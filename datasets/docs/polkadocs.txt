General General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General General General Information to get started with Polkadot and Web3. üìÑÔ∏è Getting Started Get started with Polkadot and Web3. üóÉÔ∏è Stay Safe Good-practices to Stay Safe while Surfing in Web3. üóÉÔ∏è Wallets Wallet Options in the Polkadot Ecosystem. üóÉÔ∏è Dashboards Dashboards in the Polkadot Ecosystem. üìÑÔ∏è Polkadot-JS The Polkadot-JS Tool Collection. üóÉÔ∏è Community & Contributors Polkadot Community and Wiki Contributors. üóÉÔ∏è Programmes Programmes and Initiatives within the Polkadot Ecosystem. üìÑÔ∏è Research Pages Latest Research Developments on Polkadot. üìÑÔ∏è Metadata Explorer Visualize Metadata and related Documentation. üìÑÔ∏è FAQ FAQs to Help you Navigate on Polkadot. üìÑÔ∏è Glossary Common Terms and Concepts in the Polkadot Ecosystem. Next Getting Started ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Getting Started General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Getting Started On this page Getting Started Welcome to the Polkadot Wiki The Polkadot Wiki serves as the central source of truth for Polkadot. It is a community-focused initiative led by Web3 Foundation to keep an up-to-date resource on the best information for learning, building, and maintaining on Polkadot. The Wiki has three main sections: Learn (for learners), Build (for people that want to build on Polkadot), and Maintain (for people that want to maintain the network). What is Polkadot? ‚Äã Polkadot is the first fully-sharded blockchain, i.e. it is a multi-chain environment. Polkadot enables scalability by allowing specialized blockchains to communicate with each other in a secure, trust-free environment. Polkadot is built to connect and secure unique blockchains, whether they be public, permissionless networks, private consortium chains, or other Web3 technologies. It enables an internet where independent blockchains can exchange information under common security guarantees. Polkadot is a living network with the core pillars of governance and upgradability. The network has an advanced suite of governance tools and, using the WebAssembly standard as a "meta-protocol", can autonomously deploy network upgrades. Polkadot adapts to your growing needs without the risks of network forks. Learn more about Governance If you haven't heard of Governance before, a great place to start is the Governance page Polkadot serves as a foundational layer of a decentralized web, where users control their data and are not limited by trust bounds within the network. See the video below made by DeFi Teller to know more about Polkadot. Interact with Polkadot ‚Äã Creating an Account Balance Transfers Staking Parachains Bridges Polkadot OpenGov Set an Identity Proxy Accounts Become an Ambassador What can I do with my DOT? ‚Äã DOT is the native token of the Polkadot Network. DOT can be used for transaction fees, staking, governance, acquisition of a parachain slot and for enabling several key functionalities on Polkadot. : the minimum balance required to have an active account on Polkadot Network. If your account balance drops below the minimum, your account will be reaped. Learn more about Accounts and the Existential Deposit requirement. : the minimum contribution required to join a nomination pool and earn staking rewards for contributing to the security of the network. Learn more about nomination pools . : the minimum contribution required to participate in crowdloans for parachain slot auctions . Around 20 DOT : : register an on-chain identity : create a proxy account . : the minimum stake required to submit your intent to directly nominate validators. : the minimum amount of DOT required to become an active nominator and earn rewards, i.e. the minimum active bond. To increase the chance of earning staking rewards, your stake should not be less than the minimum stake among the active nominators, which is a dynamic threshold. If you have lesser DOT than the minimum active nomination, please consider contributing to nomination pools . Learn more about becoming a nominator . : you can create your own nomination pool . DOT has utility in Polkadot's democracy . Bonding DOT is a requirement to create proposals, to endorse them and to vote on them when they become referendums. Bonding or 5% of requested funding is a requirement to make a treasury proposal . DOT can also enable you to participate in programs like the Thousand Validators Program . Polkadot Gifts ‚Äã Polkadot Gifts provide an easy way to: Onboard friends or family who are curious about blockchain but haven‚Äôt made the leap yet. Share your love of Polkadot and send any amount of DOT. Say ‚Äòthank you‚Äô or send someone tokens when you don‚Äôt know their address. Get friends and family set up to participate in crowdloans. Learn more about how you can create and send Polkadot Gifts here . Why Polkadot? ‚Äã Back in the early 2000's, when the internet was gaining popularity for the first time, the internet featured read-only, static, basic web pages. The online connected world at the time was only the beginning of virtual data, identities, and more. The internet during this time was also called the Web 1.0. As social media platforms and online businesses began to emerge, the internet transformed into the Web 2.0. This upgraded internet, which we still use today, features dynamic, interactive web pages, where users can read and write information plus publish their own for others to see. However, this version of the web comes with downsides, dealing with data control, privacy issues, and the consequences of trust. This is where the WEB3 comes into the picture. The Web 3.0 is taking centralized infrastructure and applications and turning them into decentralized, trust-free protocols. The goal is to transform the internet into a decentralized web, where users control their own data and identity in a trust-free environment. The Web3 movement aims to remove intermediaries and build trustless infrastructure. The Web3 Movement To learn more of the Web3 movement, check out this video from the Web3 Summit How does Polkadot Work? ‚Äã The Polkadot network uses a sharded model where shards - called " parachains ", allow transactions to be processed in parallel instead of sequentially. Each parachain in the network has a unique state transition function (STF). Polkadot has a Relay Chain acting as the main chain of the system. Based on Polkadot's design, as long as a chain's logic can compile to Wasm and adheres to the Relay Chain API, then it can connect to the Polkadot network as a parachain. Parachains construct and propose blocks to validators on the Relay Chain, where the blocks undergo rigorous availability and validity checks before being added to the finalized chain. As the Relay Chain provides the security guarantees, collators - full nodes of these parachains - don't have any security responsibilities, and thus do not require a robust incentive system. This is how the entire network stays up to date with the many transactions that take place. In order to interact with chains that want to use their own finalization process (e.g. Bitcoin), Polkadot has bridge parachains that offer two-way compatibility, meaning that transactions can be made between different parachains. The Cross-Consensus Messaging Format (XCM) allows parachains to send messages of any type to each other. The shared security and validation logic of the Relay Chain provide the environment for trust-free message passing that opens up true interoperability. Polkadot Primer To watch a short, beginner-friendly animation on Polkadot, check out our Polkadot Explainer video Why should you use Polkadot? ‚Äã Whether you're a blockchain developer or if you're interested in taking part of Polkadot's community , Polkadot offers a platform for everyone. This wiki offers a place for builders and maintainers to utilize tools and for brand-new learners to dive into educational material. Where to start learning? ‚Äã For brand-new learners of Blockchain technology: The Blockchain Fundamentals MOOC course is a great introduction to start familiarizing yourself with blockchain concepts such as cryptography and networks, and how these play into things like decentralization and cryptocurrency. This is recommended for users with backgrounds of all levels, and the course is free! Brand-New Polkadot learners ‚Äã Polkadot's original white paper is a technical summary around one possible direction of implementing the Polkadot network. This paper uses rationale and technical details to support why this direction is beneficial. This original white paper also explains how Polkadot's core components work together to build this decentralized network. Polkadot's overview paper is an updated version of the white paper that describes the protocol in more technical terms. We would recommend reading this overview paper if you are interested in digging more into the protocol itself. Polkadot's light paper is a visual, easy to read, and less technical introduction into its blockchain technology. This paper dives into the components of Polkadot but is understandable for both a non-technical and technical reader. Polkadot for Beginners: A non-technical guide to decentralization, blockchains & Polkadot - a book funded by the Polkadot Treasury Polkadot's specification is a GitHub repository that holds the latest Polkadot Host protocol specification, Polkadot's specification tests of the many components of the network, and the Polkadot Runtime specification. This repo holds algorithms and explores how various processes function in the Polkadot network. The Polkadot specification takes Polkadot's ideas and concepts from the light and the white paper but focuses on the technical specs of the technology. Watching the Technical Explainer Videos : These are great introductory videos that explain and demonstrate how to use Polkadot and its User Interface . Reading What is Polkadot? A Brief Introduction on Medium. There are also other great articles to read on Polkadot's Medium or Web3 Foundation's Medium . For brand-new learners of Kusama, Polkadot's canary cousin network: To learn more about how to build and maintain on the Kusama network, please head over to our Kusama Guide . Resources ‚Äã Polkadot Crowdcast - List of all Crowdcast webinars that the Web3 Foundation has done. Polkadot Explorer - Browser for the Polkadot network; can be used for Polkadot, Kusama, or any Substrate-based chain. Polkascan - Real-time multi-chain data for Polkadot Relay Chain and Parity Substrate chains. Subscan.io - Explorer for Substrate based chains. Polkadot Overview - Dr. Gavin Wood presents an overview of Polkadot. (Video) Polkadot Overview - Dr. Jutta Steiner presents Polkadot. (Video) Polkadot & Substrate Overview - Dr. Gavin Wood presents Substrate (blockchain in-a-box + VM) and Polkadot, and builds a blockchain on-stage in 30 minutes using Substrate. (Video) Community / Ecosystem - List of community rooms and channels to talk to others about Polkadot. Sample Applications - Sample applications that are built on or currently being built for Polkadot. Contributing Guide - Rules for contributing to the wiki. Polkadot Knowledge Base - Troubleshooting resources for specific errors and problems. Edit this page Last updated on Jul 5, 2023 by Filippo Previous General Next Stay Safe What is Polkadot? Interact with Polkadot What can I do with my DOT? Polkadot Gifts Why Polkadot? How does Polkadot Work? Why should you use Polkadot? Where to start learning? Brand-New Polkadot learners Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Stay Safe General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Scam Protection Do Your Own Research Wallets Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Stay Safe Stay Safe Learn about good-practices to stay safe while surfing in Web3. üìÑÔ∏è Scam Protection How to Protect yourself against Scams. üìÑÔ∏è Do Your Own Research Importance of Doing Your Own Research (DYOR). Previous Getting Started Next Scam Protection ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
How to Protect Yourself from Scams General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Scam Protection Do Your Own Research Wallets Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Stay Safe Scam Protection On this page How to Protect Yourself from Scams Scams are an unfortuante reality of the crypto industry. It's important to stay alert and protect yourself and your non-refundable crypto assets from scammers. If you ever feel you are being scammed, please get in contact with Polkadot Support . Essential rules: ‚Äã Never, ever, ever share your seed phrase or account password. Do not trust anyone online. It is trivial for them to lie and change their identities. If you are scammed , there is likely nothing that can be done to recover your funds. If a scammer gets a hold of your seed phrase, they can transfer all of your funds to their account in seconds. It is better to be safe than to risk all of your tokens. If it sounds too good to be true, it probably is. People, especially celebrities, do not give away crypto for free. Even if they wanted to, they could just ask for your address as opposed to having you send them tokens. Scams are absolutely rife in this space. It is easy and cheap to set a scam up, and hard to shut one down. Therefore, the onus is on the user to be as diligent as possible in avoiding them. If you can, try to always verify new information that you see with an official source, such as Polkadot network's official blog or Polkadot's Official Support . Often scammers will fake a website or a blog post, but if you check it against a secondary source you will reduce the chances of being scammed. caution With crowdloans active on the network, it is very important to know how to safely participate. Fake crowdloan campaigns may present an attractive target for scammers. If you are participating in a network-native crowdloan, never send your tokens to an address. Native crowdloan contributions are made with a special transaction using a campaign index, where the contributed assets remain locked until the end of the lease period. Legitimate teams will not ask you to send your assets to an address to participate in a network-native crowdloan. Some Common Types of Scams ‚Äã Private messages sent to you over Telegram, Twitter, and other social media - admins or employees will never contact you. "Giveaways" advertising that you "send us some DOT/KSM, we'll send you double back". Sites where you need to enter your seed phrase in order to "sync" your account, claim tokens, unblock transactions etc. Emails asking for DOT/KSM private keys/seeds/etc., posing as a member of any of our teams. Scammers will take official videos, add "giveaway" text around it so that it looks like the giveaway is supported by Polkadot, Kusama, Web3 Foundation, Parity, or another well-known entity. Many scammers will create nearly perfect imitations of sites - always triple-check the URL. People offering to help you stake or get rewards. People responding to questions that you asked publicly in a private chat. Advertisements pointing to imitations of sites asking you to enter your seed words. These are just some of the types of scams. Scammers are inventing new ones all the time. In general, do not trust anyone messaging you that you did not message yourself, and be wary of anyone attempting to help you or offer you a "deal". Scammers will often imitate usernames, profile pictures, etc. of well-known members of the community. Often the differences in these accounts will be very minor, such as joe_sm1th or jo_smith instead of joe_smith . Scammers will often make it seem like the "deal" is only available for a limited time. Do not be tricked by this, it is always better to confirm than to risk losing everything. Admins will never contact you directly ‚Äã If you've received a message from an admin over Telegram, ignore it. Our team members will never personally message you. Our social media accounts are posted on our website and any new social media accounts will be announced by our team. We will never offer to sell you DOT at a discount, air-drop "rewards", or message you privately to help with a problem you posted publicly. Our social media can be found below: Twitter Reddit Discord Polkadot YouTube Polkadot Medium Our Community Page also lists more social media accounts that Web3 Foundation, Polkadot, and Kusama have. Keep your data secure ‚Äã You should never share your seed phrase, password, private keys, or any other personal data with anyone. If you are concerned a wallet could be fake, please check out our list of well-known wallets . Some simple things that you can do to keep your assets and information secure from hackers: Keep your seed phrase only on paper, in a secret and secure location. DO NOT keep your seed phrase on any electronic medium, like the cloud, on your computer, on a USB drive, etc. Never enter your seed or mnemonic phrase directly into a website. Your seed phrase is meant as a backup in case you lose access to your wallet. Use it only for that purpose and only in wallets you've used before and trust. Your passwords should be strong and unique. It is recommended that you use a password manager app to create and store your passwords. Keep your computer free of malware. Although an antivirus can be of great help, it's not a panacea. Safe browsing and downloading is the only way to be sure your computer is clean. Avoid installing browser extensions from sources you don't trust explicitly. Store your assets in cold storage, like a hardware wallet or Parity Signer . Always check the source ‚Äã For any potential scam, always be sure to do a background check on the source, i.e, look at any username, email, YouTube channel name, URL, etc. If something seems fishy, that's because it likely is. Never enter any personal data if you feel the source could be a scam. Feel free to check with Polkadot's official support .. Check twice before sending DOT/KSM ‚Äã A good practice to take into consideration is to verify the address you are sending crypto to. If you don't know that account, you probably shouldn't be sending your assets there. It's your responsibility to make sure that you understand where you are sending your funds. Crypto is a decentralized space and your only recourse if a mistake is made is by appealing to the council (who usually will not get involved in matters of mistaken transfers - see below). Install the Polkadot{.js} extension ‚Äã The extension uses crowd-sourced anti-phishing measures to automatically prevent your browser from displaying known phishing or scam sites. They will be blocked upon loading, helping to prevent you from visiting these sites and thus falling for them. Our official sites ‚Äã You can use the following list of our official domains to make sure that you're visiting an official site: https://polkadot.network https://polkadot.io https://kusama.network https://web3.foundation https://parity.io https://substrate.io Of course, many projects building on Polkadot and Kusama that use similar names. If, however, a site poses as Polkadot, Kusama, Web3 Foundation, or Parity on a domain not listed above, then it's most likely a scam. Besides those, there are also polkadot.js.org and dotapps.io that host our web wallet and other tools. I Got Scammed - What Can I Do? ‚Äã In the unfortunate case of having fallen for a scam, there is likely nothing that can be done to recover your funds. However, there is still help and support that you can receive. The Polkadot Support team stands ready to help you in this difficult situation. Please check this Support article for steps you should take to prevent further loss and contact Polkadot Support from the same page. Finally, make sure to read the present article carefully to learn how to avoid falling victim in the future. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Stay Safe Next Do Your Own Research Essential rules: Some Common Types of Scams Admins will never contact you directly Keep your data secure Always check the source Check twice before sending DOT/KSM Install the Polkadot{.js} extension Our official sites I Got Scammed - What Can I Do? ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
How to Do Your Own Research (DYOR) General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Scam Protection Do Your Own Research Wallets Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Stay Safe Do Your Own Research On this page How to Do Your Own Research (DYOR) Many projects are being built, or intend to build, on both Polkadot and Kusama. However, many claim to do so but have either no such intention, they do not have the resources to pull it through, or they are outright trying to scam people by misusing the Polkadot and Kusama brands. Distinguishing legitimate projects from the dishonest ones is not always an easy task. This guide is meant to help you find out how to do your research better when you come across a project that seems interesting. What it's not meant to do, is label any single project as legitimate or not, or make that decision for you. Furthermore, a legitimate project does not necessarily mean it will also be successful, and this guide is not meant to be viewed as financial or investment advice. "Powered by Polkadot" or "Polka" prefix clarification ‚Äã The statement "Powered by Polkadot" on many projects' sites is often a cause of confusion. This usually means that the project is building, or intends to build, on the Polkadot ecosystem, using Substrate . But any project can claim that, so the existence of this statement on a project's site infers no information about the project's legitimacy, and it's certainly not a "seal of approval" by Web3 Foundation. This also applies for projects with a "Polka" prefix in their name. Many projects use that to associate themselves with the ecosystem, some legitimately and others only to piggyback on Polkadot's reputation. "Hard" metrics to look for when you DYOR ‚Äã 1. Association with entities you trust ‚Äã New projects usually try to increase their credibility by associating themselves with well-known entities. The thinking is simple: "These entities that have a good reputation trust us, so if you trust them, by association, you should trust us too". Indeed, association with a trusted entity can be a strong indicator of the legitimacy of a project. For example, if a project had received a Web3 Foundation Grant , this is an indication that the project is indeed building on the Polkadot ecosystem, and if they have delivered all of their milestones, then their code is most likely of reasonable quality. Furthermore, Web3 Foundation is not the only entity in the ecosystem that provides grants. Other reputable teams in the ecosystem that have developed platforms or prospective parachains provide grants for projects to build on or expand their project. These are also indicators that a project is committed to building on the broader Polkadot ecosystem. Receiving funding from reputable VCs and are known to be involved with other reputable Polkadot projects can also be a good indicator. Or participating in the Substrate Builders Program . However , claiming such associations and having them is not always the same thing. You always need to verify any claims a project makes , and that is probably the most critical takeaway from this guide. For example, if a project has the Web3 Foundation Grant badge on their site or claims to have received a grant, check to see if they have received one and that it has not been terminated. The complete list of projects that have successfully applied for a grant can be found here , where you can see what each project has delivered and if, perhaps, their grant has been terminated. The same thing goes for VC funding or another grant, or any advertised association for that matter. Check on the corresponding sites to make sure such claims are valid. Also, make sure you understand the scope of the association. Going back to the Web3 grants example, they have a precise scope. They are granted for specific deliverables, and the review team only checks the code and evaluates these deliverables of the particular project . So, having received a Web3 grant provides no information about the general practices of a team, the longevity of the project besides the scope of the grant, or other projects the team might be building, which is why the badge rules clearly state that it should not be displayed on the team's landing page. Similarly, if a project claims to have partnered with a reputable entity, verify its scope and if it is indeed a partnership by searching their site for projects they have partnered with, their press releases, or by contacting them directly. And if you see such claims about Web3 Foundation, you can be sure they're false because Web3 Foundation does not partner with, or endorse, ecosystem projects. 2. Open source ‚Äã An open-source project promotes transparency, builds trust, and potentially ensures project team honesty. Additionally, it makes it very easy to track the project's progress and see how active the team is in developing it. However, that does not mean that any closed source project is not legitimate or the team behind it has something to hide. Many teams choose to keep their code private to protect their intellectual property. And several teams that do so have gotten a General Grant , under which members of the grants review team review their private code. Another thing that an open-source project allows you to see is if they have copied any code from other open sources. This isn't necessarily bad, since no one wants to re-invents the wheel, but copied code should attribute to the source. If it doesn't, this should raise some red flags because the project team tries to feign expertise by passing someone else's code as their own. A forked repo is easy to spot since it points to the original repo, but partially copied code might not be as easy to find. A quick search can provide you with some ways and tools to look for plagiarism. So, a project being closed source is not necessarily a red flag. It just limits the ability to verify the project in that regard, but there are indirect ways as described below. However, a project being open source is undoubtedly a potentially strong indicator of its legitimacy because shady or poor practices seldom stay hidden for long in open source code. 3. Active development ‚Äã If a project team constantly updates their product, it is always a good indication that they are serious and passionate about building. Regularly releasing new features and upgrades, fixing bugs, updating their site and notifying the community of these changes are good earmarks of a legitimate project. Additionally, active development usually also means good development to be used as an indirect indicator for a closed source project. An open-source project allows anyone to monitor the development activity through its code repository directly, such as through GitHub. 4. Comprehensive documentation ‚Äã The existence of comprehensive documentation should be considered mandatory for any serious project. A couple of years ago, this meant a whitepaper, but lately, we have seen a shift to other forms of documentation, like wiki pages describing the various aspects. No matter the form of the documentation, its existence and completion is necessary, and the more detailed it is, the better. This is where the details of the project or parts of the project are explained in full for prospective investors or users. The documentation will also give you an idea of the technical expertise of the team. If the team analyses their technology and technical aspects, this is a potential indication of technical prowess. On the other hand, if the team focuses only on tokenomics or analyses their project only in broad, vague terms, this is potentially an indication that there is not a clear path to their goals. If you are looking for an example of good documentation, look no further than our own wiki . Of course, you should not expect to find such extensive documentation on newly launched projects. Our wiki, after all, covers a whole ecosystem and was populated over the course of multiple years. Updates are also constantly being pushed out and edits are consistently being made. Nevertheless, this wiki provides a good example of the documentation a legitimate project should provide. 5. Reputable team ‚Äã Some teams display their team members prominently on their site, along with their social media profiles (usually LinkedIn) and GitHub accounts. This gives prospective users and investors the ability to verify the team's credentials, track records, and expertise. But the keyword here is verify ! Do not take what you see on the project's team at face value. Look them up and verify their track record. Do a Google search for the team members mentioned. If it comes up empty, or the only results are regarding the project you are researching, it is an indication that their team members are potentially fake. Their photos on their site, if there are any, may also be stock photos, or in other words, also fake. These are usually easily recognisable, but here is a guide on how to do a reverse image search, if you want to be thorough. In some other cases, some developers prefer to maintain their anonymity, using pseudonyms, or the team members are not mentioned at all. This is not necessarily a bad thing. Perhaps the team is a strong proponent of privacy, or they want their work to speak for itself. Still, you should try and find out who is behind the project and what they are doing. For developers, their GitHub activity may be a stronger indicator of honesty. Other team members might be heavily engaged in their community, providing guidance and answers, which is always a good sign. But if the team are ghosts that do not show up anywhere and only engage with the community through proxies, this can be considered a red flag and extra precaution should be taken. Besides their community, projects that are serious about building on Polkadot usually engage with the broader Polkadot community. They are active in the various Polkadot and Kusama channels , and some of them are Polkadot Ambassadors , or generally prominent members of the ecosystem. 6. Clear integration ‚Äã There are many ways for a project to build on Polkadot. Perhaps the most direct one is to aim to become a parachain. Some of the most notable Polkadot projects are already parachains on Kusama or gearing up to become one, and most of them may also bid for Polkadot parachain slots when live. Of course, getting a parachain slot on either of the two main networks is not guaranteed, and all projects will need to win an auction for a parachain slot. Verifying which projects are currently parachains on Kusama can be quickly done by visiting the parachains page on polkadot.js.org/apps . In the parathreads page you can see which projects are preparing to claim a parachain slot, the auctions page shows which projects are bidding for the next slot, and the crowdloan page shows which projects are gathering funds from their community to participate in auctions. But not all projects that build a chain using Substrate aim to become a parachain. Some use it simply because of its infrastructure to build their customised chain, without any plans to connect to the Relay Chain. And other projects may aim to become a parachain only on Kusama or directly on Polkadot. However, building a potential parachain is not the only way to build on Polkadot and expand its ecosystem. A project might aim to build a DeFi platform on a parachain, launch a stablecoin or other token on the Asset Hub, build a decentralized exchange, or any other dApp that one might think of, without ever touching the Relay Chain. And that's the beauty of Polkadot! But in all of those cases, their plans to build on Polkadot, whatever they may be, should be clearly stated on their site and in their documentation. Most importantly, you should look for an explanation of how they plan to achieve that integration. A roadmap that places the integration at some point in the future means close to nothing without clearly stating the steps to get there. These plans should be evaluated in tandem with your research on the technical expertise of the team. This is especially true for projects that are already running on another network, like on Ethereum or Binance Smart Chain, and have issued tokens there. Many projects do that either to raise funds and test their infrastructure or because they aim to build a "multi-chain" solution or both. But because those projects are not currently built on Substrate, the existence of a clear and robust integration plan with Polkadot should be essential in your research to ensure that they will indeed build on Polkadot one day. "Soft" metrics ‚Äã The items above are what you should look at first when evaluating a project and should carry most of the weight in your decision. The reason is that they are hard to fake or manipulate, assuming that you are able to verify the information found. Hence we called them "hard" metrics. But there are other things to look for that might point to a project's legitimacy but can be more easily manipulated, so they should not affect your decision heavily. These are called "soft" metrics. 1. Site quality ‚Äã The quality of a project's site could sometimes provide insights into the legitimacy of a project. A poorly constructed site, may: have typos, grammatical errors, or poor styling be a template without any serious effort to improve or change it hold little information about the project, without links to their GitHub or other resources not "feel" professional These are all potential indications that the team is not serious about this project. But that does not mean that all well-designed sites are also solid projects. This is a soft metric, after all. Many projects that do not have any plans to build anything substantial still have excellent, or even beautiful-looking, sites. They put many resources into how they present themselves visually to mislead. So, an excellent site does not necessarily indicate a legitimate project, a poor site might indicate an illegitimate one, but the site quality alone usually is not enough to reach a conclusion. None of these metrics are sufficient alone; you need to look into all of them to make an educated decision. 2. Social media presence ‚Äã Having a vibrant community is a good indication of a legitimate project. A team that engages with their community, gives updates, answers questions, holds AMAs, and posts articles, is a team that is interested in keeping their community members updated and informed. Though at the same time, social media presence and engagement can be easily faked and manipulated. Creating a Telegram group or a Discord server and filling it with thousands of bots is very easy. Although bot users need to be identified on Discord according to its terms, scammers have little regard for terms and conditions. A team that tweets five times a day or posts a Medium article every other day may not necessarily be the building something substantial. So, make sure that you verify that their social media presence and engagement is genuine. Join their channels, ask questions and see first-hand what the community and the admins look like. If you are seeing a lot of users posting very brief comments, like "Good project", "To the moon", "Thank you" etc, without really engaging, remain skeptical and maintain a critical eye, as these are probably bots. Additionally, verify any information shared by the team on social media and also verify the comments of users. 3. Media presence ‚Äã Related to social media presence is media presence: third-party articles, mentions in YouTube videos, and general promotions of the project. When it comes to articles, the first thing to check is if the article is genuine coverage or a paid press release, especially when a project displays this coverage prominently on their page. Or if the author has any vested interest in promoting the project. You can check their previous articles to see if they systematically "shill" this project or other projects in general. This especially applies for YouTubers and influencers in general, who may be dishonest. Many of them do this for a living or as a way to "pump" projects they have invested in. Finding good influencers that provide as objective info as possible usually involves its own separate research. That is not to say that media exposure is terrible. It is probably the most abundant source of information outside the project itself, but at the same time, it requires extensive cross-checking and verification of information. 4. Email communication ‚Äã Nowadays, many projects use Telegram, Discord, or similar apps for community engagement, as well as the sole channel for communication, updates, and support. But having an email registered with their domain, besides providing another channel of communication, can be considered an additional credibility criterion. Furthermore, receiving emails from the project's domain makes it easy to verify that the communication is authentic (but look out for spoofed emails!). On the other hand, communicating through personal emails or using a public email provider, like Google or Yahoo, might indicate a less serious team or one that is spread too thin. Crowdloans ‚Äã With the recent launch of parachains on Kusama, many projects that aim to become a parachain launched a crowdloan to gather the necessary funds to participate in the parachain auctions . But with all the buzz around the Kusama parachain launch and the imminent Polkadot launch, many scams will very likely also surface. So, crowdloans require their own section to ensure participant safety. First of all, only projects that aim to become a parachain should have a crowdloan. If a project is not a parachain candidate, there should not be a crowdloan associated with it. The optimal way to participate in a parachain crowdloan is natively through the Crowdloan module on Polkadot-JS Apps. This issues a special extrinsic that locks your funds until the parachain slot lease period ends and guarantees that your stake will be returned afterwards. You can learn more about crowdloans through the link above and here for Kusama. Many parachain candidates offer a way to participate through their site as well. However, you should ensure that they are using the crowdloan pallet in the background and that they are simply wrapping that in a nicer, more user-friendly interface. If their crowdloan interface transfers funds to an account instead, these funds will be totally under their control, and this means you need to fully trust that the team will use the funds for the crowdloan, will return your share to you when the lease period ends or if they do not win a slot, and will secure the funds properly. If their crowdloan involves this kind of mechanism, it should be explicitly mentioned in their site and documentation. That being said, some teams have been doing token sales or swaps in an attempt to get a head start in raising funds for the auctions, but these are not crowdloans and still require full trust in the team. So, if you plan to participate in these token swaps, make sure the project is reputable and that you are getting the correct information through their official site and social media channels. Similarly, several centralized exchanges are creating ways to participate in crowdloans through their platforms, while some wallets are integrating crowdloan functionality into their apps. And more are sure to follow. Any legitimate effort should be using the native crowdloans module in the background and offering a more streamlined user experience. Confirming this is necessary before using these services, but in any case, it still involves trusting the exchange or the service provider. Additional material ‚Äã Fact-checking is a skill necessary not only for DYOR but also for filtering out the plethora of information that we come across on the internet on a daily basis. If you are interested in learning more about fact-checking and claim verification, have a look at the following material. A very nice YouTube series on the art of fact-checking that covers a lot of ground can be found here . Another great resource on fact-checking, for those who prefer to read, can be found here . Wikipedia article on fact-checking Finally, you should also check our complementary guide on how to identify scams , which explains how to identify outright scams and avoid them, as well as how to protect your sensitive information. One last piece of advice ‚Äã Once you have read through this material and have done your research and have identified a project as legitimate, it is also imperative that you understand what the project does and what novelty it aims to bring to the ecosystem. This does not fall under fact-checking and verifying claims, but it is important to mention: fully understanding what something does and its prospective impact is an integral part of making an informed decision, so do not overlook it. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Scam Protection Next Wallets "Powered by Polkadot" or "Polka" prefix clarification "Hard" metrics to look for when you DYOR "Soft" metrics Crowdloans Additional material One last piece of advice ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Wallets General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Wallets and Extensions Ledger Polkadot Vault Polkadot-JS UI Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Wallets Wallets Explore the different wallet options in the Polkadot and Kusama ecosystems. üìÑÔ∏è Wallets and Extensions An Overview of Wallets and Browser Extensions in the Polkadot Ecosystem. üìÑÔ∏è Ledger Use the Polkadot Ledger Application. üìÑÔ∏è Polkadot Vault Your old Smartphone as an Air-gapped Wallet. üìÑÔ∏è Polkadot-JS UI A Wallet for Power-users and Developers. Previous Do Your Own Research Next Wallets and Extensions ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Wallets and Extensions General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Wallets and Extensions Ledger Polkadot Vault Polkadot-JS UI Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Wallets Wallets and Extensions On this page Wallets and Extensions caution Using your due diligence in researching and using the wallets listed below would be best. For any issues related to these wallet, reach out to their support teams directly. If you are new to blockchain technology, generally a typical blockchain network account is a public-private key pair. Access to a private key gives full access to all the allowed transactions on that blockchain account. It is essential to keep the private key secure. Typically, the account keys are either stored and accessed through a browser extension or a smartphone app (which are considered as a hot wallets as they are online), or an air-gapped device or a hardware wallet (which are considered as cold wallets as they are offline). Not your keys, not your tokens! With custodial wallets (like accounts in centralized exchanges), another party controls your private keys. Private keys are used to access funds in your account, so you trust the exchange that your key will always be given to you whenever you need it. With non-custodial wallets , only you can access your account's private key. To realize the multichain vision of Polkadot, it is important to have non-custodial wallets that make it convenient to interact with multiple blockchains in Polkadot ecosystem. Below there is a list of all non-custodial treasury-funded wallets developed by the community. Overview ‚Äã Browser Extensions ‚Äã At a bare minimum, browser extension wallets act as key storage and management solution, allowing you to use your accounts with apps in the Web3 space. The wallets listed below offer functionality beyond that, allowing the featured actions to be performed directly through the extension. Some of them also allow interaction with air-gapped wallets and hardware devices. Wallet Browsers Staking and Nomination Pools NFTs Crowdloans Ledger support Governance Other features Enkrypt Brave, Chrome, Edge, Firefox, Opera, Safari No, No Yes No Yes No Enkrypt Features PolkaGate Brave , Chrome , Firefox , Edge Yes , Yes No Yes Yes Yes PolkaGate features SubWallet Brave , Chrome , Edge , Firefox Yes , Yes Yes Yes Yes No SubWallet features Talisman Brave, Chrome, Edge, Firefox Yes , Yes Yes Yes Yes No Talisman features Fearless Wallet Brave , Chrome No, No No No No No Fearless Wallet Features Ledger support only for chromium-based browsers Currently, all browser extensions support Ledger devices only on chromium-based browsers (i.e. Chrome, Brave, Edge, Opera). Mobile Wallets ‚Äã Mobile wallets are fully packaged apps that allow all the featured actions, as well as the storage and management of your accounts, through the mobile app. Unlike browser extensions, mobile wallets usually can‚Äôt connect to third-party web apps. Some mobile wallets provide support for hardware wallets through Bluetooth connectivity. Wallet Platforms Staking and Nomination Pools NFTs Crowdloans Ledger support Governance Other features Fearless Wallet iOS, Android Yes , Yes No Yes No No Fearless Wallet Features Nova Wallet * iOS, Android Yes , Yes Yes Yes Yes Yes Nova Wallet features Polkawallet iOS, Android Yes , Yes No Yes No Yes Polkawallet features SubWallet iOS , Android Yes , Yes Yes Yes Yes No SubWallet features * Note about Nova wallet There's another Nova wallet that is unrelated to the Polkadot ecosystem. Users are advised to ensure that they use the correct Nova wallet by downloading the app from their official website: https://novawallet.io/ . Before creating or restoring accounts , it is wise to double-check the wallet website URLs through official channels (most projects have it listed on their official social media handles). Enkrypt ‚Äã A multichain crypto wallet - Bitcoin, Ethereum, Polkadot, Kusama and a few parachains. Non-custodial, private, hardware wallet support (Ledger and Trezor). Private and open source. Hold, send, and receive tokens and NFTs. Swap tokens and bridge assets between chains. Manage multiple accounts on multiple networks. Connect to web3 apps. Recipient of a development grant from Web3 Foundation. Fearless Wallet ‚Äã The Fearless DeFi Wallet for the Future. Simply, intuitively, and fearlessly interact with Web3. Currently supports Staking, Crowdloans, Parachain Accounts, Nomination Pools Native Staking, dApp Signer on desktop through your mobile phone (experimental), Chromium-based browser extension. Safety features such as Scam Address Warning and Network Failure Warnings help fearlessly navigate web3. Formerly funded and supported by Kusama Treasury [ 1 , 2 , 3 , 4 , 5 ], and the SORA community [ 1 ], developed with <3 by SORAMITSU. Stay Fearless! Nova Wallet ‚Äã A user-friendly wallet for the Polkadot & Kusama ecosystems, providing a smooth web3 experience on both iOS and Android. Nova Wallet supports Polkadot OpenGov (including agile delegations), Governance v1 (including support for parachain governance), Staking, NFT management, XCM Transfers, Parity Signer & Ledger Support, DApp Support with Polkadot JS and Metamask/EVM Integration and crowdloans. Nova Wallet received funding from Kusama Treasury [ 1 , 2 ], as well as funding from the Polkadot Treasury [ 1 ]. PolkaGate ‚Äã PolkaGate extension is a user-friendly and feature-rich wallet/extension, supported by the Kusama Treasury [ 1 , 2 ]. It provides a diverse range of functionalities to enhance your experience, such as proxy management, open governance, pool staking management, spam address warning, support for address-only accounts, crowdloans, account export/import/derivation, ledger support, parachain accounts, phishing website detection, QR account import, solo staking, remote node selection, fund transfers, balance viewing in both cryptocurrency and USD, transaction history viewing, and the ability to generate QR codes for addresses. Polkawallet ‚Äã Polkawallet provides cross-chain asset one-stop management, convenient staking, governance, and multiple DeFi services; the private key is self-owned. Polkawallet received funding from Kusama Treasury [ 1 , 2 ]. SubWallet ‚Äã A non-custodial Polkadot, Substrate & Ethereum wallet. Track, send, receive, and monitor multi-chain assets on 150+ networks. Import account with seed phrase, private key, QR code, and JSON file. Import token & NFT, attach read-only account. XCM Transfer, NFT Management, Parity Signer & Ledger support, light clients support, EVM DApp support, MetaMask compatibility, custom endpoints, fiat on-ramp, phishing detection, transaction history. SubWallet received funding from Polkadot Treasury [ 1 , 2 , 3 , 4 ]. Talisman ‚Äã A better way to explore Web3. Keep your assets safe, manage your portfolio and explore Polkadot and Ethereum apps with Talisman. Interact with Web3 apps, store your favourite crypto assets and manage your accounts on over 150+ Substrate and EVM networks. NFT Management, ledger Support, fiat On-ramp, portfolio tracking. Talisman received funding from Polkadot Treasury [ 1 ]. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Wallets Next Ledger Overview Browser Extensions Mobile Wallets Enkrypt Fearless Wallet Nova Wallet PolkaGate Polkawallet SubWallet Talisman ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Using the Polkadot & Kusama Ledger Applications General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Wallets and Extensions Ledger Polkadot Vault Polkadot-JS UI Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Wallets Ledger On this page Using the Polkadot & Kusama Ledger Applications Need support with your Ledger Wallet? If you need help using Ledger you can contact the Polkadot Support Team . Please Note Because of required WebUSB support, Ledger wallets currently only work on Chromium-based browsers like Google Chrome. The Polkadot Ledger application is compatible with the Ledger Nano S and the Ledger Nano X devices. Ledger devices are hardware wallets that keep your secret key secured on a physical device that does not expose it to your computer or the internet. The private keys will not be leaked even if you connect your nano via USB to your computer. Ledger devices are also hierarchical deterministic wallets (HD wallets), that is: Deterministic means that only one seed phrase generates all the accounts for different blockchain networks. Hierarchical means that the accounts are generated in a tree-like structure for different purposes. Ledger devices can be equipped with applications that are blockchain-specific. Third parties usually develop such applications, enabling the user to transact securely on the blockchain network. The Polkadot Ledger application allows you to manage Polkadot's native token, DOT. It supports most of the transaction types of the network, including batch transactions from the Utility pallet. Ledger apps may not support all the transactions Check the Ledger Polkadot App specification for the list of transactions supported. Some transactions are supported only on a specific app version, and others are not supported by any version. For instance, the staking rebag extrinsic is supported on the XL version but not on the lite version . Also, joining a nomination pool is only possible with the XL version. If you have trouble using Ledger or following the directions below, you can try searching for your issue on the Polkadot support pages . Requirements ‚Äã For Ledger Nano S users only The lite version of the Polkadot Ledger App that you can install by default in the Ledger Nano S has limited functionality. The Ledger Nano S is no longer produced and has limited memory that is just right to accommodate the XL version of the Polkadot Ledger App, which gives the user more functionalities. If you do use a Nano S with the XL version you will not be able to: Install any other Ledger application on your device Add Polkadot accounts to the Ledger Live App, but you will be able to add them to treasury-funded wallets who support ledger devices or to the browser extension and the Polkadot-JS UI . Here is a list of what you will need before using Polkadot with Ledger: A Ledger Nano X or Nano S plus (recommended for Polkadot Ledger App space requirements and functionalities). The latest firmware of the Polkadot Ledger App installed (always check for updates in Ledger Live under the "Manager" tab, you will need to allow access with your nano). Ledger Live is installed and at version 2.1 or newer (see settings -> about to find out if you're up to date). A Chromium-based web browser is installed that you can use to access the Polkadot-JS UI . Using Ledger Live ‚Äã Check this support article to learn how to use Polkadot with ledger live. info Ledger Live will only show the main account with BIP44 path 44'/354'/0'/0'/0'. This means that if you import a derived account using Polkadot-JS with a derivation path 44'/354'/0'/0'/1', it will not be displayed on the Ledger Live App. Consequently, it is not possible to transact with derived accounts using the Ledger Live App, but it is possible to do so using Polkadot-JS. Check the accounts page for more information about derived accounts and derivation paths. Using on Polkadot-JS Apps ‚Äã Loading Your Account ‚Äã info Ledger Live should be off while using Ledger with Polkadot-JS UI as it can interfere with normal operation. You can import your Ledger account to Polkadot Extension or to the Polkadot-JS UI . For instructions on how to import Ledger accounts to the Polkadot Extension read through this support article , while if you want to import Ledger accounts to the Polkadot-JS UI you can consult this other article . Derivation paths ‚Äã When adding a Ledger account using the extension or the UI, you will be asked to select an account type and an account index . The first lets you select an account, while the second lets you pick a derivation path from that account - think of it like a formula from which child accounts are generated. When you are creating a Polkadot ledger account for the first time on Ledger Live with name Polkadot 1 , this can be added to Polkadot-JS using the 0/0 derivation path (i.e. account type = 0 and account index = 0). If you add a second account called Polkadot 2 , this will correspond to the 1/0 derivation path, and so on. We thus have multiple parent accounts that can be viewed and used in both Ledger Live and Polkadot-JS. Additionally, we can use Polkadot-JS UI to create multiple children accounts from each parent account. For example, Polkadot 1 with 0/0 derivation path can have child 0/1, 0/2, etc. that can be used within the UI. However, such children accounts cannot be used in Ledger Live, as it only scans through the parent accounts. So, remember that the balances on the children accounts cannot be viewed, and you will not be able to transact with those accounts on Ledger Live. Connecting your ledger device ‚Äã While using a ledger device to sign transactions, depending on your browser and its security settings, you might need to confirm the USB connection through a popup like the one below: If you are adding your Ledger Nano for the first time, click on the "Unknown device" line and the "Connect" button will become available. Signature error message If you have already connected your device, but an error message appears before signing a transaction, make sure you have opened the Polkadot application on your Ledger Nano device. Visit this support page for more information about signing transactions using your ledger. Confirming the Address on your Device ‚Äã To display your Polkadot ledger account address on your Ledger Nano, you can follow the guidelines on this support article . Here you can scroll through and make sure the address matches what is displayed on Polkadot-JS UI . Checking the Balance of Your Account ‚Äã There are a few methods to check the balance of your account. Check out this support article for information. Sending a Transfer ‚Äã General instructions to send a transfer can be found on this support page . To sign transactions with your Ledger nano check this support article . Receiving a Transfer ‚Äã To receive a transfer on the accounts stored on your Ledger device, you must provide the sender (i.e. the payer) with your address. To do so, follow the instructions on this support page . danger Before giving anyone your address, ensure it matches what's on the Ledger by confirming the address on your device . Some malware will intercept clicks and clipboard requests and can change your copied value in-flight, so being extra vigilant around copy-paste operations makes sense. Staking ‚Äã For staking using Ledger devices, check the section "How to stake using your Ledger" on this support article . Removing Expired Democracy Locks ‚Äã Check out this support page the learn how to remove democracy locks after the end of a Governance referendum. Please be advised : Despite the Polkadot ledger application being compatible with both the Ledger Nano S and the Ledger Nano X, none of the Democracy extrinsics are available in the light version. The following repository by Zondax lists the currently supported Democracy extrinsics on the full ledger. Ledger Developer Release ‚Äã danger This section is for developers only. It is recommended to install the application from Ledger Live unless you know precisely what you're doing . Why you might need the Developer Release ‚Äã Ledger apps for the Polkadot ecosystem are developed by Zondax . When new functionalities are added to the Ledger apps, they are made available on a developer release for testing purposes. After a successful audit and review, the apps would be available for download and installation using Ledger Live . As it takes some time for Ledger to audit and review the release, the app upgrade option may not be available on Ledger Live when the new runtime is deployed on the network. If this happens, users cannot use Ledger devices with the Polkadot-JS UI, and while signing for a transaction, will most likely incur the error message "txn version not supported". Please do not panic if this happens, as there are solutions to this problem. Suppose you cannot wait a few days until the app passes the Ledger audit, you can install the developer release from the shell using the latest version published on the Zondax GitHub repository . Install the Developer Release ‚Äã info See this video tutorial to learn how to install the developer release of your ledger app. Currently, the developer release can be installed only on the Nano S and S plus devices and can't be installed on the Nano X. To install the developer version, make sure you have the latest pip version and follow the steps below: Install ledgerblue running the command python3 -m pip install ledgerblue . Download the developer release from the Zondax GitHub repository . The file will be named installer_nanos_plus.sh or something similar, depending on your ledger device. Locate the downloaded shell script and make it executable in your shell by typing the command chmod +x installer_nanos_plus.sh . You can now use the ./installer_nanos_plus.sh --help command to visualize the available options (see below) Attach your Ledger Nano (in this case Nano S Plus) to your computer, enter the PIN code, and run the command ./installer_nanos_plus.sh load . Scroll with the right button until you see "Allow unsafe manager", left and right press to confirm. You will be asked to confirm the action of uninstalling the app and subsequently installing the newer version. After confirming both actions, the shell script will install the version on your device. You must insert the PIN code to use the device after the installation. If you wish to revert the version to the stable release, go to Ledger Live. The app will automatically detect the developer release and give the option to install the previous stable release. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Wallets and Extensions Next Polkadot Vault Requirements Using Ledger Live Using on Polkadot-JS Apps Loading Your Account Confirming the Address on your Device Checking the Balance of Your Account Sending a Transfer Receiving a Transfer Staking Removing Expired Democracy Locks Ledger Developer Release Why you might need the Developer Release Install the Developer Release ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Vault (formerly Parity Signer) General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Wallets and Extensions Ledger Polkadot Vault Polkadot-JS UI Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Wallets Polkadot Vault On this page Polkadot Vault (formerly Parity Signer) Rebranding of Parity Signer to Polkadot Vault The Polkadot Vault app rebranding is live on iOS and Android devices. The source code of all versions can be downloaded on the GitHub repo . The Polkadot Vault app is an air-gapped cold storage solution for all users, including developers and power users. See Ledger devices for other cold storage solutions. Smartphone compatibility with Polkadot Vault Note that although the Vault app is available for old smartphones, different versions will be installed according to the phone's hardware. For example, smartphones like iPhone 6 will install Parity Signer (the old brand name of the Vault app) with limited capabilities. There will be no log or warning if the phone was connected to the internet while not using the app. Also, no metadata updates are possible, and no option to add new networks. This would not be as secure as the latest version of the app. We would recommend that you use smartphones compatible with the latest Polkadot Vault app. Polkadot Vault (formerly Parity Signer) is a cold storage solution that allows you to use a phone in airplane mode as an air-gapped wallet. The Vault app is not technically a wallet, as it does not allow to transfer funds. It is more of a key-chain tool that will enable you the create, manage, and restore accounts. By default, the Vault app contains chain specifications for Polkadot, Kusama, and Westend. Metadata updates are possible via a QR code fountain . It is also possible to add other Substrate-based chains and do metadata updates for those. The app allows you to securely sign extrinsics via QR codes without exposing your private keys to the internet. Vault vs. Ledger ‚Äã The Polkadot Vault and Ledger are cold storage solutions because private keys of accounts created on the Vault app or Ledger device are not stored on your computer or, more in general, on a device that has an internet connection. However, the two solutions differ, and you should consider one or the other depending on your user type. Ledger Polkadot Vault Hardware designed to stay offline and be secure. Certified by French cybersecurity agency ANSSI . Hardware not designed to stay offline. The user must switch off all inbound and outbound connections (network, wifi, Bluetooth, NFC). Hardware is resistant to side-channel attacks via Secure Element . The secure element will destroy itself if opened. Although there is no wired connection, side-channel attacks are possible. Without a secure element, the phone can be opened, and the keys will be accessible in its storage unit. Accounts derived from one mnemonic seed phrase. One mnemonic for each account or one mnemonic for multiple accounts via account derivation or default Substrate address format . See this support article for information on whether to use the same account or different accounts on different chains based on your needs. Easy firmware and application upgrades through the Ledger Live application. The app should never be connected to the internet after installation, so the version installed on the phone should not be updated directly. For app upgrades , users must factory reset the phone and recover all accounts through seed phrases. Metadata updates for each chain must be done via the QR code fountain. Currently, not all parachains are supported. Users can add all parachains either through a third-party provider or if they have the wss endpoint and know how to extract the chain specifications and metadata . Ledger app updates on Ledger Live sometimes lag behind chain updates resulting in users only being able to transact if they install developer versions (only for advanced users). Metadata updates are always available to install once released, either through the third-party provider or manually. In this case, the installation process requires familiarity with working on the command prompt. Create and Import Accounts ‚Äã You can create a new account directly within the Vault app (Add Key Set > Add new Key Set). This will generate a new mnemonic seed phrase on the app. Alternatively, you can import a new account in the app using a seed phrase generated elsewhere with a compatible account generation scheme. For example, you can have an air-gapped laptop with the Subkey tool installed and generate a new account there. The seed phrase of that account can be imported into the Vault App (Add Key Set > Recover Key Set). We recommend importing a Vault account into the Polkadot-JS Browser Extension or Parity Signer Companion . Alternatively, you can import a Vault account directly into the Polkadot-JS UI but keep in mind that clearing the cache of your browser will remove those accounts (this will not happen for accounts imported into an extension). See the instructions in this article to learn more about creating Vault accounts, and this article on how to import them to the Polkadot extension or Polkadot-JS UI. info Importing an account into an extension will not import its private key. Only the public key will be imported and you must sign using your air-gapped phone which holds the private key. Signing Extrinsics ‚Äã Remember to always check for metadata updates before signing transactions. See this article to learn how to sign transactions and this article on how to easily add new chains and do metadata updates using the Vault app. The procedure to sign transactions with the Vault app is as follows: The wallet or browser extension will show a QR code encoding the information about what you are going to sign. After scanning the QR code with the Vault app, you will be presented with decoded information about what you will sign. Make sure the information matches what you intended to sign in the first place. If something does not feel right, do not sign. Check this page for more information and contact the Polkadot Support Team . If the information shown by the Vault app is correct, you can present the QR code (signature) to the camera on your laptop to sign for the transaction. QR codes are signature-specific Note that QR codes are signature-specific. If someone by chance has access to the QR code signature for one of your transactions, future transactions cannot be signed with that same QR code, and it is impossible to find out the private key of your account only with that QR code. Update the Vault App ‚Äã danger Ensure you always have your mnemonic seed phrase secure and available. To securely update the Polkadot Vault app follow the instructions here . Briefly, backup your accounts (Backup key Set > write down the mnemonic seed phrase), factory reset your Vault app (Settings > Wipe all Data), factory reset your phone, re-install the Vault app, go offline (airplane mode, no wifi), recover your accounts (Add Key Set > Recover Key Set). Security Notes ‚Äã Remove SIM cards and Forget Networks To avoid unintended connection of your phone to the Internet, remove SIM cards, reset eSIM settings, and forget any added WiFi networks. In this way, the only possibility of having an unintended internet connection is either through the cellular network or through WiFi connection. The Vault app has a Log that will tell you all activities performed with it. It is important to mark down the last action, you did so that you can do a security check the next time you use the app. Also, the Vault app will always tell you if the phone has been (even briefly) connected to the Internet. In case of an unrecognized connection, it is recommended to: Backup your accounts (i.e. make sure you have the mnemonic seed phrases) Follow the steps in Update the Vault app Once offline, create a new account on the Vault app Import the compromised accounts and transfer the funds to the new non-compromised account. The Vault App has the option to export private keys If an account's private key has been exported from the Vault app, the public key will be marked as "hot" and the following message will be displayed This key is marked hot because its root private key has been exported . You might consider exporting the private key if you are switching air-gapped phone. If you choose to export your private key, avoid the following: Do not export your private key to a device that is connected to the internet, as your key will not be "cold" anymore. Do not print private keys QR codes over an internet connection. Add Chains ‚Äã The Vault app contains default chain specs for Polkadot, Kusama, and Westend. It is possible to add more chains via QR-code, and update their metadata by generating your own QR-code fountain in a metadata portal similar to that signed by Parity . Alternatively, you can use a third-party provider to add chains and their metadata. Check this article for detailed instructions. If you choose this approach, you should trust the provider you choose. Advanced How-to Guides See the advanced guides for more information about how to add chain specification and do metadata updates yourself. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Ledger Next Polkadot-JS UI Vault vs. Ledger Create and Import Accounts Signing Extrinsics Update the Vault App Security Notes Add Chains ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot-JS UI (Wallet) General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Wallets and Extensions Ledger Polkadot Vault Polkadot-JS UI Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Wallets Polkadot-JS UI On this page Polkadot-JS UI (Wallet) A Wallet for Developers and Power Users If you need help using the Polkadot-JS UI, you can contact the Polkadot Support Team . Please note that this wallet is oriented toward developers and power users. Check out treasury-funded wallets on the Wallets Page for more user-friendly wallets. The Polkadot-JS UI is the native application for accessing all features available on Substrate chains as Polkadot and Kusama. The UI is one of the moving parts of Polkadot-JS tool collection , and its functionalities go beyond normal wallet functionalities such as transfers. Note that the UI may not precisely align with the functionality of individual parachains. For more information see the Polkadot-JS Page . Main Functionalities ‚Äã Here we describe those functionalities that will likely be useful to most users. Those functionalities include account generation, balance transfers, and staking. Accounts ‚Äã In this section of the UI, you can see accounts injected from a browser extension, such as the Polkadot-JS Extension or other in-browser wallets . It is also possible to expand balance details and see different account balance types . You can also: Add an account (this option must be enabled under Settings ). Note that if you clear the cache of your browser, you will lose it, and you will need to recover it through seed phrase or JSON file. Recover and account from JSON file. Add an account from QR code. Add an account from a Ledger device (this option must be enabled under Settings ). Create a multi-signature account . Add a proxied account (see Pure Proxies for more information). For each account visible in the UI, you can perform different actions, including adding a proxy , setting an identity , and sending tokens . For more information about adding and recovering accounts, see the Account Generation page. Network (Staking) ‚Äã In this section of the UI, you can participate in staking . In the Staking menu you can access: The Overview tab showing active or waiting validators. The Accounts tab (visible only if you added an account) showing accounts that are currently used for staking ( stash and staking proxy ), bonded amount, and nominations. In this tab, you can do staking-related transactions such as bonding more funds or changing nominations. If you have staking proxies here, you can use them while signing for staking-related transactions. The Payouts tab (visible only if you added an account) showing any pending rewards that still need to be paid out to you. The Pool tab showing all registered nomination pool with the respective state (open, blocked, or destroying). This tab can also be used to create and join a pool. The Targets tab showing detailed information about validators such as commission and return. More information can be found in the Nominator Page . The Bags tab (visible only if you added an account) showing the bag you are in ("My bags") and all bags within the bags list . Here (if applicable), you can rebag accounts that do not belong to the bag you are in, and you can move your account in front of the account having less stake than you. The Slashes tab showing if there are any global slashes. The Validator stats tab showing statistics for a chosen validator. Settings ‚Äã In this UI section, you can change general settings such as appearance and language. You can also: Allow local in-browser account storage. This is used to add accounts directly in the UI (which differs from having an account injected from a browser extension). Choose to attach Ledger devices. This adds an account directly in the UI from a Ledger device. Under Settings you can also do the metadata update. Usually, if there is an update, this will be shown as a notification. For more information about metadata updates, see the dedicated section . Other Functionalities ‚Äã In this section, we describe those functionalities that will less likely interest the average user. Network ‚Äã In addition to staking, in the Network section, you can also: See the Explorer with the most recent blocks and events. Inspect parachains , see parachain auctions and participate to parachain crowdloans . Governance ‚Äã In this section of the UI, you can participate in governance . Developer ‚Äã In this section of the UI, you can access advanced features such as: Query on-chain data Query the node using RPC calls Call extrinsics using your account (you are able to see this option only if you have an account in the UI) The Extrinsic tab provides the use to access more granular functions such as pure proxies and time-delayed proxies , change nomination pool settings (nomination and roles) and use non-transfer proxies , destroy assets , and much more. Beginner's Guide to Polkadot-JS ‚Äã Introduction to Polkadot-JS Create an account using Polkadot-JS Network Explorer on Polkadot-JS UI Edit this page Last updated on Jul 14, 2023 by Filippo Previous Polkadot Vault Next Dashboards Main Functionalities Accounts Network (Staking) Settings Other Functionalities Network Governance Developer Beginner's Guide to Polkadot-JS ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Dashboards General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Staking Dashboard Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Dashboards Dashboards Explore the different dashboards in the Polkadot and Kusama ecosystems. üìÑÔ∏è Staking Dashboard Everything about the Polkadot Staking Dashboard. Previous Polkadot-JS UI Next Staking Dashboard ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Staking Dashboard General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Staking Dashboard Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Dashboards Staking Dashboard On this page Polkadot Staking Dashboard Preliminary Notes The Polkadot Staking Dashboard supports proxy accounts . You can set your stash to controller with one click and import your staking proxy (for nominators) or non-transfer proxy (for nomination pool admins and members). The dashboard also has native Ledger support , meaning that you do not need an extension or a wallet to use it; you just need your Ledger device. Metadata updates are thus not necessary, as you only need to keep your Ledger apps up-to-date. Before using the dashboard, make sure you have a Polkadot account in a Ledger device with the Polkadot Ledger app installed, or a browser extension such as the Polkadot-JS Extension or other third-party extensions with a funded Polkadot account. On top of the existential deposit , you need some free balance to pay for transaction fees and the minimum amount to place your nominations (only for nominators, currently ) or join a nomination pool (only for pool members, currently ). For more information about staking on Polkadot visit the staking page and the advanced staking page . The Polkadot Staking Dashboard is a tool only dedicated to staking on Polkadot, Kusama (Polkadot's canary network) and Westend (Polkadot's test network). The dashboard is not a wallet, meaning that you cannot transfer funds between accounts. To fund accounts, you can use supported wallets or the Polkadot-JS UI . You can participate in staking by being a nominator or a member of a nomination pool . Once you have a funded account, you need to connect it to the dashboard (connect button on the top-right corner), this will allow you to interact with Polkadot's native staking. The dashboard is a Web3 dApp The dashboard is a decentralized application (dApp), and to login you do not need to sign up with an email and password but just need an account created on the supported non-custodial wallets. Any transaction to be submitted needs to be signed by you. Also, if you use light clients, which are resistant to censorship, you interact trustlessly with the network without intermediaries. Welcome to the world of true Web3! Note the pictures on this page refer to Kusama, but the same applies to Polkadot and Westend. The structure of this page follows the sidebar of the staking dashboard. Here you will learn about the main features of the dashboard. If you need more information, see the Walkthrough Tutorial . Overview ‚Äã Staking Dashboard Walk-through video tutorial This video tutorial shows you everything you can do with the Staking Dashboard. This page of the dashboard has six main panels: Section A: The Sidebar shows which page you are on (in this case, the Overview). It will also show the role you currently have in staking (in this case, active in both Pools and Nominate ). You can also change the network (currently on Kusama). Section B: The Accounts Panel allows you to connect one account to the dashboard. Once connected, the account will appear next to the Accounts button. You can see it here if you are active as a nominator and/or a pool member. In this case, the account KSM Stash is a nominator and a member of the pool Insight Finance. Proxy accounts are also shown here if applicable. Note that the dashboard will automatically fetch the stash and the proxy. Note that Sections A and B will always be visible while you use the dashboard. Section C: The Stats Panel shows the general view of current staking metrics, including the historical reward rate (including after inflation ), the supply staked and time remaining in the current era. Section D: The Summary Panel shows your current situation and gives you general tips about staking. In this case, the KSM Stash account is a nominator and a pool member, and by clicking on Manage > you can go directly to the Nominate and Pools pages, respectively. You can take actions such as changing staking preferences, bonding more funds, etc. Section E: The Balance Panel shows the bonded amount distinguishing between "Nominating", "In a Pool", and "Not Staking". The amount that is not staking is further divided into "Free" (a balance that can be transferred) and "Reserve" (a balance that is needed the keep the account on-chain, see existential deposit ). In this case, 0.301 KSM are bonded for nominating, 0.3 KSM are bonded in a pool, and 0.145 KSM are not used for staking. Of the non-staking balance, 0.144 KSM are free while are reserved for the existential deposit. Section F: The Recent Payouts Panel shows a bar chart with the rewards paid out to you in the past 15 days either as a nominator or a pool member (manually claimed). Note how the 4th of April bar has the tip that is not filled with color. This is to show a pending payout (for nominators only). Below the bar chart is a line chart showing the 10-day moving average. Additional statistics can be found at the bottom of the overview page. Pool funds are in system accounts Remember that funds bonded in a pool are transferred to the pool's system account, which can only be accessed by the protocol, and not by any individual user. Some wallets might not display the balance bonded in pools, but the dashboard will always show it. Stake ‚Äã In this category, you can access all functionalities allowing you to stake your tokens as a nominator, member of a nomination pool, or both. The payout section will enable you to inspect the most recently received rewards. Pools ‚Äã Joining a Nomination Pool - Walk-through video tutorial This video tutorial shows you how to join a nomination pool with the Staking Dashboard. You can also read the dedicated support article about joining a pool. This page of the dashboard has four main panels (Sidebar and Accounts Panels excluded): Section A: The Stats Panel shows the number of active pools and the minimum number of tokens needed to join a pool and/or create one. Section B: The Balance Panel shows the number of tokens bonded in pools and those that are free. In this case, we have 0.3 KSM bonded and 0.144 KSM free. In this panel, you can bond more funds ( + button) or unbond some funds ( - button). Unbonding will withdraw unclaimed rewards and funds will be locked for 28 eras (28 days). Once the 28 eras have passed, you can unlock the locked funds (button with a lock icon) that will be available as a free balance. No fast unstake and pool swap for pool members Note that the option to fast unstake is only available to nominators. Also, to change pool, you cannot simply swap memberships. You will need to unbond and go through the whole unbonding period. Section C: The Pool Panel shows the pool id where you have membership (in this case, Pool 82), the pool name, and next to it, a Manage button that will allow to choose between different options: Update Claim Permissions: you can allow other pool members to compound your rewards, withdraw your rewards as a free balance, or both. More details about nomination pools and updating claim permissions can be found on the dedicated wiki page . info For more information about how-to update your claim permissions with the staking dashboard see this support article . Leave the pool and unbond all the funds in the pool. If you are a pool admin you will be able to see the additional options: Claim Commission: you can claim pool commissions to an account specified under Manage Commission (below). Manage Commission: you can manage the pool commission rate, maximum commission, maximum change rate, and minimum time delay between commission updates. More details about nomination pools and managing pool commissions can be found on the dedicated wiki page . info For more information about how-to manage commission of your nomination pool with the staking dashboard see this support article . Rename Pool: you can change the pool's name. Lock Pool: you can lock the pool so that new members are not allowed. Destroy Pool: you can destroy the pool. See this wiki page and this support article for more information. You can see any unclaimed rewards in the middle of the panel. You can claim and bond the rewards ( + Compound button) or withdraw them as a free balance ( Withdraw button). In this case, there are approximately 0.0012 KSM that can be claimed. You can see the Pool Status at the bottom of the panel, currently set to "Nominating and Earning Rewards". Note that if it is the first time you log in to the dashboard, you will see two buttons Create and Join , instead of Manage . For more information about how to create a nomination pool, see this support article . Section D: The Pool Nominations Panel shows the nominations of the pool you are currently in, highlighting the validator that is active and will pay rewards to the pool at the end of the era (in this case ParaNodes.io/11 ). If you scroll down the page, there are two additional panels: Roles showing the accounts of the pool's Depositor, Root, Nominator, and Bouncer. The same account can cover all the roles. Pool Stats showing the Pool State (either Active, Closed, or Destroying), Pool Members (number of members in the pool), and Total Bonded (total number of bonded tokens). The Pools page is divided into four parts: the Overview is basically what we talked about until now, the Members section will show all accounts of the pool members, the All Pools section will show all pools (you can filter Active, Locked, and Destroying pools), and the Favorites section shows all pools that you liked (you can like a pool in the All Pools section by clicking on the heart icons). Nominate ‚Äã Walk-through video tutorial of How to Nominate This video tutorial shows you how to become a nominator with the Staking Dashboard. This page of the dashboard has four main panels (Sidebar and Accounts Panels excluded): Section A: The Stats Panel shows the number of active nominators, the minimum number of tokens to nominate (currently ) and the minimum active bond (currently ). The system keeps 12500 nomination intents and puts them into the bags list . The fact that active nominators are not 12500 is because there are nominators that have no active validator. Section B: The Balance Panel shows the number of tokens bonded in nominations and those that are free. In this case, we have 0.301 KSM bonded and 0.144 KSM free. In this panel, you can bond more funds ( + button) or unbond some funds ( - button). Unbonding will withdraw unclaimed rewards and funds will be locked for 28 eras (28 days). Once the 28 eras have passed, you can unlock the locked funds (button with a lock icon) that will be available as a free balance. Fast unstake If your account did not receive rewards in the past 28 eras, you will be eligible for fast unstake. The dashboard will automatically check this for you and prompt a banner. Section C: The Nominator Panel shows the current status of the nominator, currently set to "Nominating and Earning Rewards", and next to it, the Unstake button allows you to unstake the whole bonded amount and stop nominating. In the middle of the panel, the payout destination is currently set to "Compounding" (i.e. rewards are added to the bonded funds). The Update button will allow you to change the destination to "To Your Account" (to the stash account as a free balance), "To Another Account" (an account that is not the stash), or "None" (no payout destination). Section D: The Nominations panel shows your nominations and allows you to stop all nomination with the Stop button or to select specific validators ( Select button) and stop nominating only those. Payouts ‚Äã This page is an expanded version of Panel F on the Overview page. It also shows all validators and/or nomination pools that paid out rewards to your accounts in the past few months. After Staking ‚Äã Walk-through video tutorial - What to do after Staking This video tutorial shows you how to what you can do after staking with the Staking Dashboard. Validators ‚Äã This page of the dashboard has two main panels (Sidebar and Accounts Panels excluded): Panel A: The Stats Panel shows the total number of active validators, all validators currently registered (active and inactive), and average commission across all validators. Panel B: The Validators Panel shows all validators. You can order them by low/high commission or apply the following filters: Include only active validators Exclude validators that are oversubscribed, have 100% commission, blocked nominations, and have missing identity. Support ‚Äã The Support pages are: The Resources page provides a glossary with main definitions and links to support pages. The Feedback page can be used to send feedback through Canny.io . This can be a bug report or a feature request. We take your feedback seriously, do not hesitate to contact us. Network ‚Äã Under Network, you can connect to either Polkadot, Kusama, or Westend through public RPC nodes or light clients for a true Web3 experience. For more information about light clients, see this page . Edit this page Last updated on Jul 14, 2023 by Filippo Previous Dashboards Next Polkadot-JS Overview Stake Pools Nominate Payouts After Staking Validators Support Network ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot-JS General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Polkadot-JS On this page Polkadot-JS Polkadot-JS is a collection of tools that interfaces with the Polkadot blockchain in a granular way. Below we describe the different components of Polkadot-JS. For more information about Polkadot-JS you can consult the official documentation . Polkadot-JS UI ‚Äã Supported Wallet If you need help using the Polkadot-JS UI you can contact the Polkadot Support Team . Please note that this wallet is oriented toward developers and power users. For more user-friendly wallets, check out the supported and treasury-funded wallets on the Wallets Page . The Polkadot-JS UI is an application that loads in your browser. There is a standard DNS hosted version , which always has the latest features, and an IPFS version that is less frequently updated but is more decentralized. This is also often referred to as Polkadot-JS Apps, UI or the Apps UI. In the Wiki pages we will always refer to Polkadot-JS UI. Polkadot-JS Apps has many capabilities that go beyond basic wallet functions such as account creation and sending or receiving transactions. See the dedicated section for more information about the UI. Polkadot-JS UI Desktop App ‚Äã The Polkadot-JS UI also exists as a desktop application for Windows, Mac and Linux. The main advantage of using it is that by default it stores encrypted accounts on the filesystem instead of browser's local storage. Local storage is susceptible to attacks using XSS (Cross-Site Scripting). There's no such risk when with files stored on disk. The desktop app uses the Electron framework. It provides the same features as web app, the only difference being different account storage. Ledger not supported The desktop application does not support Ledger accounts. If you need a cold storage solution you can try Parity Signer (aka Polkadot Vault). Polkadot-JS Extension ‚Äã The Polkadot-JS browser extension is not a wallet per se but an account management tool. It allows you to create accounts and import accounts from Ledger devices or Parity Signer, allowing the signing of extrinsics using these accounts. It also allows you export existing accounts and restore lost accounts (given you have the information to restore them). The extension is not made for users to interact with on-chain functions as one would find through a wallet app, i.e. it does not allow you to transact or do anything else other than adding and managing accounts. However, it also provides a simple interface for interacting with extension-compliant dApps such as the Polkadot-JS UI and the Polkadot Staking Dashboard . The extension is a robust key-storage tool, i.e. if you clear the cache of your browser your accounts will be kept. This does not happen if you added an account directly to the Polkadot-JS UI. Also, the extension recognizes websites that have been flagged for malicious activity. Another security note, the extension will always ask you if you want a specific website to access the account information in it. Metadata Updates The browser extension is a tool that interacts with the Polkadot network, but it is disconnected from it when it is not in use. It is important you always check for metadata updates before using the extension or other account management tools such as Parity Signer (aka Polkadot Vault). Polkadot-JS Phishing List ‚Äã The Polkadot-JS phishing list website is a community-driven curation of malicious actors and operators. The Polkadot-JS extension uses this list to warn a user about suspicious URLs and addresses that are part of the list, and automatically blocks the account address. Polkadot-JS API ‚Äã The Polkadot-JS API is a JavaScript API allowing for programs to interface with the functionalities of Polkadot. While interacting with the underlying @polkadot/api , most interfaces are generated automatically when connecting to an available node. This is quite a departure from many other API designs where the interfaces are commonly static. When the API connects to a node, it initially retrieves the metadata which is used to "decorate" the API based on its contents. The metadata provides data in the form of api.<type>.<module>.<section> where type fits into one of the following categories: consts - runtime constants (these are not functions so the values are returned directly as they are defined by the endpoint) query - chain state values tx - all extrinsics The metadata also provides information on events , which can be queried using the api.query.system.events() interface. None of the information contained within the api.{consts, query, tx}.<module>.<method> endpoints are hard-coded in the API. These values are defined by the decoration applied from the initial metadata response and are therefore completely dynamic. This suggests that when you connect to different parachains, the metadata and API decoration will change and provide varying interfaces based on the chain. A developer can use Polkadot-JS Apps to test code's functionality. Interacting with the Polkadot-JS comes down to either querying on-chain data or issuing an extrinsic . Querying On-chain Data ‚Äã To populate the Apps UI, the web app queries the Polkadot-JS API. The API then queries a Polkadot node and uses JavaScript to return information that the UI will display on the screen. You can choose which node to connect to by changing it in the upper-left-hand corner of the screen. Let's see how we can query on-chain data with Polkadot-JS UI on the Polkadot network with an example. To find out the current value for existential deposit, navigate to Developer > Chain state > Constants and query the balances pallet for existential deposit as shown in the snapshot below. You need to click on the plus button to execute the query. The value displayed is in plancks Issuing Extrinsics ‚Äã Extrinsics are pieces of information that come from outside the chain and are included in a block. Extrinsics can be one of three types: inherents, signed, and unsigned transactions. Most extrinsics displayed on Polkadot-JS Apps are signed transactions. Inherits are non-signed and non-gossiped pieces of information included in blocks by the block author, such as timestamps, which are ‚Äútrue‚Äù because a sufficient number of validators have agreed about validity. Unsigned transactions are information that does not require a signature but will require some sort of spam prevention, whereas signed transactions are issued by the originator account of a transaction which contains a signature of that account, which will be subject to a fee to include it on the chain. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Staking Dashboard Next Community & Contributors Polkadot-JS UI Polkadot-JS UI Desktop App Polkadot-JS Extension Polkadot-JS Phishing List Polkadot-JS API Querying On-chain Data Issuing Extrinsics ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Community & Contributors General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Community Contributing to the Wiki Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Community & Contributors Community & Contributors Learn about how to participate in the Polkadot community and how to contribute to the Polkadot Wiki. üìÑÔ∏è Community Connect and stay updated with the Polkadot Community. üìÑÔ∏è Contributing to the Wiki Contribute to the Polkadot Wiki. üìÑÔ∏è Contributors Get to know your Wiki-slayers. Previous Polkadot-JS Next Community ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Community General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Community Contributing to the Wiki Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Community & Contributors Community On this page Community The Polkadot community is expanding every day. Below are the most essential links to the community channels. caution Keep in mind that no admin or moderator will ever DM you for any reason whatsoever without prior contact and anyone doing so is likely trying to scam you. General ‚Äã Polkadot Forum - a place for thoughtful comments and discussions on the future of Polkadot, and the steps we can take to get there together. Polkadot GitHub - Parity maintained repository that houses the Rust implementation of the Polkadot Host. Polkadot Support Knowledgebase and Polkadot Support Contact . Polkadot's Latest Research (news) Events ‚Äã Polkadot Meetup Platform - Information on hosting meetups, applying for funding, and materials for running it. Community Events - Information on events funded by the Treasury and organized by the Polkadot community (candidates or ambassadors) focused on promoting Polkadot, Kusama, and related technologies. The events are educational and informative, and their goal is to create an impact in the community. Matrix Chats ‚Äã We primarily use Matrix across the organization and to communicate with community members. The application we use most often to interact with the Matrix protocol is the Element messenger client. This Wiki's public Matrix room is here . Polkadot ‚Äã Polkadot Space - Space containing all official rooms below. Note that this is a newer feature and may not be supported by your Matrix client. If you face any issues, join the rooms individually. Polkadot Watercooler - General room for talk about Polkadot. Polkadot Validator Lounge - Room for validators learning about setting up a node. Polkadot Direction - Governance, and a place to discuss the future of Polkadot. Polkadot Digest - News about what is happening in the Polkadot ecosystem, published every weekday except holidays Kusama ‚Äã Kusama Space - Space containing all official rooms below. Note that this is a newer feature and may not be supported by your Matrix client. If you face any issues, join the rooms individually. Kusama Watercooler - General room for talk about Kusama. Kusama Validator Lounge - Room for validators learning about setting up a node. Kusama Direction - Governance, and a place to discuss the future of Kusama. Technical ‚Äã Substrate Developers Chat - A Matrix chat room for Substrate development. Substrate Developers Telegram Chat - A Telegram chat room for Substrate development, bridged to Matrix Substrate Developers Chat linked above. Substrate and Polkadot StackExchange - More advanced room for technical questions on building with Substrate. Smart Contracts & Parity Ink! - A room to discuss developing Substrate smart contracts using Parity Ink! Socials ‚Äã Chat ‚Äã Polkadot Discord (RECOMMENDED) Kusama Discord Social media ‚Äã Polkadot Twitter Kusama Twitter Web3 Foundation Twitter Polkadot Reddit Kusama Reddit Web3 Foundation YouTube Polkadot YouTube Kusama YouTube Channel Blogs and tutorials ‚Äã Web3 Medium Blog Polkadot Blog Gavin Wood's Medium Blog Dotleap.com Newsletter Newsletters ‚Äã Subscribe to the Polkadot newsletter - official, infrequent Dot Leap Newsletter - less official, weekly Ecosystem ‚Äã Polkadot & Kusama Ecosystem Map - Comprehensive list of projects building in the ecosystem made by SubWallet with support from Parity Technologies. Polkadot Deep Dive Quarterly Report - A quarterly report series that dives deep into the growth of the whole ecosystem. Made by SubWallet with sections focused on Ecosystem Overview, DeFi, NFT & Web3. Polkadot Stack - if you're interested in seeing what's under development and where there's room for your contributions. Teams Building on Polkadot - Community maintained list of teams building on Polkadot and/or Substrate. (Note that this may contain inaccuracies, as it's unofficial). Edit this page Last updated on Jun 30, 2023 by Filippo Previous Community & Contributors Next Contributing to the Wiki General Events Matrix Chats Polkadot Kusama Technical Socials Chat Social media Blogs and tutorials Newsletters Ecosystem ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Contributing to the Polkadot Wiki General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Community Contributing to the Wiki Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Community & Contributors Contributing to the Wiki On this page Contributing to the Polkadot Wiki The wiki was started and is maintained by Web3 Foundation. It is an open source project and aims to be the most extensive resource of knowledge on Polkadot and Kusama the ecosystem. A large part of the material currently focuses on Polkadot and Kusama directly but it is not opposed to covering informational material for community projects. Marketing material Please do not try to pull request any marketing material as this will be rejected. Nonetheless, pull requests, discussions, and contributions from the community are encouraged. Active community members who demonstrate a record of good contributions may be given write access to the repository. Otherwise, Web3 Foundation holds the administrative position and final say on the content that is included. Specifically, the Technical Education team at the foundation are most directly involved. Contributing Explainer Check out the How to Contribute to the Polkadot Wiki video for steps on contributions. How to Contribute ‚Äã You can contribute to the wiki on the w3f/polkadot-wiki GitHub repository . Every page is a MarkDown file, which is an easy to learn syntax extension to plain text that makes creating links, rendering images, and nice-looking formatting simple. Each page has an "Edit this page" link at the bottom of the content. By clicking it you are taken to the GitHub sign-in page, where you can either log in or create an account. Once logged in, you'll be taken to the GitHub built-in text editor where you can make your edits directly. When you've completed your changes, you can add any specific details on what was changed and commit to a new branch to create a new Pull Request to the repository. From there one of the maintainers will review your changes, and either merge them or request changes. Remember that after you click "Propose Changes", you must also click on "Create Pull Request" on the next page. Guideline and Rules for Contributing ‚Äã There are a few basic ground-rules for contributors: No --force pushes or modifying the Git history in any way. Pull requests are preferred to issues, especially for small changes such as typos. Issues should be used for generic or broad-based changes or missing content. Suggestions and requests are encouraged. Only use non-master branches . Significant modifications , even by contributors, ought to be subject to a pull request to solicit feedback from other contributors. Pull requests to solicit feedback are encouraged for any other non-trivial contribution but left to the discretion of the contributor. Contributors should attempt to adhere to the prevailing MarkDown style, language, and layout. Correct grammar should be used at all times. Pull requests with typos will not be merged until fixed. Care should be taken to remain as objective and informative as possible. There should be no editorializing, and external bias should not be present. We use a plugin called Prettier to standardize the style across documents. You can run this on your local copy with npx pretty-quick --staged , but for simplicity we also have a bot which runs this for us in your PRs. Heritage ‚Äã This document is based on the Level contribution guidelines . Helping With Translations ‚Äã Currently the Wiki is being reorganized and updated, during this time we will not be prioritizing translations. ::: The Wiki's default language is English , though, there is a desire to translate the Wiki in as many languages as possible. Another way you can contribute is by helping with translations. There is a feature at the bottom of the main page of the Polkadot Wiki that will redirect you to crowdin , which is where translation contributions can be made. There, you will see the progress of translating the docs associated with the Wiki in many different languages. The following are steps on how to interact with the crowdin interface for translations. Check out the official crowdin knowledge base as well. The following shows an example in Arabic . You can type in your desired translation for each section in the provided area, while using the suggestions generated by the crowdin built-in translator. Make sure to save your translation. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Community Next Contributors How to Contribute Guideline and Rules for Contributing Heritage Helping With Translations ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Contributors General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Community Contributing to the Wiki Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Community & Contributors Contributors On this page Contributors Current Contributors ‚Äã The following is a list of regular contributors to the Wiki. Each contributor's contact information is listed as a precaution - if one of these people ever contacts you directly, first check the contact information to see that it matches the origin of the message, and then feel free to get in touch with someone else on this list to verify the legitimacy of the original inquiry. Employee impersonation is a common type of scam For other ways of protecting yourself, please see the How to Protect Yourself from Scams guide. Technical Education ‚Äã Bill Laboon ‚Äã email: [email protected] matrix: @bill:web3.foundation twitter: @BillLaboon telegram: @BillLaboon Bill Laboon is the Head of Education and Grants at Web3 Foundation. Prior to Web3 Foundation, he spent five years teaching Computer Science at the University of Pittsburgh. Bill also has several years of experience in software engineering and management. He is the author of two books: A Friendly Introduction to Software Testing , an undergraduate textbook; and Strength in Numbers , a near-future novel set in a world in which cryptocurrency has eliminated traditional money. Radhakrishna Dasari ‚Äã email: [email protected] matrix: @radha:web3.foundation twitter: @DrW3RK Radha leads Technical Education at Web3 Foundation. Radha has a PhD in Computer Science and has three years of experience teaching Computer Science courses. He also taught a Massively Open Online Course (MOOC) on Coursera . Radha is excited to contribute to Web3 Foundation's vision by educating professionals and the general public about the Web3 Technology Stack. Filippo Franchini ‚Äã email: [email protected] matrix: @filippo:web3.foundation twitter: @filippoweb3 Filippo is a Technical Educator at the Web3 Foundation. Filippo has a PhD in Statistics and has 6 years of experience in teaching statistical computing and data analysis. Filippo strongly believes in the Web3 vision of having a privacy-focused, secure, decentralized internet where power is given back to users. The complexity of Web3 technology can be intimidating and is still a major blocking element behind mainstream adoption. Filippo is excited to contribute to Web3 Foundation's vision by educating professionals and the general public about Web3 and blockchain technology. Filippo enjoys learning about Polkadot and regularly contributes to Wiki articles. He got certified as a blockchain analyst and is currently learning about Rust and Substrate . Bader Youssef ‚Äã email: [email protected] matrix: @bader:web3.foundation twitter: @baderyo_o Bader is a Technical Educator at the Web3 Foundation. He has been an avid blockchain and web3 advocate, full-stack software engineer, and technical writer for the past 4 years. He has used a multitude of technologies to create disruptive and unique applications, mostly centering around eliminating intermediary servers in applications and creating trust-free digital interactions for the betterment of humanity. Real technology needs to solve real problems, and Bader is very strong on creating practical solutions to those problems using Web3. Bader has a strong passion for ensuring that this technology can be understood and utilized to its fullest potential and focuses on making it as simple and useful to digest as possible. Technical Integrations ‚Äã Joe Petrowski ‚Äã email: [email protected] matrix: @joe:web3.foundation telegram: @joepetrowski twitter: @joepetrowski Joe leads Technical Integrations at Web3 Foundation and was previously a Research Analyst at Parity Technologies. He focuses on making Substrate chain integration as smooth as possible, from educating users on Substrate and Polkadot fundamentals to building tools to create great user experiences and occasionally writing a bit of code. He also hosts the Relay Chain podcast. Prior to working at Parity, Joe worked in shock and vibration simulation for satellite launch, algorithmic trading, and professional cycling. He has a Bachelor of Science in Aeronautical and Mechanical Engineering. Past Contributors ‚Äã The following contributors are no longer participating in an official capacity. Keith Alfaro ‚Äã Keith was a member of the TechEd team at the Web3 Foundation. He has worked professionally as a software engineer since completing his graduate studies. Keith holds bachelor's and master's degrees in Architecture (buildings not computers). It was during this time that he discovered his passion for computational design. He was previously employed in the CAD/3D graphics software development domains. Keith is passionate about learning, developing and sharing decentralized and peer-to-peer technologies. Emre Surmeli ‚Äã twitter: @semres8 Emre was a Technical Educator at Web3 Foundation. His background is in full-stack software development, education, and blockchain technology consulting. He is passionate about learning and teaching and has always kept a foot in education by designing and delivering software development and blockchain classes at General Assembly, Code Fellows, and Columbia University. Danny Salman ‚Äã email: [email protected] matrix: @dannysalman:matrix.org telegram: @dannysalman twitter: @dannysalman _ Danny was a Technical Educator at Web3 Foundation who has a keen interest in blockchain and trust-free technologies. As a Technical Educator, Danny taught and documented Polkadot. He has a Bachelor of Engineering in Computer Engineering and a background in philosophy & politics. Alex Chau ‚Äã matrix: @a-jwc:‚Äãmatrix.org Alex was a Technical Education Intern at Web3 Foundation. He has developed a deep passion for educating others on decentralized systems and pushing forward the WEB3 vision. Alex has educational experience in cryptography, data privacy, and enterprise security. He has a Bachelor of Science in Computer Science from San Francisco State University. Jake Hemmerle ‚Äã matrix: @jake:hemmerle.dev twitter: @jakehemmerle telegram: @jakehemmerle Jake was a Technical Education Intern at Web3 Foundation and studied Computer Science at the University of Cincinnati. He has been learning about blockchain technology since 2018 and has personal interests in peer-to-peer systems, network protocols, applied cryptography and privacy-enhancing technologies. You can now find Jake cracking the code at the multi-chain Polkadot dApp hub, Astar Network . Bruno Skvorc ‚Äã telegram: @swader matrix: @bitfalls:matrix.org twitter: @bitfalls Bruno joined the crypto ecosystem full time in 2015 with the advent of Ethereum and created educational resources, tutorials, and newsletters for budding developers of the industry. From 2019, he was a Technical Educator at Web3 Foundation. He is passionate about decentralization, censorship resistance, and transparency coupled with accountability, so he has found his passion in immutable ledgers of the present and future. In a previous life, he was a web developer and senior PHP editor at SitePoint, which culminated in a book about web-dev-friendly virtual environments . In his quest for developer health , he likes to combine fitness and gaming and can be found in VR on Oculus and Steam as TheSwader. He also runs a weekly newsletter covering all things Web3 at DotLeap . You can now find Bruno hacking the metaverse by leading the way at RMRK to create the next-generation NFT standard. Anson Lau ‚Äã matrix: @anson-lau:matrix.org twitter: @Anson_LauHK Anson was a Technical Educator at Web3 Foundation. Before that, he worked as a Hyperledger Blockchain Developer to deliver B2B applications at IBM. He is passionate about staking, governance, cryptoeconomics, and privacy areas. Anson has been actively participating in the blockchain space since 2017 and creating video content to help different people understand how decentralization could deliver us a better future. Kirsten Richard ‚Äã Kirsten was a Technical Educator Intern at Web3 Foundation from Canada. She focused on curating educational content around blockchain and Polkadot network concepts. Kirsten has a Bachelor's degree in Computer Science and is passionate about teaching and technology. Logan Saether ‚Äã matrix: @logansaether:matrix.org twitter: @logansaether Logan was a Technical Educator at Web3 Foundation, where he worked on creating material for supporting the development of the next generation of distributed technologies. At Web3 Technical Education, Logan focused on creating the Polkadot Wiki as the central source of truth and knowledge for the Polkadot protocol. He contributed in creating written content and example applications that demonstrate how to use these new technologies. You can now find Logan leading efforts to define the prediction market space at Zeitgeist . Edit this page Last updated on Jun 30, 2023 by Filippo Previous Contributing to the Wiki Next Programmes Current Contributors Technical Education Bill Laboon Radhakrishna Dasari Filippo Franchini Bader Youssef Technical Integrations Joe Petrowski Past Contributors Keith Alfaro Emre Surmeli Danny Salman Alex Chau Jake Hemmerle Bruno Skvorc Anson Lau Kirsten Richard Logan Saether ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Programmes General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Grants Bug Bounty Polkadot Ambassadors Substrate Builders Thousand Validators Thousand Contributors Developer Heroes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Programmes Programmes Learn about different programmes and initiatives within the Polkadot and Kusama ecosystems. üìÑÔ∏è Grants The Grant Programs in the Polkadot Ecosystem. üìÑÔ∏è Bug Bounty An Overview of How you can Help Catch Bugs. üìÑÔ∏è Polkadot Ambassadors The Polkadot's Ambassador Program and How you can Participate in It. üìÑÔ∏è Substrate Builders The Substrate Builders Program and How it can Support your Substrate Development. üìÑÔ∏è Thousand Validators The Thousand Validators Program and How it can Support your Validator Endeavors. üìÑÔ∏è Thousand Contributors The Thousand Contributors Program and How it can Support your Contributions. üìÑÔ∏è Developer Heroes More Power to Polkadot's Developers. Previous Contributors Next Grants ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Grants General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Grants Bug Bounty Polkadot Ambassadors Substrate Builders Thousand Validators Thousand Contributors Developer Heroes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Programmes Grants On this page Grants Web3 Foundation Grants ‚Äã Web3 Foundation offers grants for open source software development and research around Substrate, Polkadot, Kusama and ink!. Applications and deliveries are tracked transparently on GitHub. Information regarding requirements, the application process, deliveries, etc. can be found in the Grants Program README . For guidance, there is also a list of previously accepted applications and a list of frequently asked questions . Alternative Funding Sources ‚Äã info Check the alternative funding sources section on the Web3 Foundation grants repo for comprehensive information. Polkadot Treasury ‚Äã The Polkadot Treasury is a pot of on-chain funds collected through transaction fees, slashing, staking inefficiencies, etc. The funds held in the treasury can be spent on spending proposals. Both Polkadot and Kusama offer everyone the opportunity to apply for funding via the treasury. See: Treasury Wiki Polkadot Treasury Guide Kusama Treasury Guide Kusama Faucet Due to high demand and spamming issues, the Kusama faucet is no longer in operation. The Web3 Foundation has considered new ways to distribute KSM for people who need KSM to build. If you are interested in obtaining KSM for building or research, you can apply through the Treasury or receive a tip for doing something cool in the community. Other Grant Programs ‚Äã Below is a list of other grant programs in the Polkadot/Substrate ecosystem. Darwinia Grants Program Moonbeam Grants Program Edgeware Grants and Bounties Crust Grants Program HydraDX Grants and Bounties Astar / Shiden Network Builders Program Picasso / Composable Grants Program SubQuery Grants Programme Acala Grants Program OAK‚Äôs Developer Grants Edit this page Last updated on Jun 30, 2023 by Filippo Previous Programmes Next Bug Bounty Web3 Foundation Grants Alternative Funding Sources Polkadot Treasury Other Grant Programs ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Bug Bounty Programme General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Grants Bug Bounty Polkadot Ambassadors Substrate Builders Thousand Validators Thousand Contributors Developer Heroes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Programmes Bug Bounty On this page Bug Bounty Programme If you discover a bug, we appreciate your cooperation in responsibly investigating and reporting it as per instructions on Web3 Foundation website . Disclosure to any third parties disqualifies bug bounty eligibility. Bug bounty program scope The bug bounty program does not cover bugs on code bases that are external to or, written on top of Polkadot, or that use Polkadot. To be eligible for the bug bounty program the bug has to be a part of the Polkadot codebase, this includes protocols that Polkadot uses such as AnV, XCM, GRANDPA, etc. We call on our community and all bug bounty hunters to help identify bugs in Polkadot. Eligibility ‚Äã Generally speaking, any bug that poses a significant vulnerability, either to the soundness of protocols and protocol/implementation compliance to network security, to classical client security, as well as security of cryptographic primitives, could be eligible for a reward. Please note that it's entirely our discretion to decide whether a bug is significant enough to qualify for a reward. note The submission quality will be a significant factor in the level of considered compensation. A high-quality submission includes explaining how the bug can be reproduced, how it was discovered, and otherwise critical details. Please disclose responsibly; disclosure to any third parties disqualifies bug bounty eligibility. Examples: An attack that could disrupt the entire network and harm the validity to the network would be considered a critical threat. An attack that would disrupt service to others would be regarded as a high threat. Responsible investigation and reporting Responsible investigation and reporting include, but isn't limited to, the following: Don't violate the privacy of other users, destroy data, etc. Don't defraud or harm Polkadot network or its users during your research; you should make a good faith effort not to interrupt or degrade our services. Don't target the validators' physical security measures, or attempt to use social engineering, spam, distributed denial of service (DDoS) attacks, etc. Initially, report the bug only to us and not to anyone else. Give us a reasonable amount of time to fix the bug before disclosing it to anyone else, and give us adequate written warning before disclosing it to anyone else. In general, please investigate and report bugs in a way that makes a reasonable, good-faith effort not to be disruptive or harmful to our users or us. Otherwise, your actions might be interpreted as an attack rather than an effort to be helpful. How to report a bug ‚Äã Please follow the instructions at web3.foundation/security-report/ . Edit this page Last updated on Jun 30, 2023 by Filippo Previous Grants Next Polkadot Ambassadors Eligibility How to report a bug ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Ambassador Programme General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Grants Bug Bounty Polkadot Ambassadors Substrate Builders Thousand Validators Thousand Contributors Developer Heroes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Programmes Polkadot Ambassadors On this page Polkadot Ambassador Programme The Polkadot Ambassador Programme is an initiative ran by Web3 Foundation to support potential contributors to the Polkadot community and ecosystem. How it works ‚Äã If you are enthusiastic about Polkadot and Web3 and are willing to help the community grow, you can join the program and receive funding for hosting events, gain access to communication channels of core team members, and gain access to Polkadot events. There are many ways you can get involved in expanding Polkadot‚Äôs ecosystem, such as: Contributing code and documentation related to the ecosystem Speaking about Polkadot at events, workshops, and meetups Writing blog posts Representing Polkadot in your region Mentoring and onboarding newcomers Identifying partnerships and opportunities for the Polkadot ecosystem Participating in calls and tasks that help drive the ecosystem forward How to apply: ‚Äã To become a Polkadot Ambassador, please fill out your application here . Find out more about the programme and how to join here . Edit this page Last updated on Jun 30, 2023 by Filippo Previous Bug Bounty Next Substrate Builders How it works How to apply: ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Substrate Builders Programme General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Grants Bug Bounty Polkadot Ambassadors Substrate Builders Thousand Validators Thousand Contributors Developer Heroes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Programmes Substrate Builders On this page Substrate Builders Programme Over 150 projects are using Substrate to create the next generation of blockchain infrastructure and application-specific chains. The Substrate Builders Program is an initiative by Parity Technologies to take your Substrate project to the next level. Projects building Substrate-based blockchains, applications, or ecosystem components can receive support for technical challenges, strategy, and ecosystem building from Parity‚Äôs extensive experience and resources. Square One Initiative ‚Äã The Square One initiative is a step behind the Substrate Builders Program as it helps early-stage teams who are committed to building on polkadot find their feet in the ecosystem. Square One's primary purpose as a program is to help tailor a specific pathway across our Substrate resources that act as a lighthouse for new and early-stage projects, developers & builders that are committed to building on Polkadot. The objective of Square One is threefold: To accelerate the development of teams in the Polkadot ecosystem. Identify needs & blockers that teams face and address them accordingly in the most efficient manner. Create a dedicated pathway for each team based on their needs and track them accordingly. There are many ways to build on Substrate, and the ecosystem comprises numerous toolkits and resources for education, development, and funding. The initiative guides teams down the most appropriate path of tools and support resources, enabling them to accomplish their goals and launch new products with minimum friction. The resources available through Square One can be grouped into various categories: Funding - Apply for grants, including access to education and support on how to access funding through the Polkadot Treasury and the Web3 Foundation Grants Program, Accelerators & VCs. Education - Gain recognized certifications, join Hackathons, apply for the Polkadot Blockchain Academy to expand one‚Äôs programming expertise to develop a project‚Äôs POC/MVP. Tech mentoring - Access mentorship groups to help make your development market fit. Product development - Get directed towards external parties interested in Substrate development projects. Development support - Completely bespoke development resources are available to aid the project development and growth. Tracks ‚Äã There are three tracks builders can take for the program. Chains track ‚Äã This track supports builders who are building application-specific chains, which include parachains, parathreads and independent chains too. Infrastructure track ‚Äã This track supports builders who are building ecosystem-based infrastructure, which may be in the form of tooling, UI, middleware, bridges, and more. Application track ‚Äã This track supports builders who are building on top of Substrate-based chains. Check out the official Substrate Builders Program site for more information and steps on how to apply. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Polkadot Ambassadors Next Thousand Validators Square One Initiative Tracks Chains track Infrastructure track Application track ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Thousand Validators Programme General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Grants Bug Bounty Polkadot Ambassadors Substrate Builders Thousand Validators Thousand Contributors Developer Heroes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Programmes Thousand Validators On this page Thousand Validators Programme The Thousand Validators Programme is an initiative by Web3 Foundation and Parity Technologies to use the funds held by both organizations to nominate validators in the community. It serves two major purposes: Give validators a structured on-ramp to join the active set of validators on Kusama and Polkadot Further decentralize the validator active set. How it Works ‚Äã The nominating backend will routinely change its nominations at every era. The backend does this by short-listing candidates by validity and then sorts validators by their weighted score in descending order. Validators with a higher weighted score are selected for any possible slots. As validators are nominated and actively validate, their weighted scores decrease allowing other validators to be selected in subsequent rounds of assessment. If a validator is active during a single nomination period (the time after a new nomination and before the next one) and does not break any of the requirements, it will have its rank increased by 1. Validators with higher rank have performed well within the programme for a longer period of time. The backend nominates as many validators as it reasonably can in such a manner to allow each nominee an opportunity to be elected into the active set. Setting up a Validator ‚Äã Please see the guide on how to set up a validator as well as additional information on how to secure a validator . How to Apply ‚Äã Entrance to the Polkadot programme requires a rank of 25 or higher in the Kusama programme. Attaining a rank of 25 usually takes around two months. The leaderboard is available here . In order to apply to the Polkadot programme, set up your Polkadot node to adhere to the requirements below and fill in the application form . You will hear back from the team shortly. Requirements ‚Äã Verified identity (see here for instructions) Connect to dedicated telemetry (use --telemetry-url 'wss://telemetry-backend.w3f.community/submit 1' when starting the node) Minimum of 5_000 DOTs self stake (exceptions by approval for good intentions) No more than 5% commission Have a staking proxy set up Must be on the latest release Maximum one node per applicant Validators must operate nodes themselves, they may not be operated by third parties or staking providers. Nominators ‚Äã The below addresses are the stash / staking proxy pairs for the primary nominators involved in the Polkadot Thousand Validators programme. They are formatted like " stash / staking proxy ". - 14Ns6kKbCoka3MS4Hn6b7oRw9fFejG8RH5rq5j63cWUfpPDJ / 12iz6aJ75KdqVZLGyvFJmgc5k74Pdokgy9UGTgWtnt67RNTg - 12RYJb5gG4hfoWPK3owEYtmWoko8G6zwYpvDYTyXFVSfJr8Y / 12iz6aJ75KdqVZLGyvFJmgc5k74Pdokgy9UGTgWtnt67RNTg - 16GMHo9HZv8CcJy4WLoMaU9qusgzx2wxKDLbXStEBvt5274B / 12iz6aJ75KdqVZLGyvFJmgc5k74Pdokgy9UGTgWtnt67RNTg - 13yk62yQYctYsRPXDFvC5WzBtanAsHDasenooLAxKvf5bNkK / 12iz6aJ75KdqVZLGyvFJmgc5k74Pdokgy9UGTgWtnt67RNTg A time delay proxy is used as the interaction method for some of these accounts. Since approximately early January 2021, the nominators will select an automatic number of validators to nominate based on the lowest amount staked for a validator and the amount of funds it holds. This can be anywhere from a few validators receiving nomination from a single nominator, to the max of 16 nominators on Polkadot. Selection ‚Äã On-chain parameters assess each candidate to produce a weighted score. It is a changing system in which new endpoints or scores are sometimes introduced. Below are some of the original weights. Weights ‚Äã Inclusion ‚Äã The inclusion weight accounts for 40 points. It is assessed by an evaluation of the validator's inclusion in the active set over the past 84 eras. A candidate can be assured of full score if there were no stints of active validation in 84 eras. Span Inclusion * ‚Äã The span inclusion weight accounts for 40 points. It is assessed by an evaluation of the validator's inclusion in the active set over the past 28 eras. A candidate can be assured of full score if there were no stints of active validation in 28 eras. Discovered * ‚Äã The discovered weight accounts for 5 points. It is determined by comparing the candidates tenure in the programme relative to other candidates. A candidate that is in the programme for a longer duration relative to the entire group of validators allows for a higher score. Nominated * ‚Äã The nominated weight accounts for 10 points, and it is assessed based on when the candidate was last nominated relative to the other candidates in the programme. Rank * ‚Äã The rank weight accounts for 5 points and is assessed relative to the ranks of other candidates within the programme. Unclaimed ‚Äã The unclaimed weight relates to the number of payouts outstanding for greater than four eras. Each payout that exceeds this threshold would attribute a negative score of 10 points. Bonded * ‚Äã Candidates with a bond size that is relatively higher than others would receive a score of 50 points. Faults * ‚Äã A fault is attained when a candidate has an offline event when actively validating. A legitimate fault is irrevocable. Faults account for 5 points in the system and are relative to others in the programme. Offline time ‚Äã Candidates who have accumulated < 200 minutes offline time during the weekly period will receive 2 points in the system. Offline time is judged by a candidate's connection to the W3F Telemetry and is reset on Sundays. Location * ‚Äã The system allocates a score of 40 points for candidates who host their validators in uniquely located data centres. A candidate's location is determined from Telemetry and is relative to the number of other candidates located at the same data centre. Council ‚Äã Candidates will receive a score of up to 50 points for voting for council members. Candidates may back as few as one candidate; to attain a full score of 50 points, the 1KV candidate should allocate > 75% of their bond to the respective council members. Democracy ‚Äã Candidates will receive 10 * 1KV points for each referendum they have voted on (Aye/Nay) to a limit of 100 points. Scores are based on votes for referendum 49 and beyond. * Scores that are based on their relative position against others are assessed as follows: The respective weight is assigned high and low percentiles. Any scores lower than the score at the low percentile and higher than the score at the high percentile are removed. The weighted score is then obtained by ((candidate_value - low_threshold) / (high_threshold - low_threshold)) * weight. The default low and high percentiles are 10 and 90%, respectively. Inclusion and Span Inclusions are measured against low and high percentiles of 20 and 75%, respectively. Bonded is measured against low and high percentiles of 5 and 85%, respectively. Finally, location is measured against low and high percentiles of 10 and 95%, respectively. Frequently asked questions ‚Äã How do I apply? See here . How long does it take for the application to process? Usually a few weeks. Sometimes a little longer. You can inspect the candidate files here and see if your info shows up. Also, you will get invited to a dedicated matrix room after being accepted. I'm in the program and am getting nominated but don't get active? The system optimizes the nominations to maximize the stake on active validators and minimize the stake variance across them. Also keep in mind that getting nominated does not mean all the nominations are with you. Nominators can choose up to 16 validators. My scoring doesn't update , I haven't been elected for a while ? Sometimes it‚Äôs best to give things time to resolve; if they don't, leave a message in the dedicated matrix room or open an issue on GitHub. The 1000 validators website is not up to date? See these resources for more up-to-date information. Resources ‚Äã To extract and display the 1KV Program scores there are various tools listed below. Resource Github source Info Main 1KV site github Main site and 1KV backend. SubVT telegram bot for polkadot and kusama github Telegram bot with overviews and alerts for various polkadot and kusama related events, including 1KV events. SubVT ios and android app github SubVT app version for mobile phones. Validator earnings overview github Overview of validators' earnings. Math Crypto's Insights github 1KV-oriented scoring overview of Kusama and Polkadot. One-T for polkadot and kusama github A performance report bot for the Polkadot and Kusama network with special focus on the 1KV programme. One-T parachains overview github One-T's parachain overview. Metaspan's 1KV overview github Overview of 1KV programme, including the newly introduced endpoints, see here for available endpoints. Decentradot's 1KV overview github Overview of the 1KV programme, including the newly introduced endpoints. Hirish 1KV overview github? Overview of the 1KV Programme. Edit this page Last updated on Jul 14, 2023 by Filippo Previous Substrate Builders Next Thousand Contributors How it Works Setting up a Validator How to Apply Selection Weights Frequently asked questions Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Thousand Contributors Programme General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Grants Bug Bounty Polkadot Ambassadors Substrate Builders Thousand Validators Thousand Contributors Developer Heroes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Programmes Thousand Contributors On this page Thousand Contributors Programme The Thousand Contributors Programme is an initiative maintained by Web3 Foundation's Technical Education and Support team to offer an off-chain contributor program to support and reward community contributions. The program focuses on rewarding contributors in the areas of technical education, community, and support that relate to the Polkadot and Kusama ecosystems. The pilot phase of Thousand Contributors Programme has ended You can still submit your intent to participate. The submissions will be reviewed and accepted on a case-by-case basis. How to Participate ‚Äã You can signal your intent to participate by: Choosing a task from this list of open tasks Then fill out this typeform Process ‚Äã Once you have submitted your contribution proposal, the team will: Evaluate. Your evaluators will differ depending on which area you want to contribute. Accept or deny. Once we review your proposal, we will get in touch with you about the status. Define your delivery. All tasks will have different delivery requirements; we will communicate what your contribution needs to fulfill. Fund. Once both parties finalize the delivery of the task, the team will deposit your rewards to the account shared with us. Submission reviews ‚Äã The review process follows the present pipeline in the 1KC public GitHub project . Learn more about the program and its components on the official 1KC repository Edit this page Last updated on Jun 30, 2023 by Filippo Previous Thousand Validators Next Developer Heroes How to Participate Process Submission reviews ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Developer Heroes Programme General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Grants Bug Bounty Polkadot Ambassadors Substrate Builders Thousand Validators Thousand Contributors Developer Heroes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Programmes Developer Heroes On this page Developer Heroes Programme For more information about the Developer Heroes Programme, see the Polkadot blog post . Sign up Use this form to sign up and become a candidate for the Polkadot Developer Heroes Program. Goal ‚Äã Build a strong community of Polkadot developers of different levels of expertise to drive the growth and evolution of the Polkadot ecosystem. Developers need to have some degree of experience with the Polkadot technology stack. The program provides: Opportunities for personal and professional growth. Resources and support to boost the growth and evolution of developers. Chance to showcase and enhance skills. Establish yourself as a valued contributor to the Polkadot ecosystem and pull someone else up on stage at the same time. Each one, teach one! Rookies and Heroes ‚Äã A rookie is a beginner or first-timer developer. Someone who has previous experience in programming but needs to gain in-depth knowledge about the Polkadot technology. The technology includes but is not limited to independent Substrate chains and ink! development , and Polkadot-related tooling. Rookies will need to learn these concepts and tools so that they can deliver the tasks required to be a Hero. A rookie becomes a hero by continuously advocating for the technology used in the Polkadot ecosystem as well as spreading awareness and educating a broader audience about the technology. Depending on what you enjoy doing, you can become a hero by doing some of the following things: Offer mentoring : Offer 1:1 or group mentoring to fellow community members. Help out in the community forums : Answer questions from the community on the Substrate and Polkadot StackExchange or on the Polkadot Forum . Contribute to the Polkadot stack : Have a look at the open source technology stack list and contribute with a pull request, an issue, or find and fix bugs. Create content : Write a blog post, record a video tutorial, and write a Twitter thread. Share about your latest coding success, something you learned and are excited about. Speak at events : Do a workshop or talk at meetups and conferences. Edit this page Last updated on Jul 4, 2023 by Filippo Previous Thousand Contributors Next Research Pages Goal Rookies and Heroes ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Research Pages General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Research Pages On this page Research Pages The Web3 Foundation research page is edited directly by the researchers at W3F and contain papers, articles and documents that are sources for the information on the wiki. The following papers and articles may be of special interest for those interested in exploring Polkadot from a research or academic perspective. Research Papers ‚Äã Efficient Aggregatable BLS Signatures with Chaum-Pedersen Proofs - describes our improvements regarding efficient verification for BLS signatures for both individual as well as aggregated signatures. Accountable Light Client Systems for PoS Blockchains - Implemented as an efficient method to aggregate BLS signatures and BLS public keys which in turn is used to build light client systems that are the core cryptographic primitive for secure proof-of-stake blockchain bridges. In turn, the efficient BLS aggregation can be securely proven to a light client verifier using a custom designed SNARK. Authors: Oana Ciobotaru, Fatemeh Shirazi, Alistair Stewart, Sergey Vasilyev Overview of Polkadot and its Design Considerations - A broad overview of the design of Polkadot. Authors: Jeff Burdges, Alfonso Cevallos, Peter Czaban, Rob Habermeier, Syed Hosseini, Fabio Lama, Handan Kƒ±lƒ±n√ß Alper, Ximin Luo, Fatemeh Shirazi, Alistair Stewart, Gavin Wood Intro to Nominated Proof of Stake - A description of the NPoS scheme which selects which validators are allowed to participate in the consensus protocol of Polkadot. Author: Alfonso Cevallos GRANDPA: A Byzantine Finality Gadget - GHOST-based Recursive Ancestor Deriving Prefix Agreement, Polkadot's finality gadget. Authors: Alistair Stewart and Lefteris Kokoris Kogias A Verifiably Secure and Proportional Committee Election Rule - Validator Election in Nominated Proof of Stake. Authors: Alfonso Cevallos and Alistair Stewart Network Time with a Consensus on Clock - Consensus on Clock in Universally Composable Timing. Author: Handan Kƒ±lƒ±n√ß Alper Delay Encryption - Delay Encryption can roughly be described as ‚Äúidentity based encryption with slow derived private key issuance‚Äù. Authors: Jeff Burdges and Luca de Feo Research Blogs ‚Äã An Updated Overview of Polkadot ‚Äì Gives a summary of the paper ‚ÄúOverview of Polkadot and its Design Considerations‚Äù Polkadot Research Update ‚Äì Gives a summary of the papers on Validator Election in Nominated Proof of Stake, Network Time with a Consensus on Clock, and Byzantine Finality Gadgets. Polkadot‚Äôs Messaging Scheme ‚Äì Gives an outline of Cross-Chain Message Passing (XCMP). Authors: Fatemeh Shirazi, Logan Saether, Alistair Stewart, Rob Habermeier, Gavin Wood How Nominated Proof of Stake Will Work in Polkadot ‚Äì Gives an outline of Nominated Proof of Stake in Polkadot. Author: Alfonzo Cevallos Messaging Protocol Workshop Outcomes ‚Äì An initiative to design and implement a privacy-preserving communication system for decentralized applications. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Developer Heroes Next Metadata Explorer Research Papers Research Blogs ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Metadata Explorer General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Metadata Explorer Metadata Explorer The Metadata Explorer tool helps visualize the metadata of various parachains by retrieving the latest data directly from the chain using the polkadot-js api . The dropdown below allows you to update the chain selection to visualize. You can search all sub-categories using the provided search field. The information is categorized by the chains Pallets , RPC and Runtime information. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Research Pages Next FAQ ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Frequently Asked Questions (FAQs) General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General FAQ On this page Frequently Asked Questions (FAQs) info This FAQ focuses on technical questions for users interested in developing applications for Polkadot. If you have a more general question, you may wish to search for the answer on our support Knowledge Base or the main Polkadot network FAQ . If you have a question that is not answered, please feel free to ask on the Polkadot Watercooler Element channel or contact Polkadot Support . Polkadot Launch ‚Äã The Genesis block of the Polkadot network was launched on May 26, 2020 at 15:36:21 UTC, as a Proof of Authority (PoA) network, with governance controlled by the single Sudo (super-user) account. During this time, validators started joining the network and signaling their intention to participate in consensus. The network evolved to become a Proof of Stake (PoS) network on June 18, 2020. With the chain secured by the decentralized community of validators, the Sudo module was removed on July 20, 2020, transitioning the governance of the chain into the hands of the token (DOT) holders. This is the point where Polkadot became decentralized. The final step of the transition to full-functioning Polkadot was the enabling of transfer functionality, which occurred on Polkadot at block number 1_205_128 on August 18, 2020, at 16:39 UTC. On August 21, 2020, Redenomination of DOT occurred. From this date, one DOT (old) equals 100 new DOT. Polkadot Roadmap ‚Äã For more information on the Polkadot roadmap please visit the official Polkadot website . Validators ‚Äã How do I apply to be a validator? ‚Äã There is no central authority that decides on validators, so there is not per se an application that you can fill out. Registering as a validator is permissionless; in order to become one you must only set up a validator node and mark your intention to validate on chain. For detailed instruction on how to do this you can consult the Kusama validator guide on validating for Kusama or the Polkadot validator guide for validating on Polkadot. However, once you've set up a validator and have registered your intention it does not mean that you will be included in the active set right away. The validators are elected to the active set based on the results of an election algorithm known as Phragm√©n's method . Phragm√©n's method tries to accomplish two goals: 1) select n members from a larger set based on stake-weighted votes and 2) equalize the stake backing each validator as much as possible. You will likely want to campaign your validator to the community in order to get more backing. You are looking for nominators that will put up their tokens to increase the stake for your validator. For validators who cannot acquire the minimum stake from the community, Parity and Web3 Foundation also run a joint program called Thousand Validators that will nominate validators if they apply and fit the requirements. How are validators rewarded? ‚Äã Validators are rewarded from the inflation of the Relay Chain, transaction fees, and tips. However, they only take a percentage of the former two. More details can be read on the page for validator payouts . What is the minimum stake necessary to be elected as an active validator? ‚Äã The minimum stake that is necessary to be elected as an active validator is dynamic and can change over time. It depends not only on how much stake is being put behind each validator, but also the size of the active set and how many validators are waiting in the pool. There are a few ways to estimate the minimum stake. One way can be to navigate to the Polkadot Apps Targets tab . The value at the top of the screen saying "Lowest" is the least staked validator. You need at least this much + 1 to enter the set. You can also use some tools some to perform estimations. Offline Election can provide exact results of running an election on the current set of validators using the same Rust code that is ran in Polkadot. Validator stats script can give you an estimate that is based on the currently elected set, as well as some statistics about Kusama validators. Why will Polkadot have only 1000 validators while other projects have hundreds of thousands? ‚Äã Polkadot's goal to have 1000 validators is set to be something that is practically achievable in the short term with high confidence of good performance in a live environment. Furthermore, validators in Polkadot are not the only stakers, and if we consider the number of stakers that can be possible on Polkadot the number can scale up to hundreds of thousands. Since validators are performing critical consensus work to maintain the security of the chain including all of its shards, a more modest number of validators is estimated to start. Upon later improvements, such as implementing signature aggregation for finalization messages, the number of validators could reasonably scale up. However, increasing validators above one thousand remains a goal for later iterations of Polkadot. It is also worth mentioning that one thousand validators is more than the number of validators of similar PoS chains with comparable levels of economic security as Polkadot. The closest contenders are operating with around 150 validators, while Polkadot is already securely running with validators. Additionally, other projects sometimes have a different definition of validator that approximates more closely to remote signing keys without the full operation of a validating node. On Polkadot, each validator is running their own validating node and performing full verification of the Relay Chain, voting on finality, producing blocks in their decided slots, and verifying parachain state transitions. Other projects may consider validators and "validating nodes" as separate entities. Finally, individuals may participate in the block production process indirectly by nominating validators. In this way, individuals who are not running a node can still share in staking rewards. Relay Chain ‚Äã What is the block time of the Relay Chain? ‚Äã Both the Kusama and Polkadot networks are currently operating at a rate of one block every six seconds. This may be changed in the future. It may go as low as two to three seconds after optimizations, or potentially increase in order to handle the capacity of the parachain networking in a live environment. Does Polkadot have smart contracts? ‚Äã No - and yes. The Polkadot Relay Chain does not implement smart contracts natively. The reason for not having smart contracts on the Relay Chain is part of the design philosophy for Polkadot that dictates that the Relay Chain should be the minimal logic required to accomplish its job. However, Polkadot will be a platform for other chains that do implement smart contracts. It's possible for parachains to enable smart contract functionality and then benefit from the security and interoperability features of Polkadot. Additionally, existing smart contract chains can connect to Polkadot as a parachain, or via a bridge. While the Polkadot Relay Chain does not implement smart contracts directly, undoubtedly there will be parachains that do. So it's better to say that the Polkadot ecosystem has smart contracts versus "Polkadot has smart contracts." How will the Polkadot Relay Chain connect to external chains in the ecosystem? ‚Äã One of the cornerstone interoperability technologies being researched and developed for deployment on Polkadot is cross-chain bridges. Bridges come in a variety of flavors with varying levels of trust associated with them. Polkadot is predominantly researching the trust-minimized flavor that imposes economic costs on the operators of the bridge, and therefore makes it economically secure. Bridge efforts are being worked on in concert with other projects in the ecosystem. Eventually, there will be bridges between Polkadot and most of the other major chains. What is Polkadot's Transactions Per Second (TPS)? ‚Äã Polkadot is a heterogeneous sharded network comprising a relay chain and numerous parachains, which are all individual blockchains built on Substrate executing in parallel. Hence, the Transactions Per Second (TPS) of Polkadot is a number that encompasses all the transactions on the relay chain as well as parachains. As the transactions on these Substrate-based blockchains are weights based , it makes sense to use TPS as a measure for the network performance if all the transactions carry the same weight. Performance benchmark tests show that Substrate-based blockchains can achieve over 1000 TPS for balance transfer transactions. Assuming Polkadot is running over 100 parachains; the projected TPS is well over 100,000. With asynchronous backing upgrade , the TPS is expected to increase tenfold to 1,000,000. It is essential to realize that TPS is inherently a subjective measurement with numerous factors that can contribute to it. It's hard to gauge the usefulness of TPS in isolation (when compared to other chains), as it depends on what a transaction does for a particular network. To view how Polkadot measures TPS see the Polkadot sTPS ( Standard Transaction Per Second ) to consider precisely how benchmarking was performed for Polkadot. DOT ‚Äã What is the difference between DOT (old) and new DOT? ‚Äã The DOT (old) unit on Polkadot was at twelve decimal places, otherwise known as 1e12 Plancks. On 21 August, 2020, Denomination Day, the DOT (old) value was redenominated to 1e10 (10_000_000_000, or ten billion) Plancks, meaning that the new DOT was valued at ten decimal places. Following the redenomination , the new DOT is called DOT. What is the inflation rate of the DOT? ‚Äã The inflation rate is approximately 10% per year. A portion of the inflation is rewarded to validators for performing their duties, while another portion may go directly to the treasury. The exact percentage that goes into both varies and is based on the amount of DOT that are staked. Please see the article on inflation for more information. Why can't crowdloaned DOT be staked? ‚Äã DOTs contributed to a sucessful crowdloan campaign by a parachain are bonded for the entire lease period, which is two years on Polkadot. The crowdloaned DOT cannot be used for any other DOT utility functionalities like staking and democracy. In exchange to the lost staking rewards or liquidity of DOTs, the parachain team may offer rewards to the contributor. The utility of crowdloaned DOT is to provide a lease for a parachain. The utility of staked DOT is to secure the network through a reward/slash mechanism. Allowing crowdloaned DOT to be staked results in complex consequences like applying a slash on crowdloaned DOT that was meant to be bonded for the entire lease period of a parachain. In a way, the inaccessibility of crowdloaned DOTs and the lack of staking rewards for the entire lease duration encourages the contributors to back projects that are valuable to the ecosystem. Governance ‚Äã What prevents Polkadot governance from failing? ‚Äã Polkadot's governance has already been shown to work. Examples can be found in the runtime upgrades that have successfully taken place through on the testnets as well as in a real economic environment on Kusama and Polkadot itself. It is fair to say that the field of on-chain blockchain governance is still new, and no one can claim to know exactly what the optimal version of on-chain governance is yet. However, Polkadot takes a brave step forward in pioneering thought-through mechanisms for evolving a blockchain. Blockchains need a method to adapt and evolve. Therefore, an on-chain governance system was necessary for the long-term success of Polkadot. Ultimately, it is the token holders that are responsible for preventing Polkadot's governance from failing by using their economic value and conviction to sway the progression of the protocol. What prevents Polkadot governance from becoming plutocratic? ‚Äã A savvy reader might have noticed that the answer to the previous question endowed the token holder with the ultimate responsibility to ensure that Polkadot's governance does not fail. By following the train of this assertion, one might assume that Polkadot's governance is susceptible to becoming ruled by a few large token holders (called whales in trading parlance) and therefore become a mere plutocracy (rule of the rich). There are several other mechanisms that are built-in to the governance system to resist this plutocratic tendency. One of these mechanisms is called conviction voting, and imbues greater voting power to token holders who are willing to lock their tokens on the protocol for longer lengths of time. Longer lock-ups display conviction in a vote. Conviction voting could allow a highly determined minority to overrule the vote of an apathetic majority in certain situations. Another mechanism is known as Adaptive Quorum Biasing. This makes proposals have a varying threshold for approval or rejection based on what part of the governance protocol the proposal originated in. For details on the subtleties of Polkadot's governance system, please see the governance page . Parachains ‚Äã How do parachain economics work? ‚Äã Parachains have the flexibility to implement their own monetary system or incentive structure for collators. However, this is not strictly necessary. Since the collator's job is to continue to give recent state transitions to the validators on the Relay Chain who validate each transition, the security of the parachain and the Polkadot network is completely separate from parachain economics. Parachains need collators to continue to progress, so it wouldn't be unreasonable to see them incentivize collator nodes in some way, but the specific mechanism is completely up to parachain implementers. Are parachains ephemeral? What happens when a parachain loses the next auction? ‚Äã Parachains are not ephemeral. As long as someone is keeping the data for a parachain, the parachain can move between being a parachain, a parathread, or a separate sovereign chain at different points of its lifetime. Especially with parathreads, parachains can be decommissioned to only produce blocks when their usage and throughput makes it necessary. When a parachain loses an auction for renewal, that parachain has a few options. In most cases, becoming a parathread instead would be a suitable choice. Parathreads are still secured by the Relay Chain, but don't need to hold a parachain slot and can produce a block when its economically feasible for them. For more on parachains please see the parachains page and for more on parathreads see the parathreads page . Networking ‚Äã What is libp2p? ‚Äã Libp2p is a modular and extensible networking stack that is used by IPFS, Substrate, and many other projects. It is a collection of peer-to-peer protocols for finding peers and connecting to them. Its modules have logic for content routing, peer routing, peer discovery, different transports, and NAT traversals. It is intended to be used by applications for building large scale peer-to-peer networks by only selecting the parts of the protocol suite that are needed. The Rust implementation of the specification was built and primarily maintained by a team of contributors at Parity Technologies. The Go and JavaScript versions are maintained by Protocol Labs as well as community contributors. A Nim version of the library also exists. Libp2p as a whole is an open source project that is actively developed and expanded on various code repositories hosted on their GitHub . Does Polkadot use libp2p? ‚Äã Yes, since Polkadot is built with Substrate. Substrate uses a networking protocol that is based on libp2p (specifically the Rust libp2p library). However, Substrate uses a mix of standard libp2p protocols and protocols that are homegrown and not official libp2p standards. Of the standards protocols, those which are shared with other implementations of libp2p such as IPFS, are connection-checking (ping), asking for information on a peer (identity), and Kademlia random walks (kad). Of the protocols that are custom to Substrate, there are the legacy Substrate stream, a request-response for getting information on blocks (sync), a light client protocol, a notification protocol for transactions, and block announcement. For detailed information on how Substrate uses libp2p and the standard and custom protocols, please see the networking documentation . How does libp2p differ from IPFS? ‚Äã The Interplanetary File System (IPFS) is a peer-to-peer hypermedia protocol used primarily for storage of files. It allows one to upload a file onto the network and share it with its content addressable URI. IPFS, like Substrate, is an application of libp2p and exists higher on the technology stack. Although both IPFS and Substrate use libp2p, it cannot be said that Substrate "uses" IPFS since besides sharing the underlying library for networking there is no native integration between the two applications. Kusama ‚Äã What is the minimum amount of KSM / DOT I can have in my account? ‚Äã Please see information about Existential Deposits . What are the transfer fees for Kusama? ‚Äã It is important to note that the cost of transferring KSM is dynamic. Currently, the minimum cost of transferring KSM is 0.01 KSM (the base fee), although this can be changed via governance. However, actual transaction fees will vary based on a variety of factors. Specifically, fee calculation follows the following formula: base_fee + (tx_length * length_fee) + WeightToFee(weight) Please see the fee calculation page in the Substrate documentation for more detailed information. Answered by Gav series ‚Äã The "Answered by Gav" series is a collection of posts uploaded to Reddit of questions that have been asked in the Polkadot Watercooler Riot channel and answered by Polkadot founder Gavin Wood. Reason for using asynchronous rather than synchronous communication? Difference in terms of TPS? How exactly do validators in an ETH parachain keep moving around and how is communication between zones trustless? What are the main issues with Bitcoin integration and will it ever be possible? Same problem with other POW chains? Is Polkadot only going to work with POS chains? How is it trust-less in comparison to Cosmos though? What are the current thoughts around governance especially since projects have to be voted in to receive the parachains security? Also is there any detailed overview of how exactly a token transfer from ETH could be exchanged with another chain's currency? Can I run multiple Validators with the same Session Key? How to tackle the concentration risk of Validators in data centers? Edit this page Last updated on Jun 30, 2023 by Filippo Previous Metadata Explorer Next Glossary Polkadot Launch Polkadot Roadmap Validators How do I apply to be a validator? How are validators rewarded? What is the minimum stake necessary to be elected as an active validator? Why will Polkadot have only 1000 validators while other projects have hundreds of thousands? Relay Chain What is the block time of the Relay Chain? Does Polkadot have smart contracts? How will the Polkadot Relay Chain connect to external chains in the ecosystem? What is Polkadot's Transactions Per Second (TPS)? DOT What is the difference between DOT (old) and new DOT? What is the inflation rate of the DOT? Why can't crowdloaned DOT be staked? Governance What prevents Polkadot governance from failing? What prevents Polkadot governance from becoming plutocratic? Parachains How do parachain economics work? Are parachains ephemeral? What happens when a parachain loses the next auction? Networking What is libp2p? Does Polkadot use libp2p? How does libp2p differ from IPFS? Kusama What is the minimum amount of KSM / DOT I can have in my account? What are the transfer fees for Kusama? Answered by Gav series ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Glossary General Learn Build Maintain Kusama Search Contribute English English Help us translate General Getting Started Stay Safe Wallets Dashboards Polkadot-JS Community & Contributors Programmes Research Pages Metadata Explorer FAQ Glossary Learn Build Maintain General Glossary On this page Glossary Active Nomination ‚Äã A validator (or validators) that a nominator has selected to nominate and is actively validating this era. The nominator is placing their stake behind this validator for this era and will potentially receive staking rewards in return for doing so. Alexander ‚Äã The fourth (now defunct) proof of concept (PoC-4) testnet for Polkadot. Asset Hub ‚Äã A system parachain used for asset management. Attestation ‚Äã In the Polkadot validity system, an attestation is a type of message that validators broadcast that says whether they think a parachain candidate block is valid or invalid. Auction (Parachain) ‚Äã Parachain auctions are how non-common-good parathreads win a slot to become a parachain. Authority ‚Äã An authority is a generic term for the role in a blockchain that can participate in the consensus mechanisms. In GRANDPA , the authorities vote on chains they consider final. In BABE, the authorities are block producers. Authority sets can be chosen to be mechanisms such as Polkadot's NPoS algorithm. Availability Cores ‚Äã Slots used to process parachains. The runtime assigns each parachain to an availability core and validators can fetch information about the cores, such as parachain block candidates, by calling the appropriate Runtime API. BABE ‚Äã Blind Assignment for Blockchain Extension (BABE) is Polkadot's block production mechanism. Bitfield Array ‚Äã A bitfield array contains single-bit values which indicate whether a candidate is available. The number of items is equal of to the number of availability cores and each bit represents a vote on the corresponding core in the given order. Block ‚Äã A collection of data, such as transactions, that together indicate a state transition of the blockchain. Block Explorer ‚Äã An application that allows a user to explore the different blocks on a blockchain. Blocks Nominations ‚Äã This indicates that a validator does not currently allow any more nominations. This is controlled by the validator. BLS ‚Äã Boneh-Lynn-Shacham (BLS) signatures have a slow signing, very slow verification, require slow and much less secure pairing friendly curves, and tend towards dangerous malleability. Yet, BLS permits a diverse array of signature aggregation options far beyond any other known signature scheme, which makes BLS a preferred scheme for voting in consensus algorithms and threshold signatures. Bonding ‚Äã A process by which tokens can be "frozen" in exchange for some other benefit. For example, staking is a form of bonding for which you receive rewards in exchange for securing the network. You can also bond tokens in exchange for a parachain slot. Bounty ‚Äã A mechanism which works in some sense as the reverse of a Treasury Proposal, allowing the Polkadot Council to indicate that there is a need to do some task for the Polkadot network and allowing users to receive DOT in return for working on that task. Bridge ‚Äã A parachain that acts as an intermediary between the Polkadot Relay Chain and an external chain, in such a way that it appears to the Relay Chain that the external chain is a parachain (i.e., meets the Polkadot Host's requirements of parachains). Bridges allow for interaction between other blockchains, such as Ethereum and Bitcoin, that are not natively compatible with Polkadot. Byzantine Fault Tolerance ‚Äã The property of a system that is tolerant of Byzantine faults; a system where not only may individual subsystems fail, but it may not be clear if a particular subsystem has failed or not. That is, different observers on the system may not agree on whether or not the system has failed. Ensuring Byzantine fault tolerance is an important part of developing any distributed system. Capacity ‚Äã The maximum number of nominators signalling intent to nominate a validator (and thus could potentially actively nominate that validator in the next session). This maximum number will equal the number of nominators necessary to oversubscribe a validator. Any validator which is "at capacity" or higher may potentially be oversubscribed in the next session; a validator that is not at capacity cannot be oversubscribed unless more nominators select it before the next election. Candidate ‚Äã A candidate is a submitted parachain block to the relay chain validators. A parachain block stops being referred to as a candidate as soon it has been finalized. Collations ‚Äã Parachain blocks or candidates that are being proposed to the Polkadot relay chain validators. More specifically, a collation is a data structure which contains the proposed parachain candidate, including an optional validation parachain Runtime update and upward messages. Collator ‚Äã A node that maintains a parachain by collecting parachain transactions and producing state transition proofs for the validators. Collectives ‚Äã The Polkadot Collectives parachain was added in Referendum 81 and exists only on Polkadot (i.e., there is no Kusama equivalent). The Collectives chain hosts on-chain collectives that serve the Polkadot network, such as the Fellowship and Polkadot Alliance . Commission ‚Äã Validators and nominators get paid from block production on the network, where validators can set a variable commission rate, which is initially subtracted from the total rewards that validator is entitled to (for that period), where the commission determines the rate of distribution for the remaining rewards set out for the nominators that are backing that validator. Common Good (Parachain) ‚Äã See System Parachains , which is generally preferred over the term "common good". Community Queue ‚Äã The queue for proposals originating from individual accounts (i.e. not the Council) which are waiting to become referenda. Compare the External queue. Consensus ‚Äã The process of a group of entities to agree on a particular data value (such as the ordering and makeup of blocks on a blockchain). There are a variety of algorithms used for determining consensus. The consensus algorithm used by Polkadot is GRANDPA . Crowdloan ‚Äã A mechanism for potential parachains to temporarily source tokens to win an auction for a parachain slot. Tokens gathered in this way are programmatically returned to the lender after the lease period is over or the crowdloan period ends. Curator ‚Äã A person, group, or other entity charged with judging and verifying the successful completion of a Bounty. Dapps ‚Äã A generic term for a decentralized application, that is, one that runs as part of a distributed network as opposed to being run on a specific system or set of systems. DOT ‚Äã The native token for Polkadot. DOT serves three purposes: network governance (allowing them to vote on-chain upgrades and other exceptional events), general operation (rewarding good actors and punishing bad actors), and bonding (adding new parachains by "freezing" DOT while they are connected the Relay Chain). Duty Roster ‚Äã A lookup table that specifies the job that a particular validator is required to do (i.e. attest to the validity of a specific parachain). The duty roster routinely shuffles the validator set into different subsets per parachain. Epoch ‚Äã An epoch is a time duration in the BABE protocol that is broken into smaller time slots. Each slot has at least one slot leader who has the right to propose a block. In Kusama, it is the same duration as a session . Era ‚Äã A (whole) number of sessions, which is the period that the validator set (and each validator's active nominator set) is recalculated and where rewards are paid out. Equivocation ‚Äã Providing conflicting information to the network. BABE equivocation entails creating multiple blocks in the same slot. GRANDPA equivocation would consist of signing multiple conflicting chains. External Queue ‚Äã The queue for proposals originating with the Polkadot Council which are waiting to become referenda. Compare the Community queue. Extrinsic ‚Äã A SCALE encoded array consisting of a version number, signature, and varying data types indicating the resulting runtime function to be called, including the parameters required for that function to be executed. These state changes are invoked from the outside world, i.e. they are not part of the system itself. Extrinsics can take two forms, " inherents " and " transactions ". For more technical details see the polkadot spec Fellowship ‚Äã A mostly self-governing expert body with a primary goal of representing humans who embody and contain the technical knowledge base of the Kusama and/or Polkadot networks and protocols. Finality ‚Äã The property of a block that cannot be reverted. Generally, created blocks are not final until some point in the future - perhaps never, in the case of "probabilistic finality". The Polkadot Relay Chain uses a deterministic finality gadget known as GRANDPA . Finality Gadget ‚Äã A mechanism that determines finality. Frame ‚Äã The collection of Substrate-provided pallets (Substrate Runtime Modules). Genesis ‚Äã The origin of a blockchain, also known as block 0. It can also be used to reference the initial state of the blockchain at origination. Example In the genesis state Alice, Bob, and Charlie had 30 tokens each. Governance ‚Äã The process of determining what changes to the network are permissible, such as modifications to code or movement of funds. The governance system in Polkadot is on-chain and revolves around stakeholder voting. Governance Council ‚Äã An on-chain entity that consists of several on-chain accounts (starting at 6, eventually moving to the final value of 24). The Council can act as a representative for "passive" (non-voting) stakeholders. Council members have two main tasks: proposing referenda for the overall stakeholder group to vote on and cancelling malicious referenda. GRANDPA Finality Gadget ‚Äã GHOST-based Recursive ANcestor Deriving Prefix Agreement. It is the finality gadget for Polkadot, which allows asynchronous, accountable, and safe finality to the blockchain. For an overview of GRANDPA, see this Medium post Hard Fork ‚Äã A permanent diversion of a blockchain occurs quickly due to a high priority change in a consensus rule. Clients who follow a hard fork always need to upgrade their clients to continue following the upgraded chain. Hard forks are considered permanent divergences of a chain for which non-upgraded clients are following consensus rules incompatible to the ones followed by upgraded clients. Hard Spoon ‚Äã Defined by Jae Kwon of Cosmos as "a new chain that takes into account state from an existing chain; not to compete, but to provide broad access." A non-contentious blockchain that inherits the state of the underlying blockchain and creates a new branch of the same blockchain . Horizontal Relay-routed Message Passing ‚Äã Horizontal Relay-routed Message Passing, also known as HRMP, is a precursor to the complete XCMP implementation, that mimics the same interface and semantics of XCMP. It is similar to XCMP except for how it stores all messages in the Relay Chain storage, therefore making it more expensive and demanding more resources than XCMP. The plan is to retire HRMP once the implementation of XCMP is complete. Inactive Nomination ‚Äã A validator (or validators) that a nominator has selected to nominate, but is not actively validating this era. This type of nomination may become active in a future era. Inherent ‚Äã Extrinsics that are "inherently true." Inherents are not gossiped on the network and are put into blocks by the block author. They are not provably true the way that the desire to send funds is, therefore they do not carry a signature. A blockchain's runtime must have rules for validating inherents. For example, timestamps are inherents. They are validated by being within some margin that each validator deems reasonable. Injected Account ‚Äã An account that is not directly managed by the Polkadot UI but can be accessed through it, such as accounts controlled by the Polkadot{.js} extension. Interoperability ‚Äã The ability for some sort of system to exchange and make use of information often compared to "cross-chain" technologies. Keep-Alive Check ‚Äã The keep-alive check is used to indicate whether or not a transfer can allow the sending account to be reduced to less than the existential deposit, causing it to be reaped. KSM ‚Äã The abbreviation for Kusama network tokens. Kusama ‚Äã The "canary network" for Polkadot. It consists of an early-release, unaudited version of the Polkadot software. It is not a testnet - after the transition to NPoS, the network is entirely in the hands of the community (i.e., Kusama token holders). Lease Period ‚Äã A particular amount of time that a parachain for which the parachain can connect to the Relay Chain. LIBP2P ‚Äã An open-source library for encrypted peer-to-peer communications and other networking functions. More information at: https://libp2p.io/ Liveness ‚Äã The property of a distributed system is that it will eventually come to some sort of consensus. A system stuck in an infinite loop would not be considered live, even if computations are taking place; a system that eventually provides a result, even if incorrect or it takes a long time, is considered to have liveness. Mainnet ‚Äã Short for "main network": the fully functional and acting chain that runs its own network. Message ‚Äã In Polkadot's XCMP protocol, a message is arbitrary data that is sent from one parachain (the egress chain) to another (the ingress chain) through a channel and ensured delivery by the validator set. Message Queue ‚Äã In Polkadot's XCMP protocol, a message queue is the list of messages waiting to be processed by a particular receiving parachain over a channel. Metadata ‚Äã Data that includes information about other data, such as information about a specific transaction. Motion ‚Äã A motion is essentially a "referendum" or "decision" being considered by the Council. The Council can vote on motions like approving Treasury Proposals or making proposals for the community to vote on. Next Session ‚Äã This indicates that the validator will be a member of the active set in the next session. Node Explorer ‚Äã A tool that gives you information about a node, such as the latest blocks sealed, finalized, and the current chain state as known by that node. Nominated Proof of Stake (NPoS) ‚Äã A Proof-of-Stake system where nominators back validators with their own stake as a show of faith in the good behavior of the validator. Nominated Proof-of-Stake differs from the more generic concept Delegated Proof-of-Stake in that nominators are subject to loss of stake if they nominate a bad validator; delegators are not subject to loss of stake based on the behavior of the validator. Note that some other blockchain technologies may use the term Delegated Proof-of-Stake, even if delegators can be slashed. Polkadot uses the Phragm√©n method to allocate stake to nominees. Nominator ‚Äã Accounts that select a set of validators to nominate by bonding their tokens. Nominators receive some of the validators' rewards, but are also liable for slashing if their nominated validators misbehave. Non-fungible Token (NFT) ‚Äã A non-fungible token is a token that does not hold the property of fungibility, which, in turn, means that it cannot be interchangeable and indistinguishable from other tokens. NFTs allow the tokenization of unique items and provide exclusive ownership for those tokens. On-chain Governance ‚Äã A governance system of a blockchain that is controlled by mechanisms on the blockchain. On-chain governance allows decisions to be made transparently. Note that there are a variety of different algorithms for making these decisions, such as simple majority voting, adaptive quorum biasing, or identity-based quadratic voting. Online Message ‚Äã This is a message that is broadcast by a validator to verify to the network that the validator is online, even if they haven't published a block this epoch. This is sometimes referred to as "ImOnline". Polkadot OpenGov ‚Äã Previously known as Governance v2 (Gov2) during early development, Polkadot OpenGov serves as the current governance protocol for both Kusama and Polkadot. Origin ‚Äã The initiator of an extrinsic. A simple origin would be the account that is sending a token to another account. Polkadot also supports more complex origin types, such as the root origin , from which privileged functions can be called. Oversubscribed ‚Äã If more than the maximum number of nominators nominate the same validator, it is "oversubscribed", and only the top staked nominators (ranked by the amount of stake, up to the maximum number of nominators) are paid rewards. Other nominators will receive no rewards for that era. The current maximum number of nominators is on Polkadot, but it can be modified via governance. Pallet ‚Äã A Substrate runtime module. Parachain ‚Äã A blockchain that meets several characteristics that allow it to work within the confines of the Polkadot Host. Also known as "parallelized chain." Parachain Development Kit (PDK) ‚Äã Similar to an SDK , parachain development kits (PDK) is a set of tools that make it easy for developers to create Polkadot compatible parachains . ParaID ‚Äã A unique numeric (non-negative integer) identifier for a parathread. Parathread ‚Äã A blockchain that works within the confines of the Polkadot Host, thus allowing it to connect to the Polkadot Relay Chain. This can be done either by becoming a parachain or connecting on a "pay as you go" basis. All parachains start their life-cycle as a parathread. Parachain Registry ‚Äã A relatively simple database-like construct that holds both static and dynamic information on each chain. Parity Technologies ‚Äã A company, founded by Dr. Gavin Wood and Dr. Jutta Steiner, that is developing Substrate, Kusama and Polkadot. It has also released several other projects including Parity Ethereum and Parity Secret Store. Polkadot ‚Äã A heterogeneous, multi-chain network allowing various blockchains of different characteristics to perform arbitrary, cross-chain communication under shared security. Polkadot Alliance ‚Äã The Polkadot Alliance is an on-chain collective founded by Acala, Astar, Interlay, Kilt, Moonbeam, Phala, and Subscan, to establish standards and ethics for open-source development in referendum #94 . It aims to support development standards and expose bad actors within the ecosystems of Polkadot. Host ‚Äã The environment in which a runtime module can be executed. Parachains must support the Polkadot Host - external chains that do not will have to use a bridge. Previously known as the Polkadot Runtime Environment. Runtime Environment ‚Äã The previous name for the Polkadot Host . Preimage ‚Äã The on-chain proposals do not require the entire image of extrinsics and data (for instance the WASM code, in case of upgrades) to be submitted, but would rather just need that image's hash. That preimage can be submitted and stored on-chain against the hash later, upon the proposal's dispatch. Proof of Stake (PoS) ‚Äã A method of selecting participation in a consensus system, in which participants are chosen based on how many tokens they have at stake (at risk of loss due to misbehavior). Normally, Proof-of-Stake systems limit the number of participants. Proof of Validity ‚Äã A proof produced by parachain collators. Based on this proof and the parachain registry, a validator can verify that a parachain has properly executed its state transition function. Proofs of Validity go into the Relay Chain blocks. Proof of Work (PoW) ‚Äã A method of selecting participants in a consensus system, typically the longest chain rule, in which participants try to solve a puzzle like finding a partial pre-image of a hash. Normally, a Proof-of-Work system can have any number of participants. Proposal ‚Äã A potential function call to be voted on in a referendum. Proposals modify the behavior of the Polkadot network, from minor parameter tuning up to replacing the runtime code. Protocol ‚Äã A system of rules that allows two or more entities of a communications system to transmit information. The protocol defines the rules, syntax, semantics, and synchronization of communication and possible recovery methods. Random Seed ‚Äã A random seed is a pseudo-random number available on-chain. It is used in various places of the Polkadot protocol, most prominently in BABE the block production mechanism. Referendum ‚Äã A vote on whether or not a proposal should be accepted by the network. Referenda may be initiated by the Governance Council, by a member of the public, or as the result of a previous proposal. Stakeholders vote on referenda, weighted by both the size of their stake (i.e. number of DOT held) and the amount of time they are willing to lock their tokens. Re-Genesis ‚Äã Re-Genesis is the process of exporting the current chain state, and creating a new chain that builds on it. Re-Genesis will involve stop-the-world migration, which results in a period of time when no actual blocks are added to the blockchain. In a way, re-genesis can be viewed as a hard fork process. A formal design of Re-Genesis on Substrate is still under development - Re-Genesis Rationale and Design . Relay Chain ‚Äã The chain that coordinates consensus and communication between parachains (and external chains, via bridges). Remarks ‚Äã Remarks are extrinsics with no effect. They provide additional information to external inputs, acting as notes . Remarks are stored alongside block records and do not change the chain's storage; the information is not stored in the chain's trie, but along blocks. Rococo ‚Äã The testnet set aside for testing parachains, cumulus, and related technology. Root Origin ‚Äã A system-level origin in Substrate . This is the highest privilege level and can be thought of as the superuser of the runtime origin. To learn about more raw origins in Substrate, visit Substrate Docs Runtime ‚Äã The state transition function of a blockchain. It defines a valid algorithm for determining the state of the next block given the previous state. Runtime Module ‚Äã A module that implements specific transition functions and features one might want to have in their runtime. Each module should have domain-specific logic. For example, a Balances module has logic to deal with accounts and balances. In Substrate, modules are called "pallets". Safety ‚Äã The property of a distributed system indicating that a particular state transition will not be reverted. GRANDPA provides deterministic safety. That is, for a state changed marked as "safe" or "final", one would require a hard fork to revert that change. Scalability ‚Äã While an ambiguous concept, [blockchain] scalability can be understood as the ability for the network to scale in capabilities (e.g. processing more transactions) when needed. Sealing ‚Äã The process of adding a block to the Relay Chain. Note that finalization is a separate process - blocks are finalized sometime after they are sealed. Session ‚Äã A session is a Substrate implementation term for a period that has a constant set of validators. Validators can only join or exit the validator set at a session change. Session Certificate ‚Äã A message containing a signature on the concatenation of all the Session keys. Session Key ‚Äã Hot keys that are used for performing network operations by validators, for example, signing GRANDPA commit messages. Shared Security ‚Äã The security model that Polkadot uses whereby all chains are equally secured. This is achieved by placing proofs of the validity of parachain blocks into the Relay Chain such that, in order to revert finality of a single parachain, an attacker would need to attack the entire Polkadot system. Slashing ‚Äã The removal of a percentage of an account's DOT as a punishment for a validator acting maliciously or incompetently (e.g., equivocating or remaining offline for an extended period). Soft Fork ‚Äã A backward compatible change to client code causes upgraded clients to start mining a new chain. Requires a "vote-by-hashrate" of a majority of miners to enact successfully. Soft forks are considered temporary divergences in a chain since non-upgraded clients do not follow the new consensus rules but upgraded clients are still compatible with old consensus rules. Software Development Kit (SDK) ‚Äã A collection of software tools (and programs) packaged together that can be used to develop software. Staking ‚Äã The act of bonding tokens for Polkadot (DOT) by putting them up as "collateral" for a chance to produce a valid block (and thus obtain a block reward). Validators and nominators stake their DOT in order to secure the network. State transition function ‚Äã A function that describes how the state of a blockchain can be transformed. For example, it may describe how tokens can be transferred from one account to another. Substrate ‚Äã A modular framework for building blockchains. Polkadot is built using Substrate . Chains built with Substrate will be easy to connect as parachains. For developers, see the Substrate GitHub repository . System Parachains ‚Äã Parachains that are part of the Polkadot core protocol. These are allocated a parachain execution core by governance rather than auction. Tabling ‚Äã In Polkadot governance, bringing a proposal to a vote via referendum. Note that this is the British meaning of "tabling", which is different from the US version, which means "to postpone" a measure. Teleport ‚Äã Send an asset from an account on one chain to an account on a different chain. This occurs by burning an amount on the sending chain and minting an equivalent amount on the destination chain. Testnet ‚Äã Short for "test network": an experimental network where testing and development takes place. Networks are often executed on a testnet before they are deployed to a mainnet . Tokenization ‚Äã The process of replacing sensitive data with non-sensitive data. Tracks ‚Äã Each Origin is associated with a single referendum class and each class is associated with a Track . The Track outlines the lifecycle for the proposal and is independent from other class's tracks. Having independent tracks allows the network to tailor the dynamics of referenda based upon their implied privilege level. Tranche ‚Äã Validators use a subjective, tick-based system to determine when the approval process should start. A validator starts the tick-based system when a new availability core candidates have been proposed, which can be retrieved via the Runtime API , and increments the tick every 500 milliseconds. Each tick/increment is referred to as a ‚Äútranche‚Äù, represented as an integer, starting at 0. Transfer ‚Äã Send an asset from one account to another. This generally refers to transfers that occur only on the same chain. Transaction ‚Äã An extrinsic that is signed. Transactions are gossiped on the network and incur a transaction fee. Transactions are "provably true", unlike inherents. For example, one can prove that Alice wants to send funds to Bob by the fact that she signed a transfer-funds message with her private key. Validator ‚Äã A node that secures the Relay Chain by staking DOT, validating proofs from collators on parachains and voting on consensus along with other validators. Vertical Message Passing ‚Äã Vertical message passing consists of two separate types of message passing, Downward Message Passing (DMP) and Upward Message Passing (UMP). Downward messages pass from the Relay Chain to a parachain, although they may also originate from another parachain via HRMP . Upward messages originate from parachains and go up to the Relay Chain via runtime entry points. Voting ‚Äã The process of stakeholders determining whether or not a referendum should pass. Votes are weighted both by the number of DOT that the stakeholder account controls and the amount of time they are willing to lock their DOT. Waiting Nomination ‚Äã The nominator has nominated this validator, but the validator was not elected into the active validator set this era and thus cannot produce blocks for the canonical chain. If the validator does get into the active set in a future era, this may turn into an active or inactive nomination. Wallet ‚Äã A program that allows one to store private keys and sign transactions for Polkadot or other blockchain networks. Wasm ‚Äã The abbreviation for WebAssembly . Watermark ‚Äã In Polkadot's parachain messaging scheme, the watermark is the minimum processed send-height of the receiving parachain. All messages on all channels that are sending to this parachain at or before the watermark are guaranteed to be processed. Web3 Foundation ‚Äã A Switzerland-based foundation that nurtures and stewards technologies and applications in the fields of decentralized web software protocols, particularly those that utilize modern cryptographic methods to safeguard decentralization, to the benefit and for the stability of the Web3 ecosystem. WebAssembly ‚Äã An instruction format for a virtual, stack-based machine. Polkadot Runtime Modules are compiled to WebAssembly. Also known as Wasm. Weights ‚Äã A permission-less system needs to implement a mechanism to measure and limit usage in order to establish an economic incentive structure, to prevent the network overload, and to mitigate DoS vulnerabilities. This mechanism must enforce a limited time-window for block producers to create a block and include limitations on block size, to prevent execution of certain extrinsics which are deemed too expensive and could decelerate the network. This is handled by the weight system, where the cost of the transactions (referred to as extrinsics ) are determined before execution. Checkout this section of the Substrate docs covering transaction weights and fees . Witness ‚Äã Cryptographic proof statements of data validity. Whitelist Pallet ‚Äã Allows one Origin to escalate the privilege level of another Origin for a certain operation. In terms of OpenGov, it allows the Fellowship to authorise a new origin (which we will call Whitelisted-Root) to be executed with Root-level privileges. Edit this page Last updated on Jul 14, 2023 by Filippo Previous FAQ Next Learn Active Nomination Alexander Asset Hub Attestation Auction (Parachain) Authority Availability Cores BABE Bitfield Array Block Block Explorer Blocks Nominations BLS Bonding Bounty Bridge Byzantine Fault Tolerance Capacity Candidate Collations Collator Collectives Commission Common Good (Parachain) Community Queue Consensus Crowdloan Curator Dapps DOT Duty Roster Epoch Era Equivocation External Queue Extrinsic Fellowship Finality Finality Gadget Frame Genesis Governance Governance Council GRANDPA Finality Gadget Hard Fork Hard Spoon Horizontal Relay-routed Message Passing Inactive Nomination Inherent Injected Account Interoperability Keep-Alive Check KSM Kusama Lease Period LIBP2P Liveness Mainnet Message Message Queue Metadata Motion Next Session Node Explorer Nominated Proof of Stake (NPoS) Nominator Non-fungible Token (NFT) On-chain Governance Online Message Polkadot OpenGov Origin Oversubscribed Pallet Parachain Parachain Development Kit (PDK) ParaID Parathread Parachain Registry Parity Technologies Polkadot Polkadot Alliance Host Runtime Environment Preimage Proof of Stake (PoS) Proof of Validity Proof of Work (PoW) Proposal Protocol Random Seed Referendum Re-Genesis Relay Chain Remarks Rococo Root Origin Runtime Runtime Module Safety Scalability Sealing Session Session Certificate Session Key Shared Security Slashing Soft Fork Software Development Kit (SDK) Staking State transition function Substrate System Parachains Tabling Teleport Testnet Tokenization Tracks Tranche Transfer Transaction Validator Vertical Message Passing Voting Waiting Nomination Wallet Wasm Watermark Web3 Foundation WebAssembly Weights Witness Whitelist Pallet ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Learn about Polkadot General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Learn about Polkadot Learn about the technology behind the Polkadot ecosystem, and the tools you can use to become a participant of Web3. üóÉÔ∏è Basics Staking, Accounts, Transactions, Tokens and NFTs, and Governance. üóÉÔ∏è Advanced Advanced Concepts & Guides about Accounts, Staking, and NFTs. üóÉÔ∏è Architecture Nominators, Validators, Collators, Parachains, and more. üóÉÔ∏è Polkadot Comparisons Comparison between Polkadot and other Networks. üìÑÔ∏è Videos Videos about Polkadot and Related Topics. üóÉÔ∏è Future Implementations Implementations that will be deployed on Polkadot. üóÉÔ∏è Archive Implementations no longer live on Polkadot. Previous Glossary Next Basics ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Basic concepts about Polkadot General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Basic concepts about Polkadot Everything about staking, accounts, transactions, tokens and NFTs. üóÉÔ∏è Accounts Basic Accounts, Identity, Multi-signature, and Proxies. üóÉÔ∏è Transactions Extrinsics, Balance Transfers, and Fees. üóÉÔ∏è Tokens and Assets DOT Token, Asset Hub, and Teleports. üóÉÔ∏è NFTs Intro to NFTs and NFT Projects. üóÉÔ∏è Components OpenGov, Treasury, WASM, Runtime Upgrades, and more. üóÉÔ∏è Staking Staking Philosophy and Mechanics. Previous Learn Next Accounts ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Accounts General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Polkadot Accounts Account Generation Account Identity Multi-Signature Accounts Proxy Accounts Transactions Tokens and Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Accounts Accounts Everything about accounts and native account abstraction on Polkadot. üìÑÔ∏è Polkadot Accounts Polkadot Accounts, Account Identity, and Account Reaping. üìÑÔ∏è Account Generation Generate a Polkadot Basic Account. üìÑÔ∏è Account Identity On-chain Identity, Judgements and Registrars. üìÑÔ∏è Multi-Signature Accounts Multi-signature Accounts on Polkadot. üìÑÔ∏è Proxy Accounts Proxy Accounts on Polkadot. Previous Basics Next Polkadot Accounts ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Accounts General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Polkadot Accounts Account Generation Account Identity Multi-Signature Accounts Proxy Accounts Transactions Tokens and Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Accounts Polkadot Accounts On this page Polkadot Accounts This document covers the basics of Polkadot accounts. See the Advanced Account page for more information about accounts such as account derivation and indices . For a more in-depth explanation of the cryptography behind Polkadot accounts, please see the cryptography page . Account Address ‚Äã An address is the public part of a Polkadot account. The private part is the key used to access this address. The public and private parts together make up a Polkadot account. You can think of the public address of your account like your mailbox and the private key like the key to open that mailbox. Anybody can send mail to your mailbox, but only you can access them as only you have access to its key. In the context of Polkadot accounts, anybody can send tokens to your public address but only you can transact with them using your private key. That is why you should keep your private key secret. Mnemonic and Address Generation ‚Äã A valid account requires a private key that can sign on to one of the supported curves and signature schemes . Without a private key an account cannot sign anything. In Polkadot there are some exceptions of accounts that do not have private keys (i.e. key-less accounts). Such accounts are multi-signature accounts and anonymous proxies that are not discussed here and are meant for an advanced audience. Most wallets generate a mnemonic phrase for users to back up their wallets and generate a private key from the mnemonic. Not all wallets use the same algorithm to convert from mnemonic phrase to private key, which affects the ability to use the same mnemonic phrase in multiple wallets. Wallets that use different measures will arrive at a different set of addresses from the exact mnemonic phrase. Not all wallets use the same algorithm to convert from mnemonic phrase to private key Subkey and Polkadot-JS based wallets use the BIP39 dictionary for mnemonic generation, but use the entropy byte array to generate the private key, while full BIP39 wallets (like Ledger) use 2048 rounds of PBKDF2 on the mnemonic. The same mnemonic may generate different private keys on other wallets due to the various cryptographic algorithms used. See Substrate BIP39 Repo for more information. A typical mnemonic phrase generated by the Subkey tool is shown below. 'caution juice atom organ advance problem want pledge someone senior holiday very' Its corresponding private/public keypair is also shown. Secret seed (Private key): 0x056a6a4e203766ffbea3146967ef25e9daf677b14dc6f6ed8919b1983c9bebbc Public key (SS58): 5F3sa2TJAWMqDhXG6jhV4N8ko9SxwGy8TpaNS1repo5EYjQX Polkadot default address format is the MultiAddress type. This means that the same mnemonic phrase will generate public keys for different parachains. For more information see the Address Format section on the Advanced Account page. Obtaining and Managing an Address ‚Äã info To learn more about generating accounts on Polkadot visit the Account Generation page . Account Balance Types ‚Äã In Polkadot there are different types of balance depending on the account activity. Different balance types indicate whether your balance can be used for transfers, to pay fees, or must remain frozen and unused due to an on-chain requirement. Below is an example that displays different balance types of a Kusama account in the Accounts tab of the Polkadot-JS UI (note that the balance types are the same for a Polkadot account). The total balance indicates the total number of tokens in the account. Note that this number does not necessarily correspond to the tokens you are allowed to transfer. In the example the total number of tokens in 0.6274 KSM. The transferrable balance indicates the number of tokens that are free to be transferred. This is calculated by subtracting the number of locked and reserved tokens from the total number of tokens. Locked funds correspond to tokens used in staking, governance, and vested transfers (see below). In the example, the transferrable balance is 0.0106 KSM. The vested balance indicates tokens that were sent to the account and that are released with a specific time schedule. The tokens are owned by the account but are locked and become available for transfer after a specific number of blocks. In the example, the vested balance is 0.25 KSM. The bonded balance indicates the number of tokens that are locked for on-chain participation to staking. In the example the bonded balance is 0.4 KSM. The democracy balance indicates the number of tokens that are locked for on-chain participation to democracy (i.e. voting for referenda and council). In the example, the democracy balance is 0.4 KSM. The redeemable balance indicates the number of tokens that are ready to be unlocked to become transferrable again. Those tokens already went through the unbonding period. In this case, the redeemable balance is 0.1 KSM. The locked balance indicates the number of tokens that are frozen for on-chain participation to staking and democracy, or for vested transfers. Locks do not stack , which means that if you have different locks the total locked balance is not the addition of all single locks. Instead, the biggest lock decides the total locked balance . In the example, the locked balance is 0.55 KSM because the biggest lock is on democracy (0.55 KSM). The reserved balance indicates the number of tokens that are frozen for on-chain activity other than staking, governance, and vested transfers. Such activity can be setting an identity or a proxy. Reserved funds are held due to on-chain requirements and can usually be freed by taking some on-chain action. For example, the "Identity" pallet reserves funds while an on-chain identity is registered, but by clearing the identity, you can unreserve the funds and make them free again. The same applies to proxies. The idea is that those actions require some network memory usage that is not given for free. In the example we created a governance proxy and the reserved funds for this are 0.0668 KSM. Query Account Data in Polkadot-JS ‚Äã In the Polkadot-JS UI, you can also query account data under Developer > Chain state . Under selected state query choose the system pallet followed by account(AccountId32): FrameSystemAccountInfo , under Option choose an account, and then click on the "+" button on the right. Account information include: nonce , the number of transactions the account sent. consumers , the number of other modules that currently depend on this account's existence. The account cannot be reaped until this is zero. providers , the number of other modules that allow this account to exist. The account may not be reaped until this and sufficients are both zero. sufficients , the number of modules that allow this account to exist for their own purposes. The account may not be reaped until this and providers are both zero. data , the additional data that belongs to this account. Used to store the balance(s) in a lot of chains. More in-depth information about the above data can be found in the substrate code base . The AccountData structure defines the balance types in Substrate. The three types of balances include free , reserved , and frozen . The usable balance of the account is the amount that is free minus any funds considered frozen , while the total balance of the account is the sum of free and reserved funds. The flags describe extra information about the account. More in-depth information about the above data can be found in the balances pallet in the Substrate code base . Unlocking Locks ‚Äã Locks do not stack! The biggest lock decides the total amount of locked funds. See this walk-through video tutorial that will guide you in the process of unlocking funds in the example above. In the example, we mentioned that the locked balance is 0.55 KSM because the biggest lock is on democracy and is 0.55 KSM. As soon as the democracy lock is removed the next biggest lock is on staking 0.5 KSM (bonded 0.4 KSM + redeemable 0.1 KSM). This means that the locked balance will be 0.5 KSM, and 0.05 KSM will be added to the transferrable balance. After redeeming the unbonded 0.1 KSM, the locked balance will be 0.4 KSM, and an additional 0.1 KSM will be added to the transferrable balance. Now the biggest lock is still the bonded one. This means that even if we remove the vested lock, the locked balance will still be 0.4 KSM and no tokens will be added to the transferrable balance. To free those bonded tokens we will need to unbond them and wait for the unbonding period to make them redeemable. If we remove the proxy the reserved funds will be automatically added to the transferrable balance. Existential Deposit and Reaping ‚Äã info See this video tutorial or visit this support page for more information about existential deposit. When you generate an account (address), you only generate a key that lets you access it. The account does not exist yet on-chain. For that, it needs the existential deposit of . Having an account go below the existential deposit causes that account to be reaped . The account will be wiped from the blockchain's state to conserve space, along with any funds in that address. You do not lose access to the reaped address - as long as you have your private key or recovery phrase, you can still use the address - but it needs a top-up of another existential deposit to be able to interact with the chain. Transaction fees cannot cause an account to be reaped. Since fees are deducted from the account before any other transaction logic, accounts with balances equal to the existential deposit cannot construct a valid transaction. Additional funds will need to be added to cover the transaction fees. Here's another way to think about existential deposits. Ever notice those Thumbs.db files on Windows or .DS_Store files on Mac? Those are junk; they serve no specific purpose other than making previews a bit faster. If a folder is empty saved for such a file, you can remove the folder to clear the junk off your hard drive. That does not mean you lose access to this folder forever - you can always recreate it. You have the key , after all - you're the computer's owner. It just means you want to keep your computer clean until you maybe end up needing this folder again and then recreate it. Your address is like this folder - it gets removed from the chain when nothing is in it but gets put back when it has the existential deposit. Account Identity ‚Äã The Identities pallet built into Polkadot allows users to attach on-chain metadata to their accounts. Independent registrars can verify this metadata to provide trustworthiness. info To learn more about how to set or release an identity, how to define sub-accounts, or how to become a registrar, please read this guide . Proxy Accounts ‚Äã Polkadot comes with a generalized proxy account system that allows users to keep keys in cold storage while proxies act on their behalf with restricted (or unrestricted) functionality. info See the proxies page for more information about proxies. Multi-signature Accounts ‚Äã Multi-signature accounts are accounts created from several standard accounts, pure proxies, and even other multi-sig accounts. info See the Multi-signature accounts page for a full explanation about multi-signature accounts, including their use-cases. Resources ‚Äã Understanding Accounts and Keys in Polkadot - An explanation of what the different kinds of accounts and keys are used for in Polkadot, with Bill Laboon and Chinmay Patel of BlockX Labs. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Accounts Next Account Generation Account Address Mnemonic and Address Generation Obtaining and Managing an Address Account Balance Types Query Account Data in Polkadot-JS Unlocking Locks Existential Deposit and Reaping Account Identity Proxy Accounts Multi-signature Accounts Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Account Generation General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Polkadot Accounts Account Generation Account Identity Multi-Signature Accounts Proxy Accounts Transactions Tokens and Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Accounts Account Generation On this page Account Generation DISCLAIMER: Key Security ‚Äã The only ways to access your account are via your secret seed or your account's JSON file in combination with a password. Keep them offline in a secure and private location. If you share these with anyone, they can access your account, including your funds. This information is always a target for hackers and malicious actors. Check out the wiki doc on how to recognize scams . info Visit this support article for more information about key security. Ways to Generate an Account ‚Äã Treasury-funded Wallets and Browser Extensions If you are a general user, you are recommended to create accounts using any of the wallets and extensions funded by the Treasury or Grants. If you are a power user or a developer, you can use Polkadot-JS Extension . When you create accounts using browser extensions like the Polkadot-JS extension , it stores your accounts in a private vault and lets you clear your browser cache without fear. Remember to back up your seed phrase - if you lose access to this computer or the extension somehow crashes beyond repair, the seed phrase will come in handy. Alternatively, you can create accounts using the Polkadot-JS UI , although this option is disabled by default. More info about the Polkadot-JS UI can be found on the dedicated page . Please note that all wallets that are connected to the internet are considered "hot wallets" that are susceptible to a wide range of attacks, so it is recommended to use cold storage when dealing with non-trivial amounts. Current popular cold storage solutions within the Polkadot ecosystem include Ledger and Polkadot Vault . The latter keeps your keys on an air-gapped mobile phone. However, it does require obtaining a dedicated Android or iOS-compatible phone that you are comfortable using only for Polkadot Vault. Besides the Polkadot-JS browser extension and the Polkadot-JS UI, Polkadot addresses can also be created with the Subkey tool . Subkey is intended for users comfortable using the command line and can seem intimidating, but it is quite approachable. Follow the instructions in the Subkey documentation . How-to guides to generate an account See our Support Articles for more information about how to create an account using the tools below. Ledger Hardware Wallet Polkadot Vault Polkadot-JS Browser Extension Polkadot-JS UI Subkey Vanity Generator Backing Up Accounts ‚Äã Depending on what software you use to access your account, there are various ways to back up and restore your account. It is a good idea to back your information up and keep it secure. In general, as long as you know how you created your account and have the seed phrase ( mnemonic phrase ) or JSON file (and password) stored securely, you can restore your account. Ledger Hardware Wallet ‚Äã To use a Ledger hardware wallet to create your address and keep your tokens in cold storage, follow the instructions on the Ledger hardware wallet guide page . Polkadot Vault ‚Äã Parity Signer is a secure way of storing your DOT on an air-gapped device. It is highly recommended that you turn off wifi, cellular network, Bluetooth, NFC, and any other communications methods after installing it. For guidelines about how to create an account using Parity Signer, see this video tutorial and visit this support article . Restore Account on Polkadot Vault ‚Äã See this video tutorial and this support page to learn how to restore your account on the Polkadot-JS UI. Polkadot-JS Browser Extension ‚Äã info For guidelines about how to create an account using the Polkadot Extension, see this video tutorial and visit this support article . The Polkadot-JS Browser Extension (the Polkadot Extension) provides a reasonable balance of security and usability. It provides a separate local mechanism to generate your address and interact with Polkadot. This method involves installing the Polkadot Extension and using it as a ‚Äúvirtual vault," separate from your browser, to store your private keys. It also allows the signing of transactions and similar functionality. It is still running on the same computer you use to connect to the internet and thus is less secure than using Parity Signer or other air-gapped approaches. Account Backup using the Polkadot-JS Browser Extension ‚Äã info See this video tutorial and visit this support page to know how to back up your account. Reset Password using the Polkadot-JS Browser Extension ‚Äã info See this video tutorial to learn how to change the password for an account that has been created on the Polkadot-JS browser extension (i.e. an injected account). danger Before following the instructions below, make sure you have your mnemonic phrase stored in a safe place accessible to you. Let's say you created ACCOUNT 1 protected by password PSW 1 . To reset the password of your ACCOUNT 1 using the browser extension, you must follow the following steps: Go to ACCOUNT 1 on the browser extension and click "Forget account". This action will delete the access to your account. Note that your tokens are still in your account on the Polkadot blockchain network. On the browser extension click the "+" button in the top right corner and select the option "Import account from pre-existing seed". After entering the mnemonic phrase, you can choose a new password, PSW 2 . JSON files do not allow changing account passwords If you add the account to the extension using the option "Restore account from backup JSON file", this will allow you to restore access to your account using a JSON file protected by the password PSW 1 , but does not let you set a new password. Thus, PSW 1 will become the account password by default. Accounts on Cold wallets do not need passwords For hardware wallets such as Ledger , you may have to set a PIN for accessing the accounts on the device, but you do not need to set a password for every individual account. When you need to make transactions with your account, you are required to sign using your Ledger device. Also, Ledger wallets let you generate multiple accounts for multiple blockchain networks without setting different passwords to access such accounts. Restore Account on the Polkadot-JS Browser Extension ‚Äã info See this video tutorial and this support page to learn how to restore your account on the Polkadot-JS UI. Polkadot-JS UI ‚Äã info For guidelines about how to create an account using Polkadot-JS UI, see this video tutorial and visit this support article . caution If you use this method to create your account and clear your cookies in your browser, your account will be lost forever if you do not back it up . Make sure you store your seed phrase in a safe place or download the account's JSON file if using the Polkadot{.js} browser extension. Learn more about account backup and restoration here . Local in-browser account storage is disabled by default on the Polkadot-JS UI. To create an account using the Polkadot-JS UI, navigate to settings > account options and click on allow local in-browser account storage in the drop-down menu. Using the Polkadot-JS user interface without a browser extension is not recommended . It is the least secure way of generating an account. It should only be used if all other methods are not feasible. Account Backup using the Polkadot-JS UI ‚Äã info See this video tutorial and visit this support page to know how to back up your account. Reset password using the Polkadot-JS UI ‚Äã To reset the password of an account created with Polkadot-JS Apps UI, you need to go to the "Accounts" tab, click the icon with three vertical dots on your account and select "Change this account's password". See this video tutorial to learn how to change the password for an account created on the Polkadot-JS UI (i.e. a non-injected account). note If you create an account first using Polkadot-JS Apps UI and then add it to the browser extension, you need to follow the guidelines for the browser extension to change the the password of such an account. Restore Account on the Polkadot-JS UI ‚Äã See this video tutorial and this support page to learn how to restore your account on the Polkadot-JS UI. Subkey ‚Äã Subkey is recommended for technically advanced users comfortable with the command line and compiling Rust code. Subkey lets you generate keys on any device that can compile the code. Subkey may also be useful for automated account generation using an air-gapped device. It is not recommended for general users. info For guidelines about how to create an account using Subkey, see this video tutorial and visit this support article . Vanity Generator ‚Äã The vanity generator is a tool on Polkadot-JS UI that lets you generate addresses that contain a specific substring. For the tutorial on how to create an account using Vanity Generator, visit this support article . Account Address Format ‚Äã An account created for Polkadot can also be used on multiple chains in the Polkadot ecosystem. More specifically, the account of a chain that uses the *25519 account address format (the latest list can be accessed on the ss58 registry repository is cross-compatible with all the chains that use the similar format. To switch between the accounts on different chains, you can follow the guidelines in this support article . Subscan has a tool you can use to convert your address between the different chain formats. Using the same account on multiple chains - Pros and Cons The address format differs from chain to chain, but that difference is only visual. The same private key can be used to sign transactions on behalf of the respective accounts on multiple chains. Using a single account on multiple chains is convenient, as you do not have to deal with multiple mnemonic phrases or private keys. But, if your account gets compromised on one chain, the attacker can gain full access to the accounts on all other chains. This also has implications for the account holder's privacy, as knowing the identity of an account on one chain can expose the account holder's identity on all the chains. In the Accounts tab, the Polkadot-JS UI displays a warning message next to each Account you are using on multiple chains and recommends using different Accounts on different chains (see below). On Polkadot-JS Extension, you can copy your address by clicking the account's icon while the desired chain format is active. E.g. selecting "Substrate" as the format will change your address, and clicking the colorful icon of your account will copy it in that format. While in Polkadot mode, that address format will be copied, and so on. Encryption Enhancement ‚Äã Some newly generated JSON account files cannot be imported (restored) into older wallet software. This is due to an enhanced encryption method, noticeable in a slight delay when encrypting/decrypting your wallet. If you cannot load a JSON file, please use the latest version of the wallet software. If you cannot load it, ensure that the wallet software uses the newest version of the Polkadot API . Edit this page Last updated on Jul 18, 2023 by bader y Previous Polkadot Accounts Next Account Identity DISCLAIMER: Key Security Ways to Generate an Account Backing Up Accounts Ledger Hardware Wallet Polkadot Vault Restore Account on Polkadot Vault Polkadot-JS Browser Extension Account Backup using the Polkadot-JS Browser Extension Reset Password using the Polkadot-JS Browser Extension Restore Account on the Polkadot-JS Browser Extension Polkadot-JS UI Account Backup using the Polkadot-JS UI Reset password using the Polkadot-JS UI Restore Account on the Polkadot-JS UI Subkey Vanity Generator Account Address Format Encryption Enhancement ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Account Identity General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Polkadot Accounts Account Generation Account Identity Multi-Signature Accounts Proxy Accounts Transactions Tokens and Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Accounts Account Identity On this page Account Identity Polkadot provides a naming system that allows participants to add personal information to their on-chain account and subsequently ask for verification of this information by registrars . Setting an Identity ‚Äã Users can set an identity by registering through default fields such as legal name, display name, website, Twitter handle, Riot handle, etc. along with some extra, custom fields for which they would like attestations (see Judgements ). Users must reserve funds in a bond to store their information on chain: and per each field beyond the legal name. These funds are locked , not spent - they are returned when the identity is cleared. These amounts can also be extracted by querying constants through the Chain state constants tab on Polkadot-JS Apps. Instructions for setting and clearing Identities The procedure to set and clear identities is explained in detail in this support article - How to set and clear an Identity note The Ledger app on Nano S doesn't support the extrinsic for setting identity. As a workaround, create a primary identity with an on-chain account and then using that primary identity, assign a sub-identity to the Ledger stash. Format Caveat ‚Äã Please note the following caveat: because the fields support different formats, from raw bytes to various hashes, a UI has no way of telling how to encode a given field it encounters. The Polkadot-JS UI currently encodes the raw bytes it encounters as UTF8 strings, which makes these values readable on-screen. However, given that there are no restrictions on the values that can be placed into these fields, a different UI may interpret them as, for example, IPFS hashes or encoded bitmaps. This means any field stored as raw bytes will become unreadable by that specific UI. As field standards crystallize, things will become easier to use but for now, every custom implementation of displaying user information will likely have to make a conscious decision on the approach to take, or support multiple formats and then attempt multiple encodings until the output makes sense. Judgements ‚Äã After a user injects their information on chain, they can request judgement from a registrar. Users declare a maximum fee that they are willing to pay for judgement, and registrars whose fee is below that amount can provide a judgement. When a registrar provides judgement, they can select up to six levels of confidence in their attestation: Unknown: The default value, no judgement made yet. Reasonable: The data appears reasonable, but no in-depth checks (e.g. formal KYC process) were performed (all the currently verified identities on-chain). Known Good: The registrar has certified that the information is correct (this step involves verification of state issued identity documents, and at the moment no account has known good identity, with the exception of registrars). Out of Date: The information used to be good, but is now out of date. Low Quality: The information is low quality or imprecise, but can be fixed with an update. Erroneous: The information is erroneous and may indicate malicious intent. A seventh state, "fee paid", is for when a user has requested judgement and it is in progress. Information that is in this state or "erroneous" is "sticky" and cannot be modified; it can only be removed by the complete removal of the identity. Registrars gain trust by performing proper due diligence and would presumably be replaced for issuing faulty judgments. To be judged after submitting your identity information, go to the Extrinsics tab in the Polkadot-JS UI and select the identity pallet, then requestJudgement . For the reg_index put the index of the registrar you want to be judged by, and for the max_fee put the maximum you're willing to pay for these confirmations. If you don't know which registrar to pick, first check the available registrars by going to Chain State tab in the Polkadot-JS UI and selecting identity.registrars() to get the full list. Instructions for requesting and cancelling Identity judgments The procedure to request and cancel identity judgments is explained in detail in this support article Requesting judgement through Web3 Foundation Registrar If you requested judgement for your on-chain identity through the Web3 Foundation Registrar (i.e. Registrar #0) you will need to complete a few additional tasks. For more information visit this support article . caution The set identity calls go on-chain. Hence, the contact information is available publicly, for both legitimate entities, like registrars or validators, but also scammers who might impersonate them. The strings in the identity fields are good candidates for homograph attacks, as someone could list a fraudulent website (web3.f0undation instead of web3.foundation for example) and still get verified by the registrar (if the checks are automated)! In a decentralized network, one should be cautious making transactions with accounts solely based on their identity. If an account on-chain claims to be of Web3 Foundation, it is wise to verify its authenticity by checking directly with Web3 Foundation or examining the established history of that account on-chain. Registrars ‚Äã Registrars can set a fee for their services and limit their attestation to certain fields. For example, a registrar could charge 1 DOT to verify one's legal name, email, and GPG key. When a user requests judgement, they will pay this fee to the registrar who provides the judgement on those claims. Users set a maximum fee they are willing to pay and only registrars below this amount would provide judgement. Becoming a Registrar ‚Äã Advanced How-to Guides See this page to learn how to become a Registrar. Current Registrars ‚Äã There are multiple registrars on Polkadot. Unless no additional information is available here, you must reach out to specific registrars individually if you want to be judged by those. Registrar 0: URL : https://registrar.web3.foundation/ Account : 12j3Cz8qskCGJxmSJpVL2z2t3Fpmw3KoBaBaRGPnuibFc7o8 Fee : 0 DOT Registrar 1: URL : https://registrar.d11d.net/ Account : 1Reg2TYv9rGfrQKpPREmrHRxrNsUDBQKzkYwP1UstD97wpJ Fee : 10 DOT Registrar 2: Account : 1EpXirnoTimS1SWq52BeYx7sitsusXNGzMyGx8WPujPd1HB Fee : 0 DOT To find out how to contact the registrar after the application for judgement or to learn who they are, we can check their identity by adding them to our Address Book. Their identity will be automatically loaded. Sub Accounts ‚Äã Users can also link accounts by setting "sub accounts", each with its own identity, under a primary account. The system reserves a bond for each sub account. An example of how you might use this would be a validation company running multiple validators. A single entity, "My Staking Company", could register multiple sub accounts that represent the Stash accounts of each of their validators. An account can have a maximum of 100 sub-accounts. Note that a deposit of is required for every sub-account. Explainer article and video on setting sub-identities Here is the how to set sub-identities article and a video tutorial on setting sub-identities using Polkadot-JS UI Clearing and Killing an Identity ‚Äã Clearing: Users can clear their identity information and have their deposit returned. Clearing an identity also clears all sub accounts and returns their deposits. Visit the section "Clear an Identity" on this support article for guidelines about clearing identities. Killing: The Council can kill an identity that it deems erroneous. This results in a slash of the deposit. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Account Generation Next Multi-Signature Accounts Setting an Identity Format Caveat Judgements Registrars Becoming a Registrar Current Registrars Sub Accounts Clearing and Killing an Identity ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Multi-Signature Accounts General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Polkadot Accounts Account Generation Account Identity Multi-Signature Accounts Proxy Accounts Transactions Tokens and Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Accounts Multi-Signature Accounts On this page Multi-Signature Accounts Introduction to Multisig Accounts ‚Äã It is possible to create multi-signature accounts (multisig) in Substrate-based chains. A multisig is composed of one or more addresses and a threshold. The threshold defines how many signatories (participating addresses) need to agree on submitting an extrinsic for the call to be successful. For example, Alice, Bob, and Charlie set up a multisig with a threshold of 2. This means Alice and Bob can execute any call even if Charlie disagrees with it. Likewise, Charlie and Bob can execute any call without Alice. A threshold is typically a number smaller than the total number of members but can also be equal to it, which means they all have to agree. Learn more about multisig accounts Check the "How to create a multisig account" section on this support page . We recommend trying out the tutorial on Westend network - Polkadot's testnet. Multi-signature accounts have several uses: securing your stash: use additional signatories as a 2FA mechanism to secure your funds. One signer can be on one computer, and another can be on another or in cold storage. This slows down your interactions with the chain but is orders of magnitude more secure. board decisions: legal entities such as businesses and foundations use multisigs to govern over the entity's treasury collectively. group participation in governance: a multisig account can do everything a regular account can. A multisig account could be a council member in Kusama's governance, where a set of community members could vote as one entity. Multi-signature accounts cannot be modified after being created . Changing the set of members or altering the threshold is not possible and instead requires the dissolution of the current multisig and creation of a new one. As such, multisig account addresses are deterministic , i.e. you can always calculate the address of a multisig by knowing the members and the threshold, without the account existing yet. This means one can send tokens to an address that does not exist yet, and if the entities designated as the recipients come together in a new multisig under a matching threshold, they will immediately have access to these tokens. Multisig Accounts with the Polkadot-JS UI ‚Äã Multisig Transactions with Accounts Tab ‚Äã Waltkthrough Video Tutorial See this video tutorial to learn how to transact with a multisig account using the Accounts Tab in the Polkadot-JS UI . You can create a multisig account directly on the Accounts Tab of the Polkadot-JS UI , and use this account to send funds. See this support article for more information. Multisig Transactions with Extrinsic Tab ‚Äã There are three types of actions you can take with a multisig account: Executing a call asMulti . This is used to begin or end a multisig transaction. Approving a call approveAsMulti . This is used to approve an extrinsic and pass-on to the next signatory (see example below for more information). Cancelling a call cancelAsMulti . info Check out this page for more information about the actions you can take with a multi-signature account. In scenarios where only a single approval is needed, a convenience method as_multi_threshold_1 should be used. This function takes only the other signatories and the raw call as arguments. Note that the Polkadot-JS UI does not have integration for this call because it is not possible to create multisig accounts with threshold=1 . If you want to create a multisig with threshold 1, you can use txwrapper-core , which is developed and supported by Parity Technologies. There is a detailed multisig example that you can try out and change to see how it works. However, in anything but the simple one approval case, you will likely need more than one of the signatories to approve the call before finally executing it. When you create a new call or approve a call as a multisig, you will need to place a small deposit. The deposit stays locked in the pallet until the call is executed. The deposit is to establish an economic cost on the storage space that the multisig call takes up on the chain and discourage users from creating dangling multisig operations that never get executed. The deposit will be reserved in the caller's accounts, so participants in multisig wallets should have spare funds available. The deposit is dependent on the threshold parameter and is calculated as follows: Deposit = depositBase + threshold * depositFactor Where depositBase and depositFactor are chain constants (in DOT units) set in the runtime code. Currently, the deposit base equals and the deposit factor equals . Example using Multisig Accounts ‚Äã Walk-through video tutorial See this video tutorial to learn how to transact with a multisig account using the Extrinsic Tab in the Polkadot-JS UI . Let's consider an example of a multisig on Polkadot with a threshold of 2 and 3 signers: Charlie, Dan, and Eleanor. First, Charlie will create the call on-chain by calling the multisig.asMulti extrinsic with the raw call, in this case, a balance transfer ( balances.transferKeepAlive extrinsic) from multisig CDE to Frank's account. When doing this, Charlie will have to deposit DepositBase + (2 * DepositFactor) = 20.152 DOT while he waits for either Dan or Eleanor also to approve the balance transfer call using the multisig.approveAsMulti or the multisig.asMulti extrinsics. If Dan submits the multisig.approveAsMulti extrinsic, he approves Charlie's call but he passes on the final approval to Eleanor. So, although the multisig has threshold 2, in this case all 3/3 signatories need to participate in the transaction approval. Eleanor will need to submit a multisig.asMulti or multisig.approveAsMulti extrinsic to transfer funds from CDE to Frank. Alternatively, Dan or Eleanor can just submit a multisig.asMulti extrinsic after Charlie to transfer the funds. In this case, 2/3 signatories will participate in the transaction approval. The accounts approving Charlie's call will not need to place the deposit, and Charlie will receive his deposit back once the transfer is successful or canceled. To cancel the transaction, Dan or Eleanor can use the multisig.cancelAsMulti extrinsic. Note that multisigs are deterministic , which means that multisig addresses are generated from the addresses of signers and the threshold of the multisig wallet. No matter the order of the signatories' accounts, the multisig will always have the same address because accounts' addresses are sorted in ascending order. Addresses that are provided to the multisig wallet are sorted Public keys of signers' wallets are compared byte-for-byte and sorted ascending before being used to generate the multisig address. For example, consider the scenario with three addresses, A, B, and C, starting with 5FUGT , 5HMfS , and 5GhKJ . If we build the ABC multisig with the accounts in that specific order (i.e. first A, then B, and C), the real order of the accounts in the multisig will be ACB. If, in the Extrinsic tab, we initiate a multisig call with C, the order of the other signatories will be first A, then B. If we put first B, then A, the transaction will fail. This has some implications when using the Extrinsics tab on the Polkadot-JS UI to perform multisig transactions. If the order of the other signatories is wrong, the transaction will fail. This does not happen if the multisig is executed directly from the Accounts tab (recommended). The Polkadot-JS UI supports multisig accounts, as documented on the Account Generation page . You can see our video tutorials for more information about creating multisig accounts and transacting with them using both the Accounts Tab and the Extrinsic Tab in the Polkadot-JS UI. Multisig with Multix Tool ‚Äã Walk-Through Video Tutorial See this video tutorial and this article for more information about using Multix for multisig accounts. The Multix tool is an easy-to-use interface to manage complex multisigs. The tool is part of the open-source Polkadot/Kusama Tech Stack . Besides being user-friendly, Multix provides several benefits compared to the Polkadot-JS UI: When one person creates the multisig account, all the signatories see the multisig account on their interface. There is no need to pass around the call data to confirm a transaction. Such information is retrieved from the chain and displayed to the users. Thanks to the power of pure proxies , adding and removing signatories or adjusting the threshold is now possible without creating new multisigs and Multix provides an intuitive interface to work with pure proxies. Decoding Multisig Call Data ‚Äã info Before signing a transaction, it is important to know the exact specifics of what is being signed. Check the "How to use a multisig account" in the support docs on how to decode the multisig call data. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Account Identity Next Proxy Accounts Introduction to Multisig Accounts Multisig Accounts with the Polkadot-JS UI Multisig Transactions with Accounts Tab Multisig Transactions with Extrinsic Tab Multisig with Multix Tool Decoding Multisig Call Data ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Proxy Accounts General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Polkadot Accounts Account Generation Account Identity Multi-Signature Accounts Proxy Accounts Transactions Tokens and Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Accounts Proxy Accounts On this page Proxy Accounts The Account Tab in the Polkadot-JS UI cannot handle complicated proxy setups The Accounts Tab in the Polkadot-JS UI cannot handle complex proxy setups (e.g. a proxy -> multisig -> an anonymous proxy which is part of another multisig). These complex setups must be done using the Extrinsics Tab directly. We recommend to use the Westend Testnet if you are testing features for the first time. By performing the complex proxy setups on the testnet, you can comfortably replicate the procedure on the main networks. Proxies allow users to use an account (it can be in cold storage or a hot wallet) less frequently but actively participate in the network with the weight of the tokens in that account. Proxies are allowed to perform a limited amount of actions related to specific substrate pallets on behalf of another account. The video below contains more information about using proxies. Why use a Proxy? ‚Äã Proxies are helpful because they let you delegate efficiently and add a layer of security. Rather than using funds in a single account, smaller accounts with unique roles can complete tasks on behalf of the main stash account. Proxies can be hotter than the initial account, which can be kept cold, but the weight of the tokens in the colder account can be used by the hotter accounts. This increases the security of your accounts by minimizing the number of transactions the cold account has to make. This also drives attention away from the stash account, although it is possible to determine the relationship between the proxy and the proxied account. From the security perspective, we can imagine proxies as bodyguards of a VIP, loyal and ready to risk their lives to ensure the VIP's protection. But proxies are also useful in other contexts such as efficient account management at the corporate level. They also provide an elegant solution to change signatories within multi-signature accounts, and they can be used within proxy calls and nested proxy calls. In this page we will explore all these interesting use cases of proxies within the Polkadot ecosystem. Shown below is an example of how you might use these accounts. Imagine you have one stash account as your primary token-holding account and don't want to access it very often, but you want to participate in staking to earn staking rewards. You could set one of your existing accounts as a staking proxy for that stash account, and use your staking proxy to sign all staking-related transactions. Having a staking proxy will make the stash account isolated within the staking context. In other words, the account assigned as a staking proxy can participate in staking on behalf of that stash. Without the proxy you will need to sign all the staking-related transactions with the stash. If the proxy is compromised, it doesn't have access to transfer-related transactions, so the stash account could just set a new proxy to replace it. You can also monitor proxies by setting a time-delay . Creating multiple proxy accounts that act for a single account, lets you come up with more granular security practices around how you protect private keys while still being able to actively participate in the network. info The maximum number of proxies allowed for a single account is . You can have the same proxy for multiple accounts. Creating Proxy ‚Äã info To create a proxy account read this support article . Proxy Types ‚Äã You can set up a proxy account via the proxy pallet. When you set a proxy, you must choose a type of proxy for the relationship. Polkadot offers: Any Non-transfer Governance Staking Identity Judgement Cancel Auction Nomination pool When a proxy account makes a transaction, Polkadot filters the desired transaction to ensure that the proxy account has the appropriate permission to make that transaction on behalf of the cold account. For example, staking proxies have permission to do only staking-related transactions. Know how to check the calls and pallets accessible by proxies For the latest information on the calls and pallets that can be fully accessed by proxies, check the source code in the runtime folder on the Polkadot repository Any Proxy ‚Äã As implied by the name, a proxy type of Any allows the proxy account to make any transaction, including balance transfers. In most cases, this should be avoided as the proxy account is used more frequently than the cold account and is therefore less secure. Non-transfer Proxy ‚Äã Proxies that are of the type Non-transfer are accounts that allow any type of transaction except balance transfers (including vested transfers). Hence, this proxy does not have permission to access calls in the Balances and XCM pallet. Governance Proxy ‚Äã The Governance type will allow proxies to make transactions related to governance (i.e., from the Democracy , Phragmen Election , Treasury , Bounties , Tips , Utility and Child Bounties pallets). Explainers on governance proxies See Governance for more information on governance proxies or watch our technical explainer video that explores this concept . Staking Proxy ‚Äã info Visit the Advanced Staking Concepts page for more detailed information about staking proxies. The Staking type allows all staking-related transactions. The stash account is meant to stay in cold storage, while the staking proxy account makes day-to-day transactions like setting session keys or deciding which validators to nominate. The staking proxy can fully access Staking, Session, Utility and Fast Unstake pallets. Do not use Staking proxy for participating in Nomination Pools Use a non-transfer instead of a staking proxy to participate in nomination pools. The staking proxy is not enabled to make successful calls to the nomination pools pallet. Identity Judgement Proxy ‚Äã The Identity Judgement proxies are in charge of allowing registrars to make judgments on an account's identity. If you are unfamiliar with judgment and identities on chain, please refer to this page . This proxy can only access provide_judgement call from the Identity pallet along with the calls from the Utility pallet. Cancel Proxy ‚Äã Proxies that are of the type Cancel allow accounts to reject and remove any time-delay proxy announcements. This proxy can only access reject_announcement call from the Proxy pallet. Auction Proxy ‚Äã Proxies that are of the type Auction are accounts that allow transactions pertaining to parachain auctions and crowdloans. The Auction proxy account can sign those transactions on behalf of an account in cold storage. If you already set up a Non-transfer proxy account, it can do everything an Auction proxy can do. Before participating in a crowdloan using an Auction proxy, it is recommended that you check with the respective parachain team for any possible issues pertaining to the crowdloan rewards distribution. Auction proxy can access Auctions, Crowdloan, Registrar and Slots pallets. Nomination Pools Proxy ‚Äã Proxies that are of the type Nomination Pools are accounts that allow transactions pertaining to Nomination Pools . Removing Proxy ‚Äã Removing proxies Read the section "Removing Proxies" on this support page to learn how to remove proxies. How to view your Proxy ‚Äã To view your proxy, just go on the Accounts menu in the Polkadot-JS UI, next to the proxied account you will notice a blue icon. Hover on it, and you will see Proxy overview . Click on it and you will be presented with a list of all proxies for that account. Additionally, you can head over to the Chain State tab (underneath the Developer menu) on Polkadot-JS Apps . If you've created your proxy on a Kusama account, it is required to change your network accordingly using the top left navigation button. On this page, the proxy pallet should be selected, returning the announcements and proxies functions. The proxies function will allow you to see your created proxies for either one account or for all accounts (using the toggle will enable this). Proxy announcements are what time lock proxies do to announce they are going to conduct an action. Proxy Deposits ‚Äã Proxies require deposits in the native currency (i.e. DOT or KSM) to be created. The deposit is required because adding a proxy requires some storage space on-chain, which must be replicated across every peer in the network. Due to the costly nature of this, these functions could open up the network to a Denial-of-Service attack. To defend against this attack, proxies require a deposit to be reserved while the storage space is consumed over the lifetime of the proxy. When the proxy is removed, so is the storage space, and therefore the deposit is returned. The required deposit amount for n proxies is equal to: ProxyDepositBase + ProxyDepositFactor * n where the ProxyDepositBase is the required amount to be reserved for an account to have a proxy list (creates one new item in storage). For every proxy the account has, an additional amount defined by the ProxyDepositFactor is reserved as well (appends 33 bytes to storage location). The ProxyDepositBase is and the ProxyDepositFactor is . Time-delayed Proxy ‚Äã We can add a layer of security to proxies by giving them a delay time. The delay will be quantified in blocks. Polkadot has approximately 6 seconds of block time. A delay value of 10 will mean ten blocks, which equals about one minute delay. The proxy will announce its intended action using the proxy.announce extrinsic and will wait for the number of blocks defined in the delay time before executing it. The proxy will include the hash of the intended function call in the announcement. Within this time window, the intended action may be canceled by accounts that control the proxy. This can be done by the proxy itself using the proxy.removeAnnouncement extrinsic or by the proxied account using the the proxy.rejectAnnouncement extrinsic. Now we can use proxies knowing that any malicious actions can be noticed and reverted within a delay period. After the time-delay, the proxy can use the proxy.proxyAnnounced extrinsic to execute the announced call. info See this video tutorial to learn how you can setup and use time-delayed proxies. The video goes through the example below. Announcing n calls using a time-delayed proxy also requires a deposit of the form: announcementDepositBase + announcementDepositFactor * n where the announcementDepositBase is the required amount to be reserved for an account to announce a proxy call. For every proxy call the account has, an additional amount defined by the announcementDepositFactor is reserved as well. The announcementDepositBase is and the announcementDepositFactor is . Let's take for example the stash account Eleanor setting Bob as a time-delayed staking proxy. In this way, if Bob submits an extrinsic to change the reward destination, such extrinsic can be rejected by Eleanor. This implies that Eleanor monitors Bob, and that within the time-delay she can spot the announced extrinsic. Eleanor can check all the proxy call announcements made by her account's proxies on-chain. On Polkadot-JS UI, go to Developer > Storage > Proxy > Announcements to check the hashes for the calls made by the proxy accounts and the block height at which they are enabled for execution. info If you try to use proxy.proxyAnnounced to execute the call within the time-delay window you will get an error "Proxy unannounced" since the announcement will be done after the time delay. Also note that regular proxy.proxy calls do not work with time-delayed proxies, you need to announce the call first and then execute the announced call on a separate transaction. Proxy calls ‚Äã Proxy calls are used by proxies to call proxied accounts. These calls are important for example in the case of pure proxies, as any attempt to sign transactions with a pure proxy will fail. For more details see the dedicated section about anonymous proxies . Nested Proxy Calls ‚Äã As the term suggests, nested proxy calls are proxy calls within proxy calls. Such calls are needed if there are proxied accounts that are proxies themselves. In the example diagram below, Alice has a stash account that has a staking proxy account, P-C. P-C is a pure proxy, a proxied account originally spawned by Charly that is now an any proxy of P-C and signs everything on its behalf. For example, to bond more funds, Charly needs to submit a prox.proxy extrinsic to P-C, which in turn submits a proxy.proxy extrinsic to Alice including for example a staking.bondExtra extrinsic, specifying the number of extra tokens that need to be bounded. If Charly wants to leave, a new account can take his place as any proxy (before Charly leaves!). There is no need to change the staking proxy account. Also, Alice is the only one who can remove P-C as a staking proxy, and P-C can only perform staking-related tasks. For example, P-C cannot send funds out from Alice's account. Proxy calls can be done using the Extrinsic Tab in the Polkadot-JS UI. Nested proxy calls can be done by calling each proxy.proxy extrinsic separately, or in some cases by just calling the last proxy.proxy extrinsic. In the diagram above, submitting the proxy call from P-C to Alice will automatically ask for Charly's signature. Thus one proxy call will trigger the second one because Charly's is the only any proxy of P-C, and P-C cannot sign anything. While if we want to use Bob's account we will need to submit all three proxy calls. Anonymous Proxy (Pure Proxy) ‚Äã Risk of loss of funds Read carefully the text below and before performing any action using anonymous proxies on Polkadot, experiment on the Westend testnet. Anonymous proxies are very different from other proxy types. The proxies we described so far are existing accounts assigned as proxies by a primary account. These proxies act on behalf of the primary account, reducing the exposure of the primary account's private key. Remember, the more often we use an account's private key to sign transactions, the more we expose that key to the internet, increasing the visibility of that account. The purpose of a proxy is thus to draw the attention of potential attackers away from the primary account, as proxies' private keys will be used most of the time to perform actions on behalf of the primary account. Anonymous proxies are new accounts that are created (not assigned) by a primary account. That primary account then acts as any proxy on behalf of the anonymous proxy. Anonymous proxies are keyless non-deterministic accounts as they do not have a private key but they have an address that is randomly generated. Also, in some sense, nobody owns an anonymous proxy as nobody has a private key to control them. Renaming Anonymous Proxies as Pure Proxies Anonymous proxies are not anonymous because they have an address that is spawned by a primary account acting as any proxy. Even if the any proxy changes, it is still possible to find who generated the anonymous proxy by going backward using a block explorer. There was thus the need to change the name of anonymous proxy. People suggested keyless accounts since they do not have a private key and are proxied accounts. However, multisig accounts are also keyless (but deterministic). Moreover, even if anonymous proxies are proxied accounts, they can still act as proxies and control other accounts via proxy calls (see multisig example below). Thus, the name that has been chosen is pure proxy . If you want to know more about the reasoning behind renaming of pure proxies, see the discussion in this PR or the discussion on Polkadot forum . From now on we will thus use the term pure proxy instead of anonymous proxy. Create and Remove Pure Proxy ‚Äã info To create a pure proxy see this support article , or watch this technical explainer video . Removing Pure Proxies The procedure for removing a pure proxy is different from the one used to remove other proxies. Visit the section "Removing an Anonymous Proxy" on this support article , or watch this technical explainer video . Explainer video on Pure Proxies Learn more about pure proxies from our technical explainer video . Use of Pure Proxy ‚Äã The use of the pure proxy is strictly bound to the relationship between the pure proxy and the any proxy. Note that the any proxy does not necessarily be the one who created the pure proxy in the first place. Hence, pure proxies are not really owned by somebody, but they can be controlled. Once that relationship between the pure proxy and its any proxy is broken, the pure proxy will be inaccessible (even if visible on the Polkadot-JS UI). Also, pure proxies are non-deterministic, meaning that if we lose one pure proxy , the next one we create from the same primary account will have a different address. Pure proxies cannot sign anything because they do not have private keys. However, although they do not have private keys and cannot sign any transaction directly, they can act as proxies (or better, proxy channels) within proxy.proxy calls (proxy calls). For example, it is possible to have pure proxies within a multisig. Using proxy calls, it is possible to use the any proxy to call the pure proxy, which in turn will do a multisig call. More about this later on. danger Once you remove the relationship with any proxy, the pure proxy will be inaccessible. Also, pure proxies cannot sign for anything. Why Pure Proxy? ‚Äã Despite their complexity and associated dangers, pure proxies have important benefits that we discuss below. Enhanced Security ‚Äã Pure proxies cannot be stolen because they do not have private keys. The only accounts that have full access to the pure proxies are any proxies. Security can be further increased if the any proxy is a multi-signature account. Simplified and Secure Account Management at a Corporate Level ‚Äã Walk-through tutorial video You can see this video tutorial that goes through this scenario. The tutorial requires some familiarity with the Extrinsic Tab of the Polkadot-JS UI. Probably the greatest benefit of using pure proxies is the management of complex account relationships at a corporate level. Let's take for example 3 accounts belonging to Charlie, Dan and Eleanor working for Company X. Charlie holds funds belonging to Company X, but he wants to leave the company and transfer the economic responsibility to Eleanor. Dan is a staking proxy of Charlie. Without Pure Proxy , Charlie must (see left side of the Figure below): Remove Dan as a staking proxy, this step requires 1 signature Stop nominating and unbound all funds , this step requires 2 signatures Transfer the funds to Eleanor, this step requires 1 signature Then Eleanor adds Dan as a staking proxy (1 signature). The whole process requires 5 signatures. Here we are presenting a simple example, in fact, with multi-signature accounts and multiple proxies the procedure would be more time-consuming and labor-intensive. With Pure Proxy (see right side of the Figure above), Charlie must add Eleanor as any proxy of the pure proxy, and remove himself (or Eleanor can remove him). The process requires just 2 signatures (1 signature to add the new any proxy and 1 signature the remove the old one). The funds remain in the pure proxy, and it is not necessary to stop nominating or unbond funds. Also, any proxy relationships with the pure proxy stay in place. Thus, if we use the pure proxy, with an increasing number of proxies we will always have to sign twice (not necessarily true in multi-signature accounts). While if we are not using the pure proxy, the more the proxies the more signatures we need to detach them from the old stash and attach them to the new stash (see Figure below). Multi-signature Account Management ‚Äã Pure proxies are useful to efficiently manage multi-signature (multisigs) accounts. In fact, multisigs are deterministic, which means that once a multisig is created the signatories cannot be changed. If one of the signatories wants to leave the multisig, a new multisig must be created. This is inconvenient, especially at corporate-level management where the chance of replacing someone within a multisig can be high. Pure proxies allow keeping the same multisig when the signatories change. Scenario One: One Anonymous Proxy within a Multisig ‚Äã Walk-through tutorial video You can see this video tutorial that goes through this scenario. The tutorial requires some familiarity with the Extrinsic Tab of the Polkadot-JS UI. It is possible to put a pure proxy within a multisig, and then transactions will be signed by the any proxy on behalf of the pure proxy (proxied account). Let's take for example the diagram below. Alice, Bob and Anon are part of the multisig ABC, a multisig account with threshold 2. P-C is a pure proxy spawned by Charlie, who now acts as any proxy and thus signs anything on behalf of P-C. The pure proxy cannot sign directly because it does not have a private key. So, for example, to send funds from the multisig to Dan, Charly needs to submit a proxy.proxy extrinsic to P-C, which in turn will submit a multisig.asMulti extrinsic to ABC containing the call data for the balances.transferKeepAlive extrinsic about the transfer of some funds from ABC to Dan. Alice can then approve the transfer by submitting a multisig.asMulti extrinsic also containing the call data for the balances.transferKeepAlive extrinsic about the transfer of some funds from ABC to Dan. If Charly wants to leave the multisig, a new any proxy can be added to P-C and Charly can be removed (by himself or by the new any proxy). Note that the multisig also contains Bob that in this specific example does not do anything. Proxy calls To use a pure proxy within a multisig you need to use the Extrinsic Tab and generate a proxy.proxy extrinsic. If you try to sign a multisig transaction using the pure proxy you will be prompted with a warning. Remember, you cannot sign something directly if you do not have a private key. Scenario Two: Multisig made of Anonymous Proxies ‚Äã Walk-through Tutorial Video You can see this video tutorial that goes through this scenario. The tutorial requires some familiarity with the Extrinsic Tab of the Polkadot-JS UI. The diagram below shows a multisig that is made only with pure proxies (P-A, P-B and P-C). In this situation Alice, Bob or Charly can leave the multisig at any time without the requirement of creating a new multisig. If for example, Bob leaves the multisig the procedure will require somebody else to be added as any proxy to P-B, and then Bob can remove himself (or the new any proxy can remove Bob). In the diagram above, Alice submits the proxy.proxy extrinsic to P-A, which in turn submits the multisig.asMulti extrinsic containing the balances.transferKeepAlive extrinsic about the transfer of some tokens from ABC to Dan. Then, Charly does the same to confirm the transaction. Note that Charly will need to pay for some weight, for the computation that is necessary to execute the transaction. Edit this page Last updated on Jul 14, 2023 by Filippo Previous Multi-Signature Accounts Next Transactions Why use a Proxy? Creating Proxy Proxy Types Any Proxy Non-transfer Proxy Governance Proxy Staking Proxy Identity Judgement Proxy Cancel Proxy Auction Proxy Nomination Pools Proxy Removing Proxy How to view your Proxy Proxy Deposits Time-delayed Proxy Proxy calls Nested Proxy Calls Anonymous Proxy (Pure Proxy) Create and Remove Pure Proxy Use of Pure Proxy Why Pure Proxy? ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Transactions General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Extrinsics Balances Transfers Transaction Fees Tokens and Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Transactions Transactions Everything about transactions between Polkadot accounts. üìÑÔ∏è Extrinsics Types of Transactions on Polkadot. üìÑÔ∏è Balances Transfers Perform Balance Transfers between Accounts. üìÑÔ∏è Transaction Fees How Transaction Fees are Calculated and Handled. Previous Proxy Accounts Next Extrinsics ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Extrinsics (a.k.a. Types of Transactions) General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Extrinsics Balances Transfers Transaction Fees Tokens and Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Transactions Extrinsics On this page Extrinsics (a.k.a. Types of Transactions) Pallets and Extrinsics ‚Äã Polkadot is built using Substrate , a modular framework to efficiently build blockchains. Substrate's FRAME development environment provides modules called pallets and support libraries that you can use, modify, and extend to build the runtime logic to suit the needs of your blockchain. You can explore Substrate's FRAME pallets on this dedicated page . Within each functional pallet on the blockchain, one can call its functions and execute them successfully, provided they have the permission to do so. Because these calls originate outside of the blockchain runtime, such transactions are referred to as extrinsics . Extrinsics normally contain a signature, some data to describe if the extrinsic has passed some validity checks and a reference to the pallet and call that it is intended for. For example, the Staking pallet contains all functions related to staking. A nominator can bond funds and nominate validators by issuing the respective extrinsics. Some extrinsics might also trigger an event on the chain such as a reward payout to the nominators. Types of Extrinsics ‚Äã Now that we introduced the term extrinsic , let us dive deeper and understand what extrinsics really are. Extrinsics can be one of 3 distinct types: Signed transactions: these must contain the signature of the account sending the inbound request to the runtime. With signed transactions, the account used to submit the request typically pays the transaction fee and must sign it using the account's private key. Unsigned transactions: these don't carry any information about who submitted the transaction, since the format of this type of transaction doesn't require a signature. You can define what conditions must be met for such a transaction to be valid. Inherents: are a special type of unsigned transaction made by block authors which carry information required to build a block such as timestamps, storage proofs and uncle blocks. Here are some key differences between the different types of extrinsics: Contrary to signed transactions, unsigned transaction types require implementing custom validation logic which can consume more resources for checking validity compared to signed transactions. Unsigned transactions have no economic deterrent to prevent spam or replay attacks, so custom logic must account for protecting the network from these types of transactions being misused. Inherents exist to address the need of adding some data to a block, whereas signed or unsigned transactions exist to potentially change the state of the blockchain. Metadata Updates ‚Äã Always check for Metadata Updates Before signing extrinsics with the Polkadot-JS Browser Extension and Parity Signer, always check for metadata updates. This video tutorial will explain how to do it. Parity Signer vs Browser Extension ‚Äã Parity Signer updates the full metadata through the QR fountain while the extension updates the metadata index (the metadata is not loaded into it). As a consequence the process of updating metadata is different in this two cases (you will notice that on the Signer app the update takes longer for example). Having outdated metadata on the Signer app will prevent you from signing, while on the extension you will be able to click the sign button but the extrinsic will likely fail (similarly of having an outdated Ledger app). In general, failing to update metadata will most likely result in you not being able to sign extrinsics. Verifying Extrinsics ‚Äã Walk-through Video Tutorial Visit the dedicated support page and see this video tutorial tutorial to learn about how to verify extrinsics before signing them. The video will also mention potential attacks that can happen to you while signing for transactions. danger Do not sign a transaction if you can't verify what you are signing or you suspect you might be signing a different extrinsic than the one intended. Verifying the extrinsic you are signing can take some more time before signing for a transaction but it allows you to add an extra security step. There are a multitude of possible attacks that will prevent you to send funds to the desired destination account (see below). How do Attacks look like ‚Äã In general, an attacker would make you think you are signing an extrinsic A when in reality you are signing an extrinsic B. An Attack might come from: Clipboard Memory ‚Äã info For a more detailed read about clipboard memory attacks see this article . This is a common attack. The clipboard memory is that memory on you computer dedicated to copy-paste operations. There is malicious software that can be remotely installed on your computer and that can detect when a cryptocurrency address is copied. For example, you want to send funds to Address A (belonging to you) but after copying address A a malicious software swaps that address with Address B (belonging to an attacker). This attack can be prevented by checking the receiver address before signing. Failing to do so could result in loss of the funds. Malicious Website/dApp ‚Äã This is a common attack that can happen if you are interacting with a malicious site (dApp). In this scenario you want to perform Extrinsic A on the website, but the dApp will send Extrinsic B to the extension for signing. In this case the extension will show Extrinsic B. If you are using a Ledger device you have a second layer of verification, as it will also display Extrinsic B. Malicious Browser Extension ‚Äã This scenario can happen if you have downloaded a malicious extension or a trusted extension, like the Polkadot-JS Browser Extension , from a non-trusted source. In this scenario the extension will display that you will sign for an Extrinsic A but in the background will execute Extrinsic B. If you are using a Ledger device this attack can be detected because you will be able to see Extrinsic B on the screen of your Ledger device. Corrupted metadata ‚Äã This attack is least common and might result in signing a non-intended extrinsic without the possibility of verifying it. Before authorizing the metadata update check who is requesting it. Metadata updates for the Polkadot-JS Browser Extension (or other extensions) might be requested by the Polkadot-JS UI and dApps (for example DeFi apps of parachains). For the extensions, you should trust the app that requests the update. When updating the metadata for Parity Signer you should trust the issuer of the metadata (or generate the QR fountain yourself). Corrupted QR-code (Parity Signer) ‚Äã This is a sub-case of the malicious dApp scenario. If your account is on Parity Signer the extrinsic will be displayed as a QR code, instead of the extension showing its details and you need to verify it on the device. The corrupted QR code will make you sign for an Extrinsic B when you want to sign for Extrinsic A. This will be showed in the Signer app and a careful user will notice it. If the metadata in the Signer is already incorrect (or the Signer is corrupted) there is the risk of signing a non-intended extrinsic without the possibility of verifying it. Defense against Attacks ‚Äã danger If you can't verify the extrinsic or you suspect you are signing something different than what you intended, don't sign it! To avoid being victim of an attack: Use only trusted extensions, sites and software in general. Use cold storage options (Ledger, Signer) and verify on them. Trust what these devices tell you over what is shown in the app or the browser extension. Update Signer metadata only from trusted sources (or do it yourself). Accept metadata updates for the extension only from trusted apps. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Transactions Next Balances Transfers Pallets and Extrinsics Types of Extrinsics Metadata Updates Parity Signer vs Browser Extension Verifying Extrinsics How do Attacks look like Clipboard Memory Malicious Website/dApp Malicious Browser Extension Corrupted metadata Corrupted QR-code (Parity Signer) Defense against Attacks ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Balance Transfers General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Extrinsics Balances Transfers Transaction Fees Tokens and Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Transactions Balances Transfers On this page Balance Transfers Balance transfers are used to send a balance from one account to another account. To start transferring a balance, we will begin by using Polkadot-JS UI . This guide assumes that you've already created an account and have some funds that are ready to be transferred. info We support only the use of the Polkadot-JS UI together with the browser extension , Ledger and Parity Signer for signing transactions. We do not provide support for third party applications. Sending Funds using UI, Extension, Parity Signer & Ledger ‚Äã See the video tutorial below to learn how to send funds using the supported tools. See the Polkadot Support pages for detailed information about transferring funds using the Polkadot-JS UI . Signing Transactions See the Polkadot Support pages for detailed information about signing transactions using: The Polkadot-JS UI The Polkadot-JS browser extension Ledger devices The Parity Signer app . Keep-Alive Checks ‚Äã info See this video tutorial and this support page to learn about keep-alive checks and existential deposit. In Polkadot there are two main ways to transfer funds from one account to another: transfer keep-alive (default option) will not allow you to send an amount that would allow the sending account to be removed due to it going below the existential deposit of . transfer will allow you to send DOT regardless of the consequence. If the balance drops below the existential deposit your account will be reaped. It may be that you do not want to keep the account alive (for example, because you are moving all of your funds to a different address). To switch the keep-alive check off visit this support article . info Attempting to send less than the existential deposit to an account with 0 DOT will always fail, no matter if the keep-alive check is on or not. For instance, attempting to transfer 0.1 DOT to an account you just generated (and thus has no balance) will fail, since 0.1 DOT is less than the existential deposit of and the account cannot be initialized with such a low balance. note Even if the transfer fails due to a keep-alive check, the transaction fee will be deducted from the sending account if you attempt to transfer. Vested Transfers ‚Äã DOT may have a lock placed on them to account for vesting funds. Like other types of locks, these funds cannot be transferred but can be used in other parts of the protocol such as voting in governance or being staked as a validator or nominator. Vesting funds are on a release schedule and unlock a constant number of tokens at each block ( linear vesting ) or can unlock the full amount after a specific block number ( cliff vesting ). Although the tokens are released in this manner, it does not get reflected on-chain automatically since locks are lazy and require an extrinsic to update. There are two ways that vesting schedules can be created. One way is through an extrinsic type available in the Vesting pallet, vested_transfer . The vested transfer function allows anyone to create a vesting schedule with a transfer of funds, as long as the account for which the vesting schedule will be created does not already have one and the transfer moves at least MinVestedTransfer funds, which is specified as a chain constant. A second way is as part of the genesis configuration of the chain. In the case of Polkadot, the chain specification genesis script reads the state of the Claims contract that exists on the Ethereum blockchain and creates vesting schedules in genesis for all the allocations registered as being vested. Vesting schedules have three parameters: locked , the amount of tokens to be transferred in Planck units ) per block , the number of tokens that are released per block starting block , the block number after which the vesting schedule starts The configuration of these three fields dictates the amount of funds that are originally locked, the slope of the unlock line and the block number for when the unlocking begins. info You can watch this video tutorial to understand how to do vested transfers, including linear and cliff vesting. Note the tutorial uses the Westend Testnet, but the same applies to both Polkadot and Kusama. Lazy Vesting ‚Äã Like simple payouts , vesting is lazy , which means that someone must explicitly call an extrinsic to update the lock that is placed on an account. The vest extrinsic will update the lock that is placed on the caller. The vest_other will update the lock that is placed on another "target" account's funds. These extrinsics are exposed from the Vesting pallet. If you are using the Polkadot-JS UI, when there are DOT available to vest for an account, then you will have the ability to unlock DOT which has already vested from the Accounts page. Batch Transfers ‚Äã Batch transfers are balances transfers to multiple accounts executed by one account. In order to construct a batch transfer you need to: Create a utility.batch(calls) extrinsic using the utility pallet , and Within the batch call you can create multiple balances.transferKeepAlive extrinsics using the balances pallet . You can specify as many receivers as you desire. info You can watch this video tutorial to learn how to do batch transfers. Note the tutorial uses the Westend Testnet, but the same applies to both Polkadot and Kusama. Existing Reference Error ‚Äã If you are trying to reap an account and you receive an error similar to "There is an existing reference count on the sender account. As such the account cannot be reaped from the state" , then you have existing references to this account that must be first removed before it can be reaped. References may still exist from: Bonded tokens (most likely) Unpurged session keys (if you were previously a validator) Token locks Existing recovery info Existing assets Bonded Tokens ‚Äã If you have tokens that are bonded, you will need to unbond them before you can reap your account. Follow the instructions at Unbonding and Rebonding to check if you have bonded tokens, stop nominating (if necessary) and unbond your tokens. Checking for Locks ‚Äã info See this video tutorial and this support page to learn how to check for locks and remove them. You can also check for locks by querying system.account(AccountId) in Chain state tab under the Developer drop-down menu in the Polkadot-JS UI . Select your account, then click the "+" button next to the dropdowns, and check the relative data JSON object. If you see a non-zero value for anything other than free , you have locks on your account that need to get resolved. Purging Session Keys ‚Äã If you used this account to set up a validator and you did not purge your keys before unbonding your tokens, you need to purge your keys. You can do this by seeing the How to Stop Validating page. This can also be checked by checking session.nextKeys in the chain state for an existing key. Existing Recovery Info ‚Äã Currently, Polkadot does not use the Recovery Pallet , so this is probably not the reason for your tokens having existing references. Existing Non-Native Assets ‚Äã Currently, Polkadot does not use the Assets Pallet , so this is probably not the reason for your tokens having existing references. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Extrinsics Next Transaction Fees Sending Funds using UI, Extension, Parity Signer & Ledger Keep-Alive Checks Vested Transfers Lazy Vesting Batch Transfers Existing Reference Error Bonded Tokens Checking for Locks Purging Session Keys Existing Recovery Info Existing Non-Native Assets ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Transaction Fees General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Extrinsics Balances Transfers Transaction Fees Tokens and Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Transactions Transaction Fees On this page Transaction Fees Several resources in a blockchain network are limited, for example, storage and computation. Transaction fees prevent individual users from consuming too many resources. Polkadot uses a weight-based fee model as opposed to a gas-metering model. As such, fees are charged prior to transaction execution; once the fee is paid, nodes will execute the transaction. Web3 Foundation Research designed the Polkadot fee system with the following objectives: Each Relay Chain block should be processed efficiently to avoid delays in block production. The growth rate of the Relay Chain should be bounded. Each block should have space for special, high-priority transactions like misconduct reports. The system should be able to handle spikes in demand. Fees should change slowly so that senders can accurately predict the fee for a given transaction. Fee Calculation ‚Äã Fees on the Polkadot Relay Chain are calculated based on three parameters: A Weight fee Base weight Call(s) weight A Length fee A Tip (optional). As a permissionless system, the Polkadot network needs to implement a mechanism to measure and to limit the usage in order to establish an economic incentive structure, to prevent the network overload, and to mitigate DoS vulnerabilities. Polkadot enforces a limited time-window for block producers to create a block, including limitations on block size, which can make the selection and execution of certain extrinsics too expensive and decelerate the network. Extrinsics which require too many resources are discarded by the network. Polkadot defines a specified block ratio ensuring that only a certain portion of the total block size gets used for regular extrinsics. The remaining space is reserved for critical, operational extrinsics required for the functionality by network itself. This is handled by a weight system, where the cost of the transactions (referred to as extrinsics ) are determined before execution. Weights are a fixed set of numbers used in Substrate-based chains to manage the time it takes to validate a block. Each transaction has a base weight that accounts for the overhead of inclusion (e.g. signature verification) and a dispatch weight that accounts for the time to execute the transaction. All weights, even the base weight, are a measure of time to execute on some standard hardware. The runtime converts weight units to balance units as part of the fee calculation. The weight fee is the sum of the base weight and the sum of the total weight consumed by call(s). A transaction can include several calls For instance, a batch can contain bond and nominate , and the weight would be one base weight and then the sum of the weights for bond and nominate . To learn more about the motivation of a weight fee, check out this Substrate doc on weights. The length fee is a per-byte fee multiplier for the size of the transaction in bytes. There is also a targeted fee adjustment that serves as a multiplier which tunes the final fee based on network congestion. This can constitute an adjusted weight fee calculated as the targeted fee adjustment times the weight fee. Together, these fees constitute the inclusion fee. The inclusion fee is the base fee plus the length fee plus the adjusted weight fee. The inclusion fee is deducted from the sender's account before transaction execution. A portion of the fee will go to the block author, and the remainder will go to the Treasury . This is 20% and 80%, respectively. Tips are an optional transaction fee that users can add. Tips are not part of the inclusion fee and are an incentive to block authors for prioritizing a transaction, and the entire tip goes directly to the block author. Final weights are assigned based on the worst case scenario of each runtime function. The runtime has the ability to "refund" the amount of weight which was overestimated once the runtime function is actually executed. The runtime only returns weights if the difference between the assigned weight and the actual weight calculated during execution is greater than 20%. Checkout some examples of how various weights are gauged in the Polkadot runtime for several different types of operations: request_judgement - from the identity pallet, allows users to request judgement from a specific registrar payout_stakers - from the staking Pallet, is invoked by a single account in order to payout the reward for all nominators who back a particular validator transfer - from the balances module, is designed to move the specified balance by the sender to the receiver withdraw_unbounded - from the staking module, is designed to move any unlocked funds from the staking management system to be ready for transfer Block Limits and Transaction Priority ‚Äã Blocks in Polkadot have both a maximum length (in bytes) and a maximum weight. Block producers will fill blocks with transactions up to these limits. A portion of each block - currently 25% - is reserved for critical transactions that are related to the chain's operation. Block producers will only fill up to 75% of a block with normal transactions. Some examples of operational transactions: Misbehavior reports Council operations Member operations in an election (e.g. renouncing candidacy) Block producers prioritize transactions based on each transaction's total fee. Since a portion of the fee will go to the block producer, producers will include the transactions with the highest fees to maximize their reward. Fees ‚Äã Block producers charge a fee in order to be economically sustainable. That fee must always be covered by the sender of the transaction. Polkadot has a flexible mechanism to determine the minimum cost to include transactions in a block. Transaction volume on blockchains is highly irregular, and therefore transaction fees need a mechanism to adjust. However, users should be able to predict transaction fees. Polkadot uses a slow-adjusting fee mechanism with tips to balance these two considerations. In addition to block limits , Polkadot also has a block fullness target. Fees increase or decrease for the next block based on the fullness of the current block relative to the target. The per-weight fee can change up to 30% in a 24 hour period. This rate captures long-term trends in demand, but not short-term spikes. To consider short-term spikes, Polkadot uses tips on top of the length and weight fees. Users can optionally add a tip to the fee to give the transaction a higher priority. Polkadot fees consists of three parts: Base fee : a fixed fee that is applied to every transaction and set by the runtime. Length fee : a fee that gets multiplied by the length of the transaction, in bytes. Weight fee : a fee for each, varying runtime function. Runtime implementers need to implement a conversion mechanism which determines the corresponding currency amount for the calculated weight. The final fee can be summarized as: fee = base_fee + length_of_transaction_in_bytes * length_fee + weight_fee For example, the Polkadot Runtime defines the following values: Base fee: 100 uDOTs Length fee: Length fee: 0.1 uDOTs So, the weight to fee conversion is calculated as follows: weight_fee = weight * (100 uDots / (10 * 10‚Äô000)) A weight of 10‚Äô000 (the smallest non-zero weight) is mapped to 1/10 of 100 uDOT. This fee will never exceed the max size of an unsigned 128 bit integer. Fee Multiplier ‚Äã Polkadot can add a additional fee to transactions if the network becomes too busy and starts to decelerate the system. This fee can create an incentive to avoid the production of low priority or insignificant transactions. In contrast, those additional fees will decrease if the network calms down and can execute transactions without much difficulties. This additional fee is known as the Fee Multiplier and its value is defined by the Polkadot runtime. The multiplier works by comparing the saturation of blocks; if the previous block is less saturated than the current block (implying an uptrend), the fee is slightly increased. Similarly, if the previous block is more saturated than the current block (implying a downtrend), the fee is slightly decreased. The final fee is calculated as: final_fee = fee * fee_multiplier The Update Multiplier defines how the multiplier can change. Each runtime has the ability to define this behavior accordingly. For example, the Polkadot runtime internally updates the multiplier after each block according to a custom formula defined here . Calcuating Fees with Polkadot-JS ‚Äã One useful utility for estimating transaction fees programmatically is the via the @polkadot/api . Check out the following script that logs some relevant fee information: // Estimate the fees as RuntimeDispatchInfo using the signer const info = await api . tx . balances . transfer ( recipient , 123 ) . paymentInfo ( sender ) ; // Log relevant info, partialFee is Balance, estimated for current console . log ( ` class= ${ info . class . toString ( ) } , weight= ${ info . weight . toString ( ) } , partialFee= ${ info . partialFee . toHuman ( ) } ` ) ; For additional information on interacting with the API, checkout Polkadot-JS . Shard Transactions ‚Äã The transactions that take place within Polkadot's shards - parachains and parathreads - do not incur Relay Chain transaction fees. Users of shard applications do not even need to hold DOT tokens, as each shard has its own economic model and may or may not have a token. There are, however, situations where shards themselves make transactions on the Relay Chain. Parachains have a dedicated slot on the Relay Chain for execution, so their collators do not need to own DOT in order to include blocks. The parachain will make some transactions itself, for example, opening or closing an XCM channel, participating in an auction to renew its slot, or upgrading its runtime. Parachains have their own accounts on the Relay Chain and will need to use those funds to issue transactions on the parachain's behalf. Parathreads will also make all the same transactions that a parachain might. In addition, the collators need to participate in an auction every block to progress their chain. The collators will need to have DOT to participate in these auctions. Other Resource Limitation Strategies ‚Äã Transaction weight must be computable prior to execution, and therefore can only represent fixed logic. Some transactions warrant limiting resources with other strategies. For example: Bonds: Some transactions, like voting, may require a bond that will be returned or slashed after an on-chain event. In the voting example, returned at the end of the election or slashed if the voter tried anything malicious. Deposits: Some transactions, like setting an identity or claiming an index, use storage space indefinitely. These require a deposit that will be returned if the user decides to free storage (e.g. clear their IDE). Burns: A transaction may burn funds internally based on its logic. For example, a transaction may burn funds from the sender if it creates new storage entries, thus increasing the state size. Limits: Some limits are part of the protocol. For example, nominators can only nominate 16 validators. This limits the complexity of Phragm√©n . Advanced ‚Äã This page only covered transactions that come from normal users. If you look at blocks in a block explorer, though, you may see some "extrinsics" that look different from these transactions. In Polkadot (and any chain built on Substrate), an extrinsic is a piece of information that comes from outside the chain. Extrinsics fall into three categories: Signed transactions Unsigned transactions Inherents This page only covered signed transactions, which is the way that most users will interact with Polkadot . Signed transactions come from an account that has funds, and therefore Polkadot can charge a transaction fee as a way to prevent spam. Unsigned transactions are for special cases where a user needs to submit an extrinsic from a key pair that does not control funds. For example, when users claim their DOT tokens after genesis, their DOT address doesn't have any funds yet, so that uses an unsigned transaction. Validators also submit unsigned transactions in the form of "heartbeat" messages to indicate that they are online. These heartbeats must be signed by one of the validator's session keys . Session keys never control funds. Unsigned transactions are only used in special cases because, since Polkadot cannot charge a fee for them, each one needs its own, custom validation logic. Finally, inherents are pieces of information that are not signed or included in the transaction queue. As such, only the block author can add inherents to a block. Inherents are assumed to be "true" simply because a sufficiently large number of validators have agreed on them being reasonable. For example, Polkadot blocks include a timestamp inherent. There is no way to prove that a timestamp is true the way one proves the desire to send funds with a signature. Rather, validators accept or reject the block based on how reasonable they find the timestamp. In Polkadot, it must be within some acceptable range of their own system clocks. Learn More ‚Äã Web3 Foundation Research Substrate Extrinsics, Weights & Fees Edit this page Last updated on Jun 30, 2023 by Filippo Previous Balances Transfers Next Tokens and Assets Fee Calculation Block Limits and Transaction Priority Fees Fee Multiplier Calcuating Fees with Polkadot-JS Shard Transactions Other Resource Limitation Strategies Advanced Learn More ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Tokens and Assets General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets Asset Hub DOT Teleporting Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Tokens and Assets Tokens and Assets Everything about tokens and assets on Polkadot. üìÑÔ∏è Asset Hub Fungible Tokens and NFTs on Polkadot. üìÑÔ∏è DOT Tokenomics of Polkadot's Native Token DOT. üìÑÔ∏è Teleporting Assets Teleport Assets between Parachains and Relay Chain. Previous Transaction Fees Next Asset Hub ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Asset Hub General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets Asset Hub DOT Teleporting Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Tokens and Assets Asset Hub On this page Asset Hub Assets in the Polkadot network can be represented on several chains. They can take many forms, from a parachain's native token to on-chain representations of off-chain reserves. This page focuses on the latter, namely assets issued by a creator (e.g. rights to audited, off-chain reserves held by the creator, or art issued as an NFT). The Asset Hub system parachain hosts data structures and logic that specialize in the creation, management, and use of assets in the Polkadot network. Although other parachains can host applications dealing with assets on the Asset Hub, the hub can be thought of as the "home base" of assets in the network. The Asset Hub uses DOT as its native token. The chain yields its governance to its parent Relay Chain and has no inflation or era-based rewards for collators (although collators receive a portion of transaction fees). As a system parachain , the Asset Hub has a trusted relationship with the Relay Chain, and as such, can teleport DOT between itself and the Relay Chain. That is, DOT on the Asset Hub is just as good as DOT on the Relay Chain. The Asset Hub does not support smart contracts. See the Advanced section at the bottom for a discussion on using proxy and multisig accounts to replicate oft-used contract logic. Creation and Management ‚Äã Walk-through video tutorial about creating assets See this technical explainer video to learn how to create fungible assets on the Asset Hub. Anyone on the network can create assets on the Asset Hub as long as they can reserve the required deposit of and around for the metadata. The network reserves the deposit on creation. The creator also must specify a unique AssetId , an integer of type u32 , to identify the asset. The AssetId should be the canonical identifier for an asset, as the chain does not enforce the uniqueness of metadata like "name" and "symbol". The creator must also specify a minimum balance, preventing accounts from having dust balances. Advanced How-to Guides See this page to learn more about creating assets using the Asset Hub. Asset classes and instances can have associated metadata. The metadata is an array of data that the class owner can add on-chain, for example, a link to an IPFS hash or other off-chain hosting service. The Uniques pallet also supports setting key/value pairs as attributes to a class or instance. An asset class has several privileged roles. The asset creator automatically takes on all privileged roles but can reassign them after creation. These roles are: The owner can set the accounts responsible for the other three roles and set asset metadata (e.g. name, symbol, decimals). The issuer can mint and burn tokens to/from their chosen addresses. The admin can make force transfers as well as unfreeze accounts of the asset class. The freezer can freeze assets on target addresses or the entire asset class. Always refer to the reference documentation for certainty on privileged roles. An asset's details contain one field not accessible to its owner or admin team, asset sufficiency . Only the network's governance mechanism can deem an asset as sufficient . A balance of a non-sufficient asset can only exist on accounts that are on-chain (i.e. accounts having the existential deposit of a sufficient asset). That is, a user could not keep an account on-chain by transferring an insufficient asset to it; the account must already be on-chain by having more than the existential deposit in DOT (or a sufficient asset). However, assets deemed sufficient can instantiate accounts and pay for transaction fees, such that users can transact on the Asset Hub without the need for DOT. Transaction Fees on Polkadot-JS UI Polkadot-JS UI doesn't support the functionality to pay with a sufficient asset yet . When using Polkadot-JS UI, transaction fee needs to be paid in DOT. Fungible Assets ‚Äã Fungible assets are interchangeable, i.e. one unit is equivalent to any other unit to claim the underlying item. The Asset Hub represents fungible assets in the Assets pallet. This pallet presents a similar interface for those familiar with the ERC20 standard. However, the logic is encoded directly in the chain's runtime. As such, operations are not gas-metered but benchmarked upon every release, leading to efficient execution and stable transaction fees. Transferring Asset Balances ‚Äã Walk-through video tutorial about transferring assets See this technical explainer video to learn how to transfer assets on the Asset Hub. For Ledger users see this video tutorial to learn how to use the Statemine Ledger app and what its current limitations are. Users have a simple interface, namely the ability to transfer asset balances to other accounts on-chain. As mentioned before, if the asset is not sufficient , then the destination account must already exist for the transfer to succeed. The chain also contains a transfer_keep_alive function, similar to that of the Balances pallet, that will fail if execution kills the sending account. The Asset Hub also sweeps dust balances into transfers. For example, if an asset has a minimum balance of 10 and an account has a balance of 25, then an attempt to transfer 20 units would transfer all 25. Non-sufficient assets Before transferring a non-sufficient asset, ensure the receiver account has enough funds to cover the existential deposit and transaction fees for future transfers. Failing to do so will cause the asset transfer to fail. The transfer will be successful for sufficient assets, but without DOT tokens, you will not be able to transfer those assets from the receiver account through Polkadot-JS UI. The feature request to enable sufficient assets for transaction fee payment on Polkadot-JS UI is yet to be implemented. info See this support article to learn more about transferring assets using the Asset Hub. Destroying an Asset ‚Äã Walk-through video tutorial about destroying assets See this technical explainer video to learn how to destroy assets on the Asset Hub. To destroy an asset, go to the Polkadot-JS UI on the Asset Hub > Developer > Extrinsics. If you created an asset without minting any unit, you could call assets.startDestroy and then the assets.finishDestroy extrinsics specifying the asset id you want to destroy. If you created an asset and minted some units, follow the steps below: assets.freezeAsset will freeze all assets on all accounts holding that asset id. Those accounts will no longer be able to transfer that asset. assets.startDestroy will start the destroying process. assets.destroyApprovals will destroy all approvals related to that asset id (if there are any approvals). assets.destroyAccounts will destroy all accounts related to that asset id. All asset units will be removed from those accounts. assets.finishDestroy will finish the destroying process. The asset id will be removed and available for another fungible token. Application Development ‚Äã The Asset Hub provides an approve_transfer , transfer_approved , and cancel_approval interface. Application developers can use this interface so that users can authorize the application to effectuate transfers up to a given amount on behalf of an account. Cross-Chain Accounting ‚Äã The Asset Hub uses a reserve-backed system to manage asset transfers to other parachains. It tracks how much of each asset has gone to each parachain and will not accept more from a particular parachain. As a result of this, asset owners can use the Asset Hub to track information like the total issuance of their asset in the entire network, as parachain balances would be included in the reserve-backed table. Likewise, for the minting and burning of tokens, an asset's team can perform all operations on the Asset Hub and propagate any minted tokens to other parachains in the network. Parachains that want to send assets to other parachains should do so via instructions to the Asset Hub so that the reserve-backed table stays up to date. For more info, see the "Moving Assets between Chains in XCM" section of the article on the XCM format . Non-Fungible Assets ‚Äã Unlike fungible assets, the particular instance of a non-fungible asset (NFT) has a separate meaning from another instance of the same class. The Asset Hub represents NFTs in the Uniques and NFTs pallets . Similar to the Assets pallet, this functionality is encoded into the chain. Operations are benchmarked before each release instead of any runtime metering, ensuring efficient execution and stable transaction fees. Transferring NFTs ‚Äã Users can transfer their NFTs to other accounts. The chain also provides an approve_transfer , transfer_approved and cancel_approval interfaces that application developers can use to allow users to authorize an application to transfer an instance on their behalf. Advanced Techniques ‚Äã Many asset creators on other networks use smart contracts to control privileged functions like minting and burning. Although the Asset Hub does not have a smart contract interface, it contains the Multisig , Proxy , and Utility pallets, which will meet most account management needs. For example, if a team wants sign-off from two groups to perform a privileged operation, it could create a 2-of-2 multisig from two pure proxies , and then set members from each group as proxies to those two accounts. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Tokens and Assets Next DOT Creation and Management Fungible Assets Transferring Asset Balances Destroying an Asset Application Development Cross-Chain Accounting Non-Fungible Assets Transferring NFTs Advanced Techniques ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
DOT General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets Asset Hub DOT Teleporting Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Tokens and Assets DOT On this page DOT What is DOT? ‚Äã DOT is the native token of the Polkadot network in a similar way that BTC is the native token of Bitcoin or Ether is the native token of the Ethereum blockchain. The Planck Unit ‚Äã The smallest unit for the account balance on Substrate based blockchains (Polkadot, Kusama, etc.) is Planck (a reference to Planck Length , the smallest possible distance in the physical Universe). You can compare DOT's Planck to BTC's Satoshi or ETH's Wei. Polkadot's native token DOT equals to 10 10 Planck and Kusama's native token KSM equals to 10 12 Planck. Polkadot ‚Äã Unit Decimal Places Conversion to Planck Conversion to DOT Planck 0 1 Planck 0.0000000001 DOT Microdot (uDOT) 4 10 4 Planck 0.0000010000 DOT Millidot (mDOT) 7 10 7 Planck 0.0010000000 DOT Dot (DOT) 10 10 10 Planck 1.0000000000 DOT Million (MDOT) 16 10 16 Planck 1,000,000.00 DOT DOT was redenominated at block #1_248_328 DOT was originally equal to 10 12 Planck just like Kusama (which is referred to as "DOT (old)"), but went through a process of redenomination which increased DOT's supply by 100x. As a consequence, 1 DOT now equals to 10 10 Planck. Kusama ‚Äã Unit Decimal Places Conversion to Planck Conversion to KSM Planck 0 1 Planck 0.000000000001 KSM Point 3 10 3 Planck 0.000000001000 KSM MicroKSM (uKSM) 6 10 6 Planck 0.000001000000 KSM MilliKSM (mKSM) 9 10 9 Planck 0.001000000000 KSM KSM 12 10 12 Planck 1.000000000000 KSM What are the uses of DOT? ‚Äã DOT serves three key functions in Polkadot: to be used for governance of the network, to be staked for the operation of the network, to be bonded to connect a chain to Polkadot as a parachain. DOT can also serve ancillary functions by being a transferrable token. For example, DOT stored in the Treasury can be sent to teams working on relevant projects for the Polkadot network. Explainer video on token utility These concepts have been further explained in the video Usage of DOT and KSM on Polkadot and Kusama . DOT for Governance ‚Äã The first function of DOT is to entitle holders to control the governance of the platform. Some functions that are included under the governance mechanism include determining the fees of the network, the addition or removal of parachains, and exceptional events such as upgrades and fixes to the Polkadot platform. Polkadot will enable any holder of DOT to participate in governance. For details on how holders can participate in governance, as well as their rights and responsibilities, see the governance page . DOT for Consensus ‚Äã DOT will be used to facilitate the consensus mechanism that underpins Polkadot. For the platform to function and allow for valid transactions to be carried out across parachains, Polkadot will rely on holders of DOT to play active roles. Participants will put their DOT at risk (via staking) to perform these functions. The staking of DOT acts as a disincentive for malicious participants who will be punished by the network by getting their DOT slashed. The DOT required to participate in the network will vary depending on the activity that is being performed, the duration the DOT will be staked for, and the total number of DOT staked. For more information about staking on Polkadot visit the dedicated staking page . DOT for Parachain Slot Acquisition ‚Äã DOT will have the ability to be locked for a duration in order to secure a parachain slot in the network. The DOT will be reserved during the slot lease and will be released back to the account that reserved them after the duration of the lease has elapsed and the parachain is removed. You can learn more about this aspect by reading about the auctions that govern parachain slots. Calculating When Vesting DOT Will Be Available ‚Äã Generally, you should be able to see from the Accounts by looking at your accounts and seeing when the vesting will finish. However, some DOT vest with "cliffs" - a single block where all the DOT are released, instead of vesting over time. In this case, you will have to query the chain state directly to see when they will be available (since technically, the vesting has not yet started - all of the vesting will occur in a single block in the future). Navigate to the Chain State page on Polkadot-JS. Query chain state for vesting.vesting(ACCOUNT_ID) Note the startingBlock where the unlock starts, and how much DOT is unlocked per block ( perBlock ). You will have to calculate the result into ‚Äúhuman time". To do this, remember that there are approximately 14‚Äô400 blocks per day, and you can see what the latest block is shown on the Explorer page. Obtaining Testnet Tokens ‚Äã DOT are required to make transactions on the Polkadot network. Tokens on Polkadot's Testnets like Westend and Rococo do not have any value besides allowing you to experiment with the features on the network. Getting Tokens on the Westend Testnet ‚Äã Polkadot's testnet is called Westend and you can obtain its native tokens (called Westies!) by posting !drip <WESTEND_ADDRESS> in the Matrix chatroom #westend_faucet:matrix.org . Your account will be credited with 1 WND by default. You can also specify to get more tokens by !drip <WESTEND_ADDRESS> X , where X is the number of tokens. Another way is to use our web-based Westend faucet . You can also earn WNDs as rewards by becoming a validator on Westend network. Watch the video below on how to get started on Westend. Unit Decimal Places Conversion to Planck Conversion to WND Planck 0 1 Planck 0.000000000001 WND Point 3 10 3 Planck 0.000000001000 WND MicroWND (uWND) 6 10 6 Planck 0.000001000000 WND MilliWND (mWND) 9 10 9 Planck 0.001000000000 WND WND 12 10 12 Planck 1.000000000000 WND Getting Tokens on the Rococo Testnet ‚Äã Rococo is a parachain testnet. Tokens are given directly to teams working on parachains or exploring the cross consensus message-passing aspects of this testnet. General users can obtain ROC by posting !drip <ROCOCO_ADDRESS> in the Matrix chatroom #rococo-faucet:matrix.org or through the web-based Rococo faucet . Learn more about Rococo on its dedicated wiki section . Getting Tokens on the Wococo Testnet ‚Äã Wococo is a bridge testnet. General users can obtain WOOK by posting !drip <WOCOCO_ADDRESS> in the Matrix chatroom #wococo-faucet:matrix.org . Faucets support ‚Äã If you require help with using faucets, or wish to report an issue, there is a support chat #faucets-support:matrix.org , or you can create an issue directly in the faucets repo Kusama Tokens ‚Äã Unlike testnet DOT, Kusama tokens are not freely given away. Kusama tokens are available via the claims process (if you had DOT at the time of Kusama genesis) or through the Treasury . Alternatively, they can be obtained on the open market. Polkadot Mainnet DOT ‚Äã Polkadot Mainnet DOT are not freely given away. If you purchased DOT in the original 2017 offering, you may claim them via the Polkadot claims process . Alternatively, they are available on the open market. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Asset Hub Next Teleporting Assets What is DOT? The Planck Unit Polkadot Kusama What are the uses of DOT? DOT for Governance DOT for Consensus DOT for Parachain Slot Acquisition Obtaining Testnet Tokens Getting Tokens on the Westend Testnet Getting Tokens on the Rococo Testnet Getting Tokens on the Wococo Testnet Faucets support Kusama Tokens Polkadot Mainnet DOT ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Teleporting Assets General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets Asset Hub DOT Teleporting Assets NFTs Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Tokens and Assets Teleporting Assets On this page Teleporting Assets One of the main properties that Polkadot and Kusama bring to the ecosystems is decentralized blockchain interoperability. This interoperability allows for asset teleportation: the process of moving assets, such as coins, tokens, or NFTs, between chains (parachains) to use them as you would any other asset native to that chain. Interoperability is possible through XCM and SPREE modules , which together ensure that assets are not lost or duplicated across multiple chain. Walk-through video tutorial about teleporting assets See this technical explainer video to learn how to teleport assets from Kusama to the Asset Hub. The same procedure applies to teleporting between Polkadot and the Polkadot Asset Hub, or between any other parachain. How Teleports work ‚Äã As you can see from the diagram above, there are only 2 actors within this model: the source and the destination. The way in which we transfer assets between the source and the destination are briefly summarized in the numbered labels on the diagram, and are explained in more detail below: Initiate Teleport ‚Äã The source gathers the assets to be teleported from the sending account and takes them out from the circulating supply, taking note of the total amount of assets that was taken out. Receive Teleported Assets ‚Äã The source then creates an XCM instruction called ReceiveTeleportedAssets containing as parameters a) the receiving account and b) the amount of assets taken out from circulation. It then sends this instruction over to the destination, where it gets processed and new assets are put back into the circulating supply. Deposit Asset ‚Äã The destination deposits the assets to the receiving account. The actions of taking out from the circulating supply and putting back into the circulating supply show the great flexibility that an XCM executor has in regulating the flow of an asset without changing its circulating supply. Assets are transferred to an inaccessible account in order to take them out from circulation. Likewise, for putting assets back into circulation, assets are released from a pre-filled and inaccessible treasury, or perform a mint of the assets. This process requires mutual trust between the source and destination. The destination must trust the source of having appropriately removed the sent assets from the circulating supply, and the source must trust the destination of having put the received assets back into circulation. The result of an asset teleportation should result in the same circulating supply of the asset, and failing to uphold this condition will result in a change in the asset's total issuance (in the case of fungible tokens) or a complete loss/duplication of an NFT. Teleporting Tokens using the Polkadot-JS UI ‚Äã Video tutorial on Teleporting Additional support article Troubleshooting ‚Äã If you do not see "Accounts > Teleport" in [Polkadot-JS UI] , the source chain that you have selected does not support teleportation yet. Edit this page Last updated on Jun 30, 2023 by Filippo Previous DOT Next NFTs How Teleports work Initiate Teleport Receive Teleported Assets Deposit Asset Teleporting Tokens using the Polkadot-JS UI Troubleshooting ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
NFTs General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Introduction to NFTs NFT Projects Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics NFTs NFTs Everything about NFTs on Polkadot. üìÑÔ∏è Introduction to NFTs The NFT Landscape of the Polkadot Ecosystem. üìÑÔ∏è NFT Projects NFT Projects in the Polkadot Ecosystem. Previous Teleporting Assets Next Introduction to NFTs ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
NFTs General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Introduction to NFTs NFT Projects Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics NFTs Introduction to NFTs On this page NFTs This page is a high-level overview of NFTs in the blockchain space and the various approaches to NFTs within the Polkadot network. Fungibility ‚Äã NFT stands for non-fungible token . Fungibility means interchangeability inside of a group. In theory, a $20 bill is always worth $20 in a store and identical in value to any other $20 bill. It is not, however, fungible with a $1 or $100 dollar bill (outside its group). A Pokemon‚Ñ¢ trading card of a Charizard is non-fungible with a card of Squirtle, whereas editions of Charizard are fungible with each other. Fungibility is a spectrum - what is fungible to some might not be fungible to others. In reality, Pokemon‚Ñ¢ cards, the canonical example of non-fungible assets are more fungible than US dollar bills, each of which has a unique serial number that may be important to a government agency. The cards have no serial numbers [1] . Additionally, a digital item like a "purple magic sword" in a game may be fungible with another visually identical sword if all the player cares about is the looks of their character. But if the other sword has a different function, and that function influences the outcome of an adventure the player is about to embark on, then visually identical swords are absolutely non-fungible. Bearing that in mind, the simplest explanation of NFTs is that NFTs are rows of arbitrary, project-specific, and non-interchangeable data that can be cryptographically proven to "belong" to someone . This data can be anything - concert tickets, attendance badges, simple words, avatars, plots of land in a metaverse, audio clips, house deeds, mortgages, and more. NFT Standards ‚Äã A general-purpose blockchain is not built to natively understand the concept of NFTs. It is only natively aware and optimized for its own native tokens, but implementations built on such a chain are essentially "hacks". For example, Ethereum is a general-purpose blockchain that does not have the concept of "tokens" (fungible or not) built-in. Tokens in Ethereum are essentially spreadsheets of information to be interpreted and read in a certain way by various user interfaces. This way in which they should read them is called a standard . The most widespread fungible token standard you may have heard of is ERC20, while the most widespread NFT standard is ERC721, followed closely by ERC1155. The downside of having to define these standards is that they are always instructions for how to read a spreadsheet pretending to serve information in a certain way, which by definition cannot be optimized. For this reason, even on a good day of extremely low network congestion, interactions with NFTs on any EVM chain will cost a few dollars but were on average around $100 per interaction (transfer, mint, sale) in 2021 on Ethereum. This prevents use cases that go beyond the current craze of digital dust gathering NFTs on Ethereum - profile pictures, generative "look once and then put away" art, ENS addresses, and proof of attendance badges (which have since moved to the xDAI chain to save on gas fees). A typical NFT on Ethereum ‚Äã For the sake of comparison, we can refer to these as NFTs 1.0: static NFTs that are almost exclusively image-based collectibles of varying rarity. NFTs in Polkadot & Kusama ‚Äã This is where Polkadot's technology shines and where NFTs 2.0 come into play. By allowing heterogeneous application-specific shards to exist, builders can natively optimize for complex NFT use cases without tradeoffs that would make interacting with the system prohibitively inefficient and expensive in other environments. info See this page for more information about specific NFT project on Polkadot and Kusama. Bridging ‚Äã Bridging to and from Substrate chains and EVM chains takes much effort but is a highly desired feature in the NFT industry. Merging the collector and customer base has significant implications, so multiple projects focus on making this possible. Apart from RMRK (Substrate-to-Substrate seamless teleportation natively with XCMP ) and Efinity (Paratoken), the following efforts are underway: MyNFT : an EVM to EVM bridging effort. RMRK <-> EVM Simplification bridge: a bridge developed during the RMRK hackathon for porting RMRK NFTs into simplified IOUs on EVM chains References ‚Äã [1] : Investopedia [2] : Unique Network's Chelobrick Edit this page Last updated on Jun 30, 2023 by Filippo Previous NFTs Next NFT Projects Fungibility NFT Standards A typical NFT on Ethereum NFTs in Polkadot & Kusama Bridging References ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
NFT projects on Polkadot and Kusama General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Introduction to NFTs NFT Projects Components Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics NFTs NFT Projects On this page NFT projects on Polkadot and Kusama List of NFT Projects ‚Äã Astar Basilisk Efinity Kodadot Moonbeam RMRK Asset Hub Unique Astar ‚Äã Astar Network and its sister network Shiden Network are the smart contract infrastructure in the Polkadot Ecosystem. Astar Ecosystem ("Astar") supports NFTs developed with EVM smart contracts and WASM smart contracts. Astar has all toolings available that every EVM NFT developer knows. The availability of those toolings makes the onboarding to Astar networks very attractive to any developer looking to explore the Polkadot Ecosystem. Astar has an active community of artists and NFT enthusiasts. Besides supporting all EVM toolings, Astar also bootstrapped the WASM smart contract environment for NFT developers writing smart contracts with ink! based on PSP34 (Polkadot Standards Proposals). The main advantage of having a multi-virtual machine environment for NFT developers is that it will give more possibilities to the builders for the use case they are developing. With the support of WASM smart contracts, developers can develop solutions like RMRK with smart contracts. Basilisk ‚Äã Basilisk is a Kusama parachain that provides liquidity for the ecosystem. It also has a full-featured NFT platform based on the Uniques pallet . One of the key features of Basilisk is that it allows minting NFTs with a royalty fee. This royalty fee is distributed to the original creator of the NFT via the runtime pallet. Additionally Basilisk offers a feature that allows creating a buy order for a specific NFT. These NFTs can be viewed and interacted instantly on KodaDot . Efinity ‚Äã Spearheaded by Enjin , the authors of Ethereum's ERC1155 standard and makers of the Enjin wallet and Unity plugin, which allows easy implementation of NFTs into 3D games, Efinity is an NFT bridging chain coming to Kusama and Polkadot in 2022. They plan to build a paratoken which would be a standard for token migration across different parachains in the Polkadot ecosystem, but also into and out of Ethereum and other EVM systems. KodaDot ‚Äã KodaDot is an open-source NFT marketplace that operates on the Dotsama (Kusama/Polkadot) network, striving to aggregate various NFT standards (Kusama, RMRK, Asset Hub, Basilisk, etc.) in the Dotsama ecosystem, enhancing user experience by abstracting these standards. KodaDot's strength lies in its commitment to open-source collaboration. It has transformed into a collaborative hub where creators, developers, and community members work collectively for decision making, amassing an extensive network of over 90 open-source contributors . This robust collaboration has earned KodaDot the number one rank as a dapp in the Polkadot ecosystem on Github . The Team Behind KodaDot ‚Äã KodaDot began as the first unofficial explorer for RMRKv0.0.1 contributing to RMRK protocol . It later received Kusama Treasury funding , which propelled the team to create the best end-user experience on the Asset hub . In the summer of 2022, KodaDot won the first prize at the Polkadot North American event for implementing MoonBeam and MoonRiver NFT EVM smart contracts and enabling read-only access to existing components for seamless end-user interaction. The team successfully launched with Basilisk NFT Marketplace pallet in Fall 2022 , where an increasing number of artist collections are emerging, providing artists the opportunity to receive offers on unlisted NFTs and earn on-chain royalties. KodaDot's upcoming integrations are based on PSP-34 , leveraging smart contracts written with ink! . Ecosystem Tools by KodaDot ‚Äã KodaDot has enriched the Polkadot ecosystem by offering a comprehensive API interface for builders , based on the SubSquid indexer. This platform also presents searchable items and collections, translating on-chain transactions into deep insights about collection ownership dynamics for end-users. For more info about KodaDot check out link . Moonbeam ‚Äã Moonbeam and its Kusama counterpart Moonriver are full EVM deployments with Ethereum RPC endpoints. This means that the entire toolkit offered to other EVM chains (stacks like Hardhat, Remix, Truffle, Metamask, etc.) are available to Moonriver / Moonbeam users and developers, giving it a noticeable head start in attracting existing userbases. Several dozen high profile teams are launching their products (or re-launching) on Moonriver / Moonbeam, however, it is essential to note that Moonbeam is an EVM chain and will therefore suffer from the same limitations as any other EVM chain in regards to customization and feature-richness of NFTs. A notable advantage, however, is that Moonriver / Moonbeam is still a Substrate chain, meaning integration of custom pallets into the runtime is still possible, making NFT specific optimizations at the chain runtime level a reliable way to keep EVM compatibility of tools while at the same time optimizing storage and interactions for rich NFTs. RMRK ‚Äã RMRK is a set of NFT 2.0 standards developed in three distinct code flavors: "Colored coins" approach, as on Bitcoin, originally developed as a "hack" on the Kusama chain. This is now deprecated, and it is recommended implementers use any of the other options. Solidity contracts, compatible with any EVM blockchain in and outside the Polkadot ecosystem. Documented here Rust code (Substrate pallets), compatible with any Substrate chain. Code is available here . Additionally, two more flavors are in development: Astar are developing the ink! version of RMRK: code here . Gear Technologies are developing the Gear implementation: code and docs here . The RMRK NFT 2.0 standards are a set of "NFT legos", primitives that, when put together, allow a builder to compose an NFT system of arbitrary complexity without smart contracts. NFT Legos ‚Äã NFTs can own other NFTs, NFTs can equip other NFTs for visual change NFTs can have multiple resources (different outputs based on context and resource priority) NFTs can have on-chain emotes (reactions) for price discovery and social mechanics NFTs have conditional rendering (e.g. show Mona Lisa as blushing if she got 50 kissy üòò emoji) NFTs can be governed by the community via fungible shareholder-tokens (fractionalization of NFTs) NFT from Kanaria ‚Äã Multi-resource NFTs A multi-resource NFT (gif of statue, and SVG-composable dynamic NFT in one) that can also equip other NFTs from within its "inventory". Two marketplaces for RMRK-based NFTs exist with hundreds of projects already launched: Singular , the official marketplace For a complete introduction into RMRK, see this presentation or read the non-technical docs . Asset Hub ‚Äã The Asset Hub is a generic assets parachain which provides functionality for deploying and transferring assets ‚Äî both Fungible and Non-Fungible Tokens (NFTs). The Asset Hub currently hosts Uniques pallet and the NFTs pallet with NFT 2.0 functionalities. Unique Network ‚Äã Unique network , an NFT-specific blockchain offering innovations such as sponsored transactions, bundling fungible tokens with non-fungibles, and splitting NFTs into fungible tokens for partial ownership. Unique Network have launched two NFT projects to date: Substrapunks as part of Hackusama , and Chelobricks as a promotion during Polkadot Decoded . Unique Network focuses on B2B use cases, aiming to be an infrastructure provider for others to build on, rather than entering the NFT space themselves as an end-product. Unique Network aims to make their marketplace technology open-source and whitelabel-friendly. In theory, it should be trivial to set up a new marketplace for your project using Unique's technology. Unique network aims to be a parachain on Polkadot, and Quartz is their Kusama counterpart. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Introduction to NFTs Next Components List of NFT Projects Astar Basilisk Efinity KodaDot The Team Behind KodaDot Ecosystem Tools by KodaDot Moonbeam RMRK NFT Legos NFT from Kanaria Asset Hub Unique Network ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Components General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Components Polkadot Host WebAssembly (Wasm) Runtime Upgrades Polkadot OpenGov Treasury Consensus Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Components Components Key components of the Polkadot ecosystem. üìÑÔ∏è Polkadot Host The Polkadot Host and its Components. üìÑÔ∏è WebAssembly (Wasm) WASM and its Role in Polkadot's Runtime. üìÑÔ∏è Runtime Upgrades Forkless Runtime Upgrades on Polkadot. üìÑÔ∏è Polkadot OpenGov Polkadot‚Äôs Latest Model for Decentralized Governance. üìÑÔ∏è Treasury Polkadot's On-chain Treasury. üìÑÔ∏è Consensus The Consensus Mechanism of Polkadot. Previous NFT Projects Next Polkadot Host ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Host (PH) General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Components Polkadot Host WebAssembly (Wasm) Runtime Upgrades Polkadot OpenGov Treasury Consensus Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Components Polkadot Host On this page Polkadot Host (PH) The architecture of Polkadot can be divided into two different parts, the Polkadot runtime and the Polkadot host . The Polkadot runtime is the core state transition logic of the chain and can be upgraded over the course of time and without the need for a hard fork. In comparison, the Polkadot host is the environment in which the runtime executes and is expected to remain stable and mostly static over the lifetime of Polkadot. The Polkadot host interacts with the Polkadot runtime in limited, and well-specified ways. For this reason, implementation teams can build an alternative implementation of the Polkadot host while treating the Polkadot runtime as a black box. For more details of the interactions between the host and the runtime, please see the specification . Components of the Polkadot host ‚Äã Networking components such as Libp2p that facilitates network interactions. State storage and the storage trie along with the database layer. Consensus engine for GRANDPA and BABE. Wasm interpreter and virtual machine. Low level primitives for a blockchain, such as cryptographic primitives like hash functions. A compiled Polkadot runtime, a blob of Wasm code, can be uploaded into the Polkadot host and used as the logic for the execution of state transitions. Without a runtime, the Polkadot host is unable to make state transitions or produce any blocks. A host node... must populate the state storage with the official genesis state. should maintain a set of around 50 active peers at any time. New peers can be found using the discovery protocols. should open and maintain the various required streams with each of its active peers. should send block requests to these peers to receive all blocks in the chain and execute each of them. should exchange neighbor packets. Consensus in the Polkadot Host is achieved during the execution of two different procedures, block-production and finality. The Polkadot Host must run these procedures if (and only if) it is running on a validator node. Additional information on each of these requirements can be found here . Polkadot Runtime ‚Äã Below is a diagram that displays the Polkadot host surrounding the Polkadot runtime. Think of the runtime (in white) as a component that can be inserted, swapped out, or removed entirely. While the parts in grey are stable and can not change without an explicit hard fork. Code Executor ‚Äã The Polkadot Host executes the calls of Runtime entrypoints inside a Wasm Virtual Machine (VM), which in turn provides the Runtime with access to the Polkadot Host API. This part of the Polkadot Host is referred to as the Executor. For additional technical implementation details, check out this section of the Polkadot Spec. Resources ‚Äã Polkadot Host Protocol Specification - Incubator for the Polkadot Host spec, including tests. Gossamer: A Go implementation of the Polkadot Host Kagome - C++ implementation of Polkadot Host Edit this page Last updated on Jun 30, 2023 by Filippo Previous Components Next WebAssembly (Wasm) Components of the Polkadot host Polkadot Runtime Code Executor Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
WebAssembly (Wasm) General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Components Polkadot Host WebAssembly (Wasm) Runtime Upgrades Polkadot OpenGov Treasury Consensus Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Components WebAssembly (Wasm) On this page WebAssembly (Wasm) WebAssembly is used in Polkadot and Substrate as the compilation target for the runtime. What is WebAssembly? ‚Äã WebAssembly, shortened to Wasm , is a binary instruction format for a stack-based virtual machine. Wasm is designed as a portable target for the compilation of high-level languages like C/C++/Rust, enabling deployment on the web for client and server applications. Why WebAssembly? ‚Äã WebAssembly is a platform-agnostic binary format, meaning it will run the exact instructions across whatever machine it operates on. Blockchains need determinacy to have reliable state transition updates across all nodes in the peer-to-peer network without forcing every peer to run the same hardware. Wasm is an excellent fit for reliability among the diverse set of machines. Wasm is both efficient and fast. The efficiency means that it can be uploaded onto the chain as a blob of code without causing too much state bloat while keeping its ability to execute at near-native speeds. Resources ‚Äã WebAssembly.org - WebAssembly homepage that contains a link to the spec. Wasmi - WebAssembly interpreter written in Rust. Parity Wasm - WebAssembly serialization/deserialization in Rust. Wasm utils - Collection of Wasm utilities used in Parity and Wasm contract development. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Polkadot Host Next Runtime Upgrades What is WebAssembly? Why WebAssembly? Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Runtime Upgrades General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Components Polkadot Host WebAssembly (Wasm) Runtime Upgrades Polkadot OpenGov Treasury Consensus Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Components Runtime Upgrades On this page Runtime Upgrades Runtime upgrades allow Polkadot to change the logic of the chain without the need for a hard fork. Forkless Upgrades ‚Äã You may have encountered the term "hard fork" before in the blockchain space. A hard fork occurs when a blockchain's logic changes such that nodes that do not include the new changes cannot remain in consensus with nodes that do. Such changes are backward incompatible. Hard forks can be political due to the nature of the upgrades and logistically demanding due to the number (potentially thousands) of nodes in the network that need to upgrade their software. Thus, hard forking is slow, inefficient, and error-prone due to the levels of offline coordination required and, therefore, the propensity to bundle many upgrades into one large-scale event. By using Wasm in Substrate (the framework powering Polkadot, Kusama, and many connecting chains), parachains are given the ability to upgrade their runtime (a chain's "business logic") without hard forking. Rather than encoding the runtime in the nodes, Polkadot nodes contain a WebAssembly execution host . They maintain consensus on a very low-level and well-established instruction set. Upgrades can be small, isolated, and very specific by deploying Wasm on-chain and having nodes auto-enact the new logic at a particular block height. The Polkadot runtime is stored on the Polkadot blockchain itself. Polkadot can upgrade its runtime by upgrading the logic stored on-chain and removes the coordination challenge of requiring thousands of node operators to upgrade in advance of a given block number. Polkadot stakeholders propose and approve upgrades through the on-chain governance system, which also enacts them autonomously. As a result of storing the Runtime as part of the state, the Runtime code itself becomes state sensitive, and calls to Runtime can change the Runtime code itself. Therefore, the Polkadot Host must always ensure it provides the Runtime corresponding to the state in which the entry point has been called. Client Releases ‚Äã The existing runtime logic is followed to update the Wasm runtime stored on the blockchain to a new version. The upgrade is then included in the blockchain itself, meaning that all the nodes on the network execute it. Generally, there is no need to upgrade your nodes manually before the runtime upgrade, as they will automatically start to follow the new logic of the chain. Nodes only need to be updated when the runtime requires new host functions, or there is a change in networking or consensus. Transactions constructed for a given runtime version will not work on later versions. Therefore, a transaction constructed based on a runtime version will not be valid in later runtime versions. If you can‚Äôt submit a transaction before the upgrade, it is better to wait and construct it afterward. Although upgrading your nodes is generally not necessary to follow an upgrade, we recommend following the Polkadot releases and upgrading promptly, especially for high-priority or critical releases. New Client Releases The details about the latest client releases can be found in the releases section on the Polkadot repository . A detailed analysis for client releases can be viewed on the Polkadot Forum . Runtime vs Client versions ‚Äã The runtime and client versions are distinct from each other. The runtime versioning typically looks like network-xxxx , whereas the client versioning looks like vx.x.xx . For instance, the runtime version shown on the top left section of Polkadot-JS UI below is kusama-9370 , and the client (node) version shown on the top right section is v0.9.36 . Querying runtime and client versions The runtime version can be queried on-chain through Polkadot-JS UI by navigating to the Developer tab > Chain State > Storage > system and query lastRuntimeUpgrade() . The node version can be queried by navigating to the Developer tab > RPC calls > system and query version() . Runtime Upgrades for Various Users ‚Äã For Infrastructure Providers ‚Äã Infrastructure services include but are not limited to the following: Validators API services Node-as-a-Service (NaaS) General infrastructure management (e.g. block explorers, custodians) Wallets For validators, keeping in sync with the network is key. At times, upgrades will require validators to upgrade their clients within a specific time frame, for example, if a release includes breaking changes to networking. It is essential to check the release notes, starting with the upgrade priority and acting accordingly. General infrastructure providers, aside from following the Polkadot releases and upgrading in a timely manner, should monitor changes to runtime events and auxiliary tooling, such as the Substrate API Sidecar . Transactions constructed for runtime n will not work for any other runtime >n . If a runtime upgrade occurs before broadcasting a previously constructed transaction, you will need to reconstruct it with the appropriate runtime version and corresponding metadata. For Nominators ‚Äã Runtime upgrades don't require any actions by a nominator, though it is always encouraged to keep up-to-date and participate with the latest runtime upgrade motions and releases while keeping an eye on how the nodes on the network are reacting to a new upgrade. Monitoring Runtime Changes ‚Äã You can monitor the chain for upcoming upgrades. The client release notes include the hashes of any proposals related to any on-chain upgrades for easy matching. Monitor the chain for: democracy(Started) events and log index and blockNumber . This event indicates that a referendum has started (although it does not mean it is a runtime upgrade). Get the referendum info * ; it should have a status of Ongoing . Find the ending block number ( end ) and the enactment delay (delay). If the referendum passes, it will execute on block number end + delay . democracy(Passed) , democracy(NotPassed) , or, democracy(Cancelled) events citing the index. If Passed , you need to look at the scheduler(Scheduled) event in the same block for the enactment block. democracy(PreimageNoted) events with the same hash as the ReferendumInfoOf(index) item. This may be up to the last block before execution, but it will not work if this is missing. democracy(Executed) events for actual execution. In the case of a runtime upgrade, there will also be a system(CodeUpdated) event. You can also monitor Polkassembly for discussions on on-chain proposals and referenda. * E.g. via pallets/democracy/storage/ReferendumInfoOf?key1=index&at=blockNumber on Sidecar. Edit this page Last updated on Jun 30, 2023 by Filippo Previous WebAssembly (Wasm) Next Polkadot OpenGov Forkless Upgrades Client Releases Runtime vs Client versions Runtime Upgrades for Various Users For Infrastructure Providers For Nominators Monitoring Runtime Changes ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot OpenGov General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Components Polkadot Host WebAssembly (Wasm) Runtime Upgrades Polkadot OpenGov Treasury Consensus Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Components Polkadot OpenGov On this page Polkadot OpenGov The content in this document is subject to change The governance protocol has already undergone iterations (see Governance V1 ). Governance is a constantly evolving protocol at this stage in its lifecycle. For technical information about Polkadot OpenGov and how to interact with it, please refer to this dedicated Wiki page . For additional support about Polkadot OpenGov see the dedicated support pages . Polkadot Delegation Dashboard You can easily delegate your votes to somebody else using the Polkadot Delegation Dashboard . See this video tutorial to learn how to use the dashboard. If you become a nomination pool member or a pool admin, you cannot participate in Governance with the bonded tokens in the pool, as they are held in a system account . Polkadot uses a sophisticated governance mechanism that allows it to evolve gracefully overtime at the ultimate behest of its assembled stakeholders. The stated goal is to ensure that the majority of the stake can always command the network. Polkadot brings together various novel mechanisms, including an amorphous (abstract) form of state-transition function stored on-chain defined in a platform-agnostic language (i.e. WebAssembly ), and several on-chain voting mechanisms such as referenda and batch approval voting. All changes to the protocol must be agreed upon by stake-weighted referenda. Premise ‚Äã Polkadot's first governance system ( Governance V1 ) included three main components. The Technical Committee : A technocratic committee to manage upgrade timelines. The Council : An approval-voted, elected executive "government" to manage parameters, admin, and spending proposals. The Public: All token holders. Over the first few years of operation, Governance V1 ensured the appropriate usage of treasury funds and enabled timely upgrades and fixes. Like most early technologies, protocols must evolve as they mature to improve their shortcomings and keep up with modern advancements. In Governance V1, all referenda carried the same weight as only one referendum could be voted on at a time (except for emergency proposals), and the voting period could last multiple weeks. Also, an alternating voting timetable allowed to vote either for a public referendum or a council motion every 28 days. This resulted in the system favoring careful consideration of very few proposals instead of broad consideration of many. Polkadot OpenGov changes how the practical means of day-to-day decisions are made, making the repercussions of referenda better scoped and agile to increase the number of collective decisions the system can make at any given time. The following content is focused on what the new Polkadot OpenGov version brings to the governance on Polkadot, and on the main differences with previous governance versions. We recommend learning about Governance v1 to better understand the need for and the direction of Polkadot OpenGov. Summary ‚Äã In Governance v1, active token holders (public) and the Council together administrated network's upgrade decisions. Whether the public or the council initiated the proposal, it would eventually have to go through a referendum to let all holders (weighted by stake and conviction) make the decision. The Council fulfilled its role as the representative of the public, guardian of the treasury and initiator of legislation, but it was often seen as a centralized entity. To further decentralize Polkadot, Polkadot OpenGov proposes the following main changes: Migrating all responsibilities of the Council to the public via a direct democracy voting system. Dissolving the current Council collective Allowing users to delegate voting power in more ways to community members Dissolving the Technical Committee and establishing the broader Polkadot Technical Fellowship The figure below shows an overview of Polkadot OpenGov's structure. info See this page for a comparison with the structure of Governance V1. In Polkadot OpenGov, all the proposals are initiated by the public. The proposal will enter a Lead-in period (for more information, see Referenda Timeline ), after which it will follow a specific Track which has a dedicated Origin. There are 15 Origins , each with a different track. The origins and tracks parameters are preset values that set the duration of a referendum as well as how many referenda can be voted on simultaneously. For example, a treasury proposal can now be submitted in different tracks depending on the amount requested. A proposal for a small tip will need to be submitted in the Small Tipper track, while a proposal requiring substantial funds will need to be submitted to the Medium or Big Spender track. The Polkadot Technical Fellowship can decide to whitelist a proposal that will be enacted through the Whitelist Caller origin. Those proposals will have a shorter Lead-in, Confirmation, and Enactment period when compared to the Root Origin track. Each track has its own preset Approval and Support curves which are based on the origin's privileges. When both the approval and support criteria are satisfied for a specific period (called the confirmation period), the referenda passes and will be executed after the enactment period. All referenda within each track and across tracks can be voted on simultaneously (assuming track maximum capacity is not reached). Polkadot OpenGov also comes with multi-role delegations where the token holder can assign voting power on different tracks to different entities who are experts in judging the referenda submitted to those tracks. For example, suppose a token holder does not have the technical background to consider the merits and vote on the referenda submitted to the Root track. In that case, they can delegate their voting power just for the Root track to a trusted expert who (according to them) acts in the best interest of the network protocol. In this way, token holders do not need to be up-to-date with governance matters and can still make their votes count through delegates. Delegation Dashboard To easily delegate your votes you can use the Polkadot Delegation Dashboard . See this video tutorial to know more about the dashboard and learn how to use it. Gov1 vs. Polkadot OpenGov ‚Äã Governance V1 Polkadot OpenGov Polkadot OpenGov Benefit Includes the Council , the Technical Committee , and the Public (i.e. token holders). Includes the Public and the Technical Fellowship . Simpler and more decentralized structure. Referenda executed only from one origin (Root). Referenda in this origin must be carefully scrutinized. Therefore, there is only one track (i.e. only one referendum at a time can be executed). Referenda executed from multiple origins , each with a different track that shapes proposals‚Äô timelines. Depending on the origin, multiple referenda within the same track are possible. Possibility to categorize proposals (based on importance and urgency) and execute them simultaneously within and between origin tracks. Proposals can be submitted by either the Council or the Public. The public submits proposals. More democratic. Uses Adaptive Quorum Biasing to define the approval threshold based on turnout. Given the same turnout, council-initiated referenda require fewer Aye votes to pass compared to public referenda. Uses origin-specific approval and support curves defining the amount of approval and support (i.e. turnout) needed as a function of time. The same curves are applied to all referenda within the same origin track. Referenda timeline depends on the origin and not on who submitted the proposal (i.e. Council or Public). This is a more democratic and equalitarian system. Uses alternating voting timetable allowing voters to cast votes for either council or public referenda every 28 eras. Multiple referenda can be voted at the same time. More flexible and agile governance system. Except for emergency proposals, all referenda have fixed voting and enactment periods of 28 eras. Periods' length is customizable and has pre-defined limits for each origin. The same limits apply to all tracks with the same origin. For example, the track in the origin Root will be longer than the track within the Small Tipper origin. Referenda‚Äôs timeline is tailored to their importance and urgency. Flexible enactment period based on origin. Emergency proposals turned referenda can be simultaneously voted on and executed with other referenda and have shorter enactment periods. They must be proposed by the Technical Committee and approved by the Council. No emergency proposals. The Technical Fellowship can whitelist proposals that will have their origin with shorter lead-in, confirmation, and enactment periods. The Technical Fellowship is a more decentralized entity when compared to the Technical Committee. Whitelisting a proposal requires a majority of approval from the fellowship. Only the most-endorsed proposal is able to transit from Launch to Voting period. The time for the transition is indeterminate, and (with no possibility of canceling endorsements) proposers and endorsers might wait a long time before a referendum is tabled, and getting their deposits back. All proposals will eventually be voted on (given track capacity and deposit are met and the Lead-in period has ended). It allows all proposals to get a chance to be voted on in a timely predictive manner. Only aye or nay votes possible. Voters can have the additional voting options of abstaining or splitting votes. More ways to engage in voting and increase turnout. Voters can decide to delegate votes to another account. Voters can use multirole delegations and delegate votes to different accounts depending on origins. More agile delegations tailored by expertise. Proposals ‚Äã Starting a proposal in Governance v1 See this page for more information about starting referenda in Governance v1. In Polkadot OpenGov, anyone can start a referendum at any time and do so as often as they wish. Previous features were expanded and improved, most notably Origins and Tracks help aid in the flow and processing of the submitted referenda. Cancelling and Blacklisting ‚Äã Cancelling Referenda in Governance v1 See this page for more information about cancelling referenda in Governance v1. In Polkadot OpenGov, there is a special operation called Cancellation for intervening with a proposal already being voted on. The operation will immediately reject an ongoing referendum regardless of its status. There is also a provision to ensure the deposit of the proposer is slashed if the proposal is malicious or spam. Cancellation is a governance operation the network must vote on to be executed. Cancellation comes with its own Origin and Track which has a low lead-time and Approval/Support curves with slightly sharper reductions in their thresholds for passing, given that it is invoked with a sense of urgency. Blacklisting Blacklisting referenda in Polkadot OpenGov is the same as in Governance v1 . Referenda ‚Äã Public and Council Referenda in Governance v1 With the Council's dissolution, council referenda are no longer present in Polkadot OpenGov. See this page for more information about public referenda in Governance v1. In Polkadot OpenGov all referenda are public. The Technical Fellowship has the option to whitelist referenda that can be then proposed in the track with whitelist origin . Referenda Timeline ‚Äã Voting timetable in Governance v1 See this page for more information about the voting timetable in Governance v1. The figure above provides a summary view of the referenda timeline for Polkadot OpenGov. In (1), when a referendum is initially created, the community can immediately vote on it. However, it is not immediately in a state where it can end or otherwise have its votes counted, be approved, and ultimately enacted. Instead, the proposal will stay within a Lead-in Period until it fulfills three criteria: Proposals must stay within the lead-in period for a pre-defined minimum amount of time. This helps mitigate against the possibility of "decision sniping" where an attacker controlling a substantial amount of voting power might seek to have a proposal passed immediately after proposing, not allowing the overall voting population adequate time to consider and participate. There must be enough room for the decision within the origin. Different origins have their limit on the number of proposals that can be decided simultaneously. Tracks that have more potent abilities will have lower limits. For example, the Root level Origin has a limit of one, implying that only a single proposal may be decided on at once. A decision deposit must be submitted. Creating a referendum is cheap as the deposit value consists of only the value required for the on-chain storage needed to track it. But, having a referendum reviewed and decided upon carries the risk of using up the limited spots available in the referenda queue. Having a more significant but refundable deposit requirement makes sense to help mitigate spam. Failing to submit the decision deposit will lead to a referendum timeout . Until they are in the lead-in period, proposals remain undecided. Once the criteria above are met, the referendum moves to the deciding state. The votes of the referendum are now counted towards the outcome. In (2), the proposal enters the Decision Period where votes can be cast. For a proposal to be approved, votes must satisfy the approval and support criteria for at least the Confirmation Period ; otherwise, the proposal is automatically rejected. A rejected proposal can be resubmitted anytime and as many times as needed. In (3), approved proposals will enter the Enactment Period , after which proposed changes will be executed. Note how the length of the lead-in, decision, confirmation, and enactment periods vary depending on the origin. Root origin has more extended periods than the other origins. Also, the number of referenda within each track differs, with the Root origin track only accepting one. proposal at a time (see below). This directly affects the number of proposals that can be voted on and executed simultaneously. Continuing the comparison between Root and Small Tipper, Small Tipper will allow many proposals on its track to be executed simultaneously. In contrast, Root will allow only one proposal in its track. Once the track capacity is filled, additional proposals in the lead-in period will queue until place is available to enter the decision period. Origins and Tracks ‚Äã An Origin is a specific level of privilege that will determine the Track of all referenda executed with that origin. The track is basically a pipeline in which the proposal lives and proceeds and is independent from other origins' tracks. The proposer of the referenda now selects an appropriate Origin for their request based on the proposal‚Äôs requirements. Although the track structure is the same for all origins, track parameters are not. Such parameters include: Maximum Deciding or Capacity : the limit for the number of referenda that can be decided at once (i.e. the number of tracks within each origin). Decision deposit : the amount of funds that must be placed on deposit to enter the Decision Period (note that more requirements must be met to enter the Decision Period). Preparation Period : the minimum amount of voting time needed before entering the Decision Period (given capacity and deposit are met). Decision Period : the maximum time to approve a proposal. The proposal will be accepted if approved by the end of the period. Confirmation Period : the minimum amount of time (within the Decision Period) the approval and support criteria must hold before the proposal is approved and moved to the enactment period. Minimum Enactment Period : the minimum amount of waiting time before the proposed changes are applied Approval Curve : the curve describing the minimum % of aye votes as a function of time within the Decision Period. The approval % is the portion of aye votes (adjusted for conviction) over the total votes ( aye , nay , and abstained ). Support Curve : the curve describing the minimum % of all votes in support of a proposal as a function of time within the Decision Period. The support % is defined as the portion of all votes ( aye and abstained ) without conviction over the total possible amount of votes in the system (i.e. the total issuance). For example, a runtime upgrade (requiring a set_code call, if approved) does not have the same implications for the ecosystem as the approval of a treasury tip ( reportAwesome call), and therefore different Origins for these two actions are needed in which different deposits, support, approval, and a minimum enactment periods will be predetermined on the pallet. For detailed information about origin and tracks, and parameter values in Kusama, see this page . Approval and Support ‚Äã Adaptive Quorum Biasing is deprecated In Polkadot OpenGov, Adaptive quorum biasing used in Governance V1 has been replaced with the Approval and Support system . The figure above provides a summary view of how the approval and support system works during the Decision Period. Once the proposal exits the Lead-in Period and enters the Voting Period, to be approved, it must satisfy the approval and support criteria for the Confirmation Period . Approval is defined as the share of approval ( aye votes) vote-weight (after adjustment for conviction ) against the total vote-weight ( aye , nay , and abstained ). Support is the total number of aye and abstain votes (ignoring any adjustment for conviction) compared to the total possible votes that could be made in the system. In case of split votes, only aye and abstain will count. Nay votes are not counted towards Support Support is a measure of voters who turned out either in favor of the referenda and who consciously abstained from it. Support does not include nay votes. This avoids edge situations where nay votes could push a referendum into confirming state. For example, imagine current approval is high (near 100%, way above the approval curve), and current support is just below the support curve. A nay could bump support above the support curve but not reduce approval below the approval curve. Therefore someone voting against a proposal would make it pass. Hence, a decrease in % of current approval through new votes does not directly translate into increasing support because Support needs to consider nay votes. The figure above shows the followings: Even if the approval threshold is reached (i.e. % of current approval is greater than the approval curve), the proposal only enters the confirmation period once the support threshold is also reached (i.e. % current support is greater than the underlying support curve). If the referendum meets the criteria for the confirmation period, then the proposal is approved and scheduled for enactment. The Enactment Period can be specified when the referendum is proposed but is also subject to a minimum value based on the Track. More powerful Tracks enforce a larger Enactment Period to ensure the network has ample time to prepare for any changes the proposal may bring. A referendum may exit the confirmation period when the thresholds are no longer met, due to new Nay votes or a change of existing Aye or Abstain votes to Nay . Each time it exits, the confirmation period resets. For example, if the confirmation period is 20 minutes and a referendum enters it just for 5 min, the next time it enters, it must stay for 20 minutes (not 15 minutes). During the decision period, if a referendum fails to meet the approval and support thresholds for the duration of the track-specific confirmation period, it fails and does not go to the enactment period (it may have to be resubmitted, see below). The current approval must be above 50% for a referendum to pass, and the approval curve never goes below 50%. Note that support may not increase monotonically as shown in the figure, as people might switch votes. Different Origins' tracks have different Confirmation Periods and requirements for approval and support. For additional details on the various origins and tracks, check out this table . Configuring the amount of support and overall approval required for it to pass is now possible. With proposals that use less privileged origins, it is far more reasonable to drop the required support to a more realistic amount earlier than those which use highly privileged classes such as Root . Classes with more significance can be made to require higher approval early on, to avoid controversy. Enactment ‚Äã Enactment in Governance v1 See this page for more information about enactment in Governance v1. In Polkadot OpenGov, the proposer suggests the enactment period, but there are also minimums set for each Origin Track. For example, root Origin approvals require a more extended period because of the importance of the changes they bring to the network. Voting on a Referendum ‚Äã In Governance V1, voters could cast only an aye or nay vote. In Polkadot OpenGov, voters can additionally cast a abstain and split votes. Vote splitting allows voters to allocate different votes for aye , nay , and abstain . Voluntary Locking ‚Äã Voluntary Locking Voluntary locking in Polkadot OpenGov is the same as in Governance v1 . Conviction voting locks in Governance v1 will not be carried over to OpenGov. Voting with conviction in OpenGov will create a new lock (as this will use the convictionVoting pallet), while any existing lock under Governance v1 (using the deprecated democracy pallet) will be left to expire. Delegations under Governance v1 will need to be re-issued under OpenGov. Multirole Delegation ‚Äã Polkadot OpenGov builds on the vote delegation feature from Governance v1 where a voter can delegate their voting power to another voter. It does so by introducing a feature known as multirole delegation , where voters can specify a different delegate for every class of referendum in the system. Delegation can be done per track, and accounts can choose to select different delegates (or no delegation) for each track. For example, a voter could delegate one entity for managing a less potent referenda class, choose a different delegate for another class with more powerful consequences and still retain full voting power over any remaining classes. Delegate your votes In Polkadot OpenGov you can delegate your votes to different entities, who will vote on your behalf. You can delegate your votes using the Polkadot Delegation Dashboard . See this video tutorial to learn about the features of the dashboard. If you are staking directly and not through a nomination pool, you can use bonded tokens for voting. Note that if you are voting with conviction, your tokens will have a democracy lock in addition to the staking lock. For more information about locks, see this page . Occasional delegation and undelegation calls are fee-free: creating an incentive for token holders to use this feature and ensure that wallets can do it ‚Äúby default‚Äù without any cost to end-users. It is worth noting that a user delegating their voting power does not imply that the delegate will have control over the funds of the delegating account: they can vote with a user's voting power: but they won't be able to transfer your balance, nominate a different set of validators or execute any call other than voting on the defined call/s by the user. With the new delegation features, the goal is to ensure the required support for proposals to be enacted is reached while maintaining the anonymity of voters and keeping the overall design censorship-free. For a step-by-step outline of how to delegate voting power in Polkadot OpenGov, check out the Delegating Voting Power section on the Polkadot OpenGov Maintenance page. The Technical Fellowship ‚Äã From Technical Committee to the Technical Fellowship The Polkadot Technical Fellowship is a collection of Substrate experts. This fellowship was established in 2022. In Polkadot OpenGov, this fellowship replaces the Technical Committee in Governance v1, and will serve both the Polkadot and Kusama networks. The Technical Fellowship is a mostly self-governing expert body with a primary goal of representing humans who embody and contain the technical knowledge base of the Kusama and/or Polkadot networks and protocols. This is accomplished by associating a rank with members to categorize the degree to which the system expects their opinion to be well-informed, of a sound technical basis, and in line with the interests of Polkadot and/or Kusama. Unlike the Technical Committee in Governance V1, the Fellowship is designed to be far broader in membership (i.e. to work well with even tens of thousands of members) and with far lower barriers to entry (both in terms of administrative process flow and expectations of expertise). The mechanism by which the Fellowship votes is the same as what is used for Polkadot and Kusama stakeholder voting for a proposed referendum. Members of the Fellowship can vote on any given Fellowship proposal and the aggregated opinion of the members (weighted by their rank) constitutes the Fellowship's considered opinion. Ranking System ‚Äã To prevent a small group of participants from gaining effective control over the network, this system will adhere to three main principles: The Fellowship must never have hard power over the network: it cannot change the parameters, conduct rescues or move assets. Their only power in governance is reducing the effective timeline on which a referendum takes place through whitelisting . The Fellowship weights those with a higher rank more in the aggregate opinion. However, the weight should not be so high as to make a small number of higher members‚Äô opinions be insurmountable compared to a coherent opinion from lower-ranked membership. The Fellowship should be designed to grow and develop its membership, aggregate levels of expertise and ensure that its overall decision-making capacity strengthens over time. To support these conditions, the Fellowship will have a constitution that outlines the requirements and expectations for individuals to attain and retain any given rank. Higher ranks can vote and promote lower ranks based on this constitution. Demotion occurs automatically after a given period has elapsed, and the member is unable to defend their position to their peers. Suspension can happen only through a referendum, which ensures that the Fellowship's bias alone does not necessarily result in expulsion. To prevent the Fellowship from becoming a cabal (popularity with Fellowship peers alone should not be enough to gain access to a top rank), gaining access to the top tiers of the ranks will require a referendum. Whitelisting ‚Äã Polkadot OpenGov allows the Fellowship to authorize a new origin (known as "Whitelisted-Caller") to execute with Root-level privileges for calls that have been approved by the Fellowship. The Whitelist pallet allows one Origin to escalate the privilege level of another Origin for a certain operation. The pallet verifies two things: The origin of the escalation is the Whitelisted-Root (i.e. that the referendum passed on this track), and The whitelist contains the given proposal (in the configuration of Polkadot OpenGov, it gets there via a separate Fellowship referendum). If both conditions are true, the operation executes with Root-level privileges. This system enables a new parallel Track (Whitelisted-Caller), whose parameters have less restrictive passing parameters than Root in the short term. Through an open and transparent process, a body of global experts on the Polkadot protocol have determined that the action is both safe and time-critical. Submitting Whitelisted Proposals For more information about how to submit a whitelisted proposal see the dedicated advanced how-to guides . Becoming a Fellowship Member ‚Äã Currently, the only way to become a fellowship member is through an existing member who submits a fellowshipCollective.addMember extrinsic. See more information on this Kusama SubSquare discussion , which points to these guidelines . Future plans include that public members can apply to become a Fellowship candidate by placing a small deposit (which will be returned once they become members). Their candidacy will go through a referendum to be approved to become a member. Resources ‚Äã Democracy Pallet Governance v2 Polkadot Direction Kusama Direction PolkAssembly Edit this page Last updated on Jul 13, 2023 by Filippo Previous Runtime Upgrades Next Treasury Premise Summary Gov1 vs. Polkadot OpenGov Proposals Cancelling and Blacklisting Referenda Referenda Timeline Origins and Tracks Approval and Support Enactment Voting on a Referendum Voluntary Locking Multirole Delegation The Technical Fellowship Ranking System Whitelisting Becoming a Fellowship Member Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Treasury General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Components Polkadot Host WebAssembly (Wasm) Runtime Upgrades Polkadot OpenGov Treasury Consensus Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Components Treasury On this page Treasury The Treasury is a pot of funds collected through a portion of block production rewards, transaction fees, slashing, and staking inefficiencies . Treasury funds are held in a system account that cannot be controlled by any external account; only the system internal logic can access it. Treasury Inflow and Outflow ‚Äã Tokens that are deposited into the Treasury (i.e. the inflow) is determined by the following mechanisms: Transaction fees: 80% of the transaction fees of every submitted extrinsic is diverted to the Treasury, while 20% is given to the block producers. Staking inefficiencies: the network knows an exogenously determined parameter called ideal staking rate. The APY for stakers (nominators & validators) decreases whenever the actual staking rate is not equal to the ideal staking rate. To keep inflation constant at 10%, the system does not creates less tokens, rather some share of the overall reward for stakers is diverted to the Treasury (more information here ). Slashes: whenever validators and nominators are slashed, a share of the slashed tokens are diverted to Treasury. They are typically rare and unpredictable events. Transfers: everyone can send funds to the Treasury directly. This is a rare event and typically due to grantees reimbursing some of the amount they got allocated for various reasons. The outflow is determined by the following mechanisms: Burned tokens: at the end of each spending period lasting 24 days on Polkadot, 1% of the available funds are burned, respectively. Treasury proposals & Bounties: they make up the largest share of outflow tokens to the community and need to be approved by governance. Then, payouts occur at the end of a spending period. Tips: smaller payouts directly to grantees that can happen within a spending period. Treasury Tracks ‚Äã OpenGov allows for managing funds through six tracks, each with its own origin and track parameters . Treasurer Big Spender Medium Spender Small Spender Big Tipper Small Tipper How to access Treasury funds? Access to Treasury funds requires successful enactment of referendum in the respective treasury track on-chain. Learn how to create a referendum here . Treasurer ‚Äã This track can be used for treasury spending requests up to 10M DOT per referendum. Each referendum requires a decision deposit of 1000 DOT. Big Spender ‚Äã This track can be used for treasury spending requests up to 1M DOT per referendum. Each referendum requires a decision deposit of 400 DOT. Medium Spender ‚Äã This track can be used for treasury spending requests up to 100K DOT per referendum. Each referendum requires a decision deposit of 200 DOT. Small Spender ‚Äã This track can be used for treasury spending requests up to 10K DOT per referendum. Each referendum requires a decision deposit of 100 DOT. Big Tipper ‚Äã This track can be used for treasury spending requests up to 1000 DOT per referendum. Each referendum requires a decision deposit of 10 DOT. Small Tipper ‚Äã This track can be used for treasury spending requests up to 100 DOT per referendum. Each referendum requires a decision deposit of 1 DOT. Creating a Treasury Proposal ‚Äã Your proposal should address a problem, outline a goal, give a detailed account of how you will reach that goal, and include any ongoing maintenance needs. As much as possible, you should itemize the tasks to be completed so fees can be evaluated and milestones can be followed. You can check the guidelines for a successful proposal and fill out the Treasury proposal template provided. Announcing the Proposal ‚Äã To minimize storage on chain, proposals don't contain contextual information. When a user submits a proposal, they will need to find an off-chain way to explain the proposal: Many community members participate in discussion in the Polkadot Watercooler and Polkadot Direction room . Use platforms like Polkassembly and SubSquare to initiate discussion with the community. They also offer a gauge poll to capture the community sentiment before submitting an on-chain referendum. Spreading the word about the proposal's explanation to the community is ultimately up to the proposer. Use Accounts with Verified On-Chain Identity for Treasury Proposals To ensure legitimacy, it is required that the account linked to the Treasury proposal has an identity set and is verified by an on-chain registrar . Submit Treasury Proposal Preimage ‚Äã The example below shows how to create a preimage for a transaction that requests 100 DOT from Treasury. Navigate to Polkadot-JS UI > Governance > Preimages and then click on Add Preimage. Select the account which will be used to submit the preimage. Choose treasury pallet in the "propose" dropdown and the spend(amount, beneficiary) call Enter the DOT amount. Enter the AccountID of the beneficiary (which has a verified on-chain identity). Submit preimage Sign and submit the transaction by paying the specified transaction fees. Preimage Submission Deposit A deposit is required for the preimage to be stored on chain. The deposit amount required for a preimage with a treasury spend transaction is around 41 DOT. Ensure you have enough account balance to pay for the submission deposit and the transaction fees. After successful submission of the preimage, it is displayed on Polkadot-JS UI > Governance > Preimages page. Every preimage is associated with a unique preimage hash (highlighted in a box in the image below). Take a note of this preimage hash, which is required to submit a referendum. Submit a Treasury Track Referendum ‚Äã The example below shows how to submit a Treasury track referendum. Navigate to Polkadot-JS UI > Governance > Referenda and then click on Submit proposal. Select the account which will be used to submit the proposal. Choose the appropriate submission track (The example below selected Small Spender track). Enter the preimage hash of the treasury spend transaction.(If the preimage exists on-chain, the preimage length box is automatically populated) Click on Submit proposal. Sign and submit the transaction. Once your submission is executed, your referendum will appear under your chosen track on the Polkadot-JS UI referenda page . Place a Decision Deposit for the Treasury Track Referendum ‚Äã For the referendum to move from preparing phase to the deciding phase, a decision deposit needs to be placed. The decision deposit values for each individual Treasury Tracks are listed in a section above in this document. Claiming the Preimage and Decision Deposits ‚Äã After the referendum finishes its life cycle (and gets approved or rejected or timed out), the deposits can be claimed. For claiming the preimage deposit, navigate to Polkadot-JS UI > Governance > Preimages and click on unnote button shown on the preimage you submitted. Similarly, to claim the decision deposit, navigate to Polkadot-JS UI > Governance > Referenda and scroll down to the end of the page to click on the referenda with the decision depoit and claim it. Edit this page Last updated on Jul 7, 2023 by Radha Previous Polkadot OpenGov Next Consensus Treasury Inflow and Outflow Treasury Tracks Treasurer Big Spender Medium Spender Small Spender Big Tipper Small Tipper Creating a Treasury Proposal Announcing the Proposal Submit Treasury Proposal Preimage Submit a Treasury Track Referendum Place a Decision Deposit for the Treasury Track Referendum Claiming the Preimage and Decision Deposits ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Consensus General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Components Polkadot Host WebAssembly (Wasm) Runtime Upgrades Polkadot OpenGov Treasury Consensus Staking Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Components Consensus On this page Polkadot Consensus Why do we need Consensus? ‚Äã Consensus is a method for coming to agreement over a shared state. In order for the state of the blockchain to continue to build and move forward, all nodes in the network must agree and come to consensus. It is the way that the nodes in a decentralized network are able to stay synced with each other. Without consensus for the decentralized network of nodes in a blockchain, there is no way to ensure that the state one node believes is true will be shared by the other nodes. Consensus aims to provide the objective view of the state amid participants who each have their own subjective views of the network. It is the process by which these nodes communicate and come to agreement, and are able to build new blocks. What are PoW and PoS? ‚Äã Proof of Work (PoW) and Proof of Stake (PoS) have been inaccurately used as short hand to refer to consensus mechanisms of blockchains, but that does not capture the full picture. PoW is the method for agreeing on a block author and part of the fuller Nakamoto consensus that also encompasses a chain selection algorithm (longest chain rule in Bitcoin). Similarly, PoS is a set of rules for selecting the validator set and does not specify a chain selection rule or how a chain might reach finality. PoS algorithms have traditionally been paired with an algorithm for coming to Byzantine agreement between nodes. For example, Tendermint is a practical Byzantine fault tolerant algorithm that uses PoS as its validator set selection method. Why not Proof of Work? ‚Äã Although simple and effective in coming to a decentralized consensus on the next block producer, proof of work with Nakamoto consensus consumes an incredible amount of energy, has no economic or provable finality, and has no effective strategy in resisting cartels. Nominated Proof of Stake ‚Äã In traditional PoS systems, block production participation is dependent on token holdings as opposed to computational power. While PoS developers usually have a proponent for equitable participation in a decentralized manner, most projects end up proposing some level of centralized operation, where the number of validators with full participation rights is limited. These validators are often seen to be the most wealthy, and, as a result, influence the PoS network as they are the most staked. Usually, the number of candidates to maintain the network with the necessary knowledge (and equipment) is limited; this can directly increase operational costs as well. Systems with a large number of validators tend to form pools to decrease the variance of their revenue and profit from economies of scale. These pools are often off-chain. A way to alleviate this is to implement pool formation on-chain and allow token holders to vote with their stake for validators to represent them. Polkadot uses NPoS (Nominated Proof-of-Stake) as its mechanism for selecting the validator set. It is designed with the roles of validators and nominators , to maximize chain security. Actors who are interested in maintaining the network can run a validator node. Validators assume the role of producing new blocks in BABE , validating parachain blocks, and guaranteeing finality. Nominators can choose to back select validators with their stake. Nominators can approve candidates that they trust and back them with their tokens. Probabilistic vs. Provable Finality ‚Äã A pure Nakamoto consensus blockchain that runs PoW is only able to achieve the notion of probabilistic finality and reach eventual consensus . Probabilistic finality means that under some assumptions about the network and participants, if we see a few blocks building on a given block, we can estimate the probability that it is final. Eventual consensus means that at some point in the future, all nodes will agree on the truthfulness of one set of data. This eventual consensus may take a long time and will not be able to be determined how long it will take ahead of time. However, finality gadgets such as GRANDPA (GHOST-based Recursive ANcestor Deriving Prefix Agreement) or Ethereum's Casper FFG (the Friendly Finality Gadget) are designed to give stronger and quicker guarantees on the finality of blocks - specifically, that they can never be reverted after some process of Byzantine agreements has taken place. The notion of irreversible consensus is known as provable finality. In the GRANDPA paper , it is phrased in this way: note We say an oracle A in a protocol is eventually consistent if it returns the same value to all participants after some unspecified time. Hybrid Consensus ‚Äã There are two protocols we use when we talk about the consensus protocol of Polkadot, GRANDPA and BABE (Blind Assignment for Blockchain Extension). We talk about both of these because Polkadot uses what is known as hybrid consensus . Hybrid consensus splits up the finality gadget from the block production mechanism. This is a way of getting the benefits of probabilistic finality (the ability to always produce new blocks) and provable finality (having a universal agreement on the canonical chain with no chance for reversion) in Polkadot. It also avoids the corresponding drawbacks of each mechanism (the chance of unknowingly following the wrong fork in probabilistic finality, and a chance for "stalling" - not being able to produce new blocks - in provable finality). By combining these two mechanisms, Polkadot allows for blocks to be rapidly produced, and the slower finality mechanism to run in a separate process to finalize blocks without risking slower transaction processing or stalling. Hybrid consensus has been proposed in the past. Notably, it was proposed (now defunct) as a step in Ethereum's transition to proof of stake in EIP 1011 , which specified Casper FFG . Block Production: BABE ‚Äã BABE (Blind Assignment for Blockchain Extension) is the block production mechanism that runs between the validator nodes and determines the authors of new blocks. BABE is comparable as an algorithm to Ouroboros Praos , with some key differences in chain selection rule and slot time adjustments. BABE assigns block production slots to validators according to stake and using the Polkadot randomness cycle . The chains runtime is required to provide the BABE authority list and randomness to the host via a consensus message in the header of the first block of each epoch. BABE execution happens in sequential non-overlapping phases known as epochs. Each epoch is divided into a predefined number of slots. All slots in each epoch are sequentially indexed starting from 0 (slot number). At the beginning of each epoch, the BABE node needs to run the Block-Production-Lottery algorithm to find out in which slots it should produce a block and gossip to the other block producers. Validators participate in a lottery for every slot, which will inform whether or not they are the block producer candidate for that slot. Slots are discrete units of time of approximately 6 seconds in length. Because the mechanism of allocating slots to validators is based on a randomized design, multiple validators could be candidates for the same slot. Other times, a slot could be empty, resulting in inconsistent block time. Multiple Validators per Slot ‚Äã When multiple validators are block producer candidates in a given slot, all will produce a block and broadcast it to the network. At that point, it's a race. The validator whose block reaches most of the network first wins. Depending on network topology and latency, both chains will continue to build in some capacity, until finalization kicks in and amputates a fork. See Fork Choice below for how that works. No Validators in Slot ‚Äã When no validators have rolled low enough in the randomness lottery to qualify for block production, a slot can remain seemingly blockless. We avoid this by running a secondary, round-robin style validator selection algorithm in the background. The validators selected to produce blocks through this algorithm always produce blocks, but these secondary blocks are ignored if the same slot also produces a primary block from a VRF-selected validator. Thus, a slot can have either a primary or a secondary block, and no slots are ever skipped. For more details on BABE, please see the BABE paper . BADASS BABE: SASSAFRAS ‚Äã SASSAFRAS (Semi Anonymous Sortition of Staked Assignees For Fixed-time Rhythmic Assignment of Slots) (aka SASSY BABE or BADASS BABE), is an extension of BABE and acts as a constant-time block production protocol. This approach tries to address the shortcomings of BABE by ensuring that exactly one block is produced with time-constant intervals. The protocol utilizes zk-SNARKs to construct a ring-VRF and is a work in progress. This section will be updated as progress ensues. Finality Gadget: GRANDPA ‚Äã GRANDPA (GHOST-based Recursive ANcestor Deriving Prefix Agreement) is the finality gadget that is implemented for the Polkadot Relay Chain. The Polkadot Host uses the GRANDPA Finality protocol to finalize blocks. Finality is obtained by consecutive rounds of voting by the validator nodes. Validators execute GRANDPA finality process in parallel to Block Production as an independent service. It works in a partially synchronous network model as long as 2/3 of nodes are honest and can cope with 1/5 Byzantine nodes in an asynchronous setting. A notable distinction is that GRANDPA reaches agreements on chains rather than blocks, greatly speeding up the finalization process, even after long-term network partitioning or other networking failures. In other words, as soon as more than 2/3 of validators attest to a chain containing a certain block, all blocks leading up to that one are finalized at once. Protocol ‚Äã Please refer to the GRANDPA paper for a full description of the protocol. Implementation ‚Äã The Substrate implementation of GRANDPA is part of Substrate Frame. Bridging: BEEFY ‚Äã The BEEFY (Bridge Efficiency Enabling Finality Yielder) is a secondary protocol to GRANDPA to support efficient bridging between the Polkadot network (relay chain) and remote, segregated blockchains, such as Ethereum, which were not built with the Polkadot interchain operability in mind. The protocol allows participants of the remote network to verify finality proofs created by the Polkadot relay chain validators. In other words: clients in the Ethereum network should able to verify that the Polkadot network is at a specific state. Storing all the information necessary to verify the state of the remote chain, such as the block headers, is too expensive. BEEFY stores the information in a space-efficient way and clients can request additional information over the protocol. For additional implementation details, check out this section of the Polkadot Spec. Fork Choice ‚Äã Bringing BABE and GRANDPA together, the fork choice of Polkadot becomes clear. BABE must always build on the chain that has been finalized by GRANDPA. When there are forks after the finalized head, BABE provides probabilistic finality by building on the chain with the most primary blocks. In the above image, the black blocks are finalized, and the yellow blocks are not. Blocks marked with a "1" are primary blocks; those marked with a "2" are secondary blocks. Even though the topmost chain is the longest chain on the latest finalized block, it does not qualify because it has fewer primaries at the time of evaluation than the one below it. Comparisons Nakamoto consensus ‚Äã Nakamoto consensus consists of the longest chain rule using proof of work as its Sybil resistance mechanism and leader election. Nakamoto consensus only gives us probabilistic finality. Probabilistic finality states that a block in the past is only as safe as the number of confirmations it has, or the number of blocks that have been built on top of it. As more blocks are built on top of a specific block in a Proof of Work chain, more computational work has been expended behind this particular chain. However, it does not guarantee that the chain containing the block will always remain the agreed-upon chain, since an actor with unlimited resources could potentially build a competing chain and expend enough computational resources to create a chain that did not contain a specific block. In such a situation, the longest chain rule employed in Bitcoin and other proof of work chains would move to this new chain as the canonical one. PBFT / Tendermint ‚Äã Please see the relevant section in the Cosmos comparison article. Casper FFG ‚Äã The two main differences between GRANDPA and Casper FFG are: in GRANDPA, different voters can cast votes simultaneously for blocks at different heights GRANDPA only depends on finalized blocks to affect the fork-choice rule of the underlying block production mechanism Resources BABE paper - The academic description of the BABE protocol. GRANDPA paper - The academic description of the GRANDPA finality gadget. Contains formal proofs of the algorithm. Rust implementation - The reference implementation and the accompanying Substrate pallet . Block Production and Finalization in Polkadot - An explanation of how BABE and GRANDPA work together to produce and finalize blocks on Kusama, with Bill Laboon. Block Production and Finalization in Polkadot: Understanding the BABE and GRANDPA Protocols - An academic talk by Bill Laboon, given at MIT Cryptoeconomic Systems 2020, describing Polkadot's hybrid consensus model in-depth. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Treasury Next Staking Why do we need Consensus? What are PoW and PoS? Why not Proof of Work? Nominated Proof of Stake Probabilistic vs. Provable Finality Hybrid Consensus Block Production: BABE Multiple Validators per Slot No Validators in Slot BADASS BABE: SASSAFRAS Finality Gadget: GRANDPA Protocol Implementation Bridging: BEEFY Fork Choice Nakamoto consensus PBFT / Tendermint Casper FFG ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Advanced Guides for Staking General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Components Staking Introduction to Staking Nomination Pools Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Staking Advanced Guides for Staking Advanced how-to guides about Staking. üìÑÔ∏è Introduction to Staking Overview of Staking and NPoS on Polkadot. üìÑÔ∏è Nomination Pools Staking through Polkadot's Nomination Pools. Previous Consensus Next Introduction to Staking ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Introduction to Staking General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Components Staking Introduction to Staking Nomination Pools Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Staking Introduction to Staking On this page Introduction to Staking New to Staking? Start your staking journey or explore more information about staking on Polkadot's Home Page . Discover the new Staking Dashboard that makes staking much easier and check this extensive article list to help you get started. The dashboard supports Ledger devices natively and does not require an extension or wallet as an interface. Stake through Nomination Pools The minimum amount required to become an active nominator and earn rewards may change from era to era. It is currently . If you have less DOT than the minimum active nomination and still want to participate in staking, you can join the nomination pools. You can now stake on Polkadot natively with just in the nomination pools and earn staking rewards. For additional information, check out this blog post . Check the wiki doc on nomination pools for more information. Here you will learn about what staking is, why it is important and how it works on Polkadot. Proof-of-Stake (PoS) ‚Äã Blockchain networks use consensus mechanisms to finalize blocks on the chain. Consensus is the process of agreeing on something, in this case, the progression of the blockchain or how blocks are added to the chain. Consensus consists of two actions: Block production , i.e. the way multiple blocks candidates are produced, and Block finality , i.e. the way only one block out of many candidates is selected and added to the canonical chain (see this article for more information about finality). Proof-of-Work (PoW) and Proof-of-Stake (PoS) are well-known mechanisms used to reach consensus in a secure and trustless way on public blockchains, where there are many participants who do not know each other (and probably never will). In PoW, network security relies on the fact that the miners who are responsible for adding blocks to the chain must compete to solve difficult mathematic puzzles to add blocks - a solution that has been criticized for the wastage of energy. For doing this work, miners are typically rewarded with tokens. In PoS networks like Polkadot the security of the network depends on the amount of capital locked on the chain: the more the capital locked, the lower the chance of an attack on the network, as the attacker needs to incur a heavy loss to orchestrate a successful attack (more on this later on). The process of locking tokens on the chain is called staking . Similar to the miners in PoW networks, PoS networks have validators , but they do not have to compete with each other to solve mathematical puzzles. They are instead pre-selected to produce the blocks based on the stake backing them. Token holders can lock funds on the chain and for doing so, they are getting staking rewards . There is thus an economic incentive for token holders to become active participants who contribute to the economic security and stability of the network. PoS networks in general are therefore more inclusive than PoW networks, as participants do not need to have either technical knowledge about blockchain technology or experience in running mining equipment. PoS ensures that everybody participating in the staking process has "skin in the game" and thus can be held accountable. In case of misbehavior, participants in the staking process can be punished or slashed , and depending on the gravity of the situation, their stake can be partly or fully confiscated by the network. It is not in a staker's economic interest to orchestrate an attack and risk losing tokens. Any rational actor staking on the network would want to get rewarded, and the PoS network rewards good behavior and punishes bad behavior. Nominated Proof-of-Stake (NPoS) ‚Äã Polkadot implements Nominated Proof-of-Stake (NPoS) , a relatively novel and sophisticated mechanism to select the validators who are allowed to participate in its consensus protocol. NPoS encourages DOT holders to participate as nominators . Any potential validators can indicate their intention to be a validator candidate. Their candidacies are made public to all nominators, and a nominator, in turn, submits a list of up to candidates that it supports, and the network will automatically distribute the stake among validators in an even manner so that the economic security is maximized. In the next era, a certain number of validators having the most DOT backing get elected and become active. For more information about the election algorithm go to this page on the wiki or this research article. As a nominator, a minimum of is required to submit an intention to nominate, which can be thought of as registering to be a nominator. Note that in NPoS the stake of both nominators and validators can be slashed. For an in-depth review of NPoS see this research article. Minimum Nomination to Receive Staking Rewards Although the minimum nomination intent is , it does not guarantee staking rewards. The nominated amount has to be greater than minimum active nomination , which is a dynamic value that can be much higher than . This dynamic value depends on the amount of DOT being staked, in addition to the selected nominations. Nominating Validators ‚Äã Nominating on Polkadot requires 2 actions: Locking tokens on-chain. Selecting a set of validators, to whom these locked tokens will automatically be allocated to. How many tokens you lock up is completely up to you - as are the validators you wish to select. The action of locking tokens is also known as bonding . You can also refer to your locked tokens as your bonded tokens, or staked tokens. Likewise, selecting validators is also known as backing or nominating validators. These terms are used interchangeably by the community. From now on locked tokens will be referred to as bonded tokens. Once the previous 2 steps are completed and you are nominating, your bonded tokens could be allocated to one or more of your selected validators, and this happens every time the active validator set changes. This validator set is updated every era on Polkadot. Unlike other staking systems, Polkadot automatically chooses which of your selected validators will be backed by your bonded tokens. Selecting a group of validators increases your chances of consistently backing at least one who is active. This results in your bonded tokens being allocated to validators more often, which means more network security and more rewards. This is in strong contrast to other staking systems that only allow you to back one validator; if that validator is not active, you as a staker will also not be. Polkadot's nomination model solves this. Polkadot uses tools ranging from election theory to game theory to discrete optimization, to develop an efficient validator selection process that offers fair representation and security, thus avoiding uneven power and influence among validators. The election algorithms used by Polkadot are based on the Proportional Justified Representation (PJR) methods like Phragmen . For more information about PJR methods visit this research article. Eras and Sessions ‚Äã The stake from nominators is used to increase the number of tokens held by such candidates, increasing their chance of being selected by the election algorithm for block production during a specific era . An era is a period of 24 hours during which an active set of validators is producing blocks and performing other actions on the chain. This means that not all validators are in the active set and such set changes between eras. Each era is divided into 6 epochs or sessions during which validators are assigned as block producers to specific time frames or slots . This means that validators know the slots when they will be required to produce a block within a specific session, but they do not know all the slots within a specific era. Having sessions adds a layer of security because it decreases the chance of having multiple validators assigned to a slot colluding to harm the network. Staking Rewards ‚Äã Validators who produce a block are rewarded with tokens, and they can share rewards with their nominators. Both validators and nominators can stake their tokens on chain and receive staking rewards at the end of each era. The staking system pays out rewards equally to all validators regardless of stake. Thus, having more stake in a validator does not influence the amount of block rewards it receives. This avoids the centralization of power to a few validators. There is a probabilistic component in the calculation of rewards, so they may not be exactly equal for all validators. In fact, during each era validators can earn era points by doing different tasks on chain. The more the points, the higher the reward for a specific era. This promotes validators' activity on chain. To know more about era points, and how and on which basis they are distributed visit the dedicated page . Distribution of the rewards is pro-rata to all stakers after the validator's commission is deducted. Skin in the game when Staking ‚Äã The security of PoS networks depends on the amount of staked tokens. To successfully attack the network, a malicious actor would need to accrue a large number of tokens or would need different participants to collude and act maliciously. If there is an attack in the case of NPoS, both the validator(s) and nominators will be slashed resulting in their stake being partially or fully confiscated by the network and then deposited to the treasury. There is little interest for a rational network participant to act in a harmful way because NPoS ensures that all participants can be held accountable for their bad actions. In NPoS, validators are paid equal rewards regardless of the amount of stake backing them, thus avoiding large payouts to few large validators which might lead to centralization. Being a Nominator ‚Äã Tasks and Responsibilities of a Nominator ‚Äã Validators. Since validator slots are limited, most of those who wish to stake their DOT and contribute to the economic security of the network will be nominators, thus here we focus on the role of nominators. However, it is worth mentioning that validators do most of the heavy lifting: they run the validator nodes and manage session keys , produce new block candidates in BABE , vote and come to consensus in GRANDPA , validate the state transition function of parachains, and possibly some other responsibilities regarding data availability and XCM . For more information, you can take a look at the validator docs to understand what you need to do as a validator. If you want to become a validator you can consult this guide. Nominators. Nominators have far fewer responsibilities than validators. These include selecting validators and monitoring their performance, keeping an eye on changing commission rates (a validator can change commission at any time), and general health monitoring of their validators' accounts. Thus, while not being completely set-it-and-forget-it, a nominator's experience is relatively hands-off compared to that of a validator, and even more with nomination pools . For more information, you can take a look at the nominator guide to understanding your responsibilities as a nominator. If you want to become a nominator, see this guide. If you are a beginner and would like to securely stake your tokens using the Polkadot-JS UI, refer to this support article. Polkadot Staking Dashboard The Staking Dashboard provides a more user-friendly alternative to staking. See the instructions in this support article to learn how to stake with the dashboard. Pools. Pools are "built" on top of NPoS to provide a very low barrier to entry to staking, without sacrificing Polkadot's strict security model. Selection of Validators ‚Äã The task of choosing validators is not simple, as it should take into account nominator reward and risk preferences. Ideally one aims to maximize the reward-to-risk ratio by maximizing rewards and minimizing risks, with sometimes having to compromise between the two, as minimizing risks might decrease rewards as well. Nominators should pay attention, especially to six criteria when nominating validators (not in order of importance): recent history of the era points earned across eras validator's self stake (shows skin in the game) total stake backing the validator (which is the sum of self stake and the stake coming from nominators) commission fees (i.e. how much validators charge nominators) verified identity previous slashes The diagram below shows how the selection of those criteria affects the reward-to-risk ratio. Validator Selection Criteria ‚Äã To maximize rewards and minimize risk, one could select those validators that: have era points above average (because they will get more rewards for being active), have the total stake backing the validator below the average active validator stake (because they will pay out more rewards per staked DOT), have high own stake (because if slashed they have something to lose), have low commission fees but not 0% (because it makes sense that for doing the heavy lifting, validators ask for a small commission), have on-chain registered identity (because it adds a layer of trust and possibly provides access to their website and contact details), and have not been slashed (meaning that their on-chain behavior is genuine). Network Providers ‚Äã For successful operation, a Validator node should always be ensured to meet the required software, hardware, and network bandwidth specifications . Understandably, most of the validator nodes run on cloud service providers that guarantee high hardware specifications and high levels of availability and connectivity. Keep in mind that a validator in the active set is supposed to be fully online and available for producing blocks. If the active validator node goes offline due to network interruptions or a power outage, that validator might be subject to slashing due to unresponsiveness . As Polkadot's block production mechanism is reasonably resilient to a small proportion of validators going offline, no slashing is imposed until 10% of the validators in the active set go offline. Hence, if multiple nodes are running on a single cloud service provider and go offline simultaneously due to an outage or due to a change in their terms and conditions policy regarding the support of Proof-of-Stake (PoS) operations, the offline validators and all the nominators backing them can be slashed up 7% of their stake on Polkadot. Hence, it is recommended that you check if you are nominating the validator nodes that are running on cloud service providers, and if they do, check if they allow for Proof-of-Stake operations. Checking Validators using Network Providers You can connect your stash account to the Polkawatch app . The app will show your rewards earned in the past 60 eras divided by network provider and country. You will be able to see networks used by each validator and verify if your validators are using providers who support PoS. This is also a great tool to explore how decentralized your nominations are and act accordingly. Keeping Track of Nominated Validators ‚Äã Nominators must periodically check their validators Nominating is not a "set and forget" operation. The whole NPoS system is dynamic and nominators should periodically monitor the performance and reputation of their validators. Failing to do so could result in applied slashes and/or rewards not being paid out, possibly for a prolonged period. Although the theory can be used as a general guideline, in practice it is more complicated and following the theory might not necessarily lead to the desired result. Validators might have the total stake backing them below average, low commission and above average era points in one era and then have a different profile in the next one. Selection based the criteria like on-chain identity, slash history and low commission make the staking rewards deterministic. But some criteria vary more than others, with era points being the most variable and thus one of the key probabilistic components of staking rewards. Part of this probability is directly related to the fact that a validator can produce blocks for a parachain (i.e. para-validators) or the relay chain, with para-validators earning more era points per unit time (see this page for more information). The role can switch between sessions, and you can look at the staking tab on the Polkadot-JS UI to know which validator is producing blocks for the relay chain or parachains. It is not recommended to change nominations because of the low era points of a validator in a single era. Variability in rewards due to the era points should level out over time. If a validator consistently gets era points below average, it makes sense to nominate a better-performing validator for the health of the network and increased staking rewards. See this support article to understand in detail how to select the set of validators to nominate. Stash Account and Staking Proxy ‚Äã Two different accounts can be used to securely manage your funds while staking. Stash: This account holds funds bonded for staking, but delegates all staking functions to a staking proxy account. You may actively participate in staking with a stash private key kept in a cold wallet like Ledger, meaning it stays offline all the time. Having a staking proxy will allow you to sign all staking-related transactions with the proxy instead of using your Ledger device. This will allow you: to avoid carrying around your Ledger device just to sign staking-related transactions, and to and to keep the transaction history of your stash clean Staking Proxy: This account acts on behalf of the stash account, signaling decisions about nominating and validating. It can set preferences like commission (for validators) and the staking rewards payout account. The earned rewards can be bonded (locked) immediately for bonding on your stash account, which would effectively compound the rewards you receive over time. You could also choose to have them deposited to a different account as a free (transferable) balance. If you are a validator, it can also be used to set your session keys . Staking proxies only need sufficient funds to pay for the transaction fees. danger Never leave a high balance on a proxy account which are usually "hot" as their private key is stored on the device (PC, phone) and it is always exposed to the internet for potential hacks and scams. It is good practice to deposit rewards on the stash account or to send them to another account on a cold wallet. This hierarchy of separate keys for stash and staking accounts was designed to add a layer of protection to nominators and validator operators. The more often one exposes and uses a private key, the higher its vulnerability for hacks or scams. So, if one uses a key for multiple roles on a blockchain network, it is likely that the account can get compromised. Note that the damage linked to stolen private keys is different depending on the type of account derivation. In the case of soft derivation, all derived accounts are compromised. More information about account derivation can be found here . info For Ledger users staking directly on Ledger Live, currently, there is no option to use separate stash and staking proxy accounts. Ledger devices are now supported in Talisman extension. Users can import their Ledger accounts in the extension and use them as a stash in staking. You can find more information about Talisman and other wallets that officially secured funding from the treasury here . Claiming Staking Rewards ‚Äã Rewards are calculated per era (approximately six hours on Kusama and twenty-four hours on Polkadot). These rewards are calculated based on era points, which have a probabilistic component. In other words, there may be slight differences in your rewards from era to era, and even amongst validators in the active set at the same time. These variations should cancel out over a long enough timeline. See the page on Validator Payout Guide . The distribution of staking rewards to the nominators is not automatic and needs to be triggered by someone. Typically the validators take care of this, but anyone can permissionlessly trigger rewards payout for all the nominators whose stake has backed a specific validator in the active set of that era. Staking rewards are kept available for 84 eras. The following calculation can be used to approximate this length in days on Polkadot: 84 eras √ó 24 hours in a single era √∑ 24 hours in a day = 84 days For more information on why this is so, see the page on simple payouts . Payouts Payouts are unclaimed rewards waiting to be paid out to both validators and nominators. If you go to the Staking payouts page on Polkadot-JS , you will see a list of all validators that you have nominated in the past 84 eras and for which you have not yet received a payout. The payout page is visible only to stakers. Each validator as well as their nominators have the option to trigger the payout for all unclaimed eras. Note that this will pay everyone who was nominating that validator during those eras. Therefore, you may not see anything in this tab, yet still have received a payout if somebody (generally, but not necessarily, another nominator or the validator operator) has triggered the payout for that validator for that era. Time limit to claim staking rewards If nobody claims your staking rewards within 84 eras, then you will not be able to claim them and they will be lost. Additionally, if the validator unbonds all their own stake, any pending payouts will also be lost. Since unbonding takes days, nominators should check if they have pending payouts at least this often. Rewards can be directed to the same account used to sign the payout or to a completely unrelated account. It is also possible to top-up / withdraw some bonded tokens without having to un-stake all staked tokens. If you wish to know if you received a payout, you will have to check via a block explorer. See the relevant Support page for details. For specific details about validator payouts, please see this guide . Slashing ‚Äã Slashing will happen if a validator misbehaves (e.g. goes offline, attacks the network, or runs modified software) in the network. They and their nominators will get slashed by losing a percentage of their bonded/staked DOT. Any slashed DOT will be added to the Treasury . The rationale for this (rather than burning or distributing them as rewards) is that slashes may then be reverted by the Council by simply paying out from the Treasury. This would be useful in situations such as faulty slashes. In the case of legitimate slashing, it moves tokens away from malicious validators to those building the ecosystem through the normal Treasury process. Validators with a larger total stake backing them will get slashed more harshly than less popular ones, so we encourage nominators to shift their nominations to less popular validators to reduce their possible losses. It is important to realize that slashing only occurs for active validations for a given nominator, and slashes are not mitigated by having other inactive or waiting nominations. They are also not mitigated by the validator operator running separate validators; each validator is considered its own entity for purposes of slashing, just as they are for staking rewards. In rare instances, a nominator may be actively nominating several validators in a single era. In this case, the slash is proportionate to the amount staked to that specific validator. With very large bonds, such as parachain liquid staking accounts, a nominator has multiple active nominations per era (Acala's LDOT nominator typically has 7-12 active nominations per era). Note that you cannot control the percentage of stake you have allocated to each validator or choose who your active validator will be (except in the trivial case of nominating a single validator). Staking allocations are controlled by the Phragm√©n algorithm . Once a validator gets slashed, it goes into the state as an "unapplied slash". You can check this via Polkadot-JS UI . The UI shows it per validator and then all the affected nominators along with the amounts. While unapplied, a governance proposal can be made to reverse it during this period (7 days on Kusama, 28 days on Polkadot). After the grace period, the slashes are applied. The following levels of offense are defined . However, these particular levels are not implemented or referred to in the code or in the system; they are meant as guidelines for different levels of severity for offenses. To understand how slash amounts are calculated, see the equations in the section below. Level 1: isolated unresponsiveness , i.e. being offline for an entire session. Generally no slashing, only chilling . Level 2: concurrent unresponsiveness or isolated equivocation , slashes a very small amount of the stake and chills. Level 3: misconducts unlikely to be accidental, but which do not harm the network's security to any large extent. Examples include concurrent equivocation or isolated cases of unjustified voting in GRANDPA . Slashes a moderately small amount of the stake and chills. Level 4: misconduct that poses serious security or monetary risk to the system, or mass collusion. Slashes all or most of the stake behind the validator and chills. If you want to know more details about slashing, please look at our research page . Chilling ‚Äã Chilling is the act of stepping back from any nominating or validating. It can be done by a validator or nominator at any time, taking effect in the next era. It can also specifically mean removing a validator from the active validator set by another validator, disqualifying them from the set of electable candidates in the next NPoS cycle. Chilling may be voluntary and validator-initiated, e.g. if there is a planned outage in the validator's surroundings or hosting provider, and the validator wants to exit to protect themselves against slashing. When voluntary, chilling will keep the validator active in the current session, but will move them to the inactive set in the next. The validator will not lose their nominators. When used as part of a punishment (initiated externally), being chilled carries an implied penalty of being un-nominated. It also disables the validator for the remainder of the current era and removes the offending validator from the next election. Polkadot allows some validators to be disabled, but if the number of disabled validators gets too large, Polkadot will trigger a new validator election to get a full set. Disabled validators will need to resubmit their intention to validate and re-garner support from nominators. For more on chilling, see the " How to Chill " page on this wiki. Fast Unstake ‚Äã Fast Unstaking feature is live! If you accidentally bonded your DOT or your bonded DOT never backed any active validator, you can now unbond them immediately. If your bonded balance did not back any validators in the last 28 days on Polkadot, you are eligible to perform fast unstaking. The staking dashboard will automatically check if you qualify. For more information, visit the "Fast Unstake" section in this support article . Why and Why not to Stake? ‚Äã Pros of Staking ‚Äã Earn rewards for contributing to the network's security through staking. Low barrier of entry through Nomination Pools . Can choose up-to validators which can help to decentralize the network through the sophisticated NPoS system 10% inflation/year of the tokens is primarily intended for staking rewards. When the system staking rate matches with the ideal staking rate, the entire inflation of the network is given away as the staking rewards. Up until now, the network has been following an inflation model that excludes the metric of active parachains. The ideal staking rate is a dynamic value - as the number of active parachains influences the available liquidity that is available to secure the network. Any divergence from the ideal staking rate will result in the distribution of a proportion of the newly minted tokens through inflation to go to the treasury. Keep in mind that when the system's staking rate is lower than the ideal staking rate, the annual nominal return rate will be higher, encouraging more users to use their tokens for staking. On the contrary, when the system staking rate is higher than the ideal staking rate, the annual nominal return will be less, encouraging some users to withdraw. For in-depth understanding, check the inflation section on the Wiki. Cons of Staking ‚Äã Tokens will be locked for about 28 days on Polkadot. No rewards will be earned during the unbonding period. Possible punishment in case of the active validator found to be misbehaving (see slashing ). Lack of liquidity i.e. You would not be able to use the tokens for participating in crowdloans or transfer them to different account etc. How many Validators? ‚Äã Polkadot currently has validators. The top bound on the number of validators has not been determined yet, but should only be limited by the bandwidth strain of the network due to peer-to-peer message passing. The estimate of the number of validators that Polkadot will have at maturity is around 1000. Kusama is already operating at this threshold. Why am I not receiving rewards? ‚Äã Nominating on Polkadot is not a set-and-forget action. Nominators need to monitor their nominations and ensure they are eligible to receive staking rewards. Otherwise, they would be risking their funds to secure the chain with no reward. If you are bonding significantly more than the Minimum Active Bond and yet not receiving rewards, your nominations are all waiting, or your active validator has 100% commission. However, if you bond funds close to the Minimum Active Bond, there could be several possibilities for not receiving staking rewards. The table below can be used to troubleshoot why you might not be receiving staking rewards using Polkadot-JS UI. Nomination Status What's happening? Causes What to do? Nominated validators are all in waiting status. Your stake has not been assigned to any of the nominated validators. You cannot earn rewards, nor be slashed in that era. Waiting validators are not in the active set in the current era and the stake backing them is not used to secure the network. In simple words, NPoS "does not see them". Change your nominations. Try to select validators (with reasonable commission) that have high chances to end up in the active set. You have some inactive, and some waiting nominations. Validators shown as "Inactive" in your staking dashboard are still in the active set and are producing blocks in the current era, but your stake has not been assigned to any of them. You will not earn rewards if your stake is not backing an active validator. In this case, you cannot be slashed either. Scenario 1: You have bonded less than the Minimum Active Bond. Scenario 2: You have more than the Minimum Active Bond, but your account is at the tail end of the bags list and within your bag there are acounts with less stake than you, in front of you. Scenario 1: Try bonding more funds. Scenario 2: Try to put your account in front of the accounts with less stake than you. Instructions available here You have one active validator. Active validators are producing blocks in the current era, and your stake has been assigned to them. Even if you are not earning rewards, you can be slashed. Your validator is oversubscribed, meaning that it has more than 512 nominators (ranked by stake), and your stake is less than that of those nominators. You can try to select validators that are not oversubscribed but in the long term you might want to bond more funds (even more than the Minimum Active Bond) to increase the chance of earning rewards also with oversubscribed validators. Join a Nomination Pool By joining a nomination pool that is active and earning rewards, you can start earning staking rewards with as low as 1 DOT. The nomination pools typically have a dedicated pool operator who ensures that the pool's stake is always backing an active validator and is receiving rewards. info You can find information about why you might not receive staking rewards on this support page . Staking FAQ ‚Äã info See this support page for the FAQs about staking. Resources ‚Äã How Nominated Proof of Stake will work in Polkadot - Blog post by Web3 Foundation researcher Alfonso Cevallos covering NPoS in Polkadot. Validator setup Edit this page Last updated on Jul 14, 2023 by Filippo Previous Staking Next Nomination Pools Proof-of-Stake (PoS) Nominated Proof-of-Stake (NPoS) Nominating Validators Eras and Sessions Staking Rewards Skin in the game when Staking Being a Nominator Tasks and Responsibilities of a Nominator Selection of Validators Stash Account and Staking Proxy Claiming Staking Rewards Slashing Chilling Fast Unstake Why and Why not to Stake? Pros of Staking Cons of Staking How many Validators? Why am I not receiving rewards? Staking FAQ Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Nomination Pools General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Accounts Transactions Tokens and Assets NFTs Components Staking Introduction to Staking Nomination Pools Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Basics Staking Nomination Pools On this page Nomination Pools Nomination Pools are live on Polkadot! Nomination pools are a new feature for Polkadot‚Äôs staking system that allows users to pool their DOT tokens together on-chain to nominate validators and receive rewards, significantly improving the system‚Äôs scalability. Now, anyone with as little as 1 DOT can receive rewards for staking natively on Polkadot . Note that rewards are not guaranteed for those pools that do not have enough bonded funds to be included within the bags list . Only members of active pools will receive rewards. There are currently members. (There can be a maximum of members.) There are currently pools. (There can be a maximum of pools) No limit on members per pool. note Learn the key differences between Staking directly vs Joining a Nomination Pool . For Ledger users: Joining a nomination pool is possible only with the XL version of the Polkadot Ledger App. This should be installed by default on Ledger Nano X and S Plus, but not on the Nano S. If you become a nomination pool member or a pool admin, you cannot participate in Governance with the bonded tokens in the pool, as they are held in a system account . Have questions on Nomination Pools? Please join the Polkadot Discord for asking general questions about Nomination Pools. If you are a developer, please join our nomination pools support channel . Nomination pools are one of the key features from the roadmap of staking improvements on Polkadot. They are designed to permissionlessly allow members to pool their funds together and act as a single nominator account. Due to the current runtime constraints, Polkadot can only handle nominators comfortably in the electing set . As one of the objectives of the NPoS algorithm is to maximize the overall stake on the network, it can be inferred that the staking system on Polkadot favors nominators with a larger stake. Only the nominator accounts which back the validators in the active set are eligible for receiving staking rewards. This leaves out nomination intents from the accounts with lower token balance than the min-active nomination and places them in a waiting queue to enter electing set. Nomination pools will be handy for members who want to participate in the staking system with a stake much lower than the dynamic min-active nomination threshold on the network. All operations are constant space and time complexity relative to the number of members, eliminating any theoretical upper bound on the number of members the system can handle and thus scaling the number of accounts that can participate and earn rewards in the staking system on Polkadot. In summary, each nomination pool is viewed as a single nominator from the NPoS system point of view. Why aren't the members in the nomination pools called delegators? The term delegator is associated too much with Delegated Proof of Staking (DPoS), and since Polkadot implements Nominated Proof of Staking (NPoS), naming them delegators would be misleading. The term member is our generic replacement for delegator . In action, members are quite similar to delegators and delegate their nomination power to the pool. The pool‚Äôs earnings are split pro rata to a member's stake in the bonded pool (and thus, the staking rewards for members will be the same as if they were a nominator). Importantly, slashes are also applied proportionally to members who may have been actively bonded. Key Components ‚Äã Bonded Pool: Tracks the distribution of actively staked funds. Reward Pool: Tracks rewards earned by actively staked funds. Unbonding Sub Pools: Collection of pools at different phases (i.e. eras) of the unbonding lifecycle. Members: Accounts that nominate to the pools. Point: Unit of measure for a member‚Äôs portion of a pool's funds. All pools start with a point to Planck ratio of 1. Over time, if the pool receives rewards, they increase in value, and if the pool is slashed, it decreases in value. Pool Member Lifecycle ‚Äã Join a pool ‚Äã A member delegates funds to a pool by transferring some amount to the pool‚Äôs bonded account with the join extrinsic. The pool then increases its bond with the new funds. A member is afforded the ability to bond additional funds or re-stake rewards as long as they are already actively bonded. Note that a member may only belong to one pool at a time. The current minimum bond to join a pool on Polkadot is . info The funds nominated to a pool will not be visible in the member's account balance on Polkadot JS Apps UI. This is because the member funds are transferred from their account to the pool's system account . This pool account is not accessible by anyone (including the pool root or depositor) and only the pool's internal logic can access the account. Use Non-Transfer or Nomination Pools Proxy Accounts to join Nomination Pools Only non-transfer proxies and nomination pools proxy can be used to participate in nomination pools. Staking proxies cannot be used as they cannot make calls to the nomination pools pallet. (The nomination pools will be supported through a staking proxy when the changes made in this PR are released on the network. Thus, depending on how much control you want to give your proxy, you might choose between non-transfer > staking > nomination pool proxy, with the latter being only able to sign transactions related to the NominationPool pallet. Check the "How to join a pool" section in this support article for guidelines. Claim rewards ‚Äã The member can claim their portion of any rewards that have accumulated since the previous time they claimed (or in the case that they have never claimed, any rewards that have accumulated since the era after they joined). Rewards are split pro rata among the actively bonded members. Check the "How to claim rewards" section in this support article for guidelines. Claim Permissions ‚Äã As a pool member, you can grant permission to any other account to claim and compound rewards on your behalf. There are four permission options: Permissioned (default): you need to claim and compound your rewards. PermissionlessCompound : you grant permission to any other account to compound (claim and bond) your rewards on your behalf. PermissionlessWithdraw : you grant permission to any other account to withdraw (claim and keep as a free balance) your rewards on your behalf. PermissionlessAll : you grant permission to any other account to compound or withdraw your rewards on your behalf. See the Staking Dashboard page for more information about how to set your claim permissions. See the advanced guides to learn how to claim rewards for another pool member. Unbond and withdraw funds ‚Äã At any point in time after joining the pool, a member can start the process of exiting by unbonding. unbond will unbond part or all of the member's funds. After unbond has been called and the unbonding duration has passed ( eras which correspond to days on Polkadot), a member may withdraw their funds with withdrawUnbonded . Withdrawing effectively ends a member's relationship with their pool, allowing them to join a different pool if desired. Check the "Withdraw unbonded funds" section in this support article for guidelines. Unbonding transaction automatically triggers withdrawal of rewards When there is a change in the bonded balance, the accumulated rewards in the pool thus far are automatically withdrawn to the account. The rewards are then accrued based on the updated bonded balance. Limitations of Nomination Pools ‚Äã A member cannot vote (e.g. in Referenda or for Council members) with their nominated funds. This may be changed in the future once accounts are afforded the ability to split votes. For a member to switch pools, all funds from the account must be unbonded. This process takes 28 eras. A member can partially unbond the staked funds in the pool (at most 16 partial unbonds). Pool Administration ‚Äã States ‚Äã Open: The pool is open to be joined by anyone. Blocked: The pool is blocked; no joiners are permitted. Destroying: The pool is in the process of being destroyed. Once in this state, the pool may never revert to any other state; it can only proceed to be destroyed. All members can be permissionlessly unbonded; this allows the pool to be dismantled regardless of any member‚Äôs proactivity. Roles ‚Äã Depositor: Creates the pool and is the initial member. The depositor can only leave the pool once all other members have left. Once they leave by withdrawing, the pool is fully removed from the system. Nominator: Can select the validators the pool nominates. Bouncer: Can change the pool‚Äôs state and kick (permissionlessly unbond/withdraw) members if the pool is blocked. Root: Can change the nominator, bouncer, or itself. Further, it can perform any of the actions the nominator or bouncer can. Pool Commissions ‚Äã Live on Kusama Pool commissions are currently live on Kusama. On Polkadot Referendum 55 will enable a globalMaxCommission of 10% on Polkadot and enable pool commission. As the pool root role, you can set pool commissions that will be applied to the staking rewards paid out to the pool's system account before rewards are allocated for the pool members. You can set pool commissions through the Polkadot Staking Dashboard . Three methods can be used when setting the pool commission: Commission Rate ( nominationPools.setCommission extrinsic): the start or new commission rate ( newCommission parameter) that can be set between 0% and the Max Commission (decided through governance referendum ). You will need to specify an Input Payee Account, i.e. the account that will receive the commission. Max Commission ( nominationPools.setCommissionMax extrinsic): the maximum commission ( maxCommission parameter) the pool will apply to its members (between 0% and Max Commission). Note that once set, the pool admin can only lower it . Change Rate ( nominationPools.setCommissionChangeRate extrinsic): the maximum rate increase ( maxIncrease parameter) allowed for a single commission update. Note that once set, the pool admin can only lower it . When setting the Change Rate, it will also be possible to set a minDelay quantified as the number of blocks (since last commission update) after which it is possible to change the commission (i.e. the minimum delay between commission updates). Note that once set, the pool admin can only increase it . Max Commission and Change Rate must not be necessarily set. It is the choice of the pool admin to set those parameters and provide transparency to the pool members about the pool's commission policy. Max Commission and Change Rate are currently permanent Once the Max Commission and the Change Rate are set, the pool admin currently can only decrease those values. The minimum delay between commission updates can only be increased. The situation can change in the future and a forceSetCommissionMax method can be proposed through governance referendum. Let's take, for example, Pool A, which sets the Commission Rate to 10%, the Max Commission to 100%, and the Change Rate to 1% every 300 blocks (which equates to approximately 30 minutes). The following statements are true: The pool commission can be increased by 1% every 30 minutes. Bigger increases are not allowed. Increases of less than or equal to 1% are not allowed sooner than 30 minutes since the last commission update. The Max Commission can only be decreased from 100%. Once decreased, it can be decreased again but it cannot be increased. The Change Rate's maximum increase can only be decreased from 1%. Once decreased, it can be decreased again but it cannot be increased. The Change Rate's minimum delay between updates of 30 min can only be increased. Once increased, it can be increased again but it cannot be decreased. Pool Lifecycle ‚Äã Advanced How-to Guides See this page for more information about the lifecycle of nomination pools. The cycle includes creation, upkeep and destruction. Nomination Pools - Slashing ‚Äã Suppose the staking system slashes a pool‚Äôs underlying nomination account. In that case, the slash is distributed evenly across the bonded pool, and the unbonding pools from slash era+1 through the slash apply era. Thus, any member who either a) was unbonding or b) was actively bonded in the aforementioned range of eras will be affected by the slash. In other words, a member who may have been actively bonded during the offence is slashed pro rata based on its stake relative to the total slash amount. Unbonding pools need to be slashed to ensure all nominators who were in the bonded pool while it was backing a validator that committed an offense are punished. Without these measures a nominator could unbond right after a validator equivocated with no consequences. This strategy is unfair to members who joined after the slash because they get slashed as well, but it spares members who unbond. The latter is much more important for security: if a pool's validators attack the network, their members need to unbond fast! Avoiding additional slashes gives them an incentive to do that if validators get repeatedly slashed. Nominating vs Joining a Pool ‚Äã Nominating is the action of choosing validators. It does not simply involve bonding tokens. Nominating is an active task, which implies that you regularly monitor that your stake is backing an active validator in all the eras and check if you are receiving your staking rewards. More importantly, ensure that the validators you chose always act in the best interests of the network protocol and have less chance of getting slashed. To nominate, you need a minimum of , and to receive rewards, you need at least a balance greater than the minimum active bond. Depending on your validators, if your active validator is oversubscribed, you will earn rewards only if your stake is within that of the top nominators. If the validator misbehaves, It is worth noting that your stake is subject to slashing, irrespective of whether you are at the top nominators or not. As the minimum active bond is a dynamic value, it can make your nomination inactive when the threshold goes above your bonded balance. Hence, to be eligible to earn rewards while nominating, you would need to stake a much higher balance than the minimum active bond. Nomination pools are a way to participate in staking with as little as 1 DOT and earn staking rewards. Nomination pools differ from custodial solutions (like staking through central exchanges) because they are non-custodial, native to Polkadot's protocol, permissionless, transparent, and run in a decentralized way by the community. Before joining a nomination pool, you must ensure that the pool is earning rewards and nominating the validators that match your preferences. Participating in pools is more of a set-and-forget action than nominating by yourself. The pool operator maintains the list of validators nominated by the pool, and so, in a way, you are trusting the pool operator to act in your best interests. However, it is advised to check the validators nominated by the pool from time to time and change the pool if necessary. Minimum Active Nomination Value is Dynamic The minimum amount required to become an active nominator and earn rewards is . If you have less DOT than the minimum active nomination and still want to participate in staking, you can join the nomination pools. You can now stake on Polkadot natively with just in the nomination pools and earn staking rewards. For additional information, see this blog post . Check the wiki doc on nomination pools for more information. Nominating Joining a Pool Minimum 250 DOT to nominate. Minimum 1 DOT to be a member. Rewards can be compounded automatically or sent to any account. Rewards can be manually claimed to the pool member's account and be bonded in the pool again to compound them. If the active validator gets slashed, all active nominators are subjected to slashing, also those that do not receive rewards due to the oversubscription issue. If the active validator gets slashed, all pool members are subjected to slashing. Can bond and stake DOT indefinitely. Can bond and stake DOT until the pool exists. Unbonding period of 28 days. Can switch validators without unbonding. Unbonding period of 28 days. Need to unbond before switching to a different pool. Maximum uncapped. Maximum uncapped. Should bond more than the minimum active nomination in an era to be eligible to earn staking rewards, although it can depend on multiple other factors outlined in the linked document. A nomination pool earns rewards in an era if it satisfies all the conditions mentioned for the nominator (as the nomination pool is just a nominator from the NPoS system perspective). Staked tokens can be used for participation in Governance. Staked tokens cannot be used for participation in Governance. Rewards payout can be triggered permissionlessly by anyone (typically done by the validator). The pool member must claim the rewards. Bonded funds remain in your account. Bonded funds are transferred to a pool account which is administered by the network protocol and is not accessible to anyone else. See System Accounts for more information. Nominator manages the list of staked validators (up to 16). Nominations managed by the pool operator. Edit this page Last updated on Jul 11, 2023 by Filippo Previous Introduction to Staking Next Advanced Key Components Pool Member Lifecycle Join a pool Claim rewards Claim Permissions Unbond and withdraw funds Limitations of Nomination Pools Pool Administration States Roles Pool Commissions Pool Lifecycle Nomination Pools - Slashing Nominating vs Joining a Pool ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Advanced Concepts General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced Advanced Concepts Advanced concepts to better understand Polkadot. üìÑÔ∏è Accounts Advanced Concepts about Polkadot's Accounts. üìÑÔ∏è Advanced Staking Concepts Advanced Concepts about Staking on Polkadot. üìÑÔ∏è NFT Pallets Functionalities of the NFT Pallets in the Polkadot Ecosystem. üìÑÔ∏è Cryptography Cryptographic Functions used in Polkadot. üìÑÔ∏è NPoS Election Algorithms The Election Methods used in Polkadot's NPoS Mechanism. üóÉÔ∏è How-to Guides Advanced Guides about Staking, Asset Hub, Vault App, and more. Previous Nomination Pools Next Accounts ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Accounts In-Depth General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced Accounts On this page Polkadot Accounts In-Depth Address Format ‚Äã The address format used in Substrate-based chains is SS58. SS58 is a modification of Base-58-check from Bitcoin with some minor changes. Notably, the format contains an address type prefix that identifies an address belonging to a specific network. For example: Polkadot addresses always start with the number 1 . Kusama addresses always start with a capital letter, such as C, D, F, G, H, J . Generic Substrate addresses always start with the number 5 . These prefixes, including how to validate addresses, are embedded in the Substrate SS58 format. Never use regular expressions for address validation. It's important to understand that different network formats are merely other representations of the same public key in a private-public keypair generated by an address generation tool. As a result, the addresses across Substrate-based chains are compatible if the format is converted correctly. As of Runtime 28, the default address format is the MultiAddress type. This enum is a multi-format address wrapper for on-chain accounts and allows us to describe Polkadot's default address format to represent many different address types. This includes 20 byte , 32 byte , and arbitrary raw byte variants. It also enhances the original indices lookup. info Many wallets allow you to convert between formats. Stand-alone tools exist as well; you can find them in the address conversion tools section. Derivation Paths ‚Äã If you want to create and manage several accounts on the network using the same seed, you can use derivation paths. We can think of the derived accounts as child accounts of the root account created using the original mnemonic seed phrase. Many Polkadot key generation tools support hard and soft derivation. For instance, if you intend to create an account to be used on the Polkadot chain, you can derive a hard key child account using // after the mnemonic phrase. 'caution juice atom organ advance problem want pledge someone senior holiday very//0' and a soft key child account using / after the mnemonic phrase 'caution juice atom organ advance problem want pledge someone senior holiday very/0' If you want to create another account using the Polkadot chain using the same seed, you can change the number at the end of the string above. For example, /1 , /2 , and /3 will create different derived accounts. You can use any letters or numbers in the derivation path as long as they make sense to you; they do not have to follow any specific pattern. You may combine multiple derivations in your path, as well. For instance, //bill//account//1 and //john/polkadot/initial are both valid. To recreate a derived account, you must know both the seed and the derivation path, so you should either use a well-defined sequence (e.g. //0, //1, //2...) or be sure to write down any derivation paths you use. info It is only possible to generate a derived account by knowing the derivation path. There is an additional type of derivation called password derivation. On Polkadot you can derive a password key account using /// after the mnemonic phrase 'caution juice atom organ advance problem want pledge someone senior holiday very///0' In this type of derivation, if the mnemonic phrase leaks, accounts cannot be derived without the initial password. In fact, for soft- and hard-derived accounts, if someone knows the mnemonic phrase and the derivation path, they will have access to your account. For password-derived accounts, the password is applied on the derivation path. You can know the mnemonic phrase and the derivation path, but without the password, it is impossible to access the account. In mathematical terms, if we have a written derivation path and a password , we can calculate the real derivation path as f(written derivation path, password) , where f is a function. We can then calculate the account key pair using f(seed, real derivation path) . Unlike hard and soft derivations that can be mixed, only a single password should be specified per derivation. info Password-derived account are as secure as the chosen password. Soft vs. Hard Derivation ‚Äã A soft derivation allows someone to potentially "go backward‚Äù to figure out the initial account's private key if they know the derived account's private key. It is also possible to determine that different accounts generated from the same seed are linked to that seed. A hard derivation path does not allow either of these - even if you know a derived private key, it's not feasible to figure out the private key of the root address, and it's impossible to prove that the first account is linked with the second. These derivation methods have their use cases, given that the private keys for all the derived accounts are fully secure. Unless you have a specific need for a soft derivation, it is recommended to generate the account using a hard derivation path. See the Subkey documentation for details and examples of derivation path formats. The Polkadot-JS Apps and Extension and Parity Signer support custom derivation paths using the same syntax as Subkey. Some wallets will automatically add derivation paths to the end of the generated mnemonic phrase. This will generate separate seeds for different paths, allowing separate signing keys with the same mnemonic, e.g. <mnemonic phrase>//polkadot and <mnemonic phrase>//kusama . Although you may correctly save the mnemonic phrase, using it in another wallet will generate the same addresses only if both wallets use the same derivation paths. Polkadot and Kusama both have paths registered in the BIP44 registry . danger You must have the parent private key and the derivation path to arrive at the key for an address. Only use custom derivation paths if you are comfortable with your knowledge of this topic. For the Curious: How Prefixes Work ‚Äã The SS58 registry states that: Polkadot has an address type of 00000000b ( 0 in decimal). Kusama (Polkadot Canary) has an address type of 00000010b ( 2 in decimal). Generic Substrate has 00101010b as the address type ( 42 in decimal). Because the Base58-check alphabet has no number 0, the lowest value is indeed 1. So 00000000b is 1 in Base58-check. If we try to decode a Polkadot address like 1FRMM8PEiWXYax7rpS6X4XZX1aAAxSWx1CrKTyrVYhV24fg , the result is 000aff6865635ae11013a83835c019d44ec3f865145943f487ae82a8e7bed3a66b29d7 . The first byte is 00 , which is indeed 00000000 in binary and 0 in decimal and thus matches the address type of Polkadot. Let's take a look at Substrate addresses. If we decode 5CK8D1sKNwF473wbuBP6NuhQfPaWUetNsWUNAAzVwTfxqjfr , we get 2a0aff6865635ae11013a83835c019d44ec3f865145943f487ae82a8e7bed3a66b77e5 . The first byte is 2a which when converted from hex to decimal is 42. 42 is 00101010 in binary, just as the SS58 document states. Finally, let's look at Kusama addresses. Decoding CpjsLDC1JFyrhm3ftC9Gs4QoyrkHKhZKtK7YqGTRFtTafgp gives us 020aff6865635ae11013a83835c019d44ec3f865145943f487ae82a8e7bed3a66b0985 with the first byte being 02 , just as specified. If we try a Kusama address that starts with a completely different letter, like J4iggBtsWsb61RemU2TDWDXTNHqHNfBSAkGvVZBtn1AJV1a , we still get 02 as the first byte: 02f2d606a67f58fa0b3ad2b556195a0ef905676efd4e3ec62f8fa1b8461355f1142509 . It seems counterintuitive that some addresses always have the same prefix and others like Kusama can vary wildly, but it's just a quirk of Base58-check encoding. System Accounts ‚Äã As the word suggests, system accounts are used by the system. They are used, for example, for the treasury, crowdloans, and nomination pools. From the point of view of the runtime, these accounts are like any other account on-chain. These special system accounts are just public keys, with the private key being unknown (and unattainable). So, that means that only the pallet itself can interact with this account. These accounts can never issue a signed extrinsic since they do not have a private key. info You can view system accounts on Subscan . Let us take a look at how system accounts are generated under the hood. For instance, to generate the treasury account, the raw bytes of the strings "modl" and "py/trsry" are combined to create the AccountID . For more information, check the post on Substrate StackExchange on Treasury accounts . Similarly, to generate the crowdloan account, the raw bytes of the strings "modl" and "py/cfund" along with the fund index are combined to create the AccountID . Similar logic applies to nomination pool and parachain accounts as well. Portability ‚Äã The above information brings us to portability: the ability to use a mnemonic phrase or seed across multiple wallets. Portability depends on several factors: Derivation path Mnemonic format Seed derivation Signature scheme To use the exact mnemonic across multiple wallets, ensure they follow compatible methods for generating keys and signing messages. If you are still looking for understandable documentation, contact the project maintainers. Mnemonic Format Derivation Path Seed Derivation Signature Support Polkadot{.js} Extension Standard User-Defined BIP32 sr25519 Polkadot-JS Apps Standard * User-Defined BIP32 sr25519, ed25519, secp256k Ledger BIP39 BIP44 ‚Ä† BIP32 ‚Ä° ed25519 ¬ß Subkey Standard * User-Defined BIP32 sr25519, ed25519, secp256k1 * Ed25519 keys have limited compatibility with BIP39. ‚Ä† BIP44 Registry ‚Ä° Ed25519 and BIP32 based on Khovratovich ¬ß Sr25519 planned Address Conversion Tools ‚Äã You can use the tools below to convert any SS58 address for any network for use on different networks handy subscan tool simple address converter How to Verify a Public Key's Associated Address ‚Äã You can verify your public key's associated address through a series of inspection steps, where the key is a base-16 (hexadecimal) address. Using Subkey to Retrieve Public Key from SS58 Address ‚Äã This is to showcase that the SS58 address is based on the public key (aka "Account ID") The Subkey Tool's The Inspecting Keys the section explains how to use the inspect command to recalculate your key pair's public key and address. Start by inspecting your account's Polkadot address by running the inspect command against your account's address: $ subkey inspect 1a1LcBX6hGPKg5aQ6DXZpAHCCzWjckhea4sz3P1PvL3oc4F Public Key URI ` 1a1LcBX6hGPKg5aQ6DXZpAHCCzWjckhea4sz3P1PvL3oc4F ` is account: Network ID/version: polkadot Public key ( hex ) : 0x192c3c7e5789b461fbf1c7f614ba5eed0b22efc507cda60a5e7fda8e046bcdce Account ID: 0x192c3c7e5789b461fbf1c7f614ba5eed0b22efc507cda60a5e7fda8e046bcdce SS58 Address: 1a1LcBX6hGPKg5aQ6DXZpAHCCzWjckhea4sz3P1PvL3oc4F Take note of the hexadecimal string for "Public key (hex)". This is your account's public key. Running the inspect command on your public key along with the --public parameter the SS58 address for the default network (substrate) is returned. $ subkey inspect --public 0x192c3c7e5789b461fbf1c7f614ba5eed0b22efc507cda60a5e7fda8e046bcdce Network ID/version: substrate Public key ( hex ) : 0x192c3c7e5789b461fbf1c7f614ba5eed0b22efc507cda60a5e7fda8e046bcdce Account ID: 0x192c3c7e5789b461fbf1c7f614ba5eed0b22efc507cda60a5e7fda8e046bcdce Public key ( SS58 ) : 5CdiCGvTEuzut954STAXRfL8Lazs3KCZa5LPpkPeqqJXdTHp SS58 Address: 5CdiCGvTEuzut954STAXRfL8Lazs3KCZa5LPpkPeqqJXdTHp Using the --network flag, you can define the network that you would like to inspect, where the SS58 address will be based on that network. Now, running the inspect command with --network polkadot return your original Polkadot address, thus verifying the public key. $ subkey inspect --network polkadot 5CdiCGvTEuzut954STAXRfL8Lazs3KCZa5LPpkPeqqJXdTHp Public Key URI ` 5CdiCGvTEuzut954STAXRfL8Lazs3KCZa5LPpkPeqqJXdTHp ` is account: Network ID/version: polkadot Public key ( hex ) : 0x192c3c7e5789b461fbf1c7f614ba5eed0b22efc507cda60a5e7fda8e046bcdce Account ID: 0x192c3c7e5789b461fbf1c7f614ba5eed0b22efc507cda60a5e7fda8e046bcdce Public key ( SS58 ) : 1a1LcBX6hGPKg5aQ6DXZpAHCCzWjckhea4sz3P1PvL3oc4F SS58 Address: 1a1LcBX6hGPKg5aQ6DXZpAHCCzWjckhea4sz3P1PvL3oc4F You will notice that the Subkey Tool recognizes the correct address network and returns the associated public key. The public key is returned as a hexadecimal string (i.e. prefixed with "0x" ). For both SS58 addresses, the same public key is returned. Address Verification ‚Äã Consider the following example: ‚Äã If you are comfortable enough to distinguish between each account parameter, you can prefix the public-key string with "0x" on your own: From: Pay DOTs to the Polkadot account:192c3c7e5789b461fbf1c7f614ba5eed0b22efc507cda60a5e7fda8e046bcdce , we prefix the address by "0x" -> 0x192c3c7e5789b461fbf1c7f614ba5eed0b22efc507cda60a5e7fda8e046bcdce . Using the handy subscan tool , you can verify both address associations to your public key. Copy your public key into the "Input Account or Public Key" textbox and click "Transform" at the bottom. On the right-hand side, the addresses for Polkadot and Substrate that are returned based on your public key should match the ones you inspected. note You may have to scroll down to the bottom of the menu to find the Substrate address based on the menu listings. You will notice that many networks that also use the same Substrate address. You can verify your public key verification by recalling that Polkadot addresses start with a '1', whereas Substrate addresses generally start with a '5' (Kusama addresses start with a capital letter). See Addresses for more details. Furthermore, the Utility Scripts can be referenced for how the verification is performed: pubkeyToAddress.js demonstrates how a single public key interprets a Polkadot, Substrate, or Kusama address. Indices ‚Äã Polkadot addresses can have indices. An index is like a short and easy-to-remember version of an address. Claiming an index requires a deposit of that is released when the index is cleared. Any index can be claimed if it is not taken by someone else. But what if an account gets reaped, as explained above? In that case, the index is emptied. In other words, the slot frees up again, making it available for anyone to claim. It is possible to freeze an index and permanently assign it to an address. This action consumes a deposit, and the index will be permanently bound to the address with no option to release it. Lookup Account Index on-chain When someone shares their account index, their actual account address on-chain can be looked up through Polkadot-JS Apps UI > Developer > Chain state > Storage and selecting state query on indices pallet for the account corresponding to the index. Here is an example snapshot that shows how to lookup the corresponding account address of the account index 1988 on Westend network (Polkadot Test Network) , using Polkadot-JS Apps UI. The output shows the account address, deposit amount, and a boolean flag indicating whether this is claimed permanently. Submit a claim extrinsic to the indices pallet to register an index. The easiest way to do this is via Polkadot-JS UI through the Developer -> Extrinsics menu: To find available indices to claim on Polkadot or Kusama, this helper tool may come in handy . For keeping the index permanently, you can follow up with a freeze extrinsic as mentioned above. Using ENS with DOT/KSM accounts ‚Äã ENS (Ethereum Name Service) is a distributed and open system of smart contracts on the Ethereum blockchain which allows users to claim domain names like bruno.eth . The ENS is equivalent to a DNS (Domain Name System) domain. Instead, it offers a decentralized and secure way to translate text via smart contracts. Supporting wallets can then allow senders to input ENS domains instead of long and unwieldy addresses. This prevents phishing, fraud, and typos and adds a layer of usability to the regular wallet user experience. note You will need an ENS name and an Ethereum account with some ether to follow along with this guide. To register an ENS name, visit the ENS App or any number of subdomain registrars like Nameth . Note that if you're using an older ENS name, you should make sure you're using the new resolver . Visiting the ENS App will warn you about this. You will also need some way to use your Ethereum address - following this guide on a personal computer is recommended. Wallets like Frame and Metamask are safe and will make interacting with the Ethereum blockchain through your browser very easy. Despite living on the Ethereum blockchain, the ENS system has multi-chain support. In this guide, you'll go through the process of adding a KSM and DOT address to ENS. We cover both KSM and DOT to show two different approaches. note DOT can currently only be added using the Resolver method. KSM can be added through both methods are described below. This guide is also available in video format on YouTube . Adding via the UI ‚Äã The ENS App allows an ENS domain owner to inspect all records bound to the domain, and to add new ones. In the example above, the domain bruno.eth has an Ethereum and a Bitcoin address attached. Let's attach a KSM account. First, click the [+] icon in the Records tab. Then, pick "Other Addresses", "KSM", and input the Kusama address: After clicking Save, your Ethereum wallet will ask you to confirm a transaction. Once processed, the record will show up on the domain's page: The exact process applies to adding your DOT address. Once the transaction is confirmed, your address will be bound to your ENS domain. Wallet Support ‚Äã There is no wallet support for ENS names for either KSM or DOT at this time, but the crypto accounting and portfolio application Rotki does support KSM ENS resolution. Relevant links ‚Äã ENS docs ENS Multi-chain announcement Address encoder Namehash calculator Address to pubkey converter Transferring Polkadot-JS Apps Accounts/Addresses From One Computer to Another ‚Äã caution This will overwrite existing accounts with the same pubkey on your new computer. This generally should not make a difference (since it can still access the same account), but it might if you have e.g. an account that was stored externally in the extension on the old computer but was created directly in the browser on the new one. This has been tested on Brave and Chrome, but not other browsers. Go to Polkadot-JS Apps Go to JavaScript console on the browser (Available in Developer Tools) Type in the command: JSON.stringify(localStorage) Copy and paste the returned string to a text editor and save the file. Check that the string you pasted begins and ends with a tick mark ('). If not, add one to the beginning and end. Save and send that file with the copied string to the new computer. On the new computer, go to Polkadot-JS Apps Open the Javascript console on the browser (Available in Developer Tools) Set a variable raw equal to the string from the text file raw = ... copy-pasted json from original computer ... Run the following code on the console: accounts = JSON.parse(raw); for (var key in accounts) { if (accounts.hasOwnProperty(key)) { val = JSON.stringify(accounts[key]).replace(/\\/g,'').slice(1,-1); console.log(key + " -> " + val); localStorage.setItem(key, val); } } Refresh Polkadot-JS App browser and check the Accounts and Addresses pages. All of your accounts and addresses should now be available. Edit this page Last updated on Jul 3, 2023 by Filippo Previous Advanced Next Advanced Staking Concepts Address Format Derivation Paths Soft vs. Hard Derivation For the Curious: How Prefixes Work System Accounts Portability Address Conversion Tools How to Verify a Public Key's Associated Address Indices Using ENS with DOT/KSM accounts Adding via the UI Wallet Support Relevant links Transferring Polkadot-JS Apps Accounts/Addresses From One Computer to Another ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Advanced Staking Concepts General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced Advanced Staking Concepts On this page Advanced Staking Concepts New to Staking? Start your staking journey or explore more information about staking on Polkadot's Home Page . Discover the new Staking Dashboard that makes staking much easier and check this extensive article list to help you get started. You can now stake on Polkadot natively with just and earn staking rewards. For additional information, check out this blog post . This page is meant to be an advanced guide to staking with Polkadot. For a more general introduction, checkout the Introduction to Staking page. Staking Proxies ‚Äã Polkadot makes it possible to create accounts having special permissions also called proxy accounts . For mode details about proxy accounts visit the dedicated page on this wiki. Proxy accounts are special accounts which can sign extrinsic calls made to specific pallets on behalf of the proxied account. There is thus the possibility to create staking proxy accounts that can be used to sign extrinsic calls specific to the staking, session and utility pallets. Staking on Polkadot is not a set-and-forget action, as a nominator you will need to monitor the performance of your validators and make changes if needed. There will be this transactions such as nominating that will be needed to regularly signed. Each time you sign with an account, in the case of hot accounts, you expose the private key of that account to the internet with consequent risk of attack. A hot stash will be exposed all the time a transaction is signed. Even in the case of a cold stash created with a Ledger device, signing with the stash will build a transaction history that might tell something about your habits and preferences, or even your location. Ideally, accounts with high economic power like the stash must be and remain as isolated as possible. With a staking proxy, the stash account is fully isolated when signing for staking-related transactions. The proxy private key will be used to sign staking-related transactions, the stash private key will stay isolated and the staking transaction history will be built by the proxy. For a practical perspective we need to use only one account and remember one password to sign for all staking-related transactions. From a security perspective who controls the staking proxy controls our staking actions. It is important to remember that actions that can be performed by the proxy accounts are limited, and in the case of staking proxy, extrinsic calls to the balances pallet cannot be signed. This means it is not possible to do balance transfers on the proxied account through a staking proxy. Note that to change the staking proxy you will need to sign with the stash or an any proxy. Bags List ‚Äã info On Polkadot and Kusama, the instance of the pallet Bags-List is named as 'voterList'. For a demo about bags list see this video tutorial . In Polkadot's NPoS nomination intents are placed in a semi-sorted list called bags-list . The Bags-List substrate pallet is designed to be self-maintaining, with minimal effort from the blockchain, making it extremely scalable. The bags list has two primary components, bags and nodes (or nominators' accounts), with bags containing the nodes with bonded balance within a specific range. In the figure below the 1st empty bag will contain nominators whose bonded balance is in the range of 21 - 30 DOT, the 2nd bag 11 - 20 DOT, and the 3rd bag 0-10 DOT. The nomination intents are the nominators' accounts with bonded tokens (in the example shown below, there are eight nomination intents) that will be put inside each of those three bags depending on their stake. The bags list is semi-sorted, meaning that sorting is only partially done. When the nomination intents are submitted to the network, they are automatically put into each bag based on the number of bonded tokens, but within each bag, those nodes are arranged based on the time they are inserted and not based on their stake (see figure below). When the nomination intent of 19 DOT is submitted, it gets placed at the last spot in the 2nd bag (shown in the yellow circle). The same scenario applies for the node with 8 DOT (green circle) in the 3rd bag. Placing the node above all nodes with a lesser stake requires an additional step (more on this later). The mentioned two nodes (19 DOT and 8 DOT) have the option to move up in their respective bags, which can put them in front of the nodes with less stake than them (see figure below). This action must be done manually by submitting the putInFrontOf extrinsic within the voterList pallet instance. Moreover, if the node with 19 DOT bonds an additional 2 DOT, that node will be put automatically in the 1st bag (i.e. automatic rebag ) because the total number of bonded tokens will now be within the range of the 1st bag. That node with now 21 DOT will be put at the tail end of the 1st bag with the possibility to manually put itself in front of "older" nodes with less than 21 DOT (if there are any). If one decides to send staking rewards to the stash account and automatically bond them (i.e. compounding the staking rewards), the position within a bag does not change automatically. The same scenario applies to a slashing event, i.e., when a nominator gets slashed, their position within a bag does not change. This might result in a scenario where the node is in the wrong bag and needs to be placed in the right bag. To address this issue, any account on-chain can submit the permissionless extrinsic rebag within the voterList pallet instance to update the positions of the nodes that do not belong to their bag and place them in the correct one. To reiterate, actions like bonding/unbonding tokens automatically rebag the nominator node, but events like staking rewards/slashing do not. See the bags-list section for more information. The bags-list is capable of including an unlimited number of nodes, subject to the chain's runtime storage. In the current staking system configuration, the bags list keeps nomination intents, of which, at most come out as the electing nominators. See Staking Election Stages section for more info. This means that only a portion of the nomination intents is kept. Once the nomination period ends, the NPoS election system takes all nomination intents and their associated votes as input, and it outputs a set of validators. The bags are iterated from the most staked to the least staked. If the accounts are not appropriately sorted, this could leave the last touched bag to only be partially iterated. Thus, in some edge cases, the order of the members within a bag is important. Continuing with the example used in the previous figures, there are 8 nomination intents of which only 7 will be kept. If the bags list stays semi-sorted (i.e. no accounts call the putInFrontOf and rebag extrinsics), the nomination of the node with 8 DOT in the 3rd bag will not be considered while that of the preceding node with 5 DOT will be. Nomination of the node with 8 DOT will be kept only if it puts itself in front of the one with 5 DOT. Note how the nomination of the node with 19 DOT in the 2nd bag will be considered regardless of changing its position inside the bag. The sorting functionality of nomination intents using bags is extremely important for the long-term improvements of the staking/election system. Minimum active nomination threshold to earn rewards is dynamic Submitting a nomination intent does not guarantee staking rewards. The stake of the top nominators is applied to the validators in the active set. To avail of staking rewards, ensure that the number of tokens bonded is higher than the minimum active bond. For more information, see the nominator guide . The "election solution" which is a connected graph between nominators and validators with the stake as edge weights, has to meet certain requirements, such as maximizing the amount of stake to nominate validators and distributing the stake backing validators as evenly as possible. The objectives of this election mechanism are to maximize the security of the network, and achieve fair representation of the nominators. If you want to know more about how NPoS works (e.g. election, running time complexity, etc.), please read here . Rewards Distribution ‚Äã info The general rule for rewards across validators is that two validators get paid essentially the same amount of tokens for equal work, i.e. they are not paid proportional to their total stakes. There is a probabilistic component to staking rewards in the form of era points and tips but these should average out over time. Validators are paid the same regardless of stake backing them. Validators with less stake will generally pay more to nominators per-token than the ones with more stake. This gives nominators an economic incentive to gradually shift their preferences to lower-staked validators that gain a sufficient amount of reputation. A consequence of this is that the stake across validators will be as evenly distributed as possible which avoids concentration of power among a few validators. In the long term, validators will have similar levels of stake, with the stake being higher for validators with higher reputation. A nominator who is willing to risk more by backing a validator with a lower reputation will get paid more, provided there are no slashing events. Before distributing rewards to nominators, validators can create a cut of the reward (a commission) that is not shared with the nominators. This cut is a percentage of the block reward, not an absolute value. After the commission gets deducted, the remaining portion is distributed pro-rata based on their staked value and split between the validator and all of the nominators whose stake has backed this validator. For example, assume the block reward for a validator is 10 DOT. A validator may specify validator_commission = 50% , in which case the validator would receive 5 DOT. The remaining 5 DOT would then be split between the validator and their nominators based on the proportion of stake each nominator had. Note that for this calculation, validator's self-stake acts just as if they were another nominator. Thus, a percentage of the reward goes thus to pay the validator's commission fees and the remainder is paid pro-rata (i.e. proportional to stake) to the nominators and validator. If a validator's commission is set to 100%, no tokens will be paid out to any of the nominators. Notice in particular that the validator is rewarded twice: once in commission fees for validating (if their commission rate is above 0%), and once for nominating itself with own stake. The following example should clarify the above. For simplicity, we have the following assumptions: These validators do not have a stake of their own. They each receive the same number of era points. There are no tips for any transactions processed. They do NOT charge any commission fees. Total reward amount is 100 DOT tokens. The current minimum amount of DOT to be a validator is 350 (note that this is not the actual value, which fluctuates, but merely an assumption for purposes of this example; to understand how the actual minimal stake is calculated, see here ). Validator A Nominator (4) Stake (600) Fraction of the Total Stake Rewards Jin 100 0.167 16.7 Sam 50 0.083 8.3 Anson 250 0.417 41.7 Bobby 200 0.333 33.3 Validator B Nominator (4) Stake (400) Fraction of the Total Stake Rewards Alice 100 0.25 25 Peter 100 0.25 25 John 150 0.375 37.5 Kitty 50 0.125 12.5 Both validators A & B have 4 nominators with a total stake 600 and 400 respectively. Based on the above rewards distribution, nominators of validator B get more rewards per DOT than those of validator A because A has more overall stake. Sam has staked 50 DOT with validator A, but he only gets 8.3 in return, whereas Kitty gets 12.5 with the same amount of stake. To estimate how many tokens you can get each month as a nominator or validator, you can use this tool as a reference and play around with it by changing some parameters (e.g. how many days you would like to stake with your DOT, provider fees, compound rewards, etc.) to have a better estimate. Even though it may not be entirely accurate since staking participation is changing dynamically, it works well as an indicator. Oversubscription, Commission Fees & Slashes ‚Äã There is an additional factor to consider in terms of rewards. While there is no limit to the number of nominators a validator may have, a validator does have a limit to how many nominators to which it can pay rewards. In Polkadot this limit is currently , although this can be modified via runtime upgrade. A validator with more than nominators is oversubscribed . When payouts occur, only the top nominators as measured by the amount of stake allocated to that validator will receive rewards. All other nominators are essentially "wasting" their stake - they used their nomination to elect that validator to the active stake, but receive no rewards in exchange for doing so. Note that the network slashes a validator for a misbehavior (e.g. validator offline, equivocation, etc.) the slashed amount is a fixed percentage (and not a fixed amount), which means that validators with more stake get slashed more DOT. Again, this is done to provide nominators with an economic incentive to shift their preferences and back less popular validators whom they consider to be trustworthy. Also, note that each validator candidate is free to name their desired commission fee (as a percentage of rewards) to cover operational costs. Since validators are paid the same, validators with lower commission fees pay more to nominators than validators with higher fees. Thus, each validator can choose between increasing their fees to earn more, or decreasing their fees to attract more nominators and increase their chances of being elected. In the long term, we expect that all validators will need to be cost-efficient to remain competitive, and that validators with higher reputation will be able to charge slightly higher commission fees (which is fair). Slashing ‚Äã Unresponsiveness ‚Äã For every session, validators will send an "I'm online" heartbeat to indicate they are live. If a validator produces no blocks during an epoch and fails to send the heartbeat, it will be reported as unresponsive. Slashing may occur depending on the repeated offenses and how many other validators were unresponsive or offline during the epoch. Validators should have a well-architected network infrastructure to ensure the node runs to reduce the risk of slashing or chilling. A high availability setup is desirable, preferably with backup nodes that kick in only once the original node is verifiably offline (to avoid double-signing and being slashed for equivocation - see below). A comprehensive guide on validator setup is available here . Here is the formula for calculating slashing due to unresponsiveness: Let x = offenders, n = total no. validators in the active set min((3 * (x - (n / 10 + 1))) / n, 1) * 0.07 The examples demonstrate how to calculate the slashing penalty for unresponsiveness. note In all of the examples, assume that there are 100 validators in the active set. No slashing would enact if < 10% of all validators are unresponsive. For example, if exactly 10 validators were unresponsive, the expression 3 (x - (n / 10 + 1))) / n would be 3 (10 - (100 / 10 + 1)) / 100 = 3 * (10 - (10 + 1)) / 100 = -0.03 which is rounded to 0. note The minimum value between 0 and 1 is 0. 0 multiplied by 0.07 is 0. If 14 validators are unresponsive, then slashing would occur, as > 10% of validators are unresponsive. The slashing penalty would be min((3 (14 - (100 / 10 + 1))) / 100, 1) 0.07 = min((3 (14 - 11))/100, 1) 0.07 = min(0.09, 1) * 0.07 = 0.6% Similarly, if one-third of the validator set (around 33/100) are unresponsive, the slashing penalty would be about 5%. The maximum slashing that can occur due to unresponsiveness is 7%. After around 45% of the validators go offline, the expression 3 (x - (n / 10 + 1))) / n will go beyond 1. Hence, min((3 (x - (n / 10 + 1))) / n, 1) * 0.07 will be ceiled to 7%. Equivocation ‚Äã GRANDPA Equivocation : A validator signs two or more votes in the same round on different chains. BABE Equivocation : A validator produces two or more blocks on the Relay Chain in the same time slot. Both GRANDPA and BABE equivocation use the same formula for calculating the slashing penalty: Let x = offenders, n = total no. validators in the active set min( (3 * x / n )^2, 1) As an example, assume that there are 100 validators in the active set, and one of them equivocates in a slot (for our purposes, it does not matter whether it was a BABE or GRANDPA equivocation). This is unlikely to be an attack on the network, but much more likely to be a misconfiguration of a validator. The penalty would be min(3 * 1 / 100)^2, 1) = 0.0009, or a 0.09% slash for that validator (i.e., the stake held by the validator and its nominators). Now assume that there is a group running several validators, and all of them have an issue in the same slot. The penalty would be min((3 * 5 / 100)^2, 1) = 0.0225, or a 2.25% slash. If 20 validators equivocate, this is a much more serious offense and possibly indicates a coordinated attack on the network, and so the slash will be much greater - min((3 * 20 / 100)^2, 1) = 0.36, or a 36% slash on all of these validators and their nominators. All slashed validators will also be chilled. From the example above, the risk of nominating or running many validators in the active set are apparent. While rewards grow linearly (two validators will get you approximately twice as many staking rewards as one), slashing grows exponentially. A single validator equivocating causes a 0.09% slash, two validators equivocating does not cause a 0.09 * 2 = 0.18% slash, but rather a 0.36% slash - 4x as much as the single validator. Validators may run their nodes on multiple machines to make sure they can still perform validation work in case one of their nodes goes down, but validator operators should be extremely careful in setting these up. If they do not have good coordination to manage signing machines, equivocation is possible, and equivocation offenses are slashed at much higher rates than equivalent offline offenses. If a validator is reported for any one of the offenses they will be removed from the validator set ( chilled ) and they will not be paid while they are out. They will be considered inactive immediately and will lose their nominators. They need to re-issue intent to validate and again gather support from nominators. Slashing Across Eras ‚Äã There are 3 main difficulties to account for with slashing in NPoS: A nominator can nominate multiple validators and be slashed via any of them. Until slashed, the stake is reused from era to era. Nominating with N coins for E eras in a row does not mean you have N * E coins to be slashed - you've only ever had N. Slashable offenses can be found after the fact and out of order. To balance this, we only slash for the maximum slash a participant can receive in some time period, rather than the sum. This ensures protection from overslashing. Likewise, the period over which maximum slashes are computed is finite and the validator is chilled with nominations withdrawn after a slashing event, as stated in the previous section. This prevents rage-quit attacks in which, once caught misbehaving, a participant deliberately misbehaves more because their slashing amount is already maxed out. Simple Payouts ‚Äã Polkadot makes stakers claim their rewards for past eras by submitting a transaction. This naturally leads to spreading out reward distribution, as people make transactions at disparate times, rather than updating the accounts of all stakers in a single block. Even if everyone submitted a reward claim at the same time, the fact that they are individual transactions would allow the block construction algorithm to process only a limited number per block and ensure that the network maintains a constant block time. If all rewards were sent out in one block, this could cause serious issues with the stability of the network. Simple payouts require one transaction per validator, per era , to claim rewards. The reason Polkadot requires this is to avoid an attack where someone has several thousand accounts nominating a single validator. The major cost in reward distribution is mutating the accounts in storage, and Polkadot cannot pay out several thousand accounts in a single transaction. Claiming Rewards ‚Äã Polkadot stores the last 84 eras of reward information (e.g. maps of era number to validator points, staking rewards, nomination exposure, etc.). Rewards will not be claimable more than 84 eras after they were earned. This means that all rewards must be claimed within a maximum of 84 eras, although under certain circumstances (described below) this may be as low as 28 eras. If a validator kills their stash, any remaining rewards will no longer be claimable. Before doing this, however, they would need to first stop validating and then unbond the funds in their stash, which takes 28 eras. If a validator were to immediately chill and start unbonding after rewards are calculated, and nobody issued a payout for that era from that validator in the next 28 eras, the reward would no longer be claimable. Advanced How-to Guides In order to be absolutely sure that staking rewards can be claimed, users should trigger a payout before 28 eras have passed. See this page for more information about how to claim rewards using the Polkadot-JS UI. FAQ and Cautionary Notes ‚Äã Rewards expire after 84 eras. On Polkadot, that's about 84 days. On Kusama, it is approximately 21 days. Validators should claim all pending rewards before killing their stash in the event the validator decides to chill -> unbonds all -> withdraws unbonded . Nominators will not miss out on rewards if they claim the pending rewards for a validator within 28 days. Essentially, the deadline to ensure you get staking rewards is 28 eras. If the validator verifies its intent and does not unbond and withdraw, the 84 era timeline holds. Claiming rewards (or neglecting to claim rewards) does not affect nominations in any way. Nominations will persist after claiming rewards or after the rewards expire. Rewards are not minted until they are claimed. Therefore, if your reward destination is "stash, increasing amount at stake", then your staked amount does not reflect your rewards until you claim them. If you want to maximize compounding, then you will need to claim often or nominate validators which regularly claim for you. Staking operations at the end of an era are closed to allow the off-chain validator election to take place. See Off-chain Phragm√©n for more information. Inflation ‚Äã DOT is an inflationary token. In fact, there is no maximum number of DOT. On Polkadot network, inflation is set to be 10% annually , with validator rewards being a function of the amount staked and the remainder going to the treasury. info DOT went through redenomination in 2020 that saw the DOT token supply increase by 100 times. The current token supply on Polkadot is (Over 1.2 Billion DOT). There is an ideal staking rate that the network tries to maintain. The goal is to have the system staking rate meet the ideal staking rate . The system staking rate would be the total amount staked over the total token supply, where the total amount staked is the stake of all validators and nominators on the network. The ideal staking rate accounts for having sufficient backing of DOT to prevent the possible compromise of security while keeping the native token liquid. DOT is inflated according to the system staking rate of the entire network. info According to the inflation model, this would suggest that if you do not use your DOT for staking, your tokens dilute over time. The ideal staking rate on Polkadot also varies with the number of parachains (50% is the current estimation of all DOT that should be staked, per parachain slot). The ideal staking rate varies based on the number of parachains The current staking rate on Polkadot still assumes the absence of parachains, with the suggested ideal staking rate of 75%. You can track the progress on the issue to adjust it here . This code assumes that the number of slots auctioned correspond to the number of parachains on the relaychain, which may not be true as new slots can be occupied by old parachains that are renewing their lease. You can also track the progress on resolving this specific issue here . If the amount of tokens staked goes below the ideal rate, then staking rewards for nominators go up, incentivizing them to stake more tokens on the network. On the contrary, if it goes above the ideal rate, staking rewards drop. This is a result of the change in the percentage of staking rewards that go to the Treasury. Source: Research - Web3 Foundation x-axis : Proportion of DOT staked y-axis : Inflation, annualized percentage Blue line : Annual inflation rate of NPoS, i.e. total amount of tokens minted to pay validators and nominators. Green line : Annual rewards rate for stakers. For instance, 0.2 corresponds to 20% of annual returns on the staked tokens. You can determine the staking rewards by looking at the top bar of the staking overview on Polkadot-JS UI . The above chart shows the inflation model of the network. Depending on the staking participation, the distribution of the inflation to validators/nominators versus the treasury will change dynamically to provide incentives to participate (or not participate) in staking. For instance, assuming that the ideal staking rate is 50%, all of the inflation would go to the validators/nominators if 50% of all KSM / DOT are staked. Any deviation from the 50% - positive or negative - sends the proportional remainder to the treasury and effectively reduces staking rewards. For those who are interested in knowing more about the design of the inflation model for the network, please see here . Staking Miner ‚Äã caution The staking-miner code is experimental and it is still in the development phase. Use is at your own discretion, as there is a risk of losing some funds. At the end of each era on Polkadot and Kusama, using NPoS , a new set of validators must be elected based on the nominator preferences. This is a computationally intensive process, hence the usage of the term "mining" for computing the solution. The validators use off-chain workers to compute the result and submit a transaction to propose the set of winners. This can also be delegated to stand-alone programs, whose task is to mine the optimal solution. Staking miners compete with each other to produce election solutions which consist of a validator set, stake distribution across that set, and a score indicating how optimal the solution is. Staking miners run any given staking algorithms (as of now, sequential Phragm√©n or PhragMMS, subject to change if improved algorithms are introduced) to produce results, which are then sent as a transaction to the relay chain via a normal signed extrinsic. The transaction requires a bond and a transaction fee. The best solution is rewarded, which the least covers the transaction fee, and the bond is returned to the account. The bond and the fee are lost if the solution is invalid. Staking miner uses a pallet called pallet_election_provider_multi_phase and can only produce solutions during the SignedPhase of the pallet's life cycle. Once the SignedPhase is over and the UnsignedPhase starts, only the off-chain workers can provide election results. Running the staking miner requires passing the seed of a funded account in order to pay the fees for the transactions that will be sent. The same account's balance is used to reserve deposits as well. The best solution in each round is rewarded. All correct solutions will get their deposit back and the ones that submit invalid solutions will lose their deposit. NPoS election optimization ‚Äã A basic election solution is a simple distribution of stake across validators, but this can be optimized for better distribution equaling a higher security score. The staking miner does not act as a validator and focuses solely on the election result and optimization of the solution. It connects to a specified chain and keeps listening to new signed phase of the election pallet in order to submit solutions to the NPoS election. When the correct time comes, it computes its solution and submits it to the chain. The default miner algorithm is sequential Phragm√©n with a configurable number of balancing iterations that improve the score. Signed Phase of the election pallet ‚Äã The election provider pallet pallet_election_provider_multi_phase is divided into two phases, signed and unsigned . At the end of the pallet's timeline, the function elect() is called. elect() + <--T::SignedPhase--> + <--T::UnsignedPhase--> + +-------------------------------------------------------------------+ Phase::Off + Phase::Signed + Phase::Unsigned + Solutions provided by the staking miner can only be submitted during the signed phase. Solutions are submitted and queued on the chain as a RawSolution . Once submitted, a solution cannot be retracted by the originating account. RawSolution struct definition: pub struct RawSolution<S> { pub solution: S, // The solution itself pub score: ElectionScore, // The claimed score of the solution. pub round: u32, // The round at which this solution should be submitted. } A maximum of pallet::Config::MaxSignedSubmissions will be stored on-chain and they will be sorted based on score. Higher the score the more optimal the election solution is. On both Polkadot and Kusama the 'MaxSignedSubmissions' is set to submissions. This variable can be modified if needed through governance. Upon arrival of a new solution: If the queue is not full, it is stored in the appropriate sorted index. If the queue is full but the submitted solution is better than one of the queued ones, the worse solution is discarded, the deposit of the outgoing solution is returned, and the new solution is stored in the correct index. If the queue is full and the solution is not an improvement compared to any of the queued ones, it is instantly rejected and no deposit is reserved. Upon the end of the SignedPhase , no more solutions can be submitted and the solutions in the queue will be checked using Pallet::feasibility_check which ensures the score is indeed correct, and marks them as valid or invalid. By checking each solution in the queue, the queue will be reorganized by score. The highest valid score will be rewarded. Invalid solutions with higher score than the winning solution will be slashed. The rest of the solutions will be discarded and their deposit will be returned. Once the staking miner with a winning solution is ready to be rewarded the runtime will automatically execute finalize_signed_phase_accept_solution which reward account associated with the winning solution. Queue +-------------------------------+ |Solution(score=20, valid=false)| +--> Slashed +-------------------------------+ |Solution(score=15, valid=true )| +--> Rewarded, Saved +-------------------------------+ |Solution(score=10, valid=true )| +--> Discarded +-------------------------------+ |Solution(score=05, valid=false)| +--> Discarded +-------------------------------+ | None | +-------------------------------+ Deposit and reward mechanics ‚Äã The staking miners are required to pay a deposit to post their solutions. Deposit amount is the sum of SignedDepositBase + SignedDepositByte + SignedDepositWeight . All good solutions are subject to receiving a SignedRewardBase . Deposit ‚Äã Current deposit( SignedDepositBase ) is which is a fixed amount. Current deposit per byte( SignedDepositByte ) is and the total is variable depending on the size of the solution data. For example, a solution weighing 200KB would yield 200 x 0.0000097656 = 0.00195312 DOT . And the weight deposit( SignedDepositWeight ) is currently set to 0 and has no effect. Reward ‚Äã Current reward( SignedRewardBase ) is which is a fixed amount. Further Resources ‚Äã If you want to run a staking miner on your validator, refer to the repository provided in the resources section below. Staking Miner repository Election Pallet definition Signed phase parameter configuration on Polkadot Edit this page Last updated on Jul 14, 2023 by Filippo Previous Accounts Next NFT Pallets Staking Proxies Bags List Rewards Distribution Slashing Unresponsiveness Equivocation Slashing Across Eras Simple Payouts Claiming Rewards FAQ and Cautionary Notes Inflation Staking Miner NPoS election optimization Signed Phase of the election pallet Deposit and reward mechanics Further Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
NFT Pallets General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced NFT Pallets On this page NFT Pallets NFT pallets allow developers to easily implement NFT-related actions within their dApp. NFTs Pallet ‚Äã For Developers Only The information presented here below is for developers. A user-friendly portal for NFTs, DEX and Assets is under construction. NFTs is a FRAME pallet currently deployed on Westmint, which provides a multitude of functions to interact with NFTs. The pallet comes with a new way to configure NFTs, as well as configure collections and items. Pallet-level feature flags allow disabling functionalities not needed in the runtime. Roles ‚Äã Setting up a collection implies different roles with different permissions: Owner: destroy collection (to destroy the collection, there should be 0 items left). redeposit: re-evaluate the deposit on some items. set team: change the collection‚Äôs Issuer, Admin, Freezer. set collection max supply: set the maximum number of items for a collection. lock collection: this can include making a collection‚Äôs items non-transferable, fixing its max supply, and locking collection metadata and attributes. Admin: set attributes and metadata of a collection. set attributes pre-signed: set attributes for an item by providing the Admin pre-signed approval. lock item properties: lock item metadata and attributes . Freezer: lock item transfer: disallow further item transfers. unlock item transfer: lift a previous lock to transfer an item. Issuer mint force mint (with custom item configuration). mint pre-signed: mint an item by providing the Issuer pre-signed approval. update mint settings. Those roles can also be set to none without the ability to change them back. This is useful when a collection is created and all the items are minted. Now, by setting roles to none we remove the possibility of minting any more items, changing the metadata, or disallowing some item's transfer. Attributes ‚Äã An item can hold the following types of attributes: System attributes. These attributes can only be set or unset by the pallet. Examples include locking an item for runtimes that use the fractionalization pallet. This is also how users can mint from a collection if they hold a valid item from another collection (the system attribute UsedToClaim is set). Collection owner‚Äôs attributes. These are attributes that can only be set or unset by the collection's admin. User attributes. These are attributes used to store various user-defined settings/values that can only be changed by the NFT's owner. No other account can restrict modifying those attributes. External attributes. These are attributes that an NFT owner can use to allow external services (e.g. oracles, smart contracts on another chain, etc..) to set or modify. Creating a Collection ‚Äã You can use the NFTs pallet to create NFT collections. In the Polkadot-JS UI, go to Developer > Extrinsic and select the nfts.create extrinsic. When you create a collection, you must specify who the admin is. Then, under config: PalletNftsCollectionConfig , you can configure your collection by specifying different settings: settings you can specify (in a bitflag-format) settings for your collection: Transferrable items : When disabled, the items will be non-transferrable (good for soul-bound NFTs), Unlocked metadata : When disabled, the metadata will be locked, Unlocked attributes : When disabled, the attributes in the CollectionOwner namespace will be locked, Unlocked max supply : allows to change the max supply until it gets locked (i.e. the possibility to change the supply for a limited amount of time), Deposit required : when disabled, no mint deposit will be taken for items. This option can be set by a super-user only. info Note that currently, Polkadot-JS UI does not support bitflags. Leave the settings field as it is. Everything is unlocked by default (bitflag value 0 ). maxSupply (toggle option) allows you to specify the maximum number of items that can be minted. mintSettings: PalletNftsMintSettings allows you to specify different settings for your collection. mintType gives you the possibility to specify who can mint in your collection: √åssuer : only you can mint in your collection. Public : everyone can mint in your collection. HoderOf : only holders of items in another collection can mint in your collection. This requires knowledge about the ID of the other collection. This avoids looping through all existing collections and spamming RPC nodes with requests to find available IDs. price (toggle option) allows you to specify the price of the items. startBlock and endBlock give you the possibility to specify a time frame during which the collection's configuration is valid (i.e. all options within config: PalletNftsCollectionConfig ). other mint settings include: wave minting, for example mint X number of items that go to collection owners and Y number of items for the public force mint: minting bypassing mint settings info The user can decide to lock an item or collection‚Äôs metadata, attributes, and settings. Also, a locking mechanism can prevent unauthorized and unprivileged transfers (unprivileged actions can be re-allowed anytime). With all these options, one can decide to modify the price of the collection's items and who can mint, receive or buy items in that collection. Time constraints are available with startBlock and endBlock parameters. It is thus possible, for example, to create a schedule in which holders of items in collection A ( HolderOf parameter) will be able to claim a limited number of NFTs from Collection X ( maxSupply parameter) only within a specific time frame. In Collection X, people can mint the number of NFTs they have in Collection A. It's a one-to-one ratio. So if they have 3 nfts in collection A, they can mint 3 nfts in collection X. Each time they use one nft in Collection A, the said NFT will have an attribute that will block its further use to mint in Collection X. But it will be possible to mint in another collection Y if it also uses collection A as a HolderOf . You can modify the parameters, so anyone can buy more NFTs from Collection X. To buy an NFT you must pay the item price + transaction fee. Even if the item is free, the transaction fee always apply. This can be useful for events such as Hackathons where participants who bought a ticket receive the NFT ticket from Collection A. Then, all holders of at least one item in Collection A (i.e. all ticket holders) will be given free avatar NFT from Collection X within the event schedule. After the event, any additional remaining items in Collection X can be made available to the public through a marketplace. The requirement to get the free avatar is to hold at least one NFT in Collection A. One can only claim the avatar specifying which NFT (i.e. the ID) they own in Collection A. The same NFT cannot be used twice. Holders of multiple NFTs in Collection A (for example, participants in multiple Hackathons) can claim multiple avatars specific to each event. Time frame must be updated Someone trying to mint an NFT outside the specified time frame will trigger a NoConfig error, as the collection‚Äôs admin has specified no configuration after the time frame ends. The collection's admin must call the updateMintSettings extrinsic and add a new schedule or disable the block number option. After you minted an NFT, check which NFT IDs you own under which collection. In the Polkadot-JS UI go to Developer > Chain State > Storage, select the nfts.account extrinsic, and specify the account owning the NFT and the collection ID. You can also see all your collections by selecting the collectionAccount extrinsic. When a new collection is created, a new ID will be generated and assigned to it. When a collection is destroyed, no one can pick up the collection ID again (including the owner). Minting an NFT ‚Äã You can mint an NFT using the nfts.mint extrinsic. You must then specify the following: collection , the collection ID where you want to mint item , the item ID mintTo , the account witnessData (toggle option), you can specify if you own an NFT in another collection Creating an item usually involves setting some attributes specific to that item. Uploading Files and Metadata ‚Äã When you have a collection ID and an item ID you need to: Open an account on Pinata . Follow these steps to upload the file you want to mint. After uploading your file, get the Content Identifier (CID) . This unique string of letters and numbers will act as a marker to link the data uploaded onto IPFS to the collection or item ID you own. Prepare the metadata file and add your CID (see below): { "name":"Your Collection Name", "description":"Collection's Description", "image":"Your Collection CID" } Upload the metadata file to Pinata and get the updated CID. After minting your NFT on the Polkadot-JS UI, you can add the CID. Go to Developer > Extrinsics and select the nfts.setCollectionMetadata (for collections) or nfts.setMetadata (for single NFTs) extrinsic. Under the data: Bytes field you can enter the CID or upload the metadata file. The collection can be created and its item minted before uploading the NFT file and related metadata. The minting process on-chain will assign a collection and item ID to your account. Those IDs will later be populated with NFT files, metadata, and attributes. Once you upload the NFT files and related data, the above-mentioned extrinsics can be used to update a collection or item. NFT/DEX/Asset Portal With the new NFT/DEX/Asset portal, all the above steps will be executed "under the hood" and the user will not have to worry about all technicalities. Other Actions ‚Äã Buying an item up for sale. Burning (i.e., destroy) items or a single item (burning must be signed either by the admin of the collection or the owner). Smart attributes allow an NFT owner to grant permission to other entities (another account, an application, an oracle, etc.) to update attributes of an NFT. An example could be that all Polkadot fellowship members have an NFT badge that gets updated over time (sort of a rank) with a consequent upgrade in membership permissions. A collection is managed by the Issuer, the Admin, and the Freezer . Those roles can be changed anytime. Setting metadata for an item or collection (metadata includes all essential information about the item or the collection). Metadata could consist of any arbitrary data like the IPFS hash. Setting or re-setting the price of an item. Clearing attributes and metadata of a collection or an item. Changing the owner of an item or a collection. Transferring an item, as well as creating and canceling transfer approvals of a specific item, or an atomic swap . Transferring ownership of an item. Delegating accounts: Delegated accounts can approve changes to an item's attributes and transfer an item. The item owner always has control and can decide to cancel approvals from a delegated account. One can also execute pending atomic swaps created by a counterpart. Work in Progress ‚Äã NFTs fractionalization will allow the user to: Take ownership of an NFT from the pallet-nfts Create a new asset in pallet-assets Mint the input amount to the previous owner of the NFT as the beneficiary Mass minting: Minting multiple items in one single transaction. This will require the user to provide a .csv file with two columns: NFT ID and CID of metadata. Uniques Pallet ‚Äã info The Uniques Pallet is deprecated. Everything related to NFTs will be covered by the the NFTs Pallet . Uniques is a FRAME pallet deployed on the Asset Hub system parachain. It implements the most basic kind of NFT -- a data record referencing some metadata. This metadata reference is mutable until frozen, so NFTs and their classes (entities derived from) are mutable unless specifically made immutable by the issuer. Uniques takes a very bare-bones approach on purpose to keep the Asset Hub chain a simple balance-keeping chain for both fungible and non-fungibles. These NFTs can be viewed and interacted with on RMRK's Singular platform , by switching the top right menu from Kusama to the Asset Hub. They can also be interacted with directly through the extrinsics tab of the Asset Hub : Edit this page Last updated on Jun 30, 2023 by Filippo Previous Advanced Staking Concepts Next Cryptography NFTs Pallet Roles Attributes Creating a Collection Minting an NFT Uploading Files and Metadata Other Actions Work in Progress Uniques Pallet ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Cryptography on Polkadot General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced Cryptography On this page Cryptography on Polkadot This is a high-level overview of the cryptography used in Polkadot. It assumes that you have some knowledge about cryptographic primitives that are generally used in blockchains such as hashes, elliptic curve cryptography (ECC), and public-private keypairs. For detailed descriptions on the cryptography used in Polkadot please see the more advanced research wiki . Hashing Algorithm ‚Äã The hashing algorithm used in Polkadot is Blake2b . Blake2 is considered to be a very fast cryptographic hash function that is also used in the cryptocurrency Zcash . Keypairs and Signing ‚Äã Polkadot uses Schnorrkel/Ristretto x25519 ("sr25519") as its key derivation and signing algorithm. Sr25519 is based on the same underlying Curve25519 as its EdDSA counterpart, Ed25519 . However, it uses Schnorr signatures instead of the EdDSA scheme. Schnorr signatures bring some noticeable benefits over the ECDSA/EdDSA schemes. For one, it is more efficient and still retains the same feature set and security assumptions. Additionally, it allows for native multisignature through signature aggregation . The names Schnorrkel and Ristretto come from the two Rust libraries that implement this scheme, the Schnorrkel library for Schnorr signatures and the Ristretto library that makes it possible to use cofactor-8 curves like Curve25519. Keys ‚Äã Public and private keys are an important aspect of most crypto-systems and an essential component that enables blockchains like Polkadot to exist. Account Keys ‚Äã Account keys are keys that are meant to control funds. They can be either: The vanilla ed25519 implementation using Schnorr signatures. The Schnorrkel/Ristretto sr25519 variant using Schnorr signatures. ECDSA signatures on secp256k1 There are no differences in security between ed25519 and sr25519 for simple signatures. We expect ed25519 to be much better supported by commercial HSMs for the foreseeable future. At the same time, sr25519 makes implementing more complex protocols safer. In particular, sr25519 comes with safer version of many protocols like HDKD common in the Bitcoin and Ethereum ecosystem. Stash and Staking Proxy Keys ‚Äã When we talk about stash and staking proxy keys, we usually talk about them in the context of running a validator or nominating, but they are useful concepts for all users to know. Both keys are types of account keys. They are distinguished by their intended use, not by an underlying cryptographic difference. All the info mentioned in the parent section applies to these keys. When creating new staking proxy or stash keys, all cryptography supported by account keys are an available option. The staking proxy key is a semi-online key that will be in the direct control of a user, and used to submit manual extrinsics. For validators or nominators, this means that the proxy key will be used to start or stop validating or nominating. Proxy keys should hold some DOT to pay for fees, but they should not be used to hold huge amounts or life savings. Since they will be exposed to the internet with relative frequency, they should be treated carefully and occasionally replaced with new ones. The stash key is a key that will, in most cases, be a cold wallet, existing on a piece of paper in a safe or protected by layers of hardware security. It should rarely, if ever, be exposed to the internet or used to submit extrinsics. The stash key is intended to hold a large amount of funds. It should be thought of as a saving's account at a bank, which ideally is only ever touched in urgent conditions. Or, perhaps a more apt metaphor is to think of it as buried treasure, hidden on some random island and only known by the pirate who originally hid it. Since the stash key is kept offline, it must be set to have its funds bonded to a particular staking proxy. For non-spending actions, the staking proxy has the funds of the stash behind it. For example, in nominating, staking, or voting, the proxy can indicate its preference with the weight of the stash. It will never be able to actually move or claim the funds in the stash key. However, if someone does obtain your proxy key, they could use it for slashable behavior, so you should still protect it and change it regularly. Session Keys ‚Äã Session keys are hot keys that must be kept online by a validator to perform network operations. Session keys are typically generated in the client, although they don't have to be. They are not meant to control funds and should only be used for their intended purpose. They can be changed regularly; your staking proxy only need create a certificate by signing a session public key and broadcast this certificate via an extrinsic. Polkadot uses six session keys: Authority Discovery: sr25519 GRANDPA: ed25519 BABE: sr25519 I'm Online: sr25519 Parachain Assignment: sr25519 Parachain Validator: ed25519 BABE requires keys suitable for use in a Verifiable Random Function as well as for digital signatures. Sr25519 keys have both capabilities and so are used for BABE. In the future, we plan to use a BLS key for GRANDPA because it allows for more efficient signature aggregation. FAQ about Keys ‚Äã Why was ed25519 selected over secp256k1 ? ‚Äã The original key derivation cryptography that was implemented for Polkadot and Substrate chains was ed25519 , which is a Schnorr signature algorithm implemented over the Edward's Curve 25519 (so named due to the parameters of the curve equation). Most cryptocurrencies, including Bitcoin and Ethereum, currently use ECDSA signatures on the secp256k1 curve. This curve is considered much more secure than NIST curves, which have possible backdoors from the NSA . The Curve25519 is considered possibly even more secure than this one and allows for easier implementation of Schnorr signatures. A recent patent expiration on it has made it the preferred choice for use in Polkadot. The choice of using Schnorr signatures over using ECDSA is not so cut and dried. Jeff Burdges (a Web3 researcher) provides additional details on the decision in this research post on the topic: Choosing Schnorr signatures over ECDSA signatures There is one sacrifice we make by choosing Schnorr signatures over ECDSA signatures for account keys: Both require 64 bytes, but only ECDSA signatures communicate their public key. There are obsolete Schnorr variants that support recovering the public key from a signature, but they break important functionality like hierarchical deterministic key derivation. In consequence, Schnorr signatures often take an extra 32 bytes for the public key. But ultimately the benefits of using Schnorr signatures outweigh the tradeoffs, and future optimizations may resolve the inefficiencies pointed out in the quote above. What is sr25519 and where did it come from? ‚Äã Some context: The Schnorr signatures over the Twisted Edward's Curve25519 are considered secure, however Ed25519 has not been completely devoid of its bugs. Most notably, Monero and all other CryptoNote currencies were vulnerable to a double spend exploit that could have potentially led to undetected, infinite inflation. These exploits were due to one peculiarity in Ed25519, which is known as its cofactor of 8. The cofactor of a curve is an esoteric detail that could have dire consequences for the security of implementation of more complex protocols. Conveniently, Mike Hamburg's Decaf paper provides a possible path forward to solving this potential bug. Decaf is basically a way to take Twisted Edward's Curves cofactor and mathematically change it with little cost to performance and gains to security. The Decaf paper approach by the Ristretto Group was extended and implemented in Rust to include cofactor-8 curves like the Curve25519 and makes Schnorr signatures over the Edward's curve more secure. Web3 Foundation has implemented a Schnorr signature library using the more secure Ristretto compression over the Curve25519 in the Schnorrkel repository. Schnorrkel implements related protocols on top of this curve compression such as HDKD, MuSig, and a verifiable random function (VRF). It also includes various minor improvements such as the hashing scheme STROBE that can theoretically process huge amounts of data with only one call across the Wasm boundary. The implementation of Schnorr signatures that is used in Polkadot and implements the Schnorrkel protocols over the Ristretto compression of the Curve25519 is known as sr25519 . Are BLS signatures used in Polkadot? ‚Äã Not yet, but they will be. BLS signatures allow more efficient signature aggregation. Because GRANDPA validators are usually signing the same thing (e.g. a block), it makes sense to aggregate them, which can allow for other protocol optimizations. From the BLS library's README Boneh-Lynn-Shacham (BLS) signatures have slow signing, very slow verification, require slow and much less secure pairing friendly curves, and tend towards dangerous malleability. Yet, BLS permits a diverse array of signature aggregation options far beyond any other known signature scheme, which makes BLS a preferred scheme for voting in consensus algorithms and for threshold signatures. Even though Schnorr signatures allow for signature aggregation, BLS signatures are much more efficient in some fashions. For this reason it will be one of the session keys that will be used by validators on the Polkadot network and critical to the GRANDPA finality gadget. Randomness ‚Äã Randomness in Proof of Stake blockchains is important for a fair and unpredictable distribution of validator responsibilities. Computers are bad at random numbers because they are deterministic devices (the same input always produces the same output). What people usually call random numbers on a computer (such as in a gaming application), are pseudo-random - that is, they depend on a sufficiently random seed provided by the user or another type of oracle , like a weather station for atmospheric noise , your heart rate , or even lava lamps , from which it can generate a series of seemingly-random numbers. But given the same seed, the same sequence will always be generated. Though, these inputs will vary based on time and space, and it would be impossible to get the same result into all the nodes of a particular blockchain around the world. If nodes get different inputs on which to build blocks, forks happen. Real-world entropy is not suitable for use as a seed for blockchain randomness. There are two main approaches to blockchain randomness in production today: RANDAO and VRF . Polkadot uses VRF. VRF ‚Äã A verifiable random function (VRF) is a mathematical operation that takes some input and produces a random number along with a proof of authenticity that this random number was generated by the submitter. The proof can be verified by any challenger to ensure the random number generation is valid. The VRF used in Polkadot is roughly the same as the one used in Ouroboros Praos. Ouroboros randomness is secure for block production and works well for BABE . Where they differ is that Polkadot's VRF does not depend on a central clock (the problem becomes - whose central clock?), rather, it depends on its own past results to determine present and future results, and it uses slot numbers as a clock emulator, estimating time. Here's how it works in detail: ‚Äã Slots are discrete units of time six seconds in length. Each slot can contain a block, but may not. Slots make up epochs - on Polkadot, 2400 slots make one epoch, which makes epochs four hours long. In every slot, each validator "rolls a die". They execute a function (the VRF) that takes as input the following: The "secret key", a key specifically made for these die rolls. An epoch randomness value, which is the hash of VRF values from the blocks in the epoch before last (N-2), so past randomness affects the current pending randomness (N). The slot number. The output is two values: a RESULT (the random value) and a PROOF (a proof that the random value was generated correctly). The RESULT is then compared to a threshold defined in the implementation of the protocol (specifically, in the Polkadot Host). If the value is less than the threshold, then the validator who rolled this number is a viable block production candidate for that slot. The validator then attempts to create a block and submits this block into the network along with the previously obtained PROOF and RESULT . Under VRF, every validator rolls a number for themselves, checks it against a threshold, and produces a block if the random roll is under that threshold. The astute reader will notice that due to the way this works, some slots may have no validators as block producer candidates because all validator candidates rolled too high and missed the threshold. We clarify how we resolve this issue and make sure that Polkadot block times remain near constant-time in the wiki page on consensus . RANDAO ‚Äã An alternative method for getting randomness on-chain is the RANDAO method from Ethereum. RANDAO requires each validator to prepare by performing many thousands of hashes on some seed. Validators then publish the final hash during a round and the random number is derived from every participant's entry into the game. As long as one honest validator participates, the randomness is considered secure (non-economically viable to attack). RANDAO is optionally augmented with VDF. VDFs ‚Äã Verifiable Delay Functions are computations that take a prescribed duration of time to complete, even on parallel computers. They produce unique output that can be independently and efficiently verified in a public setting. By feeding the result of RANDAO into a VDF, a delay is introduced that renders any attacker's attempt at influencing the current randomness obsolete. VDFs will likely be implemented through ASIC devices that need to be run separately from the other types of nodes. Although only one is enough to keep the system secure, and they will be open source and distributed at nearly no charge, running them is neither cheap nor incentivized, producing unnecessary friction for users of the blockchains opting for this method. Resources ‚Äã Key discovery attack on BIP32-Ed25519 - Archive of forum post detailing a potential attack on BIP32-Ed25519. A motivation for transition to the sr25519 variant. Account signatures and keys in Polkadot - Research post by Web3 researcher Jeff Burdges. Are Schnorr signatures quantum computer resistant? Polkadot's research on blockchain randomness and sortition - contains reasoning for choices made along with proofs Discussion on Randomness used in Polkadot - W3F researchers discuss the randomness in Polkadot and when it is usable and under which assumptions. Appendix A: On the security of curves ‚Äã From the Introduction of Curve25519 into libssl The reason is the following: During summer of 2013, revelations from ex- consultant at [the] NSA Edward Snowden gave proof that [the] NSA willingly inserts backdoors into software, hardware components and published standards. While it is still believed that the mathematics behind ECC (Elliptic-curve cryptography) are still sound and solid, some people (including Bruce Schneier [SCHNEIER] ), showed their lack of confidence in NIST-published curves such as nistp256, nistp384, nistp521, for which constant parameters (including the generator point) are defined without explanation. It is also believed that [the] NSA had a word to say in their definition. These curves are not the most secure or fastest possible for their key sizes [DJB] , and researchers think it is possible that NSA have ways of cracking NIST curves. It is also interesting to note that SSH belongs to the list of protocols the NSA claims to be able to eavesdrop. Having a secure replacement would make passive attacks much harder if such a backdoor exists. However an alternative exists in the form of Curve25519. This algorithm has been proposed in 2006 by DJB [Curve25519] . Its main strengths are its speed, its constant-time run time (and resistance against side-channel attacks), and its lack of nebulous hard-coded constants. Edit this page Last updated on Jul 14, 2023 by Filippo Previous NFT Pallets Next NPoS Election Algorithms Hashing Algorithm Keypairs and Signing Keys Account Keys Stash and Staking Proxy Keys Session Keys FAQ about Keys Randomness VRF RANDAO VDFs Resources Appendix A: On the security of curves ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
NPoS Election Algorithms General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced NPoS Election Algorithms On this page NPoS Election Algorithms NPoS Election Algorithms ‚Äã Since validators are paid almost equally in Polkadot in each era, it is important that the stake behind each validator is uniformly spread out. An election algorithm for Nominated Proof of Staking (NPoS) on Polkadot will try to optimize three metrics when computing a solution graph of nominators and validators: Maximize the total amount at stake. Maximize the stake behind the minimally staked validator. Minimize the variance of the stake in the set. note Sequential Phragm√©n , Phragmms and Star balancing are a few notable algorithms used for computing the NPoS solutions in Polkadot and Kusama. What is the sequential Phragm√©n method? ‚Äã The sequential Phragm√©n method is a multi-winner election method introduced by Edvard Phragm√©n in the 1890s. The quote below taken from the reference Phragm√©n paper sums up the purpose of the sequential Phragm√©n method: note The problem that Phragm√©n‚Äôs methods try to solve is that of electing a set of a given numbers of persons from a larger set of candidates. Phragm√©n discussed this in the context of a parliamentary election in a multi-member constituency; the same problem can, of course, also occur in local elections, but also in many other situations such as electing a board or a committee in an organization. Validator Elections ‚Äã The sequential Phragm√©n is one of the methods used in the Nominated Proof-of-Stake scheme to elect validators based on their own self-stake and the stake that is voted to them from nominators. It also tries to equalize the weights between the validators after each election round. Off-Chain Phragm√©n ‚Äã Given the large set of nominators and validators, Phragm√©n's method is a difficult optimization problem. Polkadot uses off-chain workers to compute the result off-chain and submit a transaction to propose the set of winners. The reason for performing this computation off-chain is to keep a constant block time of six seconds and prevent long block times at the end of each era, when the validator election takes place. Staking Miners The process of computing the optimal solution for NPoS election can be delegated to Staking Miners . Council Elections ‚Äã Deprecated in Polkadot OpenGov Phragmen was used for Council elections in Governance v1 . The Phragm√©n method was also used in the council election mechanism. When you voted for council members, you could select up to 16 different candidates and then place a reserved bond as the weight of your vote. Phragm√©n would run once on every election to determine the top candidates to assume council positions and then again amongst the top candidates to equalize the weight of the votes behind them as much as possible. What does it mean for node operators? ‚Äã Phragm√©n is something that will run in the background and requires no extra effort from you. However, it is good to understand how it works since it means that not all the stake you've been nominated will end up on your validator after an election. Nominators are likely to nominate a few different validators that they trust to do a good job operating their nodes. You can use this offline-phragm√©n tool for predicting the outcome of a validator election ahead of a new election. Understanding Phragm√©n ‚Äã This section explains the sequential Phragm√©n method in-depth and walks through examples. Basic Phragm√©n ‚Äã Rationale ‚Äã In order to understand the Weighted Phragm√©n method, we must first understand the basic Phragm√©n method. There must be some group of candidates, a group of seats they are vying for (which is less than the size of the group of candidates), and some group of voters. The voters can cast an approval vote - that is, they can signal approval for any subset of the candidates. The subset should be a minimum size of one (i.e., one cannot vote for no candidates) and a maximum size of one less than the number of candidates (i.e., one cannot vote for all candidates). Users are allowed to vote for all or no candidates, but this will not have an effect on the final result, and so votes of this nature are meaningless. Note that in this example, all voters are assumed to have equal say (that is, their vote does not count more or less than any other votes). The weighted case will be considered later. However, weighting can be "simulated" by having multiple voters vote for the same slate of candidates. For instance, five people voting for a particular candidate is mathematically the same as a single person with weight 5 voting for that candidate. The particular algorithm we call here the "Basic Phragm√©n" was first described by Brill et al. in their paper "Phragm√©n‚Äôs Voting Methods and Justified Representation" . Algorithm ‚Äã The Phragm√©n method will iterate, selecting one seat at a time, according to the following rules: Voters submit their ballots, marking which candidates they approve. Ballots will not be modified after submission. An initial load of 0 is set for each ballot. The candidate who wins the next available seat is the one where the ballots of their supporters would have the least average (mean) cost if that candidate wins. The n ballots that approved that winning candidate get 1/n added to their load. The load of all ballots that supported the winner of this round are averaged out so that they are equal. If there are any more seats, go back to step 3. Otherwise, the selection ends. Example ‚Äã Let's walk through an example with four candidates vying for three seats, and five voters. Open Seats: 3 Candidates: A B C D L0 ------------------------- Voter V1: X 0 Voter V2: X X 0 Voter V3: X X 0 Voter V4: X X 0 Voter V5: X X X 0 In this example, we can see that voter V1 approves only of candidate B , voter V2 approves of candidates C and D , etc. Voters can approve any number of candidates between 1 and number_of_candidates - 1 . An initial "load" of 0 is set for each ballot ( L0 = load after round 0 , i.e., the "round" before the first round). We shall see shortly how this load is updated and used to select candidates. We will now run through an iterative algorithm, with each iteration corresponding to one "seat". Since there are three seats, we will walk through three rounds. For the first round, the winner is simply going to be the candidate with the most votes. Since all loads are equal, the lowest average load will be the candidate with the highest n, since 1/n will get smaller as n increases. For this first example round, for instance, candidate A had only one ballot vote for them. Thus, the average load for candidate A is 1/1 , or 1. Candidate C has two ballots approving of them, so the average load is 1/2 . Candidate B has the lowest average load, at 1/4 and they get the first seat. Ballots loads are now averaged out, although for the first iteration, this will not have any effect. Filled seats: 1 (B) Open Seats: 2 Candidates: A B C D L0 L1 ----------------------------- Voter V1: X 0 1/4 Voter V2: X X 0 0 Voter V3: X X 0 1/4 Voter V4: X X 0 1/4 Voter V5: X X X 0 1/4 We are now down to candidates A , C , and D for two open seats. There is only one voter ( V4 ) for A , with load 1/4 . C has two voters, V2 and V5 , with loads of 0 and 1/4 . D has three voters approving of them, V2 , V3 , and V5 , with loads of 0 , 1/4 , and 1/4 , respectively. If Candidate A wins, the average load would be (1/4 + 1/1) / 1 , or 5/4 . If candidate C wins, the average load would be ((0 + 1/2) + (1/4 + 1/2)) / 2 , or 5/8 . If candidate D wins, the average load would be ((0 + 1/3) + (1/4 + 1/3) + (1/4 + 1/3)) / 3 , or 1/2 . Since 1/2 is the lowest average load, candidate D wins the second round. Now everybody who voted for Candidate D has their load set to the average, 1/2 of all the loads. Filled seats: 2 (B, D) Open Seats: 1 Candidates: A B C D L0 L1 L2 --------------------------------- Voter V1: X 0 1/4 1/4 Voter V2: X X 0 0 1/2 Voter V3: X X 0 1/4 1/2 Voter V4: X X 0 1/4 1/4 Voter V5: X X X 0 1/4 1/2 There is now one seat open and two candidates, A and C . Voter V4 is the only one voting for A , so if A wins then the average load would be (1/4 + 1/1) / 1 , or 5/4 . Voters V2 and V5 (both with load 1/2 ) support C , so if C wins the average load would be ((1/2 + 1/2) + (1/2 + 1/2)) / 2 , or 1 . Since the average load would be lower with C , C wins the final seat. Filled seats: 3 (B, D, C) Open Seats: 0 Candidates: A B C D L0 L1 L2 L3 ------------------------------------ Voter V1: X 0 1/4 1/4 1/4 Voter V2: X X 0 0 1/2 1 Voter V3: X X 0 1/4 1/2 1/2 Voter V4: X X 0 1/4 1/4 1/4 Voter V5: X X X 0 1/4 1/2 1 An interesting characteristic of this calculation is that the total load of all voters will always equal the number of seats filled in that round. In the zeroth round, load starts at 0 and there are no seats filled. After the first round, the total of all loads is 1 , after the second round it is 2 , etc. Weighted Phragm√©n ‚Äã Rationale ‚Äã While this method works well if all voters have equal weight, this is not the case in Polkadot. Elections for both validators and candidates for the Polkadot Council are weighted by the number of tokens held by the voters. This makes elections more similar to a corporate shareholder election than a traditional political election, where some members have more pull than others. Someone with a single token will have much less voting power than someone with 100. Although this may seem anti-democratic, in a pseudonymous system, it is trivial for someone with 100 tokens to create 100 different accounts and spread their wealth to all of their pseudonyms. Therefore, not only do we want to allow voters to have their preferences expressed in the result, but do so while keeping as equal a distribution of their stake as possible and express the wishes of minorities as much as is possible. The Weighted Phragm√©n method allows us to reach these goals. Algorithm ‚Äã Weighted Phragm√©n is similar to Basic Phragm√©n in that it selects candidates sequentially, one per round, until the maximum number of candidates are elected. However, it has additional features to also allocate weight (stake) behind the candidates. NOTE: in terms of validator selection, for the following algorithm, you can think of "voters" as "nominators" and "candidates" as "validators". Candidates are elected, one per round, and added to the set of successful candidates (they have won a "seat"). This aspect of the algorithm is very similar to the "basic Phragm√©n" algorithm described above. However, as candidates are elected, a weighted mapping is built, defining the weights of each selection of a validator by each nominator. In more depth, the algorithm operates like so: Create a list of all voters, their total amount of stake, and which validators they support. Generate an initial edge-weighted graph mapping from voters to candidates, where each edge weight is the total potential weight (stake) given by that voter. The sum of all potential weight for a given candidate is called their approval stake . Now we start electing candidates. For the list of all candidates who have not been elected, get their score, which is equal to 1 / approval_stake . For each voter, update the score of each candidate they support by adding their total budget (stake) multiplied by the load of the voter and then dividing by that candidate's approval stake (voter_budget * voter_load / candidate_approval_stake) . Determine the candidate with the lowest score and elect that candidate. Remove the elected candidate from the pool of potential candidates. The load for each edge connecting to the winning candidate is updated, with the edge load set to the score of the candidate minus the voter's load, and the voter's load then set to the candidate's score. If there are more candidates to elect, go to Step 3. Otherwise, continue to step 8. Now the stake is distributed amongst each nominator who backed at least one elected candidate. The backing stake for each candidate is calculated by taking the budget of the voter and multiplying by the edge load then dividing by the candidate load ( voter_budget * edge_load / candidate_load ). Example ‚Äã Note: All numbers in this example are rounded off to three decimal places. In the following example, there are five voters and five candidates vying for three potential seats. Each voter V1 - V5 has an amount of stake equal to their number (e.g., V1 has stake of 1, V2 has stake of 2, etc.). Every voter is also going to have a load, which initially starts at 0 . Filled seats: 0 Open Seats: 3 Candidates: A B C D E L0 ---------------------------- Voter V1 (1): X X 0 Voter V2 (2): X X 0 Voter V3 (3): X 0 Voter V4 (4): X X X 0 Voter V5 (5): X X 0 Let us now calculate the approval stake of each of the candidates. Recall that this is merely the amount of all support for that candidate by all voters. Candidate A: 1 + 2 + 3 + 5 = 11 Candidate B: 1 + 2 + 4 = 7 Candidate C: 4 = 4 Candidate D: 4 + 5 = 9 Candidate E: 0 The first step is easy - candidate E has 0 approval stake and can be ignored from here on out. They will never be elected. We can now calculate the initial scores of the candidates, which is 1 / approval_stake : Candidate A: 1 / 11 = 0.091 Candidate B: 1 / 7 = 0.143 Candidate C: 1 / 4 = 0.25 Candidate D: 1 / 9 = 0.111 Candidate E: N/A For every edge, we are going to calculate the score, which is current score plus the total budget * the load of the voter divided by the approval stake of the candidate. However, since the load of every voter starts at 0, and anything multiplied by 0 is 0, any addition will be 0 / x , or 0. This means that this step can be safely ignored for the initial round. Thus, the best (lowest) score for Round 0 is Candidate A, with a score of 0.091 . Candidates: A B C D E L0 L1 ---------------------------------- Voter V1 (1): X X 0 0.091 Voter V2 (2): X X 0 0.091 Voter V3 (3): X 0 0.091 Voter V4 (4): X X X 0 0 Voter V5 (5): X X 0 0.091 Filled seats: 1 (A) Open Seats: 2 Candidates: A B C D E L0 ---------------------------- Voter V1 (1): X X 0 Voter V2 (2): X X 0 Voter V3 (3): X 0 Voter V4 (4): X X X 0 Voter V5 (5): X X 0 Candidate A is now safe; there is no way that they will lose their seat. Before moving on to the next round, we need to update the scores on the edges of our graph for any candidates who have not yet been elected. We elided this detail in the previous round, since it made no difference to the final scores, but we should go into depth here to see how scores are updated. We first must calculate the new loads of the voters, and then calculate the new scores of the candidates. Any voter who had one of their choices for candidate fill the seat in this round (i.e., voters V1 , V2 , V3 , and V5 , who all voted for A ) will have their load increased. This load increase will blunt the impact of their vote in future rounds, and the edge (which will be used in determining stake allocation later) is set to the score of the elected candidate minus the current voter load. edge_load = elected_candidate_score - voter_load voter_load = elected_candidate_score In this instance, the score of the elected candidate is 0.091 and the voter loads are all 0 . So for each voter who voted for A , we will calculate a new edge load Voter -> A of: Edge load: 0.091 - 0 = 0.091 and a new voter load of: Voter load: 0.091 As a reminder, here are the current scores. Loads of the voters are all 0 . Candidate B : 0.143 Candidate C : 0.25 Candidate D : 0.111 Now, we go through the weighted graph and update the score of the candidate and the load of the edge, using the algorithm: candidate_score = candidate_score + ((voter_budget * voter_load) / candidate_approval_stake) Without walking through each step, this gives us the following modifications to the scores of the different candidates. V1 updates B to 0.156 V2 updates B to 0.182 V4 updates B to 0.182 V4 updates C to 0.25 V4 updates D to 0.111 V5 updates D to 0.162 After scores are updated, the final scores for the candidates for this round are: Candidate B: 0.182 Candidate C: 0.25 Candidate D: 0.162 D , with the lowest score, is elected. You will note that even though candidate B had more voters supporting them, candidate D won the election due to their lower score. This is directly due to the fact that they had the lowest score, of course, but the root reason behind them having a lower score was both the greater amount of stake behind them and that voters who did not get one of their choices in an earlier round (in this example, voter V4) correspond to a higher likelihood of a candidate being elected. We then update the loads for the voters and edges as specified above for any voters who voted for candidate D (viz., V4 and V5 ) using the same formula as above. Filled seats: 2 (A, D) Open Seats: 1 Candidates: A B C D E L0 L1 L2 ----------------------------------- Voter V1 (1): X X 0 0.091 0.091 Voter V2 (2): X X 0 0.091 0.091 Voter V3 (3): X 0 0.091 0.091 Voter V4 (4): X X X 0 0 0.162 Voter V5 (5): X X 0 0.091 0.162 Following a similar process for Round 2, we start with initial candidate scores of: Candidate B : 0.143 Candidate C : 0.25 We can then update the scores of the remaining two candidates according to the algorithm described above. V1 updates B to 0.156 V2 updates B to 0.182 V4 updates B to 0.274 V4 updates C to 0.412 With the lowest score of 0.274 , Candidate B claims the last open seat. Candidates A , D , and B have been elected, and candidates C and E are not. Before moving on, we must perform a final load adjustment for the voters and the graph. Filled seats: 3 (A, D, B) Open Seats: 0 Candidates: A B C D E L0 L1 L2 L3 ------------------------------------------ Voter V1 (1): X X 0 0.091 0.091 0.274 Voter V2 (2): X X 0 0.091 0.091 0.274 Voter V3 (3): X 0 0.091 0.091 0.091 Voter V4 (4): X X X 0 0 0.162 0.274 Voter V5 (5): X X 0 0.091 0.162 0.162 Now we have to determine how much stake every voter should allocate to each candidate. This is done by taking the load of the each edge and dividing it by the voter load, then multiplying by the total budget of the voter. In this example, the weighted graph ended up looking like this: Nominator: V1 Edge to A load= 0.091 Edge to B load= 0.183 Nominator: V2 Edge to A load= 0.091 Edge to B load= 0.183 Nominator: V3 Edge to A load= 0.091 Nominator: V4 Edge to B load= 0.113 Edge to D load= 0.162 Nominator: V5 Edge to A load= 0.091 Edge to D load= 0.071 For instance, the budget of V1 is 1 , the edge load to A is 0.091 , and the voter load is 0.274 . Using our equation: backing_stake (A) = voter_budget * edge_load / voter_load We can fill these variables in with: backing_stake (A) = 1 * 0.091 / 0.274 = 0.332 For V1 backing stake of B , you can simply replace the edge load value and re-calculate. backing_stake (B) = 1 * 0.183 / 0.274 = 0.668 Note that the total amount of all backing stake for a given voter will equal the total budget of the voter, unless that voter had no candidates elected, in which case it will be 0. The final results are: A is elected with stake 6.807. D is elected with stake 4.545. B is elected with stake 3.647. V1 supports: A with stake: 0.332 and B with stake: 0.668. V2 supports: A with stake: 0.663 and B with stake: 1.337. V3 supports: A with stake: 3.0. V4 supports: B with stake: 1.642 and D with stake: 2.358. V5 supports: A with stake: 2.813 and D with stake: 2.187. You will notice that the total amount of stake for candidates A , D , and B equals (aside from rounding errors) the total amount of stake of all the voters ( 1 + 2 + 3 + 4 + 5 = 15 ). This is because each voter had at least one of their candidates fill a seat. Any voter whose had none of their candidates selected will also not have any stake in any of the elected candidates. Optimizations ‚Äã The results for nominating validators are further optimized for several purposes: To reduce the number of edges, i.e. to minimize the number of validators any nominator selects To ensure, as much as possible, an even distribution of stake among the validators Reduce the amount of block computation time High-Level Description ‚Äã After running the weighted Phragm√©n algorithm, a process is run that redistributes the vote amongst the elected set. This process will never add or remove an elected candidate from the set. Instead, it reduces the variance in the list of backing stake from the voters to the elected candidates. Perfect equalization is not always possible, but the algorithm attempts to equalize as much as possible. It then runs an edge-reducing algorithm to minimize the number of validators per nominator, ideally giving every nominator a single validator to nominate per era. To minimize block computation time, the staking process is run as an off-chain worker . In order to give time for this off-chain worker to run, staking commands (bond, nominate, etc.) are not allowed in the last quarter of each era. These optimizations will not be covered in-depth on this page. For more details, you can view the Rust implementation of elections in Substrate , the Rust implementation of staking in Substrate , or the seqPhragm√©nwithpostprocessing method in the Python reference implementation . If you would like to dive even more deeply, you can review the W3F Research Page on Sequential Phragm√©n Method . Rationale for Minimizing the Number of Validators Per Nominator ‚Äã Paying out rewards for staking from every validator to all of their nominators can cost a non-trivial amount of chain resources (in terms of space on chain and resources to compute). Assume a system with 200 validators and 1000 nominators, where each of the nominators has nominated 10 different validators. Payout would thus require 1_000 * 10 , or 10_000 transactions. In an ideal scenario, if every nominator selects a single validator, only 1_000 transactions would need to take place - an order of magnitude fewer. Empirically, network slowdown at the beginning of an era has occurred due to the large number of individual payouts by validators to nominators. In extreme cases, this could be an attack vector on the system, where nominators nominate many different validators with small amounts of stake in order to slow the system at the next era change. While this would reduce network and on-chain load, being able to select only a single validator incurs some diversification costs. If the single validator that a nominator has nominated goes offline or acts maliciously, then the nominator incurs a risk of a significant amount of slashing. Nominators are thus allowed to nominate up to 16 different validators. However, after the weighted edge-reducing algorithm is run, the number of validators per nominator is minimized. Nominators are likely to see themselves nominating a single active validator for an era. At each era change, as the algorithm runs again, nominators are likely to have a different validator than they had before (assuming a significant number of selected validators). Therefore, nominators can diversify against incompetent or corrupt validators causing slashing on their accounts, even if they only nominate a single validator per era. Rationale for Maintaining an Even Distribution of Stake ‚Äã Another issue is that we want to ensure that as equal a distribution of votes as possible amongst the elected validators or council members. This helps us increase the security of the system by ensuring that the minimum amount of tokens in order to join the active validator set or council is as high as possible. For example, assume a result of five validators being elected, where validators have the following stake: {1_000, 20, 10, 10, 10} , for a total stake of 1_050. In this case, a potential attacker could join the active validator set with only 11 tokens, and could obtain a majority of validators with only 33 tokens (since the attacker only has to have enough stake to "kick out" the three lowest validators). In contrast, imagine a different result with the same amount of total stake, but with that stake perfectly equally distributed: {210, 210, 210, 210, 210} . With the same amount of stake, an attacker would need to stake 633 tokens in order to get a majority of validators, a much more expensive proposition. Although obtaining an equal distribution is unlikely, the more equal the distribution, the higher the threshold - and thus the higher the expense - for attackers to gain entry to the set. Rationale for Reducing Block Computing Time ‚Äã Running the Phragm√©n algorithm is time-consuming, and often cannot be completed within the time limits of production of a single block. Waiting for calculation to complete would jeopardize the constant block production time of the network. Therefore, as much computation as possible is moved to an off-chain worker, which validators can work on the problem without impacting block production time. There are several restrictions put in place to limit the complexity of the election and payout. As already mentioned, any given nominator can only select up to validators to nominate. Conversely, a single validator can have only nominators. A drawback to this is that it is possible, if the number of nominators is very high or the number of validators is very low, that all available validators may be "oversubscribed" and unable to accept more nominations. In this case, one may need a larger amount of stake to participate in staking, since nominations are priority-ranked in terms of amount of stake. Phragmms (aka Balphragmms) ‚Äã Phragmms , formerly known as Balphragmms , is a new election rule inspired by Phragm√©n and developed in-house for Polkadot. In general, election rules on blockchains is an active topic of research. This is due to the conflicting requirements for election rules and blockchains: elections are computationally expensive, but blockchains are computationally limited. Thus, this work constitutes state of the art in terms of optimization. Proportional representation is a very important property for a decentralized network to have in order to maintain a sufficient level of decentralization. While this is already provided by the currently implemented seqPhragmen , this new election rule provides the advantage of the added security guarantee described below. As far as we can tell, at the time of writing, Polkadot and Kusama are the only blockchain networks that implement an election rule that guarantees proportional representation. The security of a distributed and decentralized system such as Polkadot is directly related to the goal of avoiding overrepresentation of any minority. This is a stark contrast to traditional approaches to proportional representation axioms, which typically only seek to avoid underrepresentation. Maximin Support Objective and PJR ‚Äã This new election rule aims to achieve a constant-factor approximation guarantee for the maximin support objective and the closely related proportional justified representation (PJR) property. The maximin support objective is based on maximizing the support of the least-supported elected candidate, or in the case of Polkadot and Kusama, maximizing the least amount of stake backing amongst elected validators. This security-based objective translates to a security guarantee for NPoS and makes it difficult for an adversarial whale‚Äôs validator nodes to be elected. The Phragmms rule, and the guarantees it provides in terms of security and proportionality, have been formalized in a peer-reviewed paper ). The PJR property considers the proportionality of the voter‚Äôs decision power. The property states that a group of voters with cohesive candidate preferences and a large enough aggregate voting strength deserve to have a number of representatives proportional to the group‚Äôs vote strength. Comparing Sequential Phragm√©n, MMS, and Phragmms ‚Äã Sequential Phragm√©n ( seqPhragmen ) and MMS are two efficient election rules that both achieve PJR. Currently, Polkadot employs the seqPhragmen method for validator and council elections. Although seqPhramen has a very fast runtime, it does not provide constant-factor approximation for the maximin support problem. This is due to seqPhramen only performing an approximate rebalancing of the distribution of stake. In contrast, MMS is another standard greedy algorithm that simultaneously achieves the PJR property and provides a constant factor approximation for maximin support, although with a considerably slower runtime. This is because for a given partial solution, MMS computes a balanced edge weight vector for each possible augmented committee when a new candidate is added, which is computationally expensive. We introduce a new heuristic inspired by seqPhragmen , PhragMMS , which maintains a comparable runtime to seqPhragmen , offers a constant-factor approximation guarantee for the maximin support objective, and satisfies PJR. This is the fastest known algorithm to achieve a constant-factor guarantee for maximin support. The New Election Rule: Phragmms ‚Äã Phragmms is an iterative greedy algorithm that starts with an empty committee and alternates between the Phragmms heuristic for inserting a new candidate and rebalancing by replacing the weight vector with a balanced one. The main differentiator between Phragmms and seqPhragmen is that the latter only perform an approximate rebalancing. Details can be found in Balanced Stake Distribution . The computation is executed by off-chain workers privately and separately from block production, and the validators only need to submit and verify the solutions on-chain. Relative to a committee A , the score of an unelected candidate c is an easy-to-compute rough estimate of what would be the size of the least stake backing if we added c to committee A . Observing on-chain, only one solution needs to be tracked at any given time, and a block producer can submit a new solution in the block only if the block passes the verification test, consisting of checking: Feasibility, Balance and Local Optimality - The least stake backing of A is higher than the highest score among unelected candidates If the tentative solution passes the tests, then it replaces the current solution as the tentative winner. The official winning solution is declared at the end of the election window. A powerful feature of this algorithm is the fact that both its approximation guarantee for maximin support and the above checks passing can be efficiently verified in linear time. This allows for a more scalable solution for secure and proportional committee elections. While seqPhragmen also has a notion of score for unelected candidates, Phragmms can be seen as a natural complication of the seqPhragmen algorithm, where Phragmms always grants higher score values to candidates and thus inserts them with higher support values. To summarize, the main differences between the two rules are: In seqPhragmen , lower scores are better, whereas in Phragmms , higher scores are better. Inspired by seqPhragmen , the scoring system of Phragmms can be considered to be more intuitive and does a better job at estimating the value of adding a candidate to the current solution, and hence leads to a better candidate-selection heuristic. Unlike seqPhragmen , in Phragmms , the edge weight vector w is completely rebalanced after each iteration of the algorithm. The Phragmms election rule is currently being implemented on Polkadot. Once completed, it will become one of the most sophisticated election rules implemented on a blockchain. For the first time, this election rule will provide both fair representation (PJR) and security (constant-factor approximation for the maximin support objection) to a blockchain network. Algorithm ‚Äã The Phragmms algorithm iterates through the available seats, starting with an empty committee of size k : Initialize an empty committee A and zero edge weight vector w = 0 . Repeat k times: Find the unelected candidate with highest score and add it to committee A . Re-balance the weight vector w for the new committee A . Return A and w . External Resources ‚Äã Phragmms - W3F research paper that expands on the sequential Phragm√©n method. W3F Research Page on NPoS - An overview of Nominated Proof of Stake as its applied to Polkadot. Python Reference Implementations - Python implementations of Simple and Complicated Phragm√©n methods. Substrate Implementation - Rust implementation used in Substrate. Phragm√©n's and Thiele's Election Methods - 95-page paper explaining Phragm√©n's election methods in detail. Phragm√©n‚Äôs Voting Methods and Justified Representation - This paper by Brill et al. is the source for the simple Phragm√©n method, along with proofs about its properties. Offline Phragm√©n - Script to generate the Phragm√©n validator election outcome before the start of an era. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Cryptography Next How-to Guides NPoS Election Algorithms What is the sequential Phragm√©n method? Validator Elections Council Elections What does it mean for node operators? Understanding Phragm√©n Basic Phragm√©n Rationale Algorithm Example Weighted Phragm√©n Rationale Algorithm Example Optimizations High-Level Description Rationale for Minimizing the Number of Validators Per Nominator Rationale for Maintaining an Even Distribution of Stake Rationale for Reducing Block Computing Time Phragmms (aka Balphragmms) External Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Advanced Guides General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Staking Identity Asset Hub Polkadot Vault Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced How-to Guides Advanced Guides Advanced how-to guides. üóÉÔ∏è Staking Advanced How-to Guides about Staking. üìÑÔ∏è Identity Advanced How-to Guides about Identity. üóÉÔ∏è Asset Hub Advanced How-to Guides about the Asset Hub. üìÑÔ∏è Polkadot Vault Advanced How-to Guides about Polkadot Vault. Previous NPoS Election Algorithms Next Staking ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Advanced Guides for Staking General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Staking Staking Nomination Pools Identity Asset Hub Polkadot Vault Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced How-to Guides Staking Advanced Guides for Staking Advanced how-to guides about Staking. üìÑÔ∏è Staking Advanced How-to Guides for Nominators. üìÑÔ∏è Nomination Pools Advanced How-to Guides about Nomination Pools. Previous How-to Guides Next Staking ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
How-to Guides for Nominators General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Staking Staking Nomination Pools Identity Asset Hub Polkadot Vault Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced How-to Guides Staking Staking On this page How-to Guides for Nominators Claiming Rewards with the Polkadot-JS UI ‚Äã Anyone can trigger a payout for any validator, as long as they are willing to pay the transaction fee. Someone must submit a transaction with a validator ID and an era index. Polkadot will automatically calculate that validator's reward, find the top nominators for that era, and distribute the rewards pro rata. note The Staking system only applies the highest nominations to each validator to reduce the complexity of the staking set. These details are handled for you automatically if you use the Polkadot-JS UI , which also allows you to submit batches of eras at once. To claim rewards on Polkadot-JS UI, you will need to be in the "Payouts" tab underneath "Staking", which will list all the pending payouts for your stashes. To then claim your reward, select the "Payout all" button. This will prompt you to select your stash accounts for payout. Once you are done with payout, another screen will appear asking for you to sign and submit the transaction. Edit this page Last updated on Jul 11, 2023 by Filippo Previous Staking Next Nomination Pools Claiming Rewards with the Polkadot-JS UI ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Nomination Pools How-to Guides General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Staking Staking Nomination Pools Identity Asset Hub Polkadot Vault Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced How-to Guides Staking Nomination Pools On this page Nomination Pools How-to Guides Pool Creation ‚Äã info You easily create a pool using the Polkadot Staking Dashboard . See this support article for more information. The depositor calls the create extrinsic, setting the administrative roles and transferring some funds to the pool to add themselves as the first member. As stated above, the depositor must always be a member as long as the pool exists; they will be the last member to leave, ensuring they always have some skin in the game. A significant stake from the depositor is always a good indicator of the pool's credibility. The current minimum bond to create a pool on Polkadot is . The pool‚Äôs ‚Äònominator role‚Äô selects validators with the nominate extrinsic. On Polkadot JS Apps UI, navigate to Network > Staking > Pools and click on Add Pool button. The UI automatically assigns an ID to the pool and allows for entering the name of the pools and the deposit to be bonded. When creating a pool using Polkadot JS Apps UI, all the roles are mapped to the Depositor account by default. If any of these roles need to be assigned to a different account, create the pool using create extrinsic available in Developer > Extrinsics > nominationPools on Polkadot JS Apps UI. Pool Upkeep ‚Äã The nominator can update the pool‚Äôs validator selection. On Polkadot JS Apps UI, navigate to Network > Staking > Accounts page and click on Pooled button. If you have any pooled accounts with the role of nominator, you will notice the option to set nominees. Select the validators to nominate like you would normally using a nominator account. The root and bouncer can update the pool‚Äôs state to blocked through setState extrinsic and kick members by calling unbond and withdrawUnbonded . (The state can also be toggled back to open). Pool Destruction ‚Äã info As a pool admin, you can easily destroy a pool and permissionlessly remove all members using the Polkadot Staking Dashboard . See this support article for more information. A pool can be pushed into the ‚Äúdestroying‚Äù state via one of: The root and bouncer set the pool to ‚Äúdestroying‚Äù. This can be done by submitting the nominationPools.setState(poolId, state) extrinsic using the Polkadot-JS UI extrinsic tab . Where poolId is the specific ID of the pool and state is the pool's state that must be set to "destroying". Other possible states are "open" and "blocked". Any account can set the pool to destroying if over 90% of the pool's active bonded balance has been slashed. When a pool is in ‚Äòdestroying‚Äô state, unbond and withdrawUnbonded become permissionless, so anyone can help all the members exit. The pool is destroyed once the depositor withdraws, no members belong to the pool, and all the pool‚Äôs resources are wiped from the state. Claim Rewards for Other Pool Members ‚Äã As a pool member you can claim rewards for any other members who set their claim permissions to one of the permissionless options. Let's take the example of ALICE setting the claim permissions to PermissionlessAll . Another account STASH can now claim ALICE's rewards (as a free balance or compound them to the existing bonded balance). To do so, STASH can go to the Polkadot-JS UI Extrinsic Tab and issue the following extrisics: nominationPools.claimPayoutOthers extrinsic specifying ALICE's account. This will claim the rewards as a free balance on ALICE's account. nominationPools.bondExtraOthers extrinsic specifying ALICE's account and the option to bond: the free balance currently available in ALICE's account ( FreeBalance ) or the pool rewards ( Rewards ) unclaimed by ALICE. Edit this page Last updated on Jul 11, 2023 by Filippo Previous Staking Next Identity Pool Creation Pool Upkeep Pool Destruction Claim Rewards for Other Pool Members ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Identity How-to Guides General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Staking Identity Asset Hub Polkadot Vault Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced How-to Guides Identity On this page Identity How-to Guides Registrars ‚Äã Becoming a Registrar ‚Äã To become a registrar, submit a pre-image and proposal into Democracy , then wait for people to vote on it. For best results, write a post about your identity and intentions beforehand, and once the proposal is in the queue ask people to endorse it so that it gets ahead in the referendum queue. Here's how to submit a proposal to become a registrar: Go to the Democracy tab, select "Submit preimage", and input the information for this motion - notably which account you're nominating to be a registrar in the identity.setRegistrar function. Copy the preimage hash. In the above image, that's 0x90a1b2f648fc4eaff4f236b9af9ead77c89ecac953225c5fafb069d27b7131b7 . Submit the preimage by signing a transaction. Next, select "Submit Proposal" and enter the previously copied preimage hash. The locked balance field needs to be at least . You can find out the minimum by querying the chain state under Chain State -> Constants -> democracy -> minimumDeposit. At this point, DOT holders can endorse the motion. With enough endorsements, the motion will become a referendum, which is then voted on. If it passes, users will be able to request judgement from this registrar. Setting Sub-Identity (Sub-ID) for your Ledger Account ‚Äã Setting an Identity is not possible on Ledger app yet, but as a workaround, you can set the identity for an on-chain account and then use it to set a sub-identity to your Ledger account. Go to https://polkadot.js.org/apps/#/accounts . Click on the three vertical dots correponding to the account to which you already set identity. You should see an option to set onchain sub-identities. Click on it. In the pop-up window, select your Ledger account from the dropdown and enter text in sub name field. Then, click on set subs button. Sign and submit the transaction from the parent account with the identity You should now see the sub-identity displayed on-chain. You need to be aware that is reserved for setting identity and for each sub-identity. This reserved account balance is freed once you clear the identities on the account. Edit this page Last updated on Jun 23, 2023 by Filippo Previous Nomination Pools Next Asset Hub Registrars Becoming a Registrar Setting Sub-Identity (Sub-ID) for your Ledger Account ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Advanced Guides for Asset Hub General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Staking Identity Asset Hub Creating Assets Asset Hub Ledger App Polkadot Vault Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced How-to Guides Asset Hub Advanced Guides for Asset Hub Advanced how-to guides about the Asset Hub. üìÑÔ∏è Creating Assets Advanced How-to Guides about Creating Assets. üìÑÔ∏è Asset Hub Ledger App Advanced How-to Guides about Ledger and the Asset Hub. Previous Identity Next Creating Assets ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Creating Assets with the Asset Hub General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Staking Identity Asset Hub Creating Assets Asset Hub Ledger App Polkadot Vault Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced How-to Guides Asset Hub Creating Assets On this page Creating Assets with the Asset Hub The Asset Hub is a generic assets system parachain which provides functionality for deploying and transferring assets ‚Äî both Fungible and Non-Fungible Tokens (NFTs). The native token of the Asset hub is DOT. The Existential Deposit (ED), transaction fees, and the deposits for proxy/multisig operations are about 1/10th of the values on the Relay chains. For example, the Existential Deposit of an Asset Hub account is , when compared to on Polkadot . Apart from the core protocol token DOT, the assets held on the Asset Hub can be broadly categorized as Assets backed by an on-chain protocol‚Äôs utility Assets with off-chain backing Assets without any backing For additional background on the Asset Hub check out this support article . Creating Assets on the Asset Hub ‚Äã info Before minting assets on the Asset Hub, we recommend that you try out this tutorial on Westmint, which is a parachain on Westend. The WND tokens (Westies) are free and are available through a faucet . The images in the guides below are for Polkadot, but they also apply to Kusama. To create an asset on the Asset Hub, you would need a deposit of and around for the metadata. Before you create an asset on the Asset Hub, ensure that your Asset Hub account balance is a bit more than the sum of those two deposits, which should seamlessly account for the required deposits and transaction fees. You can send DOT from a Polkadot account to a the Asset Hub account using the teleport functionality. For instructions on teleporting DOT, check this tutorial on Teleports . Assuming you have the required DOT balance on your Asset Hub account, the following instructions should let you successfully create an asset on the Asset Hub Access the Asset Hub through Polkadot-JS UI . Navigate to Network > Assets. Click on the create button and you will be presented with a pop-up window. Choose the creator account, name of the asset to be displayed on the Asset Hub, the asset's symbol, number of decimals for the asset, the minimum balance required to hold this asset on an Asset Hub account and the most important field of your asset - the unique asset ID. The UI would not let you enter an ID that has already been taken. After all the details are entered, click on the next button. Choose the admin, issuer and the freezer accounts for your asset and click on the create button. Sign and submit the transaction (If you like to verify the transaction details before signing, you can click on the dropdown button pointed by the arrow in the snapshot below). If the transaction is successful, you should see the asset and its details displayed in the Network > Assets page on the Asset Hub. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Asset Hub Next Asset Hub Ledger App Creating Assets on the Asset Hub ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Using the Asset Hub Ledger App General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Staking Identity Asset Hub Creating Assets Asset Hub Ledger App Polkadot Vault Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced How-to Guides Asset Hub Asset Hub Ledger App On this page Using the Asset Hub Ledger App info Because of required WebUSB support, Ledger wallets currently only work on Chromium-based browsers like Brave, Chrome or Edge. The Asset Hub has a Ledger application that is compatible with the Ledger Nano S and Ledger Nano X devices. The Ledger devices are hardware wallets that keep your private key secured on a physical device that does not get directly exposed to your computer or the internet. The Statemine application allows you to manage your KSM and other tokens on the Asset Hub parachain. It supports most of the available transaction types of the network in the XL version of the app (details below ). If you have trouble using Ledger or following the directions below, you can try searching for your issue on the Polkadot Support page . Intro to Ledger Explainer Please check out our intro to Ledger video on YouTube for more information . Requirements ‚Äã Here is a list of what you will need before starting: A Ledger Nano S or a Ledger Nano X. The latest firmware installed. Ledger Live is installed and at version 2.29 or newer (see settings -> about to find out if you're up to date). A web browser is installed that you can use to access Polkadot-JS Apps UI . Installing the Ledger Application ‚Äã Using Ledger Live ‚Äã Ledger Nano S There are two versions of the Statemine app: the normal (light) version and the XL version. The light version has smaller size but it supports only basic functionality. If you want access to all the supported extrinsics, you need to install the XL version of the app. You can see here a full list of the extrinsics supported by both versions. Open the "Manager" tab in Ledger Live. Connect and unlock your Ledger device. If asked, allow the manager on your device by pressing both buttons on the YES screen. Search for Statemine in the app catalog. If you are using a Ledger Nano S, install either the normal (light) version or the XL version. For Ledger Nano X, there will only be one app available. Please proceed to the usage instructions below. Using on Polkadot-JS Apps UI ‚Äã info The Polkadot extension doesn't support Statemine Ledger accounts at this point, so you need to add your account on Polkadot-JS UI as described below. Adding Your Account ‚Äã note These instructions will guide you on how to add a Ledger account that's only available on the Asset Hub. If you want to use the same Ledger account on both Polkadot and the Asset Hub check the instructions below . Polkadot-JS Apps UI already has an integration with the Ledger application so that your device will work with the browser interface after installation. The functionality is currently gated behind a feature setting that you will need to turn on. In order to turn on the interoperability with the Statemine Ledger application, go to the "Settings" tab in Polkadot-JS Apps UI . Find the option for attaching Ledger devices and switch the option from the default "Do not attach Ledger devices" to "Attach Ledger via WebUSB" ( but see note above ). Click "Save" to keep your settings. Now when you go to the "Accounts" tab you will see a new button that says "Add via Ledger". Ensure that your Ledger device is unlocked, Ledger Live is closed and you have switched over to the Statemine application, then click this button. A popup will appear asking you to select an account and derivation path. The first option lets you select an account. You can have multiple accounts on a single Ledger device. The second dropdown lets you pick a derivation path - think of it like a formula from which child accounts are generated. If in doubt, pick the default option for both. Once you confirm your selection, depending on your browser and its security settings, you might need to confirm the USB connection through a popup like the one below when adding the Ledger device for the first time: You should now be able to scroll down and find a new account on the page with the type "ledger". You can now use this account to interact with the Asset Hub on Polkadot-JS Apps UI and it will prompt your ledger for confirmation when you initiate a transaction. Working on Relay Chains and Asset Hubs ‚Äã Polkadot and Kusama accounts from the extension will show up in the Asset Hub, even if they're not set to be used on all chains. The Asset Hub is a system parachains on Polkadot :polkadot}} However , as mentioned above, the Polkadot extension does not support Statemine Ledger accounts at the moment, but the Polkadot-JS UI does. So, if your Kusama account is in the Polkadot extension, you'll need to remove it from there and re-add it on Polkadot-JS UI directly as a Kusama account , as described here . After that, you will be able to make transactions with that account. To remove an account from the Polkadot extension: Open the extension Click on the three dots next to the account Select "Forget account". info As mentioned above, a Polkadot :polkadot}} Confirming the Address on your Device ‚Äã On the "Accounts" tab, find your Ledger-connected account. Click on the three vertical dots at the end of the row. This will open a new menu, here you can click the "Show address on hardware device" option to display the address on your device. Here you can scroll through and make sure the address matches to what is displayed on Polkadot-JS Apps UI . Using Polkadot-JS Apps ‚Äã Once you have your account loaded on the "Accounts" tab it should show a row with your Ledger account. At the far right of the row is located your account's DOT balance. If you expand the balance arrow, it will show details of your balance such as locks or reserved amounts. Sending a Transfer ‚Äã If you would like to send a transfer from your account housed on the Ledger device, the easiest method is to use Polkadot-JS Apps UI . Transfers Transferring in this way sends tokens to another account on the Asset Hub parachain. If you need to transfer KSM between the Asset Hub and Polkadot, see the Teleporting section below. Click on the "Send" button next to your account. In the second input, select one of the accounts from the drop-down menu or paste the address that you want to transfer funds to. In the third input, enter the amount of KSM you want to transfer. Click the "Make Transfer" button. Confirm the transaction on your device. A green success notification will be displayed when the transaction is included in a block. The "Transfer with Keep-Alive Checks" toggle Note the "Transfer with Keep-Alive Checks" toggle. While this toggle is in the On state, your account will be unable to make transactions which would get its balance below the existential deposit. This prevents reaping of accounts with low balances. If you toggle this to Off , you will be able to go below existential deposit balance, causing your account to be deleted and any dust amount of KSM to be burned. If you encounter KeepAlive errors when making transactions, this might be the reason. A detailed guide on doing transfers is available here . Receiving a Transfer ‚Äã In order to receive a transfer on the accounts stored on your Ledger device, you will need to provide the sender (i.e. the payer) with your address. The easiest way to get your address is to click on the account name which will open a sidebar. Your address will be shown in this sidebar, along with some other information. Another method is just clicking on your account's avatar icon - this immediately copies your address to the clipboard. Your Asset Hub address is the same as your Relay Chain address Make sure that you clarify to the sender that you wish to receive your tokens on the Asset Hub parachain, otherwise (if you're receiving DOT tokens) they could be sent on the Polkadot chain. Before giving anyone your address Make sure it matches what's really on the Ledger by confirming the address on your device . Some malware will intercept clicks and clipboard requests and can change your copied value in-flight, so being extra vigilant around copy-paste operations makes sense. Teleporting ‚Äã Teleporting allows you to send tokens between the Relay Chain and a parachain, or between different parachains. The Statemine Ledger app doesn't support the teleport extrinsic at this point, so an intermediary account needs to be created first. To teleport KSM to the Relay Chain follow these steps: Create an account outside your Ledger. Instructions can be found here . Transfer the desired amount as described above . If you want to send exactly the amount you want to teleport, don't forget take into account the fees for teleporting that will be deducted in the next step. Teleport your tokens following the instructions you will find here . Teleporting to a Ledger account from a non-Ledger account doesn't require these extra steps. Support ‚Äã If you need support, please visit the Polkadot Support page . Edit this page Last updated on Jun 30, 2023 by Filippo Previous Creating Assets Next Polkadot Vault Requirements Installing the Ledger Application Using Ledger Live Using on Polkadot-JS Apps UI Adding Your Account Working on Relay Chains and Asset Hubs Confirming the Address on your Device Sending a Transfer Receiving a Transfer Teleporting Support ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Vault How-to Guides General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Accounts Advanced Staking Concepts NFT Pallets Cryptography NPoS Election Algorithms How-to Guides Staking Identity Asset Hub Polkadot Vault Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Advanced How-to Guides Polkadot Vault On this page Vault How-to Guides This page is for developers and power users only By requesting the chain specification and metadata you trust the specific endpoint you are using (unless you are using you own node). info These guides apply to both Parity Signer and Polkadot Vault apps. The following guide bases on the Parity Signer Github page (to create the Chain Spec QR code and the metadata QR code fountain) and Metadata Portal Github page (to embed the Chain Spec and Metadata into a portal). Chain Specification ‚Äã Chain Spec QR ‚Äã To add more chains on the Vault app you can follow the instructions here . In this example we will add the Asset Hub system parachain. Briefly, fork the Parity Signer GitHub repository , start the terminal within the /generate_message folder and type the following: cargo run add-specs -d -u wss://kusama-asset-hub-rpc.polkadot.io --encryption sr25519 where wss://kusama-asset.hub-rpc.polkadot.io is the Parity RPC endpoint for the Asset Hub on Kusama. This will create the file sign_me_add_specs_statemine_sr25510 under the files/in_progress folder. See this GitHub page for a list of all endpoints listed in the Polkadot-JS UI. Generating Signature ‚Äã Use a hot account Make sure that the account used to sign the chain specification is a hot account. Never use a cold account from the Vault app or Ledger, as after typing the seed phrase into the terminal that account will be considered hot. Start the terminal within the files/in_progress folder and type the following: cat sign_me_add_specs_statemine_sr25519 | subkey sign --suri "YOUR SEED PHRASE" where "YOUR SEED PHRASE" is the seed phrase of the account that will be used to sign and authenticate both the chain spec and later on the metadata. Running the code above will return a signature similar to that below: 0xc4ce72db959000b6166af96d3bda55a927fd837747bf1bf1ae8a69e57c9ef37c25a88707c47b105a9eb1fbcf9345680eff57eb978cf73919506f6c738834e78a Signing Chain Spec ‚Äã Now, go back to the /generate_message folder and type the following: cargo run --release make --goal qr --crypto sr25519 --msg add-specs --payload sign_me_add_specs_statemine_sr25519 --verifier-hex PUBLIC KEY --signature-hex SIGNATURE where PUBLIC KEY is the public key of the account with seed "YOUR SEED PHRASE" , and SIGNATURE is the signature generated in the previous step. Running the code above will create the file add_specs_statemine-sr25519 under the files/completed folder. Metadata Updates ‚Äã Similarly to what we did for the chain specification, we now generate and sign the Asset Hub metadata. Metadata QR Fountain ‚Äã To update the chain metadata for the Asset Hub specs on the Vault app you can follow the instructions here . Briefly, in the Parity Signer repository, start the terminal within the /generate_message folder and type the following: cargo run load-metadata -d -u wss://kusama-asset-hub-rpc.polkadot.io where wss://kusama-asset-hub-rpc.polkadot.io is the Parity RPC endpoint for the Asset Hub on Kusama. This will create the file sign_me_load_metadata_statemineV9370 under the files/in_progress folder. Note that for future metadata updates the file name will change as the version at the time of writing was V9370 . info Note that the name of the file changes according to the network version. That is, ???? in sign_me_load_metadata_statemineV???? will be the latest version at fetch time. Generating Signature ‚Äã Use a hot account Make sure that the account used to sign the metadata is a hot account. Never use a cold account from the Vault app or Ledger, as after typing the seed phrase into the terminal that account will be considered hot. Start the terminal within the files/in_progress folder and type the following: cat sign_me_load_metadata_statemineV9370 | subkey sign --suri "YOUR SEED PHRASE" where "YOUR SEED PHRASE" is the seed phrase of the account you used to sign the chain specification. Running the code above will return a signature similar to that below: 0xde1ad7aeb252acb3cf42a522dcc8dc3f317a49be2ed636836dd6df8f7e47135f2c712480055822eba87e9ea5ac7d3bba96045992ae795856fdf4eea09a411f85 Do not copy the code lines above Note that the name of the file changes according to the network version. That is, ???? in sign_me_load_metadata_statemineV???? will be the latest version at fetch time. So, do not copy the code line above, but change the version with the appropriate one saved under the files/in_progress folder. The signature changes as well. Signing Metadata ‚Äã Now, go back to the /generate_message folder and type the following: cargo run --release make --goal qr --crypto sr25519 --msg load-metadata --payload sign_me_load_metadata_statemineV9370 --verifier-hex PUBLIC KEY --signature-hex SIGNATURE where PUBLIC KEY is the public key of the account with seed "YOUR SEED PHRASE" , and SIGNATURE is the signature generated in the previous step. Running the code above will create the file load_metadata_statemineV9370 under the files/completed folder. Add Chain & Update Metadata ‚Äã You can open add_specs_statemine-sr25519 on your browser (just drag the file on an open tab). This is a .png file containing the QR code to add the Asset Hub chain specification into the Vault App. You can do the same with the load_metadata_statemineV9370 . This is a .apng file containing the QR code fountain to do the metadata update for the Asset Hub on Kusama. Metadata Portal ‚Äã Modify config File ‚Äã Alternatively, you can add the chain specification QR code and the metadata QR code fountain in a metadata portal. Briefly, fork the Parity's Metadata Portal GitHub repository . You can modify the following fields of the config.toml file: name : your name / institution public_key : the public key of the account you use to sign the chain spec and the metadata. At the bottom of the file add the following information: [[chains]] name = "Statemine" title = "Kusama Asset Hub" rpc_endpoint = "wss://kusama-asset-hub-rpc.polkadot.io" color = "#f27230" [chains.github_release] owner = "paritytech" repo = "statemint" genesis_hash = "0x48239ef607d7928874027a43a67689209727dfb3d3dc5e5b03a39bdc2eda771a" For each additional chain, you need to add the respective information. Information about the genesis hash can be found on the Polkadot-JS UI > connect to the relevant chain > Developer > Chain State. Rename Chain's Files ‚Äã Rename the signed chain specification and metadata files as follow: Chain specification: add_specs_statemine-sr25519 --> statemine_specs.png Metadata updates: load_metadata_statemineV9370 --> statemine_metadata_9370.apng Thus, for chain specification the file must be renamed to chainName_specs.png while for metadata the file must be renamed to chainName_metadata_version.apng where chainName is the name of the chain and version is the version of the metadata. Add the renamed files to the /public/qr folder within the Metadata Portal repository. Run Portal ‚Äã Open the terminal within the Metadata Portal repository and run make updater . Then run make collector ; this will create the _latest.apng files for each of the chains (removed by the command make cleaner ). Finally, run yarn start to load the metadata portal on your localhost. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Asset Hub Ledger App Next Architecture Chain Specification Chain Spec QR Generating Signature Signing Chain Spec Metadata Updates Metadata QR Fountain Generating Signature Signing Metadata Add Chain & Update Metadata Metadata Portal Modify config File Rename Chain's Files Run Portal ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot's Architecture General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Polkadot's Architecture Everything about Polkadot's Architecture. üìÑÔ∏è Architecture Key Components to Polkadot's Architecture. üóÉÔ∏è Network Participants Participants in the Polkadot Ecosystem. üóÉÔ∏è Parachains Parachains' Protocol, Auctions, Crowdloans, and more. üìÑÔ∏è Bridges Bridges and Examples of Common Bridges. üóÉÔ∏è Cross Consensus Messaging (XCM) Polkadot's Cross-Consensus Messaging Format (XCM). Previous Polkadot Vault Next Architecture ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Architecture General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Architecture On this page Architecture Polkadot is a heterogeneous multichain with shared security and interoperability. Relay Chain ‚Äã The Relay Chain is the central chain of Polkadot. All validators of Polkadot are staked on the Relay Chain in DOT and validate for the Relay Chain. The Relay Chain is composed of a relatively small number of transaction types that include ways to interact with the governance mechanism, parachain auctions, and participating in NPoS. The Relay Chain has deliberately minimal functionality - for instance, smart contracts are not supported. The main responsibility is to coordinate the system as a whole, including parachains. Other specific work is delegated to the parachains, which have different implementations and features. Parachain and Parathread Slots ‚Äã Polkadot can support a number of execution slots. These slots are like cores on a computer's processor (a modern laptop's processor may have eight cores, for example). Each one of these cores can run one process at a time. Polkadot allows these slots using two subscription models: parachains and parathreads. Parachains have a dedicated slot (core) for their chain and are like a process that runs constantly. Parathreads share slots amongst a group, and are thus more like processes that need to be woken up and run less frequently. Most of the computation that happens across the Polkadot network as a whole will be delegated to specific parachain or parathread implementations that handle various use cases. Polkadot places no constraints over what parachains can do besides that they must be able to generate a proof that can be validated by the validators assigned to the parachain. This proof verifies the state transition of the parachain. Some parachains may be specific to a particular application, others may focus on specific features like smart contracts, privacy, or scalability ‚Äî still, others might be experimental architectures that are not necessarily blockchain in nature. Polkadot provides many ways to secure a slot for a parachain for a particular length of time. Parathreads are part of a pool that shares slots and must-win auctions for individual blocks. Parathreads and parachains have the same API; their difference is economic. Parachains will have to reserve DOT for the duration of their slot lease; parathreads will pay on a per-block basis. Parathreads can become parachains, and vice-versa. Shared Security ‚Äã Parachains connected to the Polkadot Relay Chain all share in the security of the Relay Chain. Polkadot has a shared state between the Relay Chain and all of the connected parachains. If the Relay Chain must revert for any reason, then all of the parachains would also revert. This is to ensure that the validity of the entire system can persist and no individual part is corruptible. The shared state ensures that the trust assumptions when using Polkadot parachains are only those of the Relay Chain validator set and no other. Since the validator set on the Relay Chain is expected to be secure with a large amount of stake put up to back it, parachains should benefit from this security. Interoperability ‚Äã XCM ‚Äã XCM, short for cross-consensus message, is a format and not a protocol. The format does not assume anything about the receiver or senders consensus mechanism, it only cares about the format in which the messages must be structured in. The XCM format is how parachains will be able to communicate with one another. Different from XCMP, which is short for cross-chain messaging protocol, XCM is what gets delivered, and XCMP is the delivery mechanism. The best way to learn more about XCM is by reading the specification . Bridges ‚Äã A blockchain bridge is a connection that allows for arbitrary data to transfer from one network to another. These chains are interoperable through the bridge but can exist as standalone chains with different protocols, rules, and governance models. In Polkadot, bridges connect to the Relay Chain and are secured through the Polkadot consensus mechanism, maintained by collators . Polkadot uses bridges to bridge the future of Web 3.0, as bridges are fundamental to Polkadot's interoperable architecture by acting as a [secure and robust] communication channel for chains in isolation. Main Actors Validators ‚Äã Validators , if elected to the validator set, produce blocks on the Relay Chain. They also accept proofs of valid state transition from collators and receive staking rewards in return. Validators are required to keep enough parachain blocks available for later use in their local storage. Those blocks are retrievable by peers who lack that information, so that they can reliably confirm the issued validity statements about parachain blocks. The Availability & Validity (AnV) protocol consists of multiple steps for successfully upholding those responsibilities. Nominators ‚Äã Nominators bond their stake to particular validators in order to help them get into the active validator set and thus produce blocks for the chain. In return, nominators are generally rewarded with a portion of the staking rewards from that validator. Collators ‚Äã Collators are full nodes on both a parachain and the Relay Chain. They collect parachain transactions and produce state transition proofs for the validators on the Relay Chain. They can also send and receive messages from other parachains using XCMP. Parachain blocks themselves are produced by collators, whereas the relay chain validators only verify their validity (and later, their availability). Whiteboard Series ‚Äã For a video overview of the architecture of Polkadot watch the video below for the whiteboard interview with W3F researcher Alistair Stewart: Edit this page Last updated on Jun 30, 2023 by Filippo Previous Architecture Next Network Participants Relay Chain Parachain and Parathread Slots Shared Security Interoperability XCM Bridges Validators Nominators Collators Whiteboard Series ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Participants in the Polkadot Ecosystem General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Collator Nominator Validator Parachains Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Network Participants Participants in the Polkadot Ecosystem Learn about collators, nominators and validators. üìÑÔ∏è Collator Role of Collators within the Polkadot Ecosystem. üìÑÔ∏è Nominator Role of Nominators in the Polkadot Ecosystem. üìÑÔ∏è Validator Role of Validators within the Polkadot Ecosystem. Previous Architecture Next Collator ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Collator General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Collator Nominator Validator Parachains Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Network Participants Collator On this page Collator Collators' Role ‚Äã Collators maintain parachains by collecting parachain transactions from users and producing state transition proofs for Relay Chain validators. In other words, collators maintain parachains by aggregating parachain transactions into parachain block candidates and producing state transition proofs (Proof-of-Validity, PoV) for validators. Collators maintain a full node for the Relay Chain and a full node for their particular parachain; meaning they retain all necessary information to be able to author new blocks and execute transactions in much the same way as miners do on PoW blockchains. Under normal circumstances, they will collate and execute transactions to create an unsealed block and provide it, together with a PoV, to one or more validators responsible for proposing a parachain block. Collators are similar to validators on any other blockchain but they do not need to provide security guarantees because Polkadot provides those. If a parachain block is invalid, it will get rejected by validators. The validators are required to check the validity of submitted candidates, followed by issuing and collecting statements about the validity of candidates to other validators. This process is known as candidate backing . Validators receive an arbitrary number of parachain candidates with associated PoV from untrusted collators. A candidate is considered backable when at least 2/3 of all assigned validators have issued a valid statement about that candidate. The validator must successfully verify the following conditions in the following order: The candidate does not exceed any parameters in the persisted validation data. The signature of the collator is valid. Validate the candidate by executing the parachain Runtime. Once a candidate meets a specified criteria for inclusion, the selected relay chain block author then choses any of the backable candidates for each parachain and includes those into the relay chain block. We say the candidate blocks are backed . The assumption that having more collators is better or more secure is not correct. On the contrary, too many collators may slow down the network. The only nefarious power collators have is transaction censorship. To prevent censorship, a parachain only needs to ensure that there are some neutral collators - but not necessarily a majority. Theoretically, the censorship problem is solved with having just one honest collator. XCM ‚Äã Collators are a key element of the XCM (Cross-Consensus Message Passing Format) . By being full nodes of the Relay Chain, they are all aware of each other as peers. This makes it possible for them to send messages from parachain A to parachain B. Taking the Case for One Parachain ‚Äã A start of a new block candidate is initiated with a block creation time. The collator aggregates all new transactions at the end of the process. When doing so, the collator signs the parachain block candidate and produces state transition proofs (Proof-of-Validity, PoV), which are a summary of the final account balances caused by the transactions in the candidate block. The collator sends the candidate block and PoV to the parachain validators, so-called para-validators. The para-validators verify the transactions within the parachain block candidate. Upon verification, and if all is well, the candidate becomes backable and a para-validator shares the candidate block with the relay chain. The validators on the relay chain will try to reach a consensus on the block candidate. Upon reaching consensus, the now validated block candidate is shared with the validators and collators, and the process repeats for new transactions. A collator cannot continue building blocks on a parachain until the block candidate they proposed to the Relay Chain validators have been validated. A block is produced every 6 seconds. Collators in the Wild ‚Äã Blockchains that are built using Substrate are unable to hook onto the Relay Chain on their own. The Parity team built the Cumulus library to address this. Collators are being used on the Rococo testnet, and you can learn more about how they are used with Cumulus via the Cumulus repository. More information can be found under the Cumulus section on the build parachain page. Guides and Tools ‚Äã Tutorial covering Cumulus and Collators Rococo testnet guide polkadot-launch - a tool to quickly spin up a local Polkadot testnet based on some parameters like number of parachains, collator setup, etc. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Network Participants Next Nominator Collators' Role XCM Taking the Case for One Parachain Collators in the Wild Guides and Tools ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Nominator General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Collator Nominator Validator Parachains Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Network Participants Nominator On this page Nominator New to Staking? Start your staking journey or explore more information about staking on Polkadot's Home Page . Discover the new Staking Dashboard that makes staking much easier and check this extensive article list to help you get started. You can now stake natively with just 1 DOT and earn staking rewards . Stake through Nomination Pools The minimum amount required to become an active nominator and earn rewards may change from era to era. It is currently . If you have less DOT than the minimum active nomination and still want to participate in staking, you can join the nomination pools. You can now stake on Polkadot natively with just in the nomination pools and earn staking rewards. For additional information, see this blog post . Check the wiki doc on nomination pools for more information. If you landed on this page, you decided to understand how you can be a good nominator. Note, this page is not for nomination pool members, although pool members might gain essential knowledge about how to choose nomination pools. The information provided on this page is complementary to that on the Staking Page and Advanced Staking Page . Make sure you read those pages as well before nominating. Why Nominate? ‚Äã You become part of the Polkadot movement, a group of diverse professionals and enthusiasts around the world aspiring to build and foster the next-gen Internet, Web3: a decentralized, privacy-focused, and trustless internet. You are an essential piece of the puzzle, keeping Polkadot secure. The bonded balance can be used to vote in Polkadot OpenGov and shape the future direction of Polkadot. You will start to understand how Polkadot works at a technical-level. When you feel comfortable with your nomination skills and knowledge, you can open your nomination pool , help others secure the network and earn rewards, and build your reputation as a trusted nomination pool operator. If you like to be more involved, the next step is to become a validator . By getting staking rewards you keep up with or (likely) stay ahead of Polkadot inflation. Nominators secure the Relay Chain by staking DOT and nominating validators. You may have an account with DOT and want to earn fresh DOT. You could do so as a validator, which requires experience setting up a node and running and maintaining it 24/7. On Polkadot you can also earn DOT by nominating one or more validators. Doing so makes you a nominator for the validator(s) you chose. Pick your validators carefully - if they do not behave properly, they will get slashed, and you will lose DOT. However, if they follow the network rules, you can share the staking rewards they generate. While your DOT are staked for nominations, they are 'locked' (bonded). You can stop nominating at any time , but remember that the action is effective in the next era and does not automatically unbond your funds. Unbonding is a separate action, and it takes effect after the unbonding period, which is 28-day long on Polkadot. This is calculated by taking the bonding duration (in eras), multiplying it by the length of a single era (in hours), and dividing by the hours in a day (24). Example: (28 √ó 24 √∑ 24 = 28 days). A staking lock will be visible on the Polkadot-JS UI during the unbonding period, and after it, the staking lock can be unlocked, and the bonded funds become free balance you can transfer. Fast Unstaking feature is live! If you accidentally bonded your DOT or your bonded DOT never backed any active validator, you can now unbond them immediately. If your bonded balance did not back any validators in the last 28 days on Polkadot (when the feature goes live), you are eligible to perform fast unstaking. The staking dashboard will automatically check if you qualify. For more information, visit the "Fast Unstake" section in this support article . Setting-up Accounts ‚Äã Stash & Staking Proxy ‚Äã The first thing you need to do before becoming a nominator is to make sure you have a stash account where you can transfer funds you want to use for staking. For these accounts, it is recommended to use a "cold wallet" solution such as Ledger or Parity Signer . After setting up the stash account, it is recommended to have a staking proxy . Although you can be a nominator with just a stash account, having a staking proxy is good practice for security reasons. A staking proxy of the stash will be able to sign for all staking-related transactions as well. The stash will be fully isolated (except if the user decides to change the staking proxy of the stash or to attach different proxies to the stash). Rewards Payout Account ‚Äã As a nominator, you will be asked to choose an account where rewards will be paid. You can select one of the following options: back to staking: rewards are compounded to the bonded amount. to stash: rewards are sent to the stash account as a free balance. to another account: rewards are sent to a user-defined account (not stash). info Being a nominator is made simpler by using the Staking Dashboard that will guide you step by step through specifying rewards destination and bonded amount, and nominating validators (more on this below). Note that staking proxies are not currently supported on the dashboard. Nominating with the Polkadot-JS UI ‚Äã Targets Page ‚Äã There are many factors to consider when deciding which of your nominations. One helpful tool to choose validators is the Staking Targets table in the Polkadot-JS UI. This allows sorting validators using various metrics. Below are the relevant metrics shown as an example, followed by a brief description of each. validator payout nominators comm. total stake own stake return A recently 1 ( active ) 4 ( all ) 3% 1.6 MDOT 8500 DOT 17.8% payout : How recently the validator made its last reward payout to nominators. nominators : This column consists of two number values. The active count (left number) is the number of nominators whose stake is baking the validator in the current era. In this case Validator A has one active nominator. The total or all count (right number) is the number of all nominators who nominated Validator A. This includes the active count and all the other nominators whose stake in the current era is baking other validators. Be cautious of validators with a high number of subscribers. A validator is considered oversubscribed when more than active nominators are assigned to the validator. In this scenario, only the top nominators (sorted by stake) will receive rewards. The remaining nominators will not be rewarded. However, they can be slashed if the validator commits a slashable offense. Every nominator can select up to a maximum of validators, which contributes towards maximizing the probability of having the nominator‚Äôs stake applied to the validators active set. Nominating too few validators could result in the nominators not receiving their rewards when none of them make it to the active set or when those validators stop validating. The election algorithm attempts to maximize the overall network stake while minimizing the variance of the active stake across the validators. For additional information on the election process, check out the research behind nominated proof-of-stake . comm. : Total commission kept by the validator (100% means the validator will keep all rewards , and thus nominators will not receive them). A validator's commission is the percentage of the validator reward taken by the validator before the rewards are split among the nominators. As a nominator, you may think that choosing validators with the lowest commission is best. However, validators must be able to run at break-even to continue operations sustainably. Independent validators that rely on the commission to cover their server costs help to keep the network decentralized. Some validators, operated by central exchanges, etc., keep 100% of the commission to payout their staking service clients and therefore do not provide any rewards to external nominators. The commission is just one piece of the puzzle you should consider when picking nominating validators. total stake : The total amount of DOT tokens staked by nominators and the validator (i.e. own stake, see below). own stake : The amount of DOT tokens the validator has put up as a stake. A higher own stake can be considered as having more "skin in the game". This can imply increased trustworthiness. However, a validator not having a large amount of "own stake" is not automatically untrustworthy, as the validator could nominate from a different address. return : Average annual yield paid out to nominators (i.e. number of rewards divided by the number of bonded tokens). Note that nominating those with a higher yield may not guarantee similar future performance. On the Targets page, you can use different filters to select validators with specific traits (where a trait is a combination of the metrics above). Available filters are: one validator per operator : Do not show groups of validators run by a single operator. It shows small operators only who will likely have a higher commission and higher self-stake. Nominating only small operators might not always guarantee staking rewards, but it helps to keep the network more resilient to attacks. Validator vs Operator A validator is the node, the physical equipment with installed software that allows to produce new blocks and earn rewards. An operator is the entity responsible for setting up, running an maintaining the node. An operator can have multiple validators under different sub-identities. For example, ZUG CAPITAL/07 is one of the numerous validators belonging to the operator Zug Capital. comm. < 20% : Do not show any validators with a commission of 20% or higher. with capacity : Do not show any validators who are currently operating at capacity (i.e., could potentially be oversubscribed). recent payouts : Only show validators that have recently caused a payout to be issued . Note that anyone can cause a payout to occur; it does not have to be the operator of a validator. currently elected : Only show validators in the active set (i.e., they have been elected to produce blocks in the current era). with an identity : Only show validators that have set an identity . Note that this identity does not have to be verified by a registrar for the validator to appear in the list. Single Operators with Multiple Validators Recall that slashing is an additive function; the more validators offline or equivocating in a given session, the harsher the penalties. Since validators that are controlled by a single operator are more at risk of a "synchronized" failure, nominating them implies a greater risk of having a large slash of your nominated funds. Generally, it is safer to nominate validators whose behavior is independent of others in many ways (different hardware, geographic location, owner, etc.). Bags-list ‚Äã info On Polkadot and Kusama, the instance of the pallet Bags-List is named as voterList . Nominating accounts are placed in a semi-sorted list called bags-list. This sorting functionality is extremely important for the long-term improvements of the staking/election system. Bags-list allows up to nominators to set their intention to nominate, of which the stake of the top nominators is considered for electing set that eventually determines the active validators. The nominator accounts in a bag are sorted based on their insertion order, not by their nomination stake. The voterList.putInFrontOf extrinsic can be issued to move up in the bag, which might be very useful for the accounts in the last bag eligible for receiving staking rewards. Balance changes due to staking rewards or slashing do not automatically rebag the account. Whenever applicable, Polkadot JS Apps UI prompts the nominator account to rebag or move up by calling the voterList.rebag extrinsic. For guidelines about how to rebag or move your account within a bag, see the followings: The "Bags List" Section on this Support Page . The Bags List Section in Advanced Staking Concepts. The dedicated technical explainer video . Validator Stats ‚Äã Nominators can query validator histories to see statistics such as era points, elected stake, rewards and slashes, and commission. It is good practice to do comprehensive research on validator candidates. This could include (but should not be limited to) checking the validators' identity (if they have set one) and going over the validators' websites to see who they are, what kind of infrastructure setup they are using, reputation, the vision behind the validator, and more. Any problematic behavior must be taken seriously. An example of problematic behavior will be if a validator is regularly offline. In this case, nominators most likely would get fewer rewards. If many validators are unreachable , such validators and corresponding nominators will be slashed. Nominating with the Staking Dashboard ‚Äã The Staking Dashboard allows to choose pre-selected lists of validators based on user preference, or to manually select validators similarly as in the Polkadot-JS UI. Pre-selected choices are: Optimal Selection: Selects a mix of majority active and inactive validators. Active Low Commission: Gets a set of active validators with low commission. From Favorites: Gets a set of your favorite validators. Staking Election Stages ‚Äã The staking election system has three stages for both validators and nominators, namely "intention", "electable/electing", and "active". intention to nominate: an account that has stated the intention to nominate; also called simply a "nominator". electing nominator: a nominator who is selected to be a part of the input to the NPoS election algorithm . This selection is based on stake and is made using the bags-list . active nominator: a nominator who came out of the NPoS election algorithm backing an active validator. Staking rewards are received by the top nominators ranked by stake. When slashing occurs, all the active nominators backing the validator get slashed (also those who do not receive rewards due to oversubscription issues). The Election Solution Set ‚Äã Determining which validators are in the active set and which nominators are nominating them creates a very large graph mapping nominators to their respective validators. This "solution set" is computed off-chain and submitted to the chain, which means it must fit in a single block. If there are a large number of nominators, this means that some nominators must be eliminated. Currently, nominators are sorted by the amount of DOT staked, and those with more DOT are prioritized. This means that you may not receive rewards if you are staking with a small amount of DOT. This minimal amount is dynamic based on the number of validators, nominators, amount nominated, and other factors. Receiving Rewards ‚Äã As long as you have nominated more than one validator candidate, at least one of them got elected, and you are nominating with enough stake to get into the solution set, your bonded stake will be fully distributed to one or more validators. That being said, you may not receive rewards if you nominated very few validator candidates and no one got elected, or your stake is small, and you only selected oversubscribed validators, or the validator you are nominating has 100% commission. It is generally wise to choose as many trustworthy validators as you can (up to ) to reduce the risk of none of your nominated validators being elected. Not receiving Staking Rewards? To explore the possible reasons for not receiving staking rewards, check out the followings: The Staking FAQ on the Support Pages. The "Why am I not receiving staking rewards?" Reddit article. The "Why am I not receiving staking rewards?" section on the Staking Page. Rewards are lazy - somebody must trigger a payout for a validator for rewards to go to all of the validator's nominators. Any account can do this, although validator operators often do this as a service to their nominators. See the page on Simple Payouts for more information and instructions for claiming rewards. Explainer videos on Nominating These concepts have been further explained in the following videos: Why Nominate on Polkadot & Kusama What to Consider when Nominating Validators on Polkadot and Kusama Nominating/Staking on Polkadot and Kusama Good Nominator Practices ‚Äã Required Minimum Stake ‚Äã Due to the way the Phragmen algorithm generates the solution set and due to the fact that the solution set must fit in a single block, a minimum number of DOT will be required to nominate with to receive staking rewards can change between the eras. min-intention-threshold: minimum stake to declare the intention to nominate. This parameter can be updated via on-chain governance, and the most recent and up-to-date version can be found on chain state (select state query > staking > minimumNominatorBond ) min-electing: minimum stake among the electing nominators. Since this is almost always the same as ‚Äúmin-active‚Äù, it might not be reported. min-active: minimum stake among the active nominators. If your stake falls below this dynamic threshold in a given era, you will not receive staking rewards for that era. Thus, for nominator counters , we have: count of nominator intentions and max possible nominator intentions (unlimited) count of electing nominators, and maximum possible electing nominators ( ) count of active nominators and maximum possible active nominators ( ) Avoiding Oversubscribed Validators ‚Äã Validators can only pay out to a certain number of nominators per era. This is currently set to but can be modified via governance. If more than nominators nominate the same validator, it is "oversubscribed", and only the top staked nominators (ranked by the amount of stake) are paid rewards. Other nominators will receive no rewards for that era, although their stake will still be used to calculate entry into the active validator set. Although it is difficult to determine how many nominators will nominate a given validator in the next era, one can estimate based on the current number of nominators. A validator with only 5 nominators in this era, for instance, is unlikely to have more than in the next era. However, an already-oversubscribed validator with 1000 nominators this era is very likely to be oversubscribed in the next era as well. If you are not nominating with a large number of DOTs, you should try to avoid oversubscribed validators. It is not always easy to calculate if the validator selected will be oversubscribed in the next session; one way to avoid choosing potentially oversubscribed validators is to filter out any that are at capacity on the Targets page. Finally, if you have a minimal amount of DOTs close to the value of minActiveNomination , you may need to stake more DOT to get into the election set. The nominator-to-validator mapping solution needs to be evaluated within a single block duration, and if there are too many nominators, the lowest-staked nominations will be dropped from even being considered to be part of the electing set. This minActiveNomination value is dynamic and will vary over time. You can read the blog post "Polkadot Staking: An Update" for more details. Active vs. Inactive Nomination ‚Äã When you go to the Account actions under staking page, you should see your bonded accounts and nomination status. If not, you can follow this guide to configure it first. Your nominations will be effective in the next era; eras are roughly 24 hours on Polkadot. Suppose you have nominated five validator candidates, and three out of five were elected to the active validator set; then you should see two of your nominations as "waiting", and most likely one as "active" and the rest as "inactive". Active or inactive nomination means your nominated validators have been elected to be in the validator set, whereas waiting means they did not get elected. Generally, you will only have a single validator have an active nomination, which means that you are directly supporting it with your stake this era and thus potentially receiving staking rewards. Inactive nominators were validators elected for this era but which you are not actively supporting. Every era, a new election will take place, and you may be assigned a different active nomination from the validators you selected. If you are committing a very large stake, you may have more than one active nomination. However, the election algorithm attempts to minimize this situation, and it should not occur often, so you should almost always see only a single active nomination per era. See the section on Phragm√©n optimization for more details. Minimum Active Nomination to Receive Staking Rewards ‚Äã Minimum DOT required to earn staking rewards The minimum DOT required to submit intent to nominate is , but the minimum active nomination required to earn staking rewards is dynamic and may be much higher, which can be viewed on Polkadot JS Apps > Network > Staking > Targets page . Guides ‚Äã Be a Nominator (Polkadot) - Guide on nominating on the Kusama canary network. Stop Being a Nominator (all networks) - Guide on stopping nominations and withdrawing tokens. Edit this page Last updated on Jul 14, 2023 by Filippo Previous Collator Next Validator Why Nominate? Setting-up Accounts Stash & Staking Proxy Rewards Payout Account Nominating with the Polkadot-JS UI Targets Page Bags-list Validator Stats Nominating with the Staking Dashboard Staking Election Stages The Election Solution Set Receiving Rewards Good Nominator Practices Required Minimum Stake Avoiding Oversubscribed Validators Active vs. Inactive Nomination Minimum Active Nomination to Receive Staking Rewards Guides ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Validator General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Collator Nominator Validator Parachains Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Network Participants Validator On this page Validator info This page provides a general overview of the role of validators in Polkadot. For more detailed information you can read the Protocol Overview Section in The Polkadot Parachain Host Implementers' Guide . Validators secure the relay chain by staking DOT, validating proofs from collators and participating in consensus with other validators. Validators play a crucial role in adding new blocks to the relay chain and, by extension, to all parachains. This allows parties to complete cross-chain transactions via the relay chain. They guarantee that each parachain follows its unique rules and can pass messages between shards in a trust-free environment. Para-validators ‚Äã Parachain validators (i.e. para-validators) participate to the Parachain Phase of the AnV Protocol , and submit candidate receipts to the Relay Chain transaction queue so that a block author can include information on the parablock in a fork of of the Relay Chain. Para-validators work in groups and are selected by the runtime in every epoch to validate parachain blocks for all parachains connected to the relay chain. The selected para-validators are one of validators randomly selected (per epoch) to participate in the validation, creating a validator pool of 200 para-validators. Para-validators verify that the information contained in an assigned set of parachain blocks is valid. They receive parachain block candidates from the collators together with a Proof-of-Validity (PoV). The para-validators perform the first round of validity checks on the block candidates. Candidates that gather enough signed validity statements are considered backable . Block Authors ‚Äã There are validators on the Relay Chain who participate in the consensus mechanism to produce the relay chain blocks based on validity statements from other validators. These validators are called block authors, they are selected by BABE and can note up to one backable candidate for each parachain to include in the relay chain. A backable candidate included in the relay chain is considered backed in that fork of the chain. In a Relay Chain block, block authors will only include candidate receipts that have a parent candidate receipt in an earlier Relay Chain block. This ensures the parachain follows a valid chain. Also, the block authors will only include a receipt for which they have an erasure coding chunk, ensuring that the system can perform the next round of availability and validity checks. Other Validators ‚Äã Validators also contribute to the so-called availability distribution . In fact, once the candidate is backed in a fork of the relay chain, it is still pending availability , i.e. it is not fully included (only tentative included) as part of the parachain until it is proven avaialable (together with the PoV). Information regarding the availability of the candidate will be noted in the following relay chain blocks. Only when there is enough information, the candidate is considered a full parachain block or parablock . Validators also participate in the so-called approval process . Once the parablock is considered available and part of the parachain, it is still pending approval . Because para-validators are a small subset of all validators, there is a risk that by chance the majority of para-validators assigned to a parachain might be dishonest. It is thus necessary to run a secondary verification of the parablock before it can be considered approved. Having a secondary verification step avoids the allocation of more para-validators that will ultimately reduce the throughput of the system. Any instances of non-compliance with the consensus algorithms result in disputes with the punishment of the validators on the wrong side by removing some or all their staked DOT, thereby discouraging bad actors. Good performance, however, will be rewarded, with validators receiving block rewards (including transaction fees) in the form of DOT in exchange for their activities. Finally, validators participate in the chain selection process within GRANDPA , ensuring that only available and valid blocks end within the finalized Relay Chain. Within an era roles can change Within the same era, a Validator can be a para-validator, block author, and participate in the availability distribution or the approval process. Those roles can change between sessions. Further Readings ‚Äã Guides ‚Äã How to Validate on Polkadot - Guide on how to set up a validator on the Polkadot live network. Validator Payout Overview - A short overview on how the validator payout mechanism works. How to run your validator as a systemd process - Guide on running your validator as a systemd process so that it will run in the background and start automatically on reboots. How to Upgrade your Validator - Guide for securely upgrading your validator when you want to switch to a different machine or begin running the latest version of client code. How to Use Validator Setup - Guide on how to use Polkadot / Kusama validator setup. Other References ‚Äã How to run a Polkadot node (Docker) A Serverless Failover Solution for Web3.0 Validator Nodes - Blog that details how to create a robust failover solution for running validators. VPS list Polkadot Validator Lounge - A place to chat about being a validator. Slashing Consequences - Learn more about slashing consequences for running a validator node. Why You Should be A Validator on Polkadot and Kusama Roles and Responsibilities of a Validator Validating on Polkadot - An explanation of how to validate on Polkadot, with Joe Petrowski and David Dorgan of Parity Technologies, along with Tim Ogilvie from Staked. Security / Key Management ‚Äã Validator Security Overview Monitoring Tools ‚Äã PANIC for Polkadot - A monitoring and alerting solution for Polkadot / Kusama node Polkadot Telemetry Service - Network information, including what nodes are running on a given chain, what software versions they are running, and sync status. Validator Stats ‚Äã HashQuark Staking Strategy - The HashQuark staking strategy dashboard helps you choose the optimal set-up to maximize rewards, and provides other useful network monitoring tools. Polkastats - Polkastats is a cleanly designed dashboard for validator statistics. YieldScan - Staking yield maximization platform, designed to minimize effort. Subscan Validators Page - Displays information on the current validators - not as tailored for validators as the other sites. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Nominator Next Parachains Para-validators Block Authors Other Validators Further Readings Guides Other References Security / Key Management Monitoring Tools Validator Stats ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot's Parachains General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Introduction to Parachains Protocol Overview System Parachains Parachain Slot Auctions Parachain Crowdloans Parachains FAQ Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Parachains Polkadot's Parachains Everything about Polkadot's Parachains. üìÑÔ∏è Introduction to Parachains An Introduction to Polkadot's Parachains. üìÑÔ∏è Protocol Overview Actors and Protocols involved in Polkadot and its Parachains' Block Finality. üìÑÔ∏è System Parachains System Parachains currently deployed on Polkadot. üìÑÔ∏è Parachain Slot Auctions Polkadot's Parachain Slot Auction Mechanism. üìÑÔ∏è Parachain Crowdloans Polkadot's Crowdloans and How to Participate. üìÑÔ∏è Parachains FAQ Parachains FAQ. Previous Validator Next Introduction to Parachains ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Parachains General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Introduction to Parachains Protocol Overview System Parachains Parachain Slot Auctions Parachain Crowdloans Parachains FAQ Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Parachains Introduction to Parachains On this page Parachains Testing on Rococo For information on how to participate in the crowdloan and parachain auction testing on Rococo, please see the Rococo Content on the parachain development guide. Definition of a Parachain ‚Äã A parachain is an application-specific data structure that is globally coherent and can be validated by the validators of the Relay Chain. They take their name from the concept of parallelized chains that run parallel to the Relay Chain. Most commonly, a parachain will take the form of a blockchain, but there is no specific need for them to be actual blockchains. Due to their parallel nature, they can parallelize transaction processing and achieve scalability of the Polkadot protocol. They inherit the security of the entire network and can communicate with other parachains through the XCM format. Parachains are maintained by a network maintainer known as a collator . The role of the collator node is to maintain a full node of the parachain, retain all necessary information about the parachain, and produce new block candidates to pass to the Relay Chain validators for verification and inclusion in the shared state of Polkadot. The incentivization of a collator node is an implementation detail of the parachain. They are not required to be staked on the Relay Chain or own the native token unless stipulated by the parachain implementation. State Transitions ‚Äã Like other blockchains, parachains are deterministic state machines . Each parachain has a state , executes a batch of transactions grouped into a block, and achieves a new state. Joe Petrowski provided in this article a good analogy of a state with a light switch that can be either on or off, which is one of the simplest examples of how a state machine functions. Each parachain has its own state, and the Relay Chain links all those states into one state, i.e. a state of states. A multi-chain network like Polkadot can be viewed like one computer's state with many light switches where a state transition function is the logic to decide which switches should be toggled. Parachains have their own transition rule, separate economies, governance mechanisms, and users. A parachain's state is stored in a Merkle tree . Merkle trees have the convenient property that if some values within the tree change, this will be reflected in the Merkle root (in this case, the state root). One can verify the change by only looking at the new values and the paths that are affected within the tree. The Polkadot Host requires that the state transitions performed on parachains be specified as a Wasm executable. Proofs of new state transitions that occur on a parachain must be validated against the registered state transition function (STF) that is stored on the Relay Chain by the validators before Polkadot acknowledges a state transition has occurred on a parachain. The key constraint regarding the logic of a parachain is that it must be verifiable by the Relay Chain validators. Verification most commonly takes the form of a bundled proof of a state transition known as a Proof-of-Verification (PoV) block, which is submitted for checking to the validators from one or more parachain collators. Why Parachains? ‚Äã Parachains are a solution to two fundamental problems in blockchains: Scalability : Having one blockchain for many purposes makes it difficult to scale as future implementations and upgrades will likely advantage some purposes and disadvantage others. Conversely, having different blockchains will allow them to implement features without affecting other chains. Flexibility : It is reasonable to state a blockchain will either be really good at solving one problem or not so good at trying to solve many problems. A blockchain specializing in solving a specific problem has more leverage toward itself and its users. Parachains are purpose-built blockchains are highly specialized and can take advantage of each other through cooperation. Shared Security ‚Äã Shared security, sometimes referred as pooled security , is one of the unique value propositions for chains considering becoming a parachain and joining the Polkadot network. On a high level, shared security means that all parachains that are connected to the Polkadot Relay Chain by leasing a parachain slot will benefit from the economic security provided by the Relay Chain validators . The notion of shared security is different from inter-chain protocols that build on an architecture of bridges. For bridge protocols, each chain is considered sovereign and must maintain its own validator set and economic security. One concern in these protocols is the point of scalability of security. For example, one suggestion to scale blockchains is that of scale by altcoins, which suggests that transaction volumes will filter down to lower market cap altcoins as the bigger ones fill their blocks. A major flaw in this idea is that the lower market cap coins will have less economic security attached and be easier to attack. A real-life example of a 51% attack occurred recently ( Ethereum Classic attack on January 10, 2019 ), in which an unknown attacker double spent 219_500 ETC (~1.1 million USD). This was followed by two more 51% attacks on ETC. Polkadot overcomes security scalability concerns since it gravitates all the economic incentives to the Relay Chain and allows the parachains to tap into stronger guarantees at genesis. Sovereign chains must expend much more effort to grow the value of their coin so that it is sufficiently secure against well-funded attackers. PoW vs Parachain Model ‚Äã Let's compare the standard sovereign security model that exists on current proof-of-work (PoW) chains to that of the shared security of Polkadot. Chains secured by their security models, like Bitcoin, Zcash, and their derivatives, must bootstrap their independent network of miners and maintain a competitive portion of honest hashing power. Since mining is becoming a larger industry that increasingly centralizes key players, it is becoming more real that a single actor may control enough hash power to attack a chain. This means that smaller chains that cannot maintain a secure amount of hash power on their networks could potentially be attacked by a large mining cartel at the simple whim of redirecting its hash power away from Bitcoin and toward a new and less secure chain. 51% attacks are viable today with attacks having been reported on Ethereum Classic (see above), Verge , Bitcoin Gold , and other cryptocurrencies. On Polkadot, this disparity between chain security will not be present. When a parachain connects to Polkadot, the relay chain validators become the securers of that parachain's state transitions. The parachain will only have the overhead of running a few collator nodes to keep the validators informed with the latest state transitions and proofs/witness. Validators will then check these for the parachains to which they are assigned. In this way, new parachains instantly benefit from the overall security of Polkadot even if they have just been launched. Parachain Economies ‚Äã Parachains may have their economies with their native tokens. Schemes such as Proof-of-Stake are usually used to select the validator set to handle validation and finalization; parachains will not be required to do either of those things. However, since Polkadot is not overly particular about what the parachain can implement, it may be the choice of the parachain to implement a staking token, but it's not generally necessary. Collators may be incentivized through the inflation of a native parachain token. There may be other ways to incentivize the collator nodes that do not involve inflating the native parachain token. Transaction fees in a native parachain token can also be an implementation choice of parachains. Polkadot makes no hard and fast rules for how the parachains decide on the original validity of transactions. For example, a parachain may be implemented so that transactions must pay a minimum fee to collators to be valid. The Relay Chain will enforce this validity. Similarly, a parachain could not include that in their implementation, and Polkadot would still enforce its validity. Parachains are not required to have their token. If they do, it is up to the parachain to make the economic case for their token, not Polkadot. Parachain Hubs ‚Äã While Polkadot enables crosschain functionality amongst the parachains, it necessitates that there is some latency between the dispatch of a message from one parachain until the destination parachain receives the message. In the optimistic scenario, the latency for this message should be at least two blocks - one block for the message to be dispatched and one block for the receiving parachain to process and produce a block that acts upon the message. However, in some cases, we may see that the latency for messages is higher if many messages are in queue to be processed or if no nodes are running both parachain networks that can quickly gossip the message across the networks. Due to the necessary latency in sending crosschain messages, some parachains plan to become hubs for an entire industry. For example, a parachain project Acala is planning to become a hub for decentralized finance (DeFi) applications. Many DeFi applications take advantage of a property known as composability which means that functions of one application can be synergistically composed with others to create new applications. One example of this includes flash loans, which borrow funds to execute some on-chain logic as long as the loan is repaid at the end of the transaction. An issue with crosschain latency means that composability property weakens among parachains compared to a single blockchain. This implication is common to all sharded blockchain designs, including Polkadot, Eth2.0, and others. The solution to this is the introduction of parachain hubs, which maintain the stronger property of single block composability. Parachain Slot Acquisition ‚Äã Polkadot supports a limited number of parachains, currently estimated to be about 100. As the number of slots is limited, there are several ways to allocate them: Governance granted parachains, or "system parachains" Auction granted parachains Parathreads System parachains are allocated by Polkadot's on-chain governance and are part of the network's protocol, such as bridges to other networks or chains. These typically do not have an economic model and help remove transactions from the Relay Chain, allowing for more efficient parachain processing. Auction granted parachains are granted in a permissionless auction. Parachain teams can either bid with their own DOT tokens, or source them from the community using the crowdloan functionality . Parathreads have the same API as parachains, but are scheduled for execution on a pay-as-you-go basis with an auction for each block. Parachain Lease Expiration ‚Äã When a parachain wins an auction, the tokens it bids get reserved until the lease's end. Reserved balances are non-transferrable and cannot be used for staking. At the end of the lease, the tokens are unreserved. Parachains that have not secured a new lease to extend their slot will automatically become parathreads . System Parachains ‚Äã System parachains are parachains that use execution cores allocated by the network's governance. These chains remove transactions from the Relay Chain, allowing network validators to allocate resources to validating parachains. System chains are Polkadot using its scaling technology to host itself. See the Polkadot blog article , this Polkadot Forum thread , and the system parachains page for more information. Parachains' Use Cases ‚Äã Note that we still have to see the true potential of parachains and what it is listed below are just a few examples. Encrypted Consortium Chains : These are possibly private chains that do not leak any information to the public but still can be interacted with trustlessly due to the nature of the XCMP protocol. High-Frequency Chains : These chains can compute many transactions in a short amount of time by taking certain trade-offs or making optimizations. Privacy Chains : These chains do not leak any information to the public through novel cryptography. Smart Contract Chains : These chains can have additional logic implemented through the deployment of code known as smart contracts . Parachain Host ‚Äã Polkadot includes a blockchain called a relay chain. A blockchain is a Directed Acyclic Graph (DAG) of state transitions, where every added block can be viewed as the head of the chain or fork with cumulative state. All paths through the DAG terminate at the Genesis Block. A blockchain is a tree, as each block can have only one parent. A blockchain network is made of nodes that have a view of many forks of the chain and must decide which fork to follow. To construct the parachain host we need to answer two categories of questions addressed by two different components: What is the state transition function of the blockchain? This is handled by the Runtime , which defines the state transition logic of the chain. The Runtime logic is divided into: Modules encapsulate particular behavior of the protocol and consist of: Storage Routines are invoked by entry points and other modules upon block initialization or closing. Routines can alter the storage of a module. The entry point defines how new information is introduced to a module and can limit the origin from which they are called (user, root, parachain). API provides means for the node-side behavior to extract meaningful information from the state of a single fork. info The Polkadot Parachain Host Implementers' Guide provides details about Runtime Architecture and Runtime API . Knowing various forks of the blockchain, what behaviors should a node take? What information should a node extract from the state of which forks, and how should that information be used? This is handled by the Node-side behavior , which defines all activities a node undertakes given its view of the blockchain. The node-side behavior can be divided into two categories: Networking behaviors , relate to how information is distributed between nodes but not how the information is used afterward. Core behaviors , relate to internal work that a specific node does. Such behavior cares about that information is distributed and received , but not how these two are achieved. These two categories often interact, but they can be heavily abstracted from each other. The node-side behavior is split into various subsystems , which perform a particular category of work. Subsystems can communicate with each other through an Overseer that prevents race conditions. info The Polkadot Parachain Host Implementers' Guide provides details about node architecture the main subsystems: Collator subsystem Backing subsystem Availability subsystem Approval subsystem Dispute subsystem Utility subsystem The Runtime and Node-side behavior are dependent on each other. The Runtime depends on Node-side behavior to author blocks, and to include extrinsics which trigger the correct entry points. The Node-side behavior relies on the Runtime APIs to extract information necessary to determine which action to take. Resources ‚Äã Polkadot: The Parachain - Blog post by Polkadot co-founder Rob Habermeier who introduced parachains in 2017 as "a simpler form of blockchain, which attaches to the security provided by a Relay Chain rather than providing its own. The Relay Chain provides security to attached parachains, but also provides a guarantee of secure message-passing between them." The Path of a Parachain Block - A technical walk-through of how parachains interact with the Relay Chain. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Parachains Next Protocol Overview Definition of a Parachain State Transitions Why Parachains? Shared Security PoW vs Parachain Model Parachain Economies Parachain Hubs Parachain Slot Acquisition Parachain Lease Expiration System Parachains Parachains' Use Cases Parachain Host Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Parachains' Protocol Overview General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Introduction to Parachains Protocol Overview System Parachains Parachain Slot Auctions Parachain Crowdloans Parachains FAQ Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Parachains Protocol Overview On this page Parachains' Protocol Overview info This page is a summary of the Protocol Overview chapter in The Polkadot Parachain Host Implementer's Guide and the Availability and Validity (AnV) chapter in The Polkadot Protocol Specification . The Parachains' Protocol aims to carry a parachain's block from authoring to inclusion through a process that can be carried out repeatedly and in parallel for each parachain connected to the Relay Chain. The protocol allows the network to be efficiently sharded among parachains while maintaining strong security guarantees. The Availability and Validity (AnV) Protocol describes the Parachain Protocol from the perspective of availability and validity. on). Main Actors ‚Äã Validators ‚Äã They are responsible for validating the proposed parachain's blocks by checking the Proof-of-Validity (PoV) of the blocks and ensuring the PoV remains available for a designated period. They have "skin in the game", meaning they have funds bonded on-chain that can be partially or fully confiscated by the network in case of misbehavior. Collators ‚Äã They create the PoV that validators know how to check. Creating PoV requires familiarity with transaction format and block authoring rules of a specific parachain, as well as having access to its full state. Fishermen: Deprecated ‚Äã Fishermen are not available on Polkadot and are not planned for formal implementation, despite previous proposals in the AnV protocol . The idea behind Fishermen is that they are full nodes of parachains, like collators, but perform a different role in relation to the Polkadot network. Instead of packaging the state transitions and producing the next parachain blocks as collators do, fishermen will watch this process and ensure no invalid state transitions are included. To address the motivation behind the Fishermen design consideration, the current secondary backing checkers perform a similar role in relation to the Polkadot network. From a security standpoint, security is based on having at least one honest validator either among parachain validators or secondary checker (more about this later on). Protocols' Summary ‚Äã Parachain Protocol ‚Äã The parachain protocol is divided into two main phases: Inclusion Pipeline : Collators send parachain blocks (parablocks) with PoV to Validators. Validators verify if the parablocks follow the state transition rules of the parachain and sign statements that can have a positive or negative outcome. With enough positive statements, the block is added to the Relay Chain, but is still pending approval. Approval Process : Validators perform additional checks that, if positive, allow the parablock to be approved. The figure below shows a representation of a parachain with collators and validators. The figure also shows the journey of a parachain block (white square) through the Inclusion Pipeline and the Approval Process. Availability and Validity (AnV) Protocol ‚Äã The Availability and Validity (AnV) Protocol is a way of looking at the Parachain Protocol from another perspective, emphasizing the importance of a parablock being available and valid before being included in the finalized Relay Chain. It is divided into five different phases, three within the Inclusion Pipeline and two within the Approval Process : Inclusion Pipeline Parachain phase Relay Chain submission phase Availability and unavailability phase Approval Process Assignments and secondary (validity) checks Chain Selection In the Inclusion Pipeline, a parablock is made available (or unavailable), while in the Approval Process a parablock is checked if it is valid or not. Inclusion Pipeline ‚Äã Overview ‚Äã The inclusion pipeline is the path of a parachain block (or parablock) from its creation to its inclusion into the non-finalized Relay Chain (i.e. in a fork of the Relay Chain). The figure above shows the path of a candidate block through the Inclusion pipeline. The block changes its status through this path as follows: Candidate: A block with its PoV is put forward by a collator to a para-validator (in this case V1). The candidate block is shown as a white square with one white tick mark at the side (PoV from the collator). Note the candidate is not valid yet and can still fail to be included in the Relay Chain. Seconded: The block is put forward by the para-validator V1 to other para-validators (in this case V2 and V3). The seconded block is shown as a white square with a white tick mark and two yellow tick marks on top of it. The yellow marks show the PoV from the para-validators. Backable: The block validity is attested by a majority of the para-validators. The backable block is shown as white square with a white tick mark and three yellow tick marks on top of it. The yellow marks show the PoV from the para-validators, while the white mark the PoV from the collator. Backed: The block is backed and noted in a fork on the Relay Chain by a relay chain block author (in this case V4). The backed block is shown as a square with white background and yellow border. The backed block can still fail to be included in the Relay Chain. Note that for simplicity here the backed parachain block is represented as a block of the Relay Chain, but in reality a relay chain block contains many parachian blocks. Pending availability: The block is backed but not considered available yet. Included: The block is backed and considered available (we have a parablock). Parablocks are shown as square with white background and yellow border enclosing a "P". Parachain Phase ‚Äã In the parachain phase, some validators are assigned to parachains by the Validator Assignment Routine (these validators are called para-validators). Para-validators establish a connection with collators, which propose candidate blocks together with Proof-of-Validity (PoV) to para-validators via the Collator Distribution Subsystem . Para-validators participate in the Candidate Backing Subsystem . A para-validator needs to check if the candidate block follows the state transition rules of the parachain. Because states are stored within Merke trees, a para-validator can verify state transitions without having access to the entire state, but it needs: The block candidate (list of state transitions) The values in the parachain's database that the block modifies The hashes of the unaffected points in the Merke tree This set of information is the proof-of-validity (PoV). Once a para-validator has the PoV, it gossips this information to the other para-validators, who check the candidate block against the PoV. Candidates that gather more than half of signed validity statements are considered backable (i.e. they seem to represent a valid state transition), and their backing is the set of signed statements. The para-validators can then start to construct the candidate receipt (this is what goes into the Relay Chain block) and an erasure coding (this is what will make the parablock available, more on this later on) that will be sent to all validators in the network. Polkadot guarantees valid state transitions, not valid states Polkadot validators do not inspect every value in a parachain's state, only those that are modified. This insures that the modification is valid. Previously, we said that backable blocks seem to represent valid state transitions because para-validators are a small subset of all validators. Thus, it is possible to have the majority of them dishonest. Later on, we will see that more validators with come in to help to make sure the parablock is fully valid. Relay Chain Submission Phase ‚Äã The receipt of the backable parablock is added to the Relay Chain transaction queue together with other receipts from other parachains. Receipts are gossiped around, and when a relay chain block author wins BABE slot leadership, it will select a candidate receipt to include in a block on a fork of the Relay Chain. A block author can note up to 1 backable candidate for each parachain to be included in the Relay Chain block alongside its backing. Once included in a fork of the Relay Chain the candidate is considered backed in that fork. The candidate is considered to be in "pending availability" status, and it can only be considered a part of the parachain once proven available. Remember, at this stage validators of the Relay Chain already received the erasure coding information of that specific parablock. Availability and Unavailability Phase ‚Äã During the availability and unavailability phases, the validators will participate to Availability Distribution Subsystem to ensure availability of the candidate. They gossip the erasure coded pieces among the network. At least 1/3 + 1 validators must report that they possess their piece of the code word. Once this threshold of validators has been reached, the network can consider the candidate block available. The block is graduated to being a full parachain block, and its header will be included in that fork of the Relay Chain. The information about the candidate availability is noted in the subsequent relay chain blocks of that fork. The availability check by the block author ensures that Polkadot will only include blocks for which the validators distributed their erasure-coded chunks, but it does not guarantee their validity. Because the number of para-validators on each parachain is so low, collusion is a reasonable concern. By separating block production ( BABE ) from finality ( GRANDPA ), Polkadot can perform extra validity checks after a block is produced but before it is finalized. Thus, once the parablock is considered available and part of the parachain, it is still "pending approval". The Inclusion Pipeline must conclude for a specific parachain before a new block can be accepted on that parachain. After inclusion, the Approval Process starts and it makes sure the block is valid, and it can run for many parachain blocks at once. Failure to Inclusion ‚Äã The candidate can fail to be included in the parachain in any of the following ways: The collator cannot propagate the block to any of the assigned validators. The candidate is not backed by validators participating in the Candidate Backing subsystem. A relay chain block author does not select the candidate. The candidate's PoV is not considered available within a timeout, and the block is discarded from the Relay Chain. Signed negative statements will lead to a dispute , and if there are false negatives, whoever will be on the wrong side (once the dispute is resolved) will be slashed. False positives can also happen; those actors responsible for it will also be slashed. To detect false positives, PoV information must be available after the block has been added to the Relay Chain so that validators can check the work. However, as a part of the data availability scheme , they are made available on the network for a certain period so that the validators can perform the required checks. Approval Process ‚Äã Overview ‚Äã Once the parablock is considered available and part of the parachain, it is still "pending approval". At this stage, the parablock is tentatively included in the parachain, although more confirmation is necessary. The validators assigned to the parachain (i.e. the parachain validators) are sampled from a validator set assumed to be 1/3 dishonest in the worst-case scenario. In this case, it is likely that the majority of the random para-validators sampled for a specific parachain are dishonest and can back a candidate wrongly. To address this, the Approval Process allows detecting misbehavior after the fact without allocating more para-validators, which would ultimately reduce the system's throughput. As a parablock can accept children blocks after being considered available, failure to pass the approval process will invalidate the parablock and its descendants (children blocks). Only the validators who backed the block in question will be slashed, not those who backed the descendants. The approval pipeline can be divided into the following steps: Parablocks included by the Inclusion Pipeline are pending approval for a time window known as the secondary checking window . During the secondary checking window, validators (secondary checkers) randomly self-select based on a VRF lottery to perform secondary checks on each of the parablock. Secondary checkers acquire the parablock with PoV (erasure codings are necessary to reconstruct PoV) and re-run the validation function. Secondary checkers gossip about the results of their checks. Contradictory results lead to an escalation in which all validators must check the block. The validators on the losing side will be slashed. At the end of the process the parablock is either approved or rejected. The figure below shows the path of a parachain block when it exits the Inclusion Pipeline, and enters the Approval Process. The parablock becomes accepted when it is backed, available and undisputed . The parablock is checked a second time by a subset of validators (V5, V6 and V7), and if there are no contradictory results the block is approved and gossiped to other relay chain validators. Note the parablock after secondary checks is shown as a square with a white background a yellow border enclosing a "P", and three white ticks (one for each secondary check). Approved para-blocks are shown as yellow squares that become grey. Assignments & Secondary Checks ‚Äã Having a bad parablock on a fork of the relay chain is not catastrophic as long as the block is not approved and finalized by the finality gadget GRANDPA . If the block is not finalized, the fork on the chain containing that block can be ignored in favor of another fork containing good blocks. Dealing with a bad parablock includes the following stages: Detection: the bad block must be detected by honest validators. Escalation: the honest validators must start a dispute . Consequences: all involved malicious validators are slashed. The result of the dispute must be transplantable to all other forks so that malicious validators are slashed in all possible histories and so that honest validators will ignore any forks containing that parablock. Parablocks vs. Relay Chain Blocks It is important to understand that a relay chain block does not contain parablocks, but para-headers. Parachain blocks are within the parachain. Thus, it makes more sense to think of relay-chain blocks as having been approved instead of parablocks that have been approved. A relay-chain block containing information about approved parablocks can be considered approved as long as its parent relay-chain block is also approved. Thus, the validity of a relay-chain block depends on the validity of its ancestry. Validators perform two main actions in the Approval Process: Assignments determine which validators perform approval checks on which candidates, ensuring each candidate receives enough random checkers. This stage tracks approval votes to identify when no-show approval checks take suspiciously long. It also tracks relay chain equivocations to determine when adversaries possibly gained foreknowledge about assignments and add more checks in those cases. Assignees determine their own assignments to check specific candidates using two or three assignment criteria , which are based on two possible stories about the relay chain block that included the candidate (i.e. declared the candidate available). Assignment notices are gossiped among nodes so that all validators know which validators should check which candidates, and if any candidate requires more checkers. Approval checks perform the checks by obtaining the candidate, verifying its validity, sending out the approval vote, or initiating a dispute. Approval checks have a no-show timeout window (i.e. longer than one relay chain slot) to succeed in reconstructing the candidate block, redo its erasure coding to check the candidate receipt, and recheck the candidate block itself. A validator becomes tagged as a no-show if it does not approve or dispute within the no-show timeout window. Because validators can be overloaded with assignments, they can intentionally delay sending their assignment notice to avoid creating no-shows (see more in Assignment postponement ). These two steps first run as off-chain consensus protocols using messages gossiped among all validators, and then as on-chain record of those protocols' progress. The on-chain protocol is needed to provide rewards for the off-chain protocol. The on-chain verification has two phases: a) assignments notices and approval votes are recorded in a relay chain block, and b) in another relay chain block notes are fed into the approval code. The gossiped messages are of two types, assignment notices, and approval votes, and are singed with approval keys . Such keys are part of the session keys used by validators. Briefly, approval keys are: Approval assignment keys that are sr25519 keys used only for assignment criteria VRF . Approval vote keys that are ed25519 and would only sign off on a candidate parablock validity. info For detailed information about the approval process, see dedicated section in The Polkadot Parachain Host Implementers' Guide . Accepting a parablock is the result of having passed through the detection stage without dispute, or having passed through and escalation/dispute stage with a positive outcome. Chain Selection ‚Äã After enough secondary checks have been performed on all candidate receipts within a block, validators can vote for that block (and all previous blocks) in GRANDPA. Once the block has more than 2/3 of positive votes, the block is finalized on chain. Chain selection is used to select blocks to build on and finalize. These processes need to consistent among nodes and resilient to a maximum proportion of malicious nodes. The parachain host uses a block authoring system and a finality gadget. The chain selection strategy involves a leaf-selection rule and a set of finality constraints . info For detailed information about chain selection, see dedicated section in The Polkadot Parachain Host Implementers' Guide . Candidate Receipts ‚Äã PoV are typically between 1 MB and 10 MB in size and are not included in the Relay Chain blocks. For Polkadot to scale to hundreds of parachains, PoV need to be represented by something smaller on the Relay Chain: candidate receipts. A para-validator constructs a candidate receipt for a parachain block by signing: The parachain ID. The collator's ID and signature. A hash of the parent block's candidate receipt. A Merkle root of the block's erasure-coded pieces. A Merkle root of any outgoing messages. A hash of the block. The state root of the parachain before block execution. The state root of the parachain after block execution. This information is of constant size, while the actual PoV block of the parachain can be variable length. It is enough information for anyone that obtains the full PoV block to verify the state transition contained inside of it. Erasure Codes ‚Äã Before sending the candidate receipt to the Relay Chain transaction queue, the para-validator who constructs the receipt must also construct an erasure coding of the parachain block. An erasure coding takes a message (in this case, the parachain block and PoV) and creates a set of smaller messages such that you can reconstruct the original message by obtaining a fraction of the smaller messages. In the case of Polkadot the total number of smaller messages is equal to the total number of validators and the fraction is 1/3. The para-validator creates the erasure coding chunks, puts them into their Merkle tree, and sends out each chunk (together with the candidate receipt) to a corresponding validator on the Relay Chain. Validators who receive the receipts with an erasure coding chunk will include the receipt in the Relay Chain queue, where an author can include it in a block. The type of erasure codes used by Polkadot's availability scheme are Reed-Solomon codes, which already enjoy a battle-tested application in technology outside the blockchain industry. One example is found in the compact disk industry. CDs use Reed-Solomon codes to correct any missing data due to inconsistencies on the disk face such as dust particles or scratches. In Polkadot, the erasure codes are used to keep parachain state available to the system without requiring all validators to keep tabs on all the parachains. Instead, validators share smaller pieces of the data and can later reconstruct the entire data under the assumption that 1/3+1 of the validators can provide their pieces of the data. note The 1/3+1 threshold of validators that must be responsive to construct the full parachain state data corresponds to Polkadot's security assumption about Byzantine nodes. Disputes ‚Äã All parachain blocks that are in the finalized relay chain should be valid. This does not apply to backed blocks that are not included. To ensure nothing invalid ends up in the finalized relay chain, there are approval checks (described above) and disputes. The latter ensures that each attempt to include something invalid is caught and the offending validators are punished. Disputes are independent from a particular fork , while backing and approval operate on particular forks. The approval voting stops if an alternative fork (which might not contain the currently-approved candidate) is finalized. The sole purpose of the approval process is to make sure invalid blocks are not finalized. However, even though the danger is past and the offending validators did not manage to get the invalid block approved, those validators need to get slashed for the attempt. A dispute stems from a disagreement between two or more validators. For this to happen, a bad actor needs to distribute an invalid block to honest validators. Scenarios leading to a dispute can be one of the followings (ordered from most to least important): A parablock included on a branch of the relay chain is bad A parablock backed on a branch of the relay chain is bad A parablock seconded, but not backed on any branch of the relay chain, is bad Checking a parachain block requires three pieces of data: the parachain validator code, the availability of data, and the candidate receipt. The validator code is available on-chain and published ahead of time. Thus, a dispute process begins with the availability to ensure the availability of the data. Such a process will conclude quickly if the data is already available, otherwise, the initiator of the dispute must make it available. Disputes have both off- and on-chain components. Slashing is handled on-chain, so votes by validators on either side of the dispute must be placed on-chain. Moreover, a dispute on one branch of the chain must be transposed to all active branches so that misbehavior can be punished in all possible histories. There is, thus, a distinction between local (the one we are looking at) and remote disputes relative to a particular branch of the relay chain. Disputes can be divided into three different phases: Dispute initiation : Disputes are initiated by any validator who finds their opinion on the validity of a parablock in opposition to another issued statement. The initiation begins off-chain by only nodes perceiving that a parablock is bad. The validator can be one of the para-validators (i.e. one of the backers) or one of the approval checkers. Note that if the dispute occurs during the backing phase, the initiator must make the data available while if the dispute occurs during the approval process the data is already available. Dispute participation : Once becoming aware of the dispute, all validators must participate. Dispute conclusion : Disputes conclude after a 2/3 supermajority is reached on either side. Disputes may also conclude after a timeout. This will only happen if the majority of validators are unable to vote for some reason. The on-chain component of the dispute can be initiated by providing any two conflicting votes and it also waits for a 2/3 supermajority on either side. The component also tracks which parablocks have already been disputed so that the same parablock can be disputed only once on any branch of the relay chain. Inclusion is halted for the parachain until the dispute resolves. info For detailed information about disputes, see dedicated section in The Polkadot Parachain Host Implementers' Guide . In the Guide, there are also more details about disputes' flows . Network Asynchrony ‚Äã We have mentioned how a relay chain block author must select the candidate and note it on the Relay Chain (we say the block is backed). The relay chain block author is selected by BABE , which is a forkful algorithm. This means that different block authors are chosen at the same time, and they may not work on the same block parent (i.e. the representations in the previous figures are simplistic). Also, the sets of validators and parachains are not fixed, and the validators' assignments to parachains is also flexible. We say that the network is asynchronous since there will be validators who have received a block and other validators who did not. Thus, the network is variable, and it exists in multiple states. In the figure below ( left ), Group 1 received block C while Group 2 did not due to network asynchrony. Validators in Group 2 can build another block on top of B, called C'. Assume that afterward, some validators become aware of both C and C' while others remain aware of one of them ( right ). Validators in Group 3 must be aware of the network state in each head (C and C‚Äô), and they may contribute to some or full extent on both. It is possible that due to network asynchrony, two forks may grow in parallel for some time, but eventually, one fork will be chosen by the finality gadget. In the absence of an adversarial network, it is unlikely that two forks will coexist for some time as there will be validators aware of both chain heads. Further Resources ‚Äã Path of a Parachain Block - Article by Parity analyst Joe Petrowski expounds on the validity checks that a parachain block must pass in order to progress the parachain. Availability and Validity - Paper by the W3F Research Team that specifies the availability and validity protocol in detail. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Introduction to Parachains Next System Parachains Main Actors Validators Collators Fishermen: Deprecated Protocols' Summary Parachain Protocol Availability and Validity (AnV) Protocol Inclusion Pipeline Overview Parachain Phase Relay Chain Submission Phase Availability and Unavailability Phase Failure to Inclusion Approval Process Overview Assignments & Secondary Checks Chain Selection Candidate Receipts Erasure Codes Disputes Network Asynchrony Further Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
System Parachains General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Introduction to Parachains Protocol Overview System Parachains Parachain Slot Auctions Parachain Crowdloans Parachains FAQ Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Parachains System Parachains On this page System Parachains Overview ‚Äã System parachains are those that contain core Polkadot protocol features, but in parachains rather than the Relay Chain. Rather than leasing an execution core by economic means (e.g., auction), execution cores are allocated by network governance . By hosting core protocol logic in parachains instead of the Relay Chain, Polkadot uses its own scaling technology -- namely, parallel execution -- to host itself . System parachains remove transactions from the Relay Chain, allowing more Relay Chain blockspace to be used for Polkadot's primary purpose: validating parachains. System parachains always defer to on-chain governance to manage their upgrades and other sensitive actions. That is, they do not have their own native tokens or governance systems separate from DOT KSM. In fact, there will likely be a system parachain specifically for network governance. note In the past, these were often called "Common Good Parachains", so you may come across articles and discussions using that term. As the network has evolved, that term has been confusing in many cases, so "System Parachains" is preferred now. A discussion on this evolution can be found in this forum thread . Existing System Chains ‚Äã Asset Hub ‚Äã The Asset Hub on both Polkadot and Kusama are the first system parachains. The Asset Hub is an asset portal for the entire network. It helps asset creators (e.g. reserve backed stablecoin issuers) to track the total issuance of their asset in the Polkadot network, including amounts that have been transferred to other parachains. It is also the point where they can transact, to mint and burn, to manage the on-chain asset. The Asset Hub also supports non-fungible assets (NFTs) via the Uniques pallet and the new nfts pallet . For more information about NFTs see the dedicated wiki page . This logic for asset management is not encoded in smart contracts, but rather directly in the runtime of the chain. Because of the efficiency of executing logic in a parachain, fees and deposits are about 1/10th of their respective value on the Relay Chain. These low fee levels mean that the Asset Hub is well suited for handling DOT balances and transfers as well as managing on-chain assets. For example, the existential deposit for Polkadot is , while only on the Asset Hub. Encointer ‚Äã Encointer is a blockchain platform for self-sovereign ID and a global universal basic income. With referendum 158 Encointer was registered as the second system parachain on Kusama's network. The functionality of Encointer adds logic to the Relay Chain that aims to bring financial inclusivity to WEB3 and mitigate Sybil attacks with a novel Proof of Personhood (PoP) system for unique identity. Encointer offers a framework that, in principle, allows for any group of real people to create, distribute, and use their own digital community tokens. Referendum 187 introduced a runtime upgrade bringing governance and full functionality for communities to be able to use the protocol. Encointer aims to invert the Cantillon Effect , where money is issued at the bottom, and not as credit to businesses or creditworthy individuals. This way, every individual gets a universal basic income (UBI) . To resist Sybil attacks, the Encointer protocol uses a PoP mechanism to foster a unique identity system. The notion is that a person can only be present at one place at a given time. Participants are requested to attend physical key-signing ceremonies with small groups of random people at randomized locations, where these local meetings are part of one global ceremony that co-occur. Participants use the Encointer wallet app to participate in these ceremonies, and the wallet enables the management of local community currencies. Watch an Encointer ceremony in action in this video . The protocol involves other mechanisms to protect the privacy of users in addition to the physical key-signing ceremonies. Encointer was accepted as a system chain based on its offer of a Sybil defense mechanism as a basis for digital democracy. This can also be adapted by other chains, which can use the unique identity system to prevent Sybil attacks and use PoP for token airdrops or faucets. tip To learn more about Encointer, check out the official Encointer book . Collectives ‚Äã The Polkadot Collectives parachain was added in Referendum 81 and exists only on Polkadot (i.e., there is no Kusama equivalent). The Collectives chain hosts on-chain collectives that serve the Polkadot network. Some of these collectives are the Polkadot Alliance and the Polkadot Technical Fellowship . These on-chain collectives will play important roles in the future of network stewardship and decentralized governance. Networks themselves can act as collectives and express their legislative voices as single opinions within other networks. This is achieved with the assistance from a bridge hub . Bridge Hubs ‚Äã Before Polkadot and Kusama supported their first parachains, the only way to design a bridge was to put the logic onto the Relay Chain itself. Since both networks now support parachains, it makes sense to have a parachain on each network dedicated to bridges. This is because of the execution isolation provided by parachains. See the Bridges page for information on the latest bridge projects. Currently, a Bridge Hub parachain is in development that will be a portal for trust-minimized bridges to other networks. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Protocol Overview Next Parachain Slot Auctions Overview Existing System Chains Asset Hub Encointer Collectives Bridge Hubs ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Parachain Slot Auctions General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Introduction to Parachains Protocol Overview System Parachains Parachain Slot Auctions Parachain Crowdloans Parachains FAQ Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Parachains Parachain Slot Auctions On this page Parachain Slot Auctions For a parachain to be added to Polkadot it must inhabit one of the available parachain slots. The number of parachain slots is are not unbounded on Polkadot, as only a limited number are available. A limited number of slots are unlocked every few months through on-chain governance. If a parachain wants to have guaranteed block inclusion at every Relay Chain block, it must acquire a parachain slot. The development of on-demand parachains (previously referred to as parathreads) is in progress. The parachain slots will be leased according to an unpermissioned candle auction , with several alterations related to improving security while operating on a blockchain. See Rationale for additional details. Auction Schedule ‚Äã Loading Auctions... Mechanics of a Candle Auction ‚Äã Candle auctions are a variant of open auctions where bidders submit bids that are increasingly higher. The highest bidder at the conclusion of the auction is considered the winner. Candle auctions were originally employed in the 16th century for the sale of ships. The name is derived from the system by which the auction length was determined. The phrase "inch of a candle" refers to the length of time required for a candle to burn down 1 inch. When the flame extinguishes and the candle goes out, the auction terminates and the standing bid at that point in time prevails the winner. When candle auctions are used online, they require a random number to decide the moment of termination. Parachain slot auctions differ slightly from a normal candle auction in that they do not randomly terminate the auction. Instead, they run for an entire fixed duration and the winner is randomly chosen retroactively. The candle auction on Polkadot is split into two parts: The opening period which is in effect immediately after the auction starts. This period lasts for one day and eighteen hours and serves as a buffer time for parachain candidates to setup their initial bids, and likely start executing their strategy on how to win the slot auction. During the opening phase, bids will continue to be accepted, but they do not have any effect on the outcome of the auction. The ending period follows the opening period for five additional days, where the auction is subject to end based on the candle auction mechanism. The auction‚Äôs ending time can occur any time within the ending period. This time is automatically and randomly chosen by the Verifiable Random Function (VRF) . The probability of winning the auction is equal to the number of blocks that contain a winning bid, divided by the total number of blocks in the ending period. The random ending is managed by propagating through the entire ending period, where a snapshot is taken at each block within the ending period to capture the winners for that given block. At the end of the period, one of the snapshots is randomly selected to determine the winner of the auction. The parachain candidate with the highest bid at the ending time chosen by the Verifiable Random Function wins the slot auction. A parachain auction on Polkadot lasts exactly one week from the start: 1 day and 18 hours for the starting period, days for the ending period (candle auction phase) and 6 hours for determining the auction winner. info Crowdloan contributions cannot be made during these six hours when the winning block for the auction is being determined on-chain. More details on this are available in the Network Implementation section. Randomness in action ‚Äã The following example will showcase the randomness mechanics of the candle auction for the ninth auction on Kusama. Keep in mind that the candle phase has a uniform termination profile and has an equal probability of ending at any given block, and the termination block cannot be predicted before or during the auction. Auction 9 starts at block 9362014 . The auction has a full duration equal to block 9362014 + 72000 Here, block 72000 is the "ending period", which is divided into 3600 samples of 20 blocks . Figuratively, the candle is lit, and the candle phase lasts for 72,000 blocks. The winning sample during the ending period had the index 1078 . Sample 1078 is the winner Sample 1078 refers to the winner as of block 9362014 + 21560 , which equals block 9383574 . The parent block was a new BABE session in the Logs , which updated the randomness that was used to select that sample index . Inspecting the block state You can inspect the state at the end of block 9434277 to see the sample indices with an archive node . The digest in the Logs of 9434277 is decodable and contains the random value as well as the BABE authorities. As a result, the winner of this auction was not the highest bid during the full duration. Rationale ‚Äã The open and transparent nature of blockchain systems opens attack vectors that are non-existent in traditional auction formats. Normal open auctions in particular can be vulnerable to auction sniping when implemented over the internet or on a blockchain. Auction sniping takes place when the end of an auction is known and bidders are hesitant to bid their true price early, in hopes of paying less than they actually value the item. For example, Alice may value an item at auction for 30 USD. She submits an initial bid of 10 USD in hopes of acquiring the items at a lower price. Alice's strategy is to place incrementally higher bids until her true value of 30 USD is exceeded. Another bidder Eve values the same item at 11 USD. Eve's strategy is to watch the auction and submit a bid of 11 USD at the last second. Alice will have no time to respond to this bid before the close of the auction and will lose the item. The auction mechanism is sub-optimal because it has not discovered the true price of the item and the item has not gone to the actor who valued it the most. On blockchains this problem may be even worse, since it potentially gives the producer of the block an opportunity to snipe any auction at the last concluding block by adding it themselves while ignoring other bids. There is also the possibility of a malicious bidder or a block producer trying to grief honest bidders by sniping auctions. For this reason, Vickrey auctions , a type of sealed-bid auction where bids are hidden and only revealed at a later phase, have emerged as a well-regarded mechanic. For example, this mechanism is leveraged to auction human readable names on the ENS . The Candle auction is another solution that does not require a two-step commit and reveal schemes (a main component of Vickrey auctions), which allows smart contracts to participate. Candle auctions allow everyone to always know the states of the bid, but they do not reveal when the auction has officially ended. This helps to ensure that bidders are willing to make their true bids early. Otherwise, they may find themselves in a situation where the auction was determined to have ended before having an opportunity to bid. Network Implementation ‚Äã Polkadot will use a random beacon based on the Verifiable Random Function (VRF) . The VRF will provide the base of the randomness, which will retroactively determine the end-time of the auction. The slot durations are capped to 2 years and divided into 3-month periods. Parachains may lease a slot for any combination of periods of the slot duration. Parachains may lease more than one slot over time, meaning that they could extend their lease to the network past the maximum duration by leasing a contiguous slot. Individual parachain slots are fungible This means that parachains do not need to always inhabit the same slot, however they always must maintain a slot to remain a parachain. Bidding ‚Äã Parachains or parachain teams, bid in the auction by specifying the slot range that they want to lease and the number of tokens they are willing to reserve. Bidders can be either ordinary accounts, or use the crowdloan functionality to source tokens from the community. For a more in-depth comparison between both of these options for gaining a parachain slot, check out this section on Crowdloan Campaigns vs Parachain Auctions . Parachain slots at genesis --3 months-- v v Slot A | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |... Slot B | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |... Slot C |__________| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |... Slot D |__________| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |... Slot E |__________|___________| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |... ^ ^ ---------------------------------------------max lease----------------------------------------- Each period of the range 1 - 4 represents a 3-month duration for a total of 2 years Bidders will submit a configuration of bids specifying the token amount they are willing to bond and for which periods. The slot ranges may be any of the periods 1 - n , where n is the number of periods available for a slot. ( n = for Polkadot) If you bond tokens with a parachain slot, you cannot stake with those tokens. In this way, you pay for the parachain slot by forfeiting the opportunity to earn staking rewards. A bidder configuration for a single bidder may look like the following pseudocode example: const bids = [ { range : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] , bond_amount : 300 , } , { range : [ 1 , 2 , 3 , 4 ] , bond_amount : 777 , } , { range : [ 2 , 3 , 4 , 5 , 6 , 7 ] , bond_amount : 450 , } , ] ; The important concept to understand from this example is that bidders may submit different configurations at different prices ( bond_amount ). However, only one of these bids would be eligible to win exclusive of the others. The winner selection algorithm will pick bids that may be non-overlapping in order to maximize the amount of tokens held over the entire lease duration of the parachain slot. This means that the highest bidder for any given slot lease period might not always win (see the example below ). A random number, which is based on the VRF used by Polkadot, is determined at each block. Additionally, each auction will have a threshold that starts at 0 and increases to 1. The random number produced by the VRF is examined next to the threshold to determine if that block is the end of the auction within the so-called ending period . Additionally, the VRF will pick a block from the last epoch to access the state of bids which can help aid in mitigating some types of attacks from malicious validators. Examples ‚Äã There is one parachain slot available. Charlie bids 75 for the range 1 - 8. Dave bids 100 for the range 5 - 8. Emily bids 40 for the range 1 - 4. Let's calculate each bidder's valuation according to the algorithm. We do this by multiplying the bond amount by the number of periods in the specified range of the bid. Charlie - 75 * 8 = 600 for range 1 - 8 Dave - 100 * 4 = 400 for range 5 - 8 Emily - 40 * 4 = 160 for range 1 - 4 Although Dave had the highest bid in accordance to token amount per period, when we do the calculations we see that since he only bid for a range of 4, he would need to share the slot with Emily who bid much less. Together Dave and Emily's bids only equals a valuation of 560 . Charlie's valuation for the entire range is 600 . Therefore Charlie is awarded the complete range of the parachain slot. Parachain Lease Extension ‚Äã Before the slot lease expires, parachains have to bid and win another auction for continuity of the lease. To avoid any downtime in connectivity and minimize the risk of losing a subsequent auction, parachain teams need to plan ahead to bid for the lease extension before their current lease period ends. Explained in the section above, each auction lets you bid for 8 LPs (Lease Periods) which enables two scenarios for the parachain's lease extension. Lease Extension with Overlapping Slots ‚Äã Acquire a slot where the first lease period is before the last lease period of the current slot. Register a new paraId Win a slot auction with the new paraId The parachain team has access to two slots: one that will end soon one that just started Both slots have at least one LP in common. When the old slot transitions to their last LP, the parachain can swap the slots. This can be done via on-chain governance . The swap call is available in the registrar pallet. Any two parachains can swap their slots via XCM The slot swap via XCM requires two live parachains to send an XCM message to the relay chain to approve the swap. A parachain team with access to two overlapping slots can start a shell parachain on the new slot and swap it with their actual parachain on the old slot, thus ensuring continuity of the lease. Lease Extension with Non-Overlapping Slots ‚Äã Acquire a slot where the first LP starts right after the end of the last LP of the current slot. In this case, the parachain can bid directly with their current paraId , and it will be automatically extended without the need of swapping. This method has the advantage of not having superfluous LP's on different slots owned by the same team, however it has the disadvantage of losing flexibility on when to win a new slot: if the team does not win the exact slot, then it will suffer some downtime until it wins a new slot. Resources ‚Äã How do Parachain Slot Auctions Work Parachain Allocation - W3F research page on parachain allocation that goes more in depth to the mechanism Research Update: The Case for Candle Auctions - W3F breakdown and research update about candle auctions Front-Running, Smart Contracts, and Candle Auctions W3F Research team discusses how to remedy current blockchain auction setbacks with candle auctions Edit this page Last updated on Jun 30, 2023 by Filippo Previous System Parachains Next Parachain Crowdloans Auction Schedule Mechanics of a Candle Auction Randomness in action Rationale Network Implementation Bidding Examples Parachain Lease Extension Lease Extension with Overlapping Slots Lease Extension with Non-Overlapping Slots Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Parachain Crowdloans General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Introduction to Parachains Protocol Overview System Parachains Parachain Slot Auctions Parachain Crowdloans Parachains FAQ Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Parachains Parachain Crowdloans On this page Parachain Crowdloans Polkadot allows parachains to source tokens for their parachain bids in a decentralised crowdloan. Contributing to a crowdloan If you are here for guidance on how to contribute for a crowdloan, watch the video below or read this support article on crowdloans . Testing on Rococo For information on how to participate in the crowdloan and parachain auction testing on Rococo, please see the Rococo content Crowdloan Campaigns vs Parachain Auctions ‚Äã It is important to recognize that starting a crowdloan campaign is optional for participating in a parachain slot auction. The parachain slot auction can also be won directly through self-funding without community involvement. To reiterate, crowdloan campaigns are just one of the means to win auctions, which allow the community to participate in a trustless and permissionless way. Let's look at a scenario where Project A is hoping to gain a parachain slot on Polkadot, but they don't have enough tokens to bid directly to win the parachain auction. Project A could benefit from starting a new crowdloan campaign to help secure a parachian slot. Crowdloans are trustless and are supported natively on Polkadot, allowing the community to bond their tokens on Project A's behalf for the entire parachain lease duration. This will allow Project A to compete with projects that may have access to greater capital, given the project has sufficient community support. In return, the community contributors are rewarded by the projects that win the parachain slot, which would typically compensate for the opportunity cost of bonding their tokens for the lease duration. On the other hand, let's say Project B, which is more established and has access to capital, is hoping to secure a parachain slot through self-funding. Project B is not relying on any community funding (at least via the crowdloan mechanism), so they will have to determine how much funding they can allocate towards winning a slot. Project B is in full control of how much they are ultimately willing to contribute towards their goal of gaining a parachain slot. Project B need not work on coming up with a rewards model for community contributors like Project A. In contrast, crowdloan campaigns are beneficial for projects that have access to limited capital but have strong community support. They are also beneficial for projects that are able to successfully bid to win the auction with self-funding, but are looking for a mechanism to bootstrap their community and get noticed by the key actors in the ecosystem. It is publicly visible on-chain whether or not a project is bidding directly or through a crowdloan campaign. More details regarding the process of creating and executing a crowdloan campaign are provided below. Starting a Crowdloan Campaign ‚Äã Anyone who has registered a parachain can create a new crowdloan campaign for a slot by depositing a specified number of tokens. A campaign is configured as a range of slots (i.e. the duration of the parachain will bid for), a cap, and a duration. The duration can last over several auctions as long as the range of slots matches those of the auction (i.e. the first lease period of the crowdloan is the same or bigger than that of the auction). This means that a team will not need to restart the campaign just because they do not secure a slot on their first attempt. Crowdloan Submission Deposit Required In order to create a new crowdloan campaign, your account must have 500 DOT transferrable which will be reserved for the duration of the crowdloan When setting the parameters of a crowdloan campaign, consider the following: A crowdloan campaign can start well before the auction slot is opened. The campaign creation form requires setting a crowdloan cap ‚Äî the maximum amount a campaign can collect. A team can still win an auction if the cap is not reached. Set the desired end of the crowdloan in the "Ending block" field. This helps to ensure that the crowdloan is live during the entire duration of the auction. For example, if an auction starts in three days and will last for five days, you may want to set your crowdloan to end in 10 days, or a similar timescale. One way of calculating the ending block number is adding: (10 * 60 * 24 * 7) * (x * 6) + y x is the number of auction periods you want the crowdloan to continue for y is the current block number (Blocks/Min * Min/Hour * Hour/Day * Day/Week) * (x[Period] * Week/Period) + y[BlockNumber] "First period" field refers to the first period you want to bid for. If the current auction encompasses periods (3, 4, 5, 6) , your first period can be at least 3 . The last slot must also be within that range. You can only cancel an ongoing crowdloan if no contributions have been made. Your deposit will be returned to you. Prior to the start of the crowdloan campaign, the owner will upload the parachain data. Once the crowdloan is live, the parachain configuration will be locked and will be deployed as the parachain's runtime. Of course, once the parachain is running it can always change via runtime upgrades (as determined through its own local governance). Supporting a Crowdloan Campaign ‚Äã Contributing to Crowdloans ‚Äã Minimum Crowdloan Contribution The minimum balance for contributions for a crowdloan campaign is currently set to . This is in an attempt to make crowdloans as accessible as possible while maintaining a balance to justify the use of the network's resources. Each created campaign will have an index. Once a crowdloan campaign is open, anyone can participate by sending a special transaction that references the campaign's index. Tokens used to participate must be transferable ‚Äî that is, not locked for any reason, including staking, vesting, and governance ‚Äî because they will be moved into a module-controlled account that was generated uniquely for this campaign. See system accounts for more information. Do not send Crowdloan contributions directly to the Parachain address All crowdloan contributions are handled by the Crowdloan module‚Äôs logic where a campaign is identified by an index, not by address. Never transfer tokens to an address in support of a campaign . It is up to individual parachain teams to decide if and how they want to reward participants who forgo staking and choose to lock their tokens in support of the parachain‚Äôs campaign. As one can imagine, rewards will take many forms and may vary widely among projects. If a crowdloan campaign is successful, that parachain will be on-boarded to the Relay Chain. The collective tokens will be locked in that parachain's account for the entire duration that it is active. Withdraw Crowdloaned Tokens ‚Äã Participants will be able to reclaim their tokens in one of two ways: If the campaign was successful, then the parachain will enter a retirement phase at the end of its lease. During this phase, participants can withdraw the tokens with which they participated. If the campaign was unsuccessful, then this retirement phase will begin at the campaign's configured end, and participants can likewise withdraw their tokens. crowdloan.contribute extrinsic is trustless Contributing to a crowdloan through Polkadot JS Apps (which uses crowdloan.contribute extrinsic) guarantees that you receive your tokens after the campaign ends. If you intend to make contributions through other websites and custodial service providers like central exchanges, review their terms and conditions thoroughly and assess the risks associated. Note: When the lease periods won by the crowdloan have finished, or the crowdloan has ended without winning a slot, anyone can trigger the refund of crowdloan contributions back to their original owners. This can be done through the permissionless crowdloan.refund extrinsic available on Polkadot JS Apps > Developer > Extrinsics page, by specifying the parachain ID. This extrinsic may need to be issued multiple times, if the list of contributors is too long. All contributions must be returned before the crowdloan is entirely deleted from the system. Many projects will have dashboards available that allow users to participate in their crowdloans. PolkadotJS apps also offers a breakdown of ongoing crowdloans on the Apps page . Here is an example of the crowdloans in play during the very first Kusama auction. Furthermore, check out this video on How to Participate in Crowdloans for steps on how to access available crowdloans on PolkadotJS apps. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Parachain Slot Auctions Next Parachains FAQ Crowdloan Campaigns vs Parachain Auctions Starting a Crowdloan Campaign Supporting a Crowdloan Campaign Contributing to Crowdloans Withdraw Crowdloaned Tokens ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Parachains FAQ General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Introduction to Parachains Protocol Overview System Parachains Parachain Slot Auctions Parachain Crowdloans Parachains FAQ Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Parachains Parachains FAQ On this page Parachains FAQ General ‚Äã What is "parachain consensus"? ‚Äã "Parachain consensus" is special in that it will follow the Polkadot Relay Chain. Parachains cannot use other consensus algorithms that provide their own finality. Only sovereign chains (that must bridge to the Relay Chain via a parachain) can control their own consensus. Parachains have control over how blocks are authored and by whom. Polkadot guarantees valid state transitions. Executing a block finality outside the context of the relay chain is outside the scope of trust that Polkadot provides. How about parachains that are not Substrate-based? ‚Äã Substrate provides FRAME Pallets as part of its framework to seamlessly build a rustic-based blockchain. Part of FRAME are pallets that can be used for consensus. Polkadot being a Substrate-based chain rely on BABE as the block production scheme and GRANDPA as the finality gadget as part of its consensus mechanism. Collectively, this is a Hybrid Consensus Model , where block production and block finality are separate. Parachains only need to produce blocks as they can rely on the relay chain to validate the state transitions. Thus, parachains can have their own block production where the collators act as the block producers, even if the parachain is not Substrate-based. Is 100 a hard limit on the number of Parachains that can be supported? ‚Äã No.Polkadot network went through a significant number of optimizations, and there are several updates planned in the near future. The exact number of parachains that the Relay Chain can support without any degradation in performance is yet to be discovered. Also, with the blockspace over blockchains paradigm which brings parathreads into the picture, there is no hard limit number on the number of blockchains that can be supported by Polkadot . What happens to parachains when the number of validators drops below a certain threshold? ‚Äã The minimal safe ratio of validators per parachain is 5:1. With a sufficiently large set of validators, the randomness of their distribution along with availability and validity will make sure security is on-par. However, should there be a big outage of a popular cloud provider or another network connectivity catastrophe, it is reasonable to expect that the number of validators per chain will drop. Depending on how many validators went offline, the outcome differs. If a few validators went offline, the parachains whose validator groups are too small to validate a block will skip those blocks. Their block production speed will slow down to an increment of six seconds until the situation is resolved and the optimal number of validators is in that parachain's validator group again. If anywhere from 30% to 50% of the validators go offline, availability will suffer because we need two-thirds of the validator set to back the parachain candidates. In other words, all parachains will stop until the situation is resolved. Finality will also stop, but low-value transactions on the Relay Chain should be safe enough to execute, despite common forks. Once the required number of validators are in the validator set again, parachains will resume block production. Given that collators are full nodes of the Relay Chain and the parachain they are running, they will be able to recognize a disruption as soon as it occurs and should stop producing block candidates. Likewise, it should be easy for them to recognize when it's safe to restart block production - perhaps based on finality delay, validator set size or some other factor that is yet to be decided within Cumulus . Parachain Development Kits (PDKs) ‚Äã Parachain Development Kits are a set of tools that enable developers to create their own applications as parachains. For more information, see the PDK content](/docs/build-pdk#parachain-development-kit-pdk) and Parachain Development page . Security ‚Äã Is security correlated to the number of validators? What about the number of parachains? ‚Äã Security is independent of the number of parachains that are connected to the Polkadot Relay Chain. The correlation of security and the number of validators exists as the higher number of validators will give the network stronger decentralization properties and make it harder to try to take down. However, the biggest indicator of the security of the network is the economic signal of the number of DOT that are bonded and staked. The greater the number of DOT staked by honest validators and nominators, the higher the minimum amount of DOT an attacker would need to acquire a validator slot. In what scenarios do parachains need their own security? ‚Äã Most parachains will not need to worry about their own security, since all state transitions will be secured by the Polkadot Relay Chain validator set. However, in some cases (which are considered more experimental), parachains may require their own security. In general, these cases will revolve around lack of data available to Relay Chain validators. One example is if the state transition function is some succinct or zero-knowledge proof, the parachain would be responsible for keeping its data available as the Relay Chain won't have it. Additionally, for chains with their own consensus, like the one that enables fast payments on Blink Network , there would probably need to be a Byzantine agreement between stakers before a parachain block is valid. The agreement would be necessary because the data associated with the fast consensus would be unknown to Relay Chain validators. Slot Auctions ‚Äã How will parachain slots be distributed? ‚Äã Parachain slots are acquirable through auction. For more information on the auction process, please see the parachain slot auctions article. Additionally, some parachain slots will be set aside to run parathreads ‚Äî chains that bid on a per-block basis to be included in the Relay Chain. (Parathreads are not implemented yet.) Why doesn't everyone bid for the max length? ‚Äã For the duration of the slot, the tokens used for bidding in the auction are locked up. This suggests there is an opportunity cost associated with bidding, as the tokens could have been leveraged for something else. How does this mechanism help ensure parachain diversity? ‚Äã The method for dividing the parachain slots into intervals was partly inspired by the desire to allow for a greater amount of parachain diversity, while preventing particularly large and well-funded parachains from hoarding slots. By making each period a three-month duration but the overall slot a 2-year duration, the mechanism can cope with well-funded parachains, ensuring they secure a slot at the end of their lease, while gradually allowing other parachains to enter the ecosystem to occupy the durations that are not filled. For example, if a large, well-funded parachain has already acquired a slot for range 1 - 8, they would be very interested in getting the next slot that would open for 2 - 9. Under this mechanism, that parachain could acquire just period 9 (since that is the only one required) and allow the 2 - 8 range of the second parachain slot to be occupied by another party. Why is randomness difficult on blockchains? ‚Äã Generating a random number trustlessly on a transparent and open network opens up the possibility for bad actors to attempt to alter or manipulate the randomness. There have been a few solutions that have been proposed, including hash-onions like RANDAO and verifiable random functions (VRFs). The latter is what Polkadot uses as a base for its randomness. Are there other ways of acquiring a slot besides the candle auction? ‚Äã Aa parachain slot can also be acquired through a secondary market where a 3rd party has already won a parachain slot and has the ability to resell the slot along with the associated deposit of tokens that are locked up to another buyer. This would allow the seller to get liquid tokens in exchange for the parachain slot and the buyer to acquire the slot as well as the deposited tokens. A number of system or common-good parachains may be granted slots by the governing bodies of the Relay Chain. System parachains can be recognized by a parachain ID lower than 1_000, and common-good parachains by a parachain ID between 1_000 and 1_999. Other parachains will have IDs 2_000 or higher. Such parachains would not have to bid for or renew their slots as they would be considered essential to the ecosystem's future. How are auctions scheduled? ‚Äã The parachain slot auctions are scheduled through the governance. At least 2/3 of the Council can initiate an auction, however, Root origin (via referendum) is needed to cancel an auction. Here is a proposal that gives a glimpse of what goes into planning auctions schedule - Proposed Polkadot Auction Schedule 2022 . Edit this page Last updated on Jun 30, 2023 by Filippo Previous Parachain Crowdloans Next Bridges General What is "parachain consensus"? How about parachains that are not Substrate-based? Is 100 a hard limit on the number of Parachains that can be supported? What happens to parachains when the number of validators drops below a certain threshold? Parachain Development Kits (PDKs) Security Is security correlated to the number of validators? What about the number of parachains? In what scenarios do parachains need their own security? Slot Auctions How will parachain slots be distributed? Why doesn't everyone bid for the max length? How does this mechanism help ensure parachain diversity? Why is randomness difficult on blockchains? Are there other ways of acquiring a slot besides the candle auction? How are auctions scheduled? ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Bridges General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Bridges Cross Consensus Messaging (XCM) Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Bridges On this page Bridges A cornerstone technology of blockchain interoperability is the blockchain bridge. Blockchain bridges are ways for two economically sovereign and technologically diverse chains to communicate with each other. Bridge designs come in a variety of flavors ranging from centralised and trusted to more decentralised and trustless. Polkadot favors the latter bridge designs for its ecosystem. However, there is nothing that blocks a development team from building and deploying the former. While bridge designs are now getting to a place where they are sufficiently planned out, there have not been too many used heavily in production. For this reason, you can consider this page a work in progress. It will be updated as more information is determined and available. Purpose of bridging Bridges are specifically for making the Polkadot ecosystem compatible with external blockchains such as Bitcoin, Ethereum, or Tezos (among others). For information on XCM, the native interoperability technology that allows parachains to communicate trustlessly, please see the dedicated cross consensus page on the Wiki. Bridging Methods ‚Äã Building a bridge that is as decentralised and trustless as possible can be done through any of the following methods (ordered by suggested methodology): Bridge pallets - For Substrate-native chains, use a bridge pallet (e.g. Kusama <-> Polkadot bridge, since both networks' parachains use Substrate). Smart contracts - If the chain is not on Substrate, you should have smart contracts on the non-Substrate chain to bridge (e.g. Ethereum mainnet will have a bridge smart contract that initiates Eth transactions based on incoming XCMP messages). Higher-order protocols - If your chain does not support smart contracts (e.g. Bitcoin), you should use XClaim or similar protocols to bridge. via Bridge Pallets ‚Äã Receiving messages on Polkadot from an external, non-parachain blockchain can be possible through a Substrate pallet. The Substrate instance can then be deployed to Polkadot either as a system-level parachain (native extension to the core Polkadot software) or as a community-operated parachain. An example of a bridge that would strictly use bridge pallets would be a Kusama <-> Polkadot bridge, since both use parachains based on Substrate. For the standalone chains that will not have a parachain bridging module on Polkadot (non-Substrate), it will be necessary to deploy bridge contracts (see below). via Smart Contracts ‚Äã Given the generality of blockchain platforms with Turing-complete smart contract languages, it is possible to bridge Polkadot and any other smart contract capable blockchain. Those who are already familiar with Ethereum may know of the now archived Parity Bridge and the efforts being made to connect PoA sidechains to the Ethereum mainnet. The Parity bridge is a combination of two smart contracts, one deployed on each chain, that allow for cross-chain transfers of value. As an example of usage, the initial Parity Bridge proof of concept connects two Ethereum chains, main and side . Ether deposited into the contract on main generates a balance denominated in ERC-20 tokens on side . Conversely, ERC-20 tokens deposited back into the contract on side can free up Ether on main . note To learn more on how Bitcoin and Ethereum can cooperate and collaborate Through Polkadot, check out this explainer video here via Higher-Order Protocols ‚Äã Higher-order protocols (like XCLAIM ) can be used to bridge but should only be used when other options are not available. XCLAIM, in particular, requires any swappable asset to be backed by a collateral of higher value than the swappable assets, which adds additional overhead. An example of a network that would be well-suited for higher-order protocols would be Bitcoin, since it does not support smart-contracts and it's not based on Substrate. Examples ‚Äã Ethereum Bridge (Smart Contracts <-> Polkadot) ‚Äã As explained by Dr. Gavin Wood in a blog post from late 2019, there are three ways that the Polkadot and Substrate ecosystem can be bridged to the Ethereum ecosystem. Polkadot <-> Ethereum Public Bridge. Substrate <-> Parity Ethereum (Openethereum) Bridge. The Substrate EVM module. Please read the blog article for fuller descriptions of each one of these options. Bitcoin Bridge (XCLAIM <-> Substrate <-> Polkadot) ‚Äã The Interlay team has written a specification on a Bitcoin bridge that is based on the XCLAIM design paper. The protocol enables a two-way bridge between Polkadot and Bitcoin. It allows holders of BTC to "teleport" their assets to Polkadot as PolkaBTC, and holders of PolkaBTC to burn their assets for BTC on the Bitcoin chain. The Bitcoin bridge, as documented in the specification, is composed of two logically different components: The XCLAIM component maintains all accounts that own PolkaBTC. The BTC-Relay is responsible for verifying the Bitcoin state when a new transaction is submitted. For full details on how it works, please refer to the specification. There is now a reference implementation and testnet available . Additional Resources and Examples ‚Äã Parity Bridges Common Resources Substrate/Ethereum Bridge - ChainSafe and Centrifuge were awarded a grant in W3F Grants Wave 5 to build a Substrate to Ethereum two-way bridge. PolkaBTC (Bitcoin <-> Polkadot Bridge) EOS Bridge - The Bifrost team was awarded a grant in W3F Grants Wave 5 to build a bridge to EOS. Tendermint Bridge - ChorusOne was awarded a grant in Wave 5 to build a GRANDPA light client in Tendermint. Interlay BTC Bridge - The Interlay team was awarded a grant in W3F grants Wave 5 to build a trust-minimized BTC bridge. ChainX BTC Bridge - ChainX have implemented a BTC to Substrate bridge for their parachain. POA Network Case study of POA Network's implementation of Parity's bridge chain solution. Edgeth Bridge - a bridge from Ethereum to Edgeware chain (a Substrate-based chain) - now defunct and not maintained, but a good example. XCLAIM - XCLAIM is a framework for achieving trustless and efficient cross-chain exchanges using cryptocurrency-backed assets. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Parachains FAQ Next Cross Consensus Messaging (XCM) Bridging Methods via Bridge Pallets via Smart Contracts via Higher-Order Protocols Examples Ethereum Bridge (Smart Contracts <-> Polkadot) Bitcoin Bridge (XCLAIM <-> Substrate <-> Polkadot) Additional Resources and Examples ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot's XCM General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Bridges Cross Consensus Messaging (XCM) Cross-Consensus Message Format (XCM) XCM Pallet XCM Use-cases XCM Transport XCM Virtual Machine (XCVM) XCM Instructions Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Cross Consensus Messaging (XCM) Polkadot's XCM Polkadot's Cross-Consensus Messaging Format (XCM). üìÑÔ∏è Cross-Consensus Message Format (XCM) XCM, The Messaging Format at the Forefront of Interoperability. üìÑÔ∏è XCM Pallet Mechanics of the XCM Pallet and its role in Polkadot's Ecosystem. üìÑÔ∏è XCM Use-cases Cross-consensus Interactions and the XCM Format. üìÑÔ∏è XCM Transport Methods to send XCM Messages across Networks. üìÑÔ∏è XCM Virtual Machine (XCVM) Design of the XCM Executor and Mechanics of the XCM Virtual Machine. üìÑÔ∏è XCM Instructions Specification for Instructions and Registers for the XCM Format. Previous Bridges Next Cross-Consensus Message Format (XCM) ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Introduction to Cross-Consensus Message Format (XCM) General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Bridges Cross Consensus Messaging (XCM) Cross-Consensus Message Format (XCM) XCM Pallet XCM Use-cases XCM Transport XCM Virtual Machine (XCVM) XCM Instructions Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Cross Consensus Messaging (XCM) Cross-Consensus Message Format (XCM) On this page Introduction to Cross-Consensus Message Format (XCM) The Cross-Consensus Message Format, or XCM , is a messaging format and language used to communicate between consensus systems. One of Polkadot's main functionalities is interoperability amongst parachains and any other participating consensus-driven systems. XCM is the language through which complex, cross-consensus interactions can occur. Two blockchains can "speak" XCM to seamlessly interact with each other using a standard messaging format. info We typically discuss XCM in the context of parachains, but please bear this in mind that it expands to the domain of all consensus systems! Remember, a consensus system here means any system or protocol that achieves finality to agree on the latest and correct state, whether it's a Polkadot parachain, an EVM smart contract, or other bridged consensus systems. XCM is not meant to be only specific to Polkadot, but rather its primary intention is to define a generic and common format amongst different consensus systems to communicate. It's important to note that XCM does not define how messages are delivered but rather define how they should look, act, and contain relative instructions to the on-chain actions the message intends to perform. XCMP , or Cross Chain Message Passing, is the actual network-layer protocol to deliver XCM-formatted messages to other participating parachains. There are also other ways to define transport layer protocols for delivering XCM messages. XCM has four high-level core design principles which it stands to follow: Asynchronous : XCM messages in no way assume that the sender will be blocking on its completion. Absolute : XCM messages are guaranteed to be delivered and interpreted accurately, in order and in a timely fashion. Once a message is sent, one can be sure it will be processed as it was intended to be. Asymmetric : XCM messages, by default, do not have results that let the sender know that the message was received - they follow the 'fire and forget' paradigm. Any results must be separately communicated to the sender with an additional message back to the origin. Agnostic : XCM makes no assumptions about the nature of the consensus systems between which the messages are being passed. XCM as a message format should be usable in any system that derives finality through consensus. These four crucial design decisions allow for XCM messages to be a reliable yet convenient way to properly convey the intentions from one consensus system to another without any compatibility issues. note XCM is constantly in development - meaning the format is expected to change over time. XCM v3 is the latest version, and is deployed on Polkadot. To view updates on the XCM format, visit the xcm-format repository to view any RFCs that have been submitted that would contribute to the next release. A Format, Not a Protocol ‚Äã What started as an approach to cross-chain communication , has evolved into a format for Cross-Consensus Communication that is not only conducted between chains, but also between smart contracts, pallets, bridges, and even sharded enclaves like SPREE . XCM cannot actually send messages between systems. It is a format for how message transfer should be performed, similar to how RESTful services use REST as an architectural style of development, where HTTP requests contain specific parameters to perform some action. Similar to UDP, out of the box XCM is a "fire and forget" model, unless there is a separate XCM message designed to be a response message which can be sent from the recipient to the sender. All error handling should also be done on the recipient side. info XCM is not designed in a way where every system supporting the format is expected to be able to interpret any possible XCM message. Practically speaking, one can imagine that some messages will not have reasonable interpretations under some systems or will be intentionally unsupported. Furthermore, it's essential to realize that XCM messages by themselves are not considered transactions. XCM describes how to change the state of the target network, but the message by itself doesn't perform the state change. This partly ties to what is called asynchronous composability , which allows XCM messages to bypass the concept of time-constrained mechanisms, like on-chain scheduling and execution over time in the correct order in which it was intended. XCM Tech Stack ‚Äã XCM can be used to express the meaning of the messages over each of these three communication channels. Core Functionality of XCM ‚Äã XCM opens the doors to a multi-hop, multi-network communications. XCM introduces some key features and additions to cross-consensus messaging, including: Programmability - the ability to have expectations for messages, which allow for more comprehensive use cases, safe dispatches for version checking, branching, and NFT/Asset support. Functional Multichain Decomposition - the ability to define mechanisms to cross-reference and perform actions on other chains on behalf of the origin chain (remote locking), context/id for these messages, and asset namespacing. Bridging - introduces the concept of a universal location, which allows for a base reference for global consensus systems for multi-hop setups. This location is above the parent relay chain or other consensus systems like Ethereum or Bitcoin. A core part of the vision that XCM provides is improving communication between the chains to make system parachains a reality. For example, the Polkadot relay chain handles more than just parachain management and shared security - it handles user balances/assets, auctions, governance, and staking. Ideally, the relay chain should be for what it's intended to be - a place for shared security. System parachains can alleviate these core responsibilities from the relay chain but only by using a standard format like XCM. This is where system parachains come in, where each of these core responsibilities can be delegated to a system parachain respectively. info XCM bridging, functional multichain decomposition, and programmability upgrades are crucial to bringing ecosystems together using a common communication abstraction. For more information on the specific intructions used for these key features, head over to the instructions and registers page . Cross-Consensus Message Format (XCM) ‚Äã For an updated and complete description of the cross-consensus message format please see the xcm-format repository on GitHub . The Anatomy of an XCMP Interaction ‚Äã A smart contract that exists on parachain A will route a message to parachain B in which another smart contract is called that makes a transfer of some assets within that chain. Charlie executes the smart contract on parachain A , which initiates a new cross-chain message for the destination of a smart contract on parachain B . The collator node of parachain A will place this new cross-chain message into its outbound messages queue, along with a destination and a timestamp . The collator node of parachain B routinely pings all other collator nodes asking for new messages (filtering by the destination field). When the collator of parachain B makes its next ping, it will see this new message on parachain A and add it into its own inbound queue for processing into the next block. Validators for parachain A will also read the outbound queue and know the message. Validators for parachain B will do the same. This is so that they will be able to verify the message transmission happened. When the collator of parachain B is building the next block in its chain, it will process the new message in its inbound queue as well as any other messages it may have found/received. During processing, the message will execute the smart contract on parachain B and complete the asset transfer as intended. The collator now hands this block to the validator, which itself will verify that this message was processed. If the message was processed and all other aspects of the block are valid, the validator will include this block for parachain B into the Relay Chain. Check out our animated video below that explores how XCMP works. Sorry, your browser doesn't support embedded videos. Resources ‚Äã Shawn Tabrizi: XCM - The Backbone Of A Multichain Future | Polkadot Decoded 2022 - High level overview which should answer ‚ÄúWhat is XCM? XCM: The Cross-Consensus Message Format - Detailed blog post by Dr. Gavin Wood about the XCM Format. XCM Format specification - The best starting point for understanding the XCM API at a technical level. Gavin Wood, Polkadot founder: XCM v3 | Polkadot Decoded 2022 - High level overview of XCM and specifically the new features available in XCM v3. XCMP Scheme - An overall overview of XCMP describing a number of design decisions. Messaging Overview - An overview of the messaging schemes from the Polkadot Parachain Host Implementor's guide. Sub0 Online: Getting Started with XCM - Your First Cross Chain Messages - Code focused workshop on how XCM v1 works, and the core concepts of XCM. XCM: Cross-Consensus Messaging Audit - Technical audit report by Quarkslab prepared for Parity. XCM pallet code - The pallet that contains XCM logic from the Polkadot code repository XCM Config & Pallet-XCM | Polkadot Deep Dives - A technical deep dive into pallet-xcm and the XCM configuration. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Cross Consensus Messaging (XCM) Next XCM Pallet A Format, Not a Protocol XCM Tech Stack Core Functionality of XCM Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
XCM FRAME Pallet Overview General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Bridges Cross Consensus Messaging (XCM) Cross-Consensus Message Format (XCM) XCM Pallet XCM Use-cases XCM Transport XCM Virtual Machine (XCVM) XCM Instructions Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Cross Consensus Messaging (XCM) XCM Pallet On this page XCM FRAME Pallet Overview The XCM pallet ( pallet-xcm ) provides a set of pre-defined, commonly used XCVM programs in the form of a set of extrinsics using FRAME . This pallet provides some default implementations for traits required by XcmConfig . The XCM executor is also included as an associated type within the pallet's configuration. Where the XCM format defines a set of instructions used to construct XCVM programs, pallet-xcm defines a set of extrinsics that can be utilized to build XCVM programs, either to target the local or external chains. pallet-xcm 's functionality is separated into three categories: note Remember, all XCMs are XCVM programs that follow the XCM format . It is the job of the XCM executor is to handle and execute these programs. Primitive, dispatchable functions to locally execute an XCM. High-level, dispatchable functions for asset transfers. Version negotiation-specific dispatchable functions. Primitive Extrinsics ‚Äã There are two primary primitive extrinsics. These extrinsics handle sending and executing XCVM programs as dispatchable functions within the pallet. execute - This call contains direct access to the XCM executor. It is the job of the executor to check the message and ensure that no barrier/filter will block the execution of the XCM. Once it is deemed valid, the message will then be locally executed, therein returning the outcome as an event. This operation is executed on behalf of whichever account has signed the extrinsic. It's possible for only a partial execution to occur. send - This call specifies where a message should be sent ( via a transport method ) externally to a particular destination, i.e. a parachain, smart contract, or any system which is governed by consensus. In contrast to execute , the executor is not called locally, as the execution will occur on the destination chain. info The XCM pallet needs the XcmRouter to send XCMs. It is used to dictate where XCMs are allowed to be sent, and which XCM transport protocol to use. For example, Kusama, the canary network, uses the ChildParachainRouter which only allows for Downward Message Passing from the relay to parachains to occur. You can read more about XCM transport protocols here . Asset Transfer Extrinsics ‚Äã Several extrinsics within the pallet handle asset transfer logic. They define a predetermined set of instructions for sending and executing XCMs. Two variants of these functions are prefixed with limited_ . They have the same functionality but can specify a weight to pay for the XCM fee. Otherwise, the fee is taken as needed from the asset being transferred. reserve_transfer_assets - Transfer some assets from the local chain to the sovereign account of a destination chain and forward an XCM containing a ReserveAssetDeposited instruction, which serves as a notification. teleport_assets - Teleport some assets from the local chain to some destination chain. Transfer Reserve vs. Teleport ‚Äã While both extrinsics deal with transferring assets, they exhibit fundamentally different behavior. Teleporting an asset implies a two-step process: the assets are taken out of circulating supply (typically by burning/destroying) in the origin chain and re-minted to whatever account is specified at the destination. Teleporting should only occur if there is an inherent and bilateral trust between the two chains, as the tokens destroyed at the origin could not necessarily be guaranteed to have the same properties when minted at the destination. There has to be trust that the a particular chain burned, or re-minted the assets. Transferring or reserving an asset implies that equivalent assets (i.e, native currency, like DOT or KSM ) are withdrawn from sovereign account of the origin chain and deposited into the sovereign account on the destination chain. Unlike teleporting an asset, it is not destroyed and re-minted, rather a trusted, third entity is used (i.e., Asset Hub) to reserve the assets, wherein the sovereign account of the destination chain on the reserve chain obtains ownership of these assets. It's worth noting that this means that some other mechanism is needed to ensure that the balance on the destination does not exceed the amount being held in reserve chain. info A sovereign account refers to an account within a particular consensus system. Even though accounts may be different in terms of factors such as an address format, XCM agnostic nature enables communication between these sovereign accounts that are in other consensus systems. Version Negotiation Extrinsics ‚Äã The following extrinsics require root, as they are only used when bypassing XCM version negotiation. They change any relevant storage aspects that enforce anything to do with XCM version negotiations. force_xcm_version - Modifies the SupportedVersion storage to change a particular destination's stated XCM version. force_default_xcm_version - Modifies the SafeXcmVersion storage, which stores the default XCM version to use when the destination's version is unknown. force_subscribe_version_notify - Sends an XCM with a SubscribeVersion instruction to a destination. force_unsubscribe_version_notify - Sends an XCM with a UnsubscribeVersion instruction to a destination. Fees in the XCM Pallet ‚Äã Message fees are only paid if the interior location does not equal the interpreting consensus system (known as Here in the context of an XCM Multilocation ). Otherwise, the chain bears the fees. If applicable, fees are withdrawn from the assets from the specified MultiLocation and used as payment to execute any subsequent instructions within the XCM. Fees are generally dependent on several factors within the XcmConfig . For example, the barrier may negate any fees to be paid at all. Before any XCM is sent, and if the destination chain‚Äôs barrier requires it, a BuyExecution instruction is used to buy the necessary weight for the XCM. XCM fee calculation is handled by the Trader, which iteratively calculates the total fee based on the number of instructions. The Trader used to calculate the weight (time for computation in consensus) to include in the message. Fee calculation in XCM is highly configurable and, for this reason, subjective to whichever configuration is in place. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Cross-Consensus Message Format (XCM) Next XCM Use-cases Primitive Extrinsics Asset Transfer Extrinsics Transfer Reserve vs. Teleport Version Negotiation Extrinsics Fees in the XCM Pallet ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
XCM Use-cases & Examples General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Bridges Cross Consensus Messaging (XCM) Cross-Consensus Message Format (XCM) XCM Pallet XCM Use-cases XCM Transport XCM Virtual Machine (XCVM) XCM Instructions Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Cross Consensus Messaging (XCM) XCM Use-cases On this page XCM Use-cases & Examples XCM has a multitude of use cases. While the wiki covers some of the key commonplace interactions, the XCM format can be used to construct many more combinations to suit the use case at hand. Example Use-Cases ‚Äã Request for specific operations to occur on the recipient system such as governance voting. Enables single use-case chains e.g. the Asset Hub as asset parachains Optionally include payment of fees on a target network for requested operation. Provide methods for various asset transfer models: Remote Transfers : control an account on a remote chain, allowing the local chain to have an address on the remote chain for receiving funds and to eventually transfer those funds it controls into other accounts on that remote chain. Asset Teleportation : movement of an asset happens by destroying it on one side and creating a clone on the other side. Reserve Asset Transfer : there may be two chains that want to nominate a third chain, where one includes a native asset that can be used as a reserve for that asset. Then, the derivative form of the asset on each of those chains would be fully backed, allowing the derivative asset to be exchanged for the underlying asset on the reserve chain backing it. Let's review two of these example asset transfer use cases: Asset Teleportation and Reserve Asset Transfer . Asset Teleportation ‚Äã An asset teleport operation from a single source to a single destination. InitiateTeleport The source gathers the assets to be teleported from the sending account and takes them out of the circulating supply, taking note of the total amount of assets that was taken out. ReceiveTeleportedAsset The source then creates an XCM instruction called ReceiveTeleportedAssets and puts the amount of assets taken out of circulation and the receiving account as parameters to this instruction. It then sends this instruction over to the destination, where it gets processed and new assets gets put back into circulating supply accordingly. DepositAsset The destination then deposits the assets to the receiving account of the asset. Reserve Asset Transfer ‚Äã When consensus systems do not have a established layer of trust over which they can transfer assets, they can opt for a trusted 3rd entity to store the assets. InitiateReserveWithdraw The source gathers the derivative assets to be transferred from the sending account and burns them, taking note of the amount of derivatives that were burned. WithdrawAsset The source sends a WithdrawAsset instruction to the reserve, instructing the reserve to withdraw assets equivalent to the amount of derivatives burned from the source's sovereign account. DepositReserveAsset The reserve deposits the assets withdrawn from the previous step to the destination's sovereign account, taking note of the amount of assets deposited. ReserveAssetDeposited The reserve creates a ReserveAssetDeposited instruction with the amount of assets deposited to the destination's sovereign account, and sends this instruction onwards to the destination. The destination receives the instruction and processes it, minting the derivative assets as a result of the process. DepositAsset The destination deposits the derivative assets minted to the receiving account. Edit this page Last updated on Jun 30, 2023 by Filippo Previous XCM Pallet Next XCM Transport Example Use-Cases Asset Teleportation Reserve Asset Transfer ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
XCM Transport Methods (XCMP, HRMP, VMP) General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Bridges Cross Consensus Messaging (XCM) Cross-Consensus Message Format (XCM) XCM Pallet XCM Use-cases XCM Transport XCM Virtual Machine (XCVM) XCM Instructions Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Cross Consensus Messaging (XCM) XCM Transport On this page XCM Transport Methods (XCMP, HRMP, VMP) With the XCM format established, common patterns for protocols of these messages are needed. Polkadot implements two message passing protocols for acting on XCM messages between its constituent parachains. There are three primary methods for message passing, one of which is under development: XCMP (Cross-Chain Message Passing) Horizontal Relay-routed Message Passing (HRMP/XCMP-lite) VMP (Vertical Message Passing) XCMP (Cross-Chain Message Passing) ‚Äã caution XCMP is currently under development, and most of the cross-chain messages pass through HRMP channels for the time being. XCM is related to XCMP in the same way that REST is related to RESTful. Cross-Chain Message Passing secure message passing between parachains. There are two variants: Direct and Relayed . With Direct , message data goes direct between parachains and is O(1) on the side of the Relay-chain and is very scalable. With Relayed , message data is passed via the Relay-chain, and piggy-backs over VMP. It is much less scalable, and parathreads in particular may not receive messages due to excessive queue growth. Cross-chain transactions are resolved using a simple queuing mechanism based around a Merkle tree to ensure fidelity. It is the task of the Relay Chain validators to move transactions on the output queue of one parachain into the input queue of the destination parachain. However, only the associated metadata is stored as a hash in the Relay Chain storage. The input and output queue are sometimes referred to in the Polkadot codebase and associated documentation as ingress and egress messages, respectively. info For detailed information about VMP see dedicated section in The Polkadot Parachain Host Implementers' Guide . VMP (Vertical Message Passing) ‚Äã Vertical Message Passing message passing between the Relay-chain itself and a parachain. Message data in both cases exists on the Relay-chain and are interpreted by the relay chain according to XCM standards. This includes: UMP (Upward Message Passing) ‚Äã Upward Message Passing message passing from a parachain to the Relay-chain. DMP (Downward Message Passing) ‚Äã Downward Message Passing message passing from the Relay-chain to a parachain. info For detailed information about VMP see dedicated section in The Polkadot Parachain Host Implementers' Guide . HRMP (XCMP-Lite) ‚Äã While XCMP is still being implemented, a stop-gap protocol (see definition below) known as Horizontal Relay-routed Message Passing (HRMP) exists in its place. HRMP has the same interface and functionality as XCMP but is much more demanding on resources since it stores all messages in the Relay Chain storage. When XCMP has been implemented, HRMP is planned to be deprecated and phased out in favor of it. note A stop-gap protocol is a temporary substitute for the functionality that is not fully complete. While XCMP proper is still in development, HRMP is a working replacement. A tutorial on how to open an HRMP channel on a parachain can be found here . XCMP Design ‚Äã Cross-chain messages will not be delivered to the Relay Chain. Cross-chain messages will be constrained to a maximum size specified in bytes. Parachains are allowed to block messages from other parachains, in which case the dispatching parachain would be aware of this block. Collator nodes are responsible for routing messages between chains. Collators produce a list of egress messages and will receive the ingress messages from other parachains. On each block, parachains are expected to route messages from some subset of all other parachains. When a collator produces a new block to hand off to a validator, it will collect the latest ingress queue information and process it. Validators will check the proof that the new candidate for the next parachain block includes the processing of the expected ingress messages to that parachain. XCMP queues must be initiated by first opening a channel between two parachains. The channel is identified by both the sender and recipient parachains, meaning that it's a one-way channel. A pair of parachains can have at most establish two channels between them, one for sending messages to the other chain and another for receiving messages. The channel will require a deposit in DOT to be opened, which will get returned when the channel is closed. Edit this page Last updated on Jun 30, 2023 by Filippo Previous XCM Use-cases Next XCM Virtual Machine (XCVM) XCMP (Cross-Chain Message Passing) VMP (Vertical Message Passing) HRMP (XCMP-Lite) ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
XCM Virtual Machine (XCVM) & XCM Executor General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Bridges Cross Consensus Messaging (XCM) Cross-Consensus Message Format (XCM) XCM Pallet XCM Use-cases XCM Transport XCM Virtual Machine (XCVM) XCM Instructions Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Cross Consensus Messaging (XCM) XCM Virtual Machine (XCVM) On this page XCM Virtual Machine (XCVM) & XCM Executor At the core of XCM lies the Cross-Consensus Virtual Machine (XCVM). A ‚Äúmessage‚Äù in XCM is an XCVM program, referred to as an "XCM" or "XCMs" for multiple messages. The XCVM is a register-based state machine. The state is tracked in domain-specific registers that hold information that is used and mutated along the execution of a particular message. Most of the XCM format comprises these registers and the instructions used to compose XCVM programs. The XCVM is an ultra-high-level non-Turing-complete computer whose instructions are designed to be roughly at the same level as transactions in terms of definition. Messages are one or more XCM instructions executed in order by the XCVM. An XCM is executed until it either runs to the end or hits an error, at which point it finishes up and halts. The first implementation of the XCVM is the xcm-executor . It follows the XCVM specification provided by Parity. It's engineered to be extendable, providing maximum customizability when configuring XCM. Because the xcm-executor is just an implementation of XCVM, it's entirely possible to create another implementation if desired. XCMs are XCVM Programs ‚Äã A cross consensus message (XCM) is just a program that runs on the XCVM : in other words, one or more XCM instructions that are executed by an XCVM implementation, such as the xcm-executor . To learn more about the XCVM and the XCM format, see the latest blog post on XCM by Dr. Gavin Wood. XCM instructions might change a register, the state of the consensus system, or both. Depending on the program's goal, whether it is to teleport assets from one chain to another or call a smart contract on another chain, XCMs usually require changes to the registers before any changes to the consensus system can be made. XCM Executor & Configuration ‚Äã The XCM Executor's implementation centers around a core piece: the XCM configuration. Each instance of the Executor must have a valid configuration, which specifies a multitude of options on how a chain may treat incoming messages via Barriers , calculate weight for a message via the Weigher , how much weight to purchase via the Trader , configure fees , how to convert origins , and more. Cross Consensus Message (XCM) Anatomy & Flow ‚Äã An XCM is made up of a list of instructions that are executed in order. There are four different kinds of XCM instructions: Instruction - Results in a state change in the local consensus system or some state change. Trusted Indication - Tells the XCVM, or the Executor, that some action has been done before already - meaning, this action is now trusted and can be acted on, i.e., in a teleport scenario. Information - Provides additional information about a particular origin, usually the result of a query, i.e., a QueryResponse instruction. System Notification - Typically used in the context of when an HRMP channel is being opened, closed, or accepted. Typically, an XCM takes the following path through the XCVM: Instructions within an XCM are read one-by-one by the XCVM. An XCM may contain one or more instructions. The instruction is executed. This means that the current values of the XCVM registers , the instruction type , and the instruction operands are all used to execute some operation, which might result in some registers changing their value, or in an error being thrown, which would halt execution. Each subsequent instruction within the XCM is read until the end of the message has been reached. Example Register: The Holding Register ‚Äã There are many instructions that depend on the Holding register . The Holding register is an XCVM register that provides a place for any assets that are in an intermediary state to be held until they are taken out of the Holding register. It requires an instruction to place assets within it and another to withdraw them. The simplest example of this occurring is the DepositAsset instruction, which in its Rust form looks like this: enum Instruction { DepositAsset { assets: MultiAssetFilter, beneficiary: MultiLocation, }, /* snip */ } This instruction specifies which assets (asset type and amount), already present in the Holding register, are going to be taken from it and deposited to the specified beneficiary (recipient). It is very common for instructions to remove and place assets into the Holding register when transacting between chains. Example: TransferAsset ‚Äã An example below illustrates how a chain may transfer assets locally, or locally on a remote chain (as part of another instruction) using an XCM. In this message, the TransferAsset instruction is defined with two parameters: assets , which are the assets to be transferred, and the beneficiary , whoever will be the sole beneficiary of these assets. More complex instructions, especially those which perform actions that target a location other than the interpreting consensus system may make use of XCVM registers. enum Instruction { TransferAsset { assets: MultiAssets, beneficiary: MultiLocation, } /* snip */ } A MultiAsset is a general identifier for an asset. It may represent both fungible and non-fungible assets, and in the case of a fungible asset, it represents some defined amount of the asset. A MultiLocation is a relative identifier, meaning that it can only be used to define the the relative path between two locations, and cannot generally be used to refer to a location universally. TransferAsset is one of the many instructions that can be contained within an XCM. For more information, please read XCM Instructions in the wiki . Locations in XCM ‚Äã XCM's generic nature involves specifying a wide array of "locations", or any body that is governed by consensus (parachains, solochains, smart contracts, accounts, etc). These are relatively abstract notions that point to where but also to who a particular action may affect. The MulitLocation type is what XCM uses to define these locations. A MultiLocation is a relative identifier that defines a relative path into some state-bearing consensus system. It is used to define the relative path between two locations, and cannot generally be used to refer to a location universally. It is very much akin to how a relative filesystem path works and is dependent on the which consensus system the location expression is being evaluated. MultiLocation has two primary fields: A series of paths, called Junctions , which define an interior portion of state to descend into it (sometimes called a "sub-consensus" system, such as a smart contract or pallet). An interior location may also be used to refer to a Junction, used in the context of "a parachain is an interior location of the relay chain", or how a UTXO is interior to Bitcoin's consensus. The number of parent junctions at the beginning of a MultiLocation 's formation - in other words, the number of parent consensus systems above it. There are a number of various Junction variants that may be used to describe a particular location - whether it's a 32 byte account, a Substrate pallet, or a pluralistic body. MultiLocation Scenario Example ‚Äã In this scenario, assume an XCM is to be sent from our parachain to the Asset Hub ( Parachain 1000 ). This XCM references an account on the Asset Hub. As a general path, the MultiLocation would look like this: ../Parachain(1000)/AccountId32(<some_account_id>) Or, as a Rust enum: MultiLocation { parents: 1, interior: X2(Parachain(1000), <some_account_id>.into()) } In the first field, parents , there is a parent of 1 . This is because our parachain has the relay chain as a parent - in other words, it will go up by one consensus system to the relay chain. This is also illustrated by the ../ of the "file path" representation. The second field, interior , defines where to go after the relay chain. In this case, from the relay chain this message will go to the Asset Hub ( Parachain 1000 ), then reference the account ( some_account_id ) located within. Keep in mind that this location is specific to this interaction. The identities may need to change if this location was defined on another consensus system, such as Kusama. On other consensus systems, such as Ethereum, it won't be able to interpret it. UniversalLocation in XCM ‚Äã A UniversalLocation refers to any global consensus system. A global consensus system is an entity that provides its top-level consensus through some non-derivative consensus algorithm that can exist without reference to any other singleton data system. Such global consensus systems include Polkadot (or other relay chains), Bitcoin, or Ethereum. It provides a point of reference for overarching consensus systems. The GlobalConsensus junction refers to a global consensus system and takes a NetworkId that specifies a particular remote network. A UniversalLocation allows overarching consensus systems to communicate using this junction. Sub-consensus systems (i.e., a parachain on Polkadot) may refer to other remote sub-consensus systems (i.e., a parachain on Kusama) using a relative path defined via a MultiLocation . Simulating XCVM using the xcm-simulator ‚Äã Within the Polkadot repository exists the xcm-simulator , which allows developers to experiment with building, executing, and simulating various XCM use scenarios. Edit this page Last updated on Jun 30, 2023 by Filippo Previous XCM Transport Next XCM Instructions XCMs are XCVM Programs XCM Executor & Configuration Cross Consensus Message (XCM) Anatomy & Flow Example Register: The Holding Register Example: TransferAsset Locations in XCM MultiLocation Scenario Example UniversalLocation in XCM Simulating XCVM using the xcm-simulator ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
XCM Instructions & Register Specification General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Architecture Network Participants Parachains Bridges Cross Consensus Messaging (XCM) Cross-Consensus Message Format (XCM) XCM Pallet XCM Use-cases XCM Transport XCM Virtual Machine (XCVM) XCM Instructions Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Architecture Cross Consensus Messaging (XCM) XCM Instructions On this page XCM Instructions & Register Specification This page can also be viewed at the xcm-format repository, where each instruction and register is explained in-depth. caution For a more practical approach to utilizing XCM registers and instructions, refer to the XCM Docs . Please keep in mind that XCM is under active development. XCVM Registers ‚Äã Programme Programme Counter Error Error Handler Appendix Origin Holding Surplus Weight Refunded Weight Transact Status Topic Transact Status Register Topic Register XCVM Instruction Set ‚Äã WithdrawAsset ReserveAssetDeposited ReceiveTeleportedAsset QueryResponse TransferAsset TransferReserveAsset Transact HrmpNewChannelOpenRequest HrmpChannelAccepted HrmpChannelClosing ClearOrigin DescendOrigin ReportError DepositAsset DepositReserveAsset ExchangeAsset InitiateReserveWithdraw InitiateTeleport QueryHolding BuyExecution RefundSurplus SetErrorHandler SetAppendix ClearError ClaimAsset Trap SubscribeVersion UnsubscribeVersion BurnAsset ExpectAsset ExpectError ExpectOrigin QueryPallet ExpectPallet ReportTransactStatus ClearTransactStatus LockAsset UnlockAsset NoteUnlockable RequestUnlock Instructions Application Example ‚Äã The following presents the practical mapping of instructions to some core functionality in XCM . Programmability Functional Multichain Decomposition These are the primary instructions that enable programmability and branching to be possible. Branching in this context is the ability for errors and logic to be handled as needed when dealing with a message. ExpectAsset(MultiAssets) - Checks if the Holding register has a specific amount of assets, throws an error if it doesn't. ExpectError(Option<(u32, Error)>) - Ensures the Error register contains the given error, and throws an error if it doesn't. ExpectOrigin(MultiLocation) - Ensures the Origin register contains the expected origin, and throws an error if it doesn't. QueryPallet - Queries the existence of a particular pallet type. ExpectPallet - Ensure that a particular pallet with a particular version exists. ReportTransactStatus(QueryResponseInfo) - Send a QueryResponse message containing the value of the Transact Status Register to some destination. ClearTransactStatus - Set the Transact Status Register to its default, cleared, value. These instructions highlight the key instructions focused on Functional Multichain Decomposition. LockAsset(MultiAsset, MultiLocation) - Lock the locally held asset and prevent further transfer or withdrawal. UnlockAsset(MultiAsset, MultiLocation) - Remove the lock over asset on this chain and (if nothing else is preventing it) allow the asset to be transferred. NoteUnlockable(MultiAsset, MultiLocation) - Asset ( asset ) has been locked on the origin system and may not be transferred. It may only be unlocked with the receipt of the UnlockAsset instruction from this chain. RequestUnlock(MultiAsset, MultiLocation) - Send an UnlockAsset instruction to the locker for the given asset . Edit this page Last updated on Jun 30, 2023 by Filippo Previous XCM Virtual Machine (XCVM) Next Polkadot Comparisons XCVM Registers XCVM Instruction Set Instructions Application Example ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Comparisons General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Kusama Ethereum Cosmos Avalanche Other Comparisons Layer Two and Rollups Videos Future Implementations Archive Build Maintain Learn Polkadot Comparisons Polkadot Comparisons Comparison between Polkadot and other Networks. üìÑÔ∏è Kusama Comparing the cousins. üìÑÔ∏è Ethereum Comparison between Polkadot and Ethereum. üìÑÔ∏è Cosmos Comparison between Polkadot and Cosmos. üìÑÔ∏è Avalanche Comparison between Polkadot and Avalanche. üìÑÔ∏è Other Comparisons Comparison between Polkadot and ETH 1.0 and BSC. üìÑÔ∏è Layer Two and Rollups Comparison between various Rollup and L2 Protocols. Previous XCM Instructions Next Kusama ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot vs. Kusama General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Kusama Ethereum Cosmos Avalanche Other Comparisons Layer Two and Rollups Videos Future Implementations Archive Build Maintain Learn Polkadot Comparisons Kusama On this page Polkadot vs. Kusama Although they are like cousins and share many parts of their code, Polkadot and Kusama are independent, standalone networks with different priorities. Kusama is wild and fast, and great for bold experimentation and early-stage deployment. Polkadot is more conservative, prioritizing stability and dependability. Cousins have their differences after all. info To get a better understanding of the key similarities and difference between Polkadot and Kusama, checkout this support article . Cost and Speed ‚Äã Teams wishing to run a parachain are required to bond tokens as security. The bonding requirement on Kusama is lower than on Polkadot, making it the more affordable development environment. Another key technical difference between Polkadot and Kusama is that Kusama has modified governance parameters that allow for faster upgrades. Kusama is up to four times faster than Polkadot. On Polkadot, the voting period on referendums lasts days, followed by an enactment period of days before the changes are enacted on-chain. On Kusama, voting lasts days, followed by an day enactment period. This means stakeholders need to stay active and vigilant if they want to keep up with all the proposals, referenda, and upgrades, and validators on Kusama often need to update on short notice. This does not mean that the Kusama blockchain itself is faster, in the sense of faster block times or transaction throughput (these are the same on both networks), but that there's a shorter amount of time between governance events such as proposing new referenda, voting, and enacting approved upgrades. This allows Kusama to adapt and evolve faster than Polkadot. Canary network ‚Äã The initial use case for Kusama was as a pre-production environment, a ‚Äúcanary network‚Äù. Canary is a type of bird: back in the day, coal miners would put canaries into coal mines as a way to measure the amount of toxic gases in the tunnels. Similarly, canary testing is a way to validate software by releasing software to a limited number of users, or perhaps, an isolated environment - without hurting a wide range of users. Releases made onto Kusama can be thought of as Canary Releases . These releases are usually staged. In Kusama's early days, the network won't just be used for parachain candidates to innovate and test changes, but a proof of concept for Polkadot's sharded model. Kusama is not simply a testnet, the blockchain is fully functional with attached economic value, and own governance. The future of Kusama is in the hands of its participants. In a typical blockchain development pipeline, Kusama would sit in between a "testnet" and a "mainnet": Testnet --> Kusama --> Polkadot As you can imagine, building on Kusama first allows teams to test things out in a live, fully decentralized, and community-controlled network with real-world conditions and lower stakes in the event of problems or bugs than on Polkadot. Many projects will maintain parachains on both networks, experimenting and testing new technologies and features on Kusama before deploying them to Polkadot. Some teams will decide just to stay on Kusama, which is likely to be a place where we see some exciting experimentation with new technologies going forward. Projects that require high-throughput but don‚Äôt necessarily require bank-like security, such as some gaming, social networking, and content distribution applications, are particularly good candidates for this use case. Kusama may also prove to be the perfect environment for ambitious experiments with new ideas and innovations in areas like governance, incentives, monetary policy, and DAOs (decentralized autonomous organizations). Future upgrades to the Polkadot runtime will also likely be deployed to Kusama before Polkadot mainnet. This way, not only will we be able to see how these new technologies and features will perform under real-world conditions before bringing them to Polkadot, but teams who have deployed to both networks will also get an advanced look at how their own technology will perform under those upgrades. Going forward ‚Äã Ultimately, Kusama and Polkadot will live on as independent, standalone networks with their own communities, their own governance, and their own complementary use cases, though they will continue to maintain a close relationship, with many teams likely deploying applications to both networks. In the future, we‚Äôre also likely to see Kusama bridged to Polkadot for cross-network interoperability. Web3 Foundation remains committed to both networks going forward, providing crucial support and guidance to teams building for the ecosystem. Explore more ‚Äã About Kusama The Kusama Wiki Kusama on Polkadot-JS Apps Polkadot and Kusama: What's the difference? Edit this page Last updated on Mar 28, 2023 by Filippo Previous Polkadot Comparisons Next Ethereum Cost and Speed Canary network Going forward Explore more ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot vs. Ethereum General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Kusama Ethereum Cosmos Avalanche Other Comparisons Layer Two and Rollups Videos Future Implementations Archive Build Maintain Learn Polkadot Comparisons Ethereum On this page Polkadot vs. Ethereum Polkadot is the first fully sharded production-grade blockchain protocol. The current protocol of Ethereum does not implement sharding and it is the main focus of the next major upgrade . Similar to Polkadot and its parachains, Ethereum has goals of being a multi-sharded network. When a network is sharded , this implies that it is capable of executing multiple (and often many) state transitions in parallel in a scalable manner. One key difference is that Polkadot parachains are heterogeneous shards while Ethereum will have homogeneous shards. In other words, each parachain can have its own state transition logic whereas on Ethereum, all the shards use the same state transition logic. Both protocols are blockchains but serve fundamentally different roles in how they are utilized: Ethereum is a general-purpose blockchain that hosts the Ethereum Virtual Machine, an environment for executing smart contracts. Polkadot is a heterogeneous sharded, multi-chain protocol that hosts multiple chains and provides a way for them to partake in a shared security model. Polkadot acts as a meta-protocol that allows for multiple protocols to coexist and work together. info It's important to note that the "Ethereum" here refers to what was previously known as "Eth2" or "Ethereum 2.0". For more concrete details regarding the subsequent iterations of Ethereum, please refer to the Ethereum Roadmap . It's relevant to mention that some upgrades may not be active for Ethereum but are described as a part of this comparison to Polkadot. Some of these goals/upgrades may change to reflect the general direction of Ethereum. High-Level Comparison ‚Äã At a high level, both protocols have fundamentally different goals, which are reflected by their architecture: Ethereum is a general-purpose blockchain for global coordination . Ethereum is not specialized nor optimized for any particular application, rather its primary focus is the Ethereum Virtual Machine for executing smart contracts. Polkadot is a sharded blockchain that introduces shared security for each one of its shards, or parachains . Each shard is usually specialized towards a specific focus and optimized towards that goal. Polkadot provides shared security and consensus to these shards through the Polkadot relay chain. Polkadot can't and does not directly run something like a virtual machine for smart contracts. However, several of its parachains can (and does). Parachains on Polkadot can even run an EVM for executing smart contracts written in Solidity, Ethereum's native smart contract language. In the context of blockchain, "sharding" refers to the parallelization of state transition (transaction) execution. The way Ethereum and Polkadot deal with scalability and sharding is quite different. Scalability: Sharding vs. Danksharding ‚Äã As part of Ethereum's roadmap, the previously dubbed "shard chains" have been forgone in favor of rollup-based approach for scaling transaction throughput. Danksharding is how Ethereum plans to create a scalable environment for an acclaimed >100,000 transactions per second. Danksharding was the chosen alternative over "shard chains" and works by storing blobs. Danksharding will allow for much more space to be utilized per block on Ethereum, where blobs of data will be verifiable for an amount of time before being pruned from the network. These blobs will have to be held for an amount of time, implying a level of data availability that validators must have. This approach will enable data availability at layer one and further enable layer two protocols on Ethereum to flourish. In contrast, Polkadot is a purely sharded network. It prioritizes data availability as an integral part of the block validation process. Parallelized interactions between parachains, the shards of the Polkadot network, also take advantage of this factor. Whereas Ethereum primarily focuses on making large amounts of data available for validation for a portion of time, Polkadot's parallelization factor allows verification to happen on the protocol level without needing a layer two solution. note The term "shards" is used here, but know that shards and parachains mean the same thing! Each parachain represents a shard in the Polkadot network. On Polkadot, each shard hosts core logic. As mentioned earlier, each shard (i.e., parachain ) has a unique state transition function (sometimes called a runtime ). Applications can exist either within a single shard or across shards by composing logic to create cross-consensus (XCM) interactions. Polkadot uses WebAssembly ( Wasm ) as a "meta-protocol". A shard's state transition function can be abstract as long as the validators on Polkadot can execute it within a Wasm environment. Architectural Differences: Polkadot and Ethereum ‚Äã As previously mentioned, Ethereum is a general-purpose virtual machine that can run sandboxed programs are written in Solidity, whereas Polkadot is a meta-protocol for other parachains to connect and interact with each other. Ethereum operates as a single, homogeneous chain. Each Ethereum node is divided into two layers: the consensus and execution layers. Each layer handles the block validation information, peer discovery, and Proof-of-Stake of the Ethereum client. Polkadot's primary component is the relay chain , which hosts heterogeneous shards called parachains. The relay chain aggregates information from its shards, the parachains, where Polkadot validators agree upon consensus and finality. In essence, one can look at Polkadot as a series of runtimes , which are state transition functions used to describe parachains (shards), as well as Polkadot itself. Like Ethereum, Polkadot clients abstract away many of their responsibilities into various components built using Substrate. Forks, Upgrades, and Governance ‚Äã Ethereum governance is done off-chain, where various stakeholders come to a consensus through some medium other than the protocol itself. Upgrades on Ethereum will follow the standard hard-fork procedure, requiring validators to upgrade their nodes to implement protocol changes. Polkadot uses on-chain governance with a multicameral system. There are several avenues to issue proposals, and all proposals ultimately pass through a public referendum, where the majority of tokens can always control the outcome. Polkadot uses adaptive quorum biasing to set the passing threshold for low-turnout referenda. Referenda can cover various topics, including fund allocation from an on-chain Treasury or modifying the underlying runtime code of the chain. Decisions get enacted on-chain and are binding and autonomous. Polkadot can enact chain upgrades and successful proposals using the Wasm meta-protocol without a hard fork. Anything within the state transition function, the transaction queue, or off-chain workers can be upgraded without forking the chain. Consensus and Finalization ‚Äã Ethereum and Polkadot use hybrid consensus models where block production and finality have their protocols. The finality protocols - Casper FFG for Ethereum and GRANDPA for Polkadot - are both GHOST-based and can both finalize batches of blocks in one round. For block production, both protocols use slot-based protocols that randomly assign validators to a slot and provide a fork choice rule for unfinalized blocks - RandDAO/LMD for Ethereum and BABE for Polkadot. There are two main differences between Ethereum and Polkadot consensus: Ethereum finalizes batches of blocks according to periods called "epochs". The current plan is to have 32 blocks per epoch and finalize them all in one round. With a predicted block time of 12 seconds, the expected time to finality is 6 minutes (12 minutes maximum). See Ethereum 2 Block Time for more information. Polkadot's finality protocol, GRANDPA, finalizes batches of blocks based on availability and validity checks that happen as the proposed chain grows. The time to finality varies with the number of checks that need to be performed (and invalidity reports cause the protocol to require extra checks). The expected time to finality is 12-60 seconds. Ethereum requires many validators per shard to provide strong validity guarantees while Polkadot can provide stronger guarantees with fewer validators per shard. Polkadot achieves this by making validators distribute an erasure coding to all validators in the system, such that anyone - not only the shard's validators - can reconstruct a parachain's block and test its validity. The random parachain-validator assignments and secondary checks were performed randomly selected validators make it less likely for the small set of validators on each parachain to collude. Staking Mechanics ‚Äã Ethereum is a proof-of-stake network that requires 32 ETH to stake for each validator instance. Validators run a primary Beacon Chain node and multiple validator clients - one for each 32 ETH. These validators get assigned to "committees," randomly selected groups to validate shards in the network. Ethereum relies on having a large validator set to provide availability and validity guarantees: They need at least 111 validators per shard to run the network and 256 validators per shard to finalize all shards within one epoch. With 64 shards, that's 16_384 validators (given 256 validators per shard). See Ethereum Economics and Eth2 shard chain simplification proposal for more information. Polkadot can provide strong finality and availability guarantees with much fewer validators. It uses Nominated Proof of Stake (NPoS) to select validators from a smaller set, letting smaller holders nominate validators to run infrastructure while still claiming the rewards of the system without running a node of their own. Polkadot needs about ten validators for each parachain in the network. Interoperability and Message Passing ‚Äã Polkadot uses Cross-Consensus Message Passing Format (XCM) for parachains to send arbitrary messages to each other. Parachains open connections with each other and can send messages via their established channels. Given that collators will need to be full nodes of the relay chain as well, they will be connected and can relay messages from parachain A to parachain B. Messages do not pass through the relay chain. Only validity proofs and channel operations do (open, close, etc.). This enhances scalability by keeping data on the edges of the system. Polkadot will add a protocol called SPREE that provides shared logic for cross-chain messages. Messages sent with SPREE carries additional guarantees about provenance and interpretation by the receiving chain. DApp Support and Development ‚Äã Ethereum mainly supports a form of smart contract development using Solidity. These contracts are immutable, and cannot be changed once published on-chain. Polkadot supports smart contracts through parachains, usually using the ink! smart contract language . On Ethereum, smart contracts can call each other; however, they are fixed on-chain to the domain of Ethereum. On Polkadot, smart contracts can call each other in the same parachain and across parachains. On Polkadot, developers have the option of either using smart contracts, calling extrinsics from pallets that modify the chain's state in some particular way or merely use Polkadot's RPC to directly retrieve and act on-chain information. DApps on Polkadot are often composed of these multiple components working together to modify, retrieve, and watch state changes live as they happen. Conclusion ‚Äã Ethereum and Polkadot both use a sharded model. Danksharding plans to utilize a rollup-centric approach by focusing on data availability. The Polkadot ecosystem is secured by a main chain, called the "relay chain," which in turn manages and connects its shards ("parachains/parathreads") into a single, homogenous solution. The primary differences between the two protocols are: All shards in Ethereum represent the same state transitions, while Polkadot lets shards have an abstract state transition function implementation. Governance processes in Ethereum are planned to be off-chain and thus require coordination for a hard fork to enact governance decisions. In contrast, in Polkadot the decisions are on-chain and enacted autonomously via forkless upgrades. Validator selection mechanisms differ as Polkadot can provide strong availability and validity guarantees with fewer validators per shard. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Kusama Next Cosmos High-Level Comparison Scalability: Sharding vs. Danksharding Architectural Differences: Polkadot and Ethereum Forks, Upgrades, and Governance Consensus and Finalization Staking Mechanics Interoperability and Message Passing DApp Support and Development Conclusion ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot vs. Cosmos General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Kusama Ethereum Cosmos Avalanche Other Comparisons Layer Two and Rollups Videos Future Implementations Archive Build Maintain Learn Polkadot Comparisons Cosmos On this page Polkadot vs. Cosmos Polkadot and Cosmos are both protocols that provide an interface for different state machines to communicate with each other. Both protocols are predicated on the thesis that the future will have multiple blockchains that need to interoperate with each other rather than individual blockchains existing in isolation. Model ‚Äã Polkadot uses a sharded model where each shard in the protocol has an abstract state transition function (STF). Polkadot uses WebAssembly (Wasm) as a "meta-protocol". A shard's STF can be abstract as long as the validators on Polkadot can execute it within a Wasm environment. The shards of Polkadot are called " parachains ". Every time a parachain wants to make a state transition, it submits a block (batch of state transitions) along with a state proof that Polkadot validators can independently verify. These blocks are finalized for the parachains when they are finalized by Polkadot's Relay Chain, the main chain of the system. As such, all parachains share state with the entire system, meaning that a chain re-organization of a single parachain would require a re-organization of all parachains and the Relay Chain. Cosmos uses a bridge-hub model that connects Tendermint chains. The system can have multiple hubs (the primary being the "Cosmos Hub"), but each hub connects a group of exterior chains, called "zones". Each zone is responsible for securing the chain with a sufficiently staked and decentralized validator set. Zones send messages and tokens to each other via the hub using a protocol called Inter-Blockchain Communication (IBC). As zones do not share state, a re-organization of one zone would not re-organize other zones, meaning each message is trust-bound by the recipient's trust in the security of the sender. Architecture ‚Äã Polkadot ‚Äã Polkadot has a Relay Chain acting as the main chain of the system. All validators in Polkadot are on the Relay Chain. Parachains have collators, who construct and propose parachain blocks to validators. Collators don't have any security responsibilities, and thus do not require a robust incentive system. Collators can submit a single parachain block for every Relay Chain block every 6 seconds. Once a parachain submits a block, validators perform a series of availability and validity checks before committing it to the final chain. Parachain slots are limited, and thus parachain candidates participate in an auction to reserve a slot for up to two years. For chains that do not have the funding for a parachain slot or the necessity to execute with a six-second block time, Polkadot also has parathreads . Parathreads execute on a pay-as-you-go basis, only paying to execute a block when they need to. In order to interact with chains that want to use their own finalization process (e.g. Bitcoin), Polkadot has bridge parachains that offer two-way compatibility. Cosmos ‚Äã Cosmos has a main chain called a "Hub" that connects other blockchains called "zones". Cosmos can have multiple hubs, but this overview will consider a single hub. Each zone must maintain its own state and therefore have its own validator community. When a zone wants to communicate with another zone, it sends packets over IBC. The Hub maintains a multi-token ledger of token balances (non-transfer messages are relayed but their state not stored in the Hub). Zones monitor the state of the Hub with a light client, but the Hub does not track zone states. Zones must use a deterministic finality algorithm (currently, all use Tendermint) and implement the IBC interface to be able to send messages to other chains through the Hub. Cosmos can also interact with external chains by using "peg zones", which are similar to bridged parachains. Consensus ‚Äã Polkadot uses a hybrid consensus protocol with two sub-protocols: BABE and GRANDPA, together called "Fast Forward". BABE (Blind Assignment for Blockchain Extension) uses a verifiable random function (VRF) to assign slots to validators and a fallback round-robin pattern to guarantee that each slot has an author. GRANDPA (GHOST-based Recursive Ancestor Deriving Prefix Agreement) votes on chains, rather than individual blocks. Together, BABE can author candidate blocks to extend the finalized chain and GRANDPA can finalize them in batches (up to millions of blocks at a time). This isolation of tasks provides several benefits. First, it represents a reduction in transport complexity for both block production and finalization. BABE has linear complexity, making it easy to scale to thousands of block producers with low networking overhead. GRANDPA has quadratic complexity, but is reduced by a factor of the latency, or how many blocks it finalizes in one batch. Second, having the capacity to extend the chain with unfinalized blocks allows other validators to perform extensive availability and validity checks to ensure that no invalid state transitions make their way into the final chain. Cosmos (both the Hub and the zones) uses Tendermint consensus, a round-robin protocol that provides instant finality. Block production and finalization are on the same path of the algorithm, meaning it produces and finalizes one block at a time. Because it is a PBFT-based algorithm (like GRANDPA), it has quadratic transport complexity, but can only finalize one block at a time. Staking Mechanics ‚Äã Polkadot uses Nominated Proof of Stake (NPoS) to select validators using the sequential Phragm√©n algorithm . The validator set size is set by governance (1_000 validators planned) and stakers who do not want to run validator infrastructure can nominate up to 16 validators. Phragm√©n's algorithm selects the optimal allocation of stake, where optimal is based on having the most evenly staked set. All validators in Polkadot have the same weight in the consensus protocols. That is, to reach greater than 2/3 of support for a chain, more than 2/3 of the validators must commit to it, rather than 2/3 of the stake. Likewise, validator rewards are tied to their activity, primarily block production and finality justifications, not their amount of stake. This creates an incentive to nominate validators with lower stakes, as they will earn higher returns on their staked tokens. The Cosmos Hub uses Bonded Proof of Stake (a variant of Delegated PoS) to elect validators. Stakers must bond funds and submit a delegate transaction for each validator they would like to delegate to with the number of tokens to delegate. The Cosmos Hub plans to support up to 300 validators. Consensus voting and rewards are both stake-based in Cosmos. In the case of consensus voting, more than 2/3 of the stake must commit, rather than 2/3 of the validators. Likewise, a validator with 10% of the total stake will earn 10% of the rewards. Finally, in Cosmos, if a staker does not vote in a governance referendum, the validators assume their voting power. Because of this, many validators in Cosmos have zero commission in order to acquire more control over the protocol. In Polkadot, governance and staking are completely disjoint; nominating a validator does not assign any governance voting rights to the validator. Message Passing ‚Äã Polkadot uses Cross-Consensus Message Passing Format (XCM) for parachains to send arbitrary messages to each other. Parachains open connections with each other and can send messages via their established channels. Collators are full nodes of parachains and full nodes of the Relay Chain, so collator nodes are a key component of message passing. Messages do not pass through the Relay Chain, only proofs of post and channel operations (open, close, etc.) go into the Relay Chain. This enhances scalability by keeping data on the edges of the system. In the case of a chain re-organization, messages can be rolled back to the point of the re-organization based on the proofs of post in the Relay Chain. The shared state amongst parachains means that messages are free from trust bounds; they all operate in the same context. Polkadot has an additional protocol called SPREE that provides shared logic for cross-chain messages. Messages sent with SPREE carry additional guarantees about provenance and interpretation by the receiving chain. Cosmos uses a cross-chain protocol called Inter-Blockchain Communication (IBC). The current implementation of Cosmos uses the Hub to pass tokens between zones. However, Cosmos does have a new specification for passing arbitrary data. Nonetheless, as chains do not share state, receiving chains must trust the security of a message's origin. Governance ‚Äã Polkadot has a multicameral governance system with several avenues to pass proposals. All proposals ultimately pass through a public referendum, where the majority of tokens can always control the outcome. For low-turnout referenda, Polkadot uses adaptive quorum biasing to set the passing threshold. Referenda can contain a variety of proposals, including fund allocation from an on-chain Treasury . Decisions get enacted on-chain and are binding and autonomous. Polkadot has several on-chain, permissionless bodies. The primary one is the Council, which comprises a set of accounts that are elected in Phragm√©n fashion. The Council represents minority interests and as such, proposals that are unanimously approved of by the Council have a lower passing threshold in the public referendum. There is also a Technical Committee for making technical recommendations (e.g. emergency runtime upgrade to fix a bug). Cosmos uses coin-vote signaling to pass referenda. The actual enactment of governance decisions is carried out via a protocol fork, much like other blockchains. All token holders can vote, however, if a delegator abstains from a vote then the validator they delegate to assume their voting power. Validators in Polkadot do not receive any voting power based on their nominators. Upgrades ‚Äã Using the Wasm meta-protocol, Polkadot can enact chain upgrades and successful proposals without a hard fork. Anything that is within the STF, the transaction queue, or off-chain workers can be upgraded without forking the chain. As Cosmos is not based on a meta-protocol, it must enact upgrades and proposals via a normal forking mechanism. Development Framework ‚Äã Both Cosmos and Polkadot are designed such that each chain has its STF and both provide support for smart contracts in both Wasm and the Ethereum Virtual Machine (EVM). Polkadot provides an ahead-of-time Wasm compiler as well as an interpreter (Wasmi) for execution, while Cosmos only executes smart contracts in an interpreter. Cosmos chains can be developed using the Cosmos SDK, written in Go. The Cosmos SDK contains about 10 modules (e.g. staking, governance, etc.) that can be included in a chain's STF. The SDK builds on top of Tendermint. The primary development framework for parachains is Substrate , written in Rust. Substrate comes with FRAME, a set of about 40 modules (called "pallets") to use in a chain's STF. Beyond simply using the pallets, Substrate adds a further layer of abstraction that allows developers to compose FRAME's pallets by adding custom modules and configuring the parameters and initial storage values for the chain. Polkadot can support an STF written in any language So long as it compiles to its meta-protocol Wasm. Likewise, it could still use the Substrate client (database, RPC, networking, etc.); it only needs to implement the primitives at the interface. Conclusion ‚Äã Polkadot was designed on the principle that scalability and interoperability require shared validation logic to create a trust-free environment. As more blockchains are developed, their security must be cooperative, not competitive. Therefore, Polkadot provides the shared validation logic and security processes across chains so that they can interact knowing that their interlocutors execute within the same security context. The Cosmos network uses a bridge-hub model to connect chains with independent security guarantees, meaning that when data is sent from one chain to another (inter-chain communication), the receiving chain must trust the sending chain. Thus, each blockchain in the Cosmos network has its independent security mechanisms. They're independently secured and do not rely on the security of other blockchains or the hub. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Ethereum Next Avalanche Model Architecture Polkadot Cosmos Consensus Staking Mechanics Message Passing Governance Upgrades Development Framework Conclusion ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot vs. Avalanche General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Kusama Ethereum Cosmos Avalanche Other Comparisons Layer Two and Rollups Videos Future Implementations Archive Build Maintain Learn Polkadot Comparisons Avalanche On this page Polkadot vs. Avalanche info To keep the content on this page factually correct and up-to-date, contributions are welcome. Polkadot and Avalanche both have an architecture that allows for application-specific blockchains to be designed and connected to a primary network. In Polkadot, the primary network is the Relay-chain and Avalanche does this with 3 main chains - the P-chain, X-chain, and C-chain. Similar to how Polkadot has its Parachains that connect to the Relay-chain, Avalanche has what‚Äôs called subnets . Similar to Polkadot, Avalanche also uses a PoS mechanism for achieving consensus. The validators stake their AVAX tokens in order to participate in the PoS system and secure the network. Architecture ‚Äã Avalanche's architecture separates the responsibility of a layer-1 smart contract platform into three chains. This allows for a separation of concern over validators and consensus, transactions, and smart contract execution. Avalanche uses a DAG (Directed Acyclic Graph) structure for one of its chains which is non-linear. Polkadot uses the linear chain structure similar to Bitcoin and Ethereum. Smart contracts in Polkadot are implemented on parachains . Polkadot being a layer-0 blockchain, is not a smart contract platform and does not have plans to support them natively. Image source: Avalanche docs . P-chain (Platform) ‚Äã The P-chain is responsible for maintaining the validator set and securing the network. AVAX token holders can spin up their own nodes and become validators by staking their tokens. Similar to the NPoS system that Polkadot uses, Avalanche uses a Delegated PoS which allows token holders to also delegate their token stake to existing validators instead of running their own nodes. X-chain (Exchange) ‚Äã The X-chain is responsible for the transaction layer of the Avalanche blockchain. It uses a UTXO model like Bitcoin whereas Polkadot uses an account model like Ethereum. This is the only chain that implements the DAG (Directed Acyclic Graph) model for its blockchain, making this the fastest chain on the Avalanche network. This chain does not support smart contract execution. C-chain (Contracts) ‚Äã The C-chain is where the most activity will happen on the Avalanche network. It allows for different virtual machines to execute smart contract code. Out of the box, it has support for EVM and AVM (Avalanche VM). C-Chain runs a fork of go-ethereum called coreth that has the networking and consensus portions replaced with Avalanche equivalents. As Polkadot does not have a smart contract layer out of the box, the EVM and WASM smart contract abilities lie in the Parachain layers. This is a major difference between Polkadot and Avalanche. The smart-contract abilities of Avalanche are baked into its three-chain model. Subnets or sub-networks ‚Äã Avalanche defines a subnet as a dynamic set of validators that achieve consensus on a set of blockchains. In Polkadot's terminology, Subnets can be viewed as public or private blockchain runtimes that can be built on top of the primary network and allow a subset of the validators to validate these runtimes. Similar to the Parachains on Polkadot, Subnets provide the freedom to choose the transaction fee model, tokenomics, and custom compile rules. One or many validators can start validating a subnet runtime, effectively becoming a subset of the overall validator set of the Primary Network. Consensus ‚Äã Image source: gyuho.dev . Avalanche consensus uses a family of protocols to achieve security, liveness, and finality. These are known as the Snow * protocols. This group of protocols composed together uses both classical and Nakamoto consensus as well as a Delegated Proof-of-Stake system for its block creators. The Snow family is a hierarchical collection of systems used to reach finality on Avalanche: Slush Snowflake Snowball Avalanche Snowman Slushie Compared to Polkadot, Avalanche uses an asynchronous hybrid system that is based on a classical and Nakomoto approach. Polkadot uses a synchronous hybrid model that combines BABE and GRANDPA , where BABE is the algorithm used to build blocks in a probabilistic way, and GRANDPA is a finality mechanism that uses a deterministic approach to adding blocks to the longest chain. In the end, validators agree to whole chains, rather than single new blocks. Snowball ‚Äã The snowball protocol is an algorithm that nodes use to come to a consensus. Each node continuously queries x number of validators and takes the majority consensus and adopts it as its own. This method, in normal circumstances, will lead to the network reaching a consensus. The scalability of Snowball is promising, as the number of participants in the network grows, the number of consensus messages being passed around remains the same. Nodes will query no more than 20 nodes at a given time. DAG(Directed Acyclic Graph) ‚Äã DAGs are graphs consisting of vertices and edges. In Avalanche they are used for partial ordering of decisions, such as transactions. Vertices point to each other using edges, and when ordered topologically vertices and edges create a sequence. Edges in the case of Avalanche can be conflicting, and nodes will use the snowball algorithm to make decisions about which edges to keep and which to not. Staking Mechanics ‚Äã Avalanche uses a Delegated Proof-of-Stake mechanism without any slashing. The barrier to entry for staking as a full node validator is 2500 AVAX, and 25 AVAX to become a delegator. With a minimum stake period being two weeks and a maximum period being a year, for both validators and delegators. It is not clear from the Avalanche documentation what happens after a year, it is likely that validators will have to re-stake and start a new period. Validators acquire points for uptime and correctness of their work, and the remuneration of rewards depends on that. In Polkadot the minimum stake needed to be a validator is variable, same for being a nominator. The true minimum need to be competitive enough to be included in the active set for validators, or successfully being chosen as a nominator depends on the minimum staked amounts on the network at a given time. Read more about this in the staking page . Message Passing ‚Äã Avalanche does not have a native trustless message-passing mechanism. Instead, it relies on bridges. Though, because it is an EVM-compatible protocol, it's able to interoperate at a token level. However, subnets do not have a messaging layer out of the box. Polkadot, with its XCM and XCMP messaging protocols, allows for a native and trustless messaging scheme, thus supporting the composability of chains and enabling the development of powerful cross-chain applications. Governance ‚Äã According to its whitepaper, Avalanche plans to have an on-chain governance mechanism. It currently does not have an on-chain or off-chain system in production. Its governance system will limited to updating only a few key protocol parameters which include: Staking amount: This value defines the minimal stake required to be placed as bond before participating in the system. Minimum staking time for a node: The minimal amount of time required for a node to stake into the system. Maximum staking time for a node: The maximal amount of time a node can stake. Minting rate: Reward rate function, also referred to as minting rate, determines the reward a participant can claim as a function of their staking amount given some number of x publicly disclosed nodes under its ownership, over a period of t consecutive minimal staking time timeframes, such that t minimal staking time ‚â§ maximum staking time . Transaction fee amount: The fee structure, which is a set of governable fees parameters that specify costs to various transactions. Limiting the governance functionality is a design choice to increase predictability and safety. Polkadot's governance mechanism has been in production from the very beginning and was used to slowly release functionality and decentralize the initial network. It is also not limited to a few parameters and in fact, the whole runtime is subject to change via protocol making Polkadot a meta-protocol. Upgrades ‚Äã The upgrades to Avalanche are administered by the protocol developers at Ava Labs . On Polkadot, the forkless upgrades are administered and deployed through the on-chain governance. When performing upgrades, every single validator on the Subnet will need to perform the identical upgrade. This requires a co-ordination effort among the Validators of the Subnet. On Polkadot, upgrades to Parachains can be deployed automatically without any coordination with the Validators on the relaychain. Conclusion ‚Äã Avalanche has made some design decisions that allow for an improved smart-contract development environment in which protocol engineers can have the freedom to create their own blockchains and include them in the Avalanche ecosystem via subnets. The trade-offs are that the autonomy of design is limited and blockchains have to buy into the design decisions of Avalanche's main chains. Unlike parachains on Polkadot, Subnets are not able to share the security of the main chains. In addition to utilizing block finality and security of the Relay-chain, parachains on Polkadot use XCM to pass native trustless messages, instead of having to rely on multiple bridging solutions. However, Subnets are easy to launch when compared to parachains, given that they only need a recommended minimum of 5 validators, which make the costs of launch predictable. Avalanche has plans to implement shared security, interoperability, composability and on-chain governance features which are already offered by Polkadot. References ‚Äã The Avalanche Platform Whitepaper The Avalanche Consensus Whitepaper The AVAX Token Dynamics Paper Nakomoto vs Snow consensus Edit this page Last updated on Jul 14, 2023 by Filippo Previous Cosmos Next Other Comparisons Architecture Consensus Snowball DAG(Directed Acyclic Graph) Staking Mechanics Message Passing Governance Upgrades Conclusion References ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Comparisons General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Kusama Ethereum Cosmos Avalanche Other Comparisons Layer Two and Rollups Videos Future Implementations Archive Build Maintain Learn Polkadot Comparisons Other Comparisons On this page Polkadot Comparisons Polkadot is a blockchain technology but makes some innovations that sets it apart from other popular chains. In-depth Comparisons for multi-chain ecosystems See the in-depth comparisons for Ethereum 2.0 , Cosmos and Avalanche . Ethereum 1.x ‚Äã Ethereum is a smart contract blockchain that allows for general computation to be deployed on-chain and operated across the p2p network. Ethereum 1.x refers to the current Ethereum release and the immediately planned future upgrades. The difference between Ethereum 1.x and Polkadot is quite large. Ethereum is a single chain that allows developers to extend its functionality through the deployment of blobs of code onto the chain (called smart contracts). Polkadot, as described in the whitepaper, is a fully extensible and scalable blockchain network that provides security and interoperability through shared state. In practical terms, this means that the layer of abstraction between these two projects is remarkably different for developers. In Ethereum, developers write smart contracts that all execute on a single virtual machine, called the Ethereum Virtual Machine (EVM). In Polkadot, however, developers write their logic into individual blockchains, where the interface is part of the state transition function of the blockchain itself. Polkadot will also support smart contract blockchains for Wasm and EVM to provide compatibility with existing contracts, but will not have smart contract functionality on its core chain, the Relay Chain. As such, Polkadot is a possible augmentation and scaling method for Ethereum 1.x, rather than competition. Binance Smart Chain ‚Äã Binance Chain is a Proof of Stake Authority (PoSA) blockchain used to exchange digital assets on Binance DEX. Binance Smart Chain is an EVM-compatible smart contract chain bridged to Binance Chain. Together, they form the Binance Dual Chain System. Binance Smart Chain is also a Proof of Stake Authority chain and allows users to create smart contracts and dapps. Both chains are built with Cosmos SDK and therefore are a part of the Cosmos ecosystem. Due to specifics of the Cosmos architecture, interoperability of Binance Smart Chain is based on bridges. This means all validators of both chains are also bridge operators, therefore the security of the system relies on trusting validators. At the moment, there are 21 Binance Smart Chain validator nodes. Polkadot has an entirely different purpose, as it was built to connect and secure unique blockchains. It is a protocol on which single blockchains (such as Binance Smart Chain) could be built and benefit from shared security, interoperability and scalability. Interoperability within Polkadot is based on pooled security on Polkadot, and the security of the entire Polkadot network, and has much stronger economic security. Scalability based on bridges relies on each bridged chain finding its own set of validators, therefore duplicate resources are required. Scalability on Polkadot is based on the security of the Relay Chain, and as the number of validators in the active set on Polkadot are increased, more parachains can be supported. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Avalanche Next Layer Two and Rollups Ethereum 1.x Binance Smart Chain ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Layer Two and Rollups General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Kusama Ethereum Cosmos Avalanche Other Comparisons Layer Two and Rollups Videos Future Implementations Archive Build Maintain Learn Polkadot Comparisons Layer Two and Rollups On this page Layer Two and Rollups note This comparison covers general information regarding two widely used rollup mechanisms that are used to scale (usually EVM-based) blockchains and compares and contrasts how Polkadot achieves scalability. Layer two (L2) networks are popular as being the way forward for blockchain scalability by off-loading the majority of computation from layer one (L1) networks. L2 solutions utilize the L1 network's security and functionality to build an additional layer that is often faster, reduces fees, and solves other platform-specific issues. In many cases, L2 solutions focus on utilizing block space on a particular blockchain efficiently and cost-effectively. Rollups are an L2 scaling solution. At the most basic level, a rollup L2 solution is responsible for "rolling up" transactions by batching them before publishing them to the L1 chain, usually through a network of sequencers . This mechanism could include thousands of transactions in a single rollup. Polkadot implements this functionality at the native level (i.e. without using L2 scaling solutions), allowing for shared security and scalability of the relay chain and respective parachains. Shared security is a concept that has similar goals to EVM-based optimistic and zero-knowledge rollups. Still, instead of being implemented as a secondary layer, Polkadot guarantees native security and scalability for each of its parachains through the Parachains Protocol . Polkadot handles the coordination of data from parachains into an aggregated, representative state, somewhat similar to L2 rollups. Optimistic Rollups ‚Äã Optimistic rollups are an interactive scaling method for L1 blockchains. They assume optimistically that every proposed transaction is valid by default. In the case of mitigating potentially invalid transactions, optimistic rollups introduce a challenge period during which participants may challenge a suspect rollup. A fraud-proving scheme is in place to allow for several fraud proofs to be submitted. Those proofs could make the rollup valid or invalid. During the challenge period, state changes may be disputed, resolved, or included if no challenge is presented (and the required proofs are in place). While optimistic rollups provide scalability, they have both benefits and drawbacks to their approach: Benefits: They are not limited by the type of state change - any state change can be included, meaning existing apps do not have to account for it. They can be parallelized for scalability. A substantial amount of data can fit within a single rollup ( in the case of Ethereum, for example , tens of thousands of transactions in a single state transition). Drawbacks: Transaction censorship and centralization are of concern, where sequencers/L2 nodes can be compromised. Challenge periods could take a substantial amount of time to pass, increasing time for the rollup to finalize onto the L1 network. Due to their generalist nature of including any state change for their parent network, optimistic rollups can run into gas limitations or cause network congestion in the case of Ethereum. Optimistic rollups are often used in the Ethereum ecosystem. Examples of optimistic EVM-based rollup solutions include: Optimisim Arbitrum Unipig Zero-knowledge Rollups ‚Äã Zero-knowledge rollups (often called ZK rollups) are a non-interactive method that utilizes zero-knowledge proofs to compute the validity of a particular set of state changes. Whereas optimistic rollups relied on fraud proofs, ZK rollups rely on cryptographic validation in the form of ZK proofs. Zero-knowledge rollups are significantly faster in finalization, as the cryptographic validity proof handles the nuance of ensuring a rollup is valid. However, the ZK rollups often suffer from performance due to their complexity and difficult implementation into resource-constrained environments. Because Turing completeness is also challenging to achieve due to this computational overhead, their ability to be generalized (in terms of blockspace) is reduced. However, they have a promising future in solving some of the problems of optimistic rollups and addressing secure scalability. Benefits: They only require a small amount of data availability. Often, the proof is enough to ensure validity. They can be proven trustlessly. Because the proof is immediately available, finality is also instantaneous. They have a promising future overall, as they have not reached maturity yet. Drawbacks: They suffer from the same problems that other L2 solutions have regarding the centralization of L2 operators. They are computationally expensive, and ZK circuits are difficult to implement. The potential for congestion is still a factor, as the amount of data could still be problematic. Polkadot - Native Shared Security ‚Äã Whereas rollups are considered solutions for L2 protocols, Polkadot include this functionality natively through its Parachains Protocol . The Parachains Protocol, which is how Polkadot handles network's sharding is meant to accomplish the combined goals of providing security, scalability, and availability. It enables parachains to verify their collective state and communicate with one another. Parachains have similarities to aspects of optimistic and ZK rollups, which are reflected in how Polkadot handles the validity and availability of the parachain state. Collators , a key part of Polkadot's architecture, are in principle similar to sequencers, as collators pass data with a proof-of-validity (PoV) function for liveness and communication with the relay chain. Each shard, or parachain, is equipped with a unique state transition function (STF). This function ensures that communication to the relay chain remains valid. Each STF, called runtime, is written in Wasm . Any state transition function is valid if it compiles to Wasm and abides by the Parachains Protocol. Each STF runs a validity proof. The proof ( the Approval Protocol ) is interactive, unlike ZK rollups, which are non-interactive. Additionally, unlike ZK rollups, there are no difficulties in creating parachains with Turing-complete logic. Each parachain is also a full-fledged state machine (usually in the form of a blockchain). Similarly to optimistic rollups, the Parachain Protocol also has cases where disputes and resolutions of potentially harmful para blocks (blocks representing the parachain) can take place, in which case validators are slashed if a bad parablock is found. Benefits: Protocol level sharding, shared security, and interoperability. Each shard has a low barrier of entry in terms of development, as anything that compiles to Wasm is a valid target. Fast Finality (usually under a minute on Polkadot). Data availability is built-in through validators and mechanisms like erasure coding . No L2 implies less of a risk of incurring centralization issues for sequencers or other L2 operators. Drawbacks: Execution of code in Wasm could be a performance bottleneck, as it is slower than making native calls. The relay chain sets a hard limit on the size and weights of the PoV (Proof of Validity) blocks which contain the parachain state transition data. Despite these drawbacks, Polkadot remains upgradable through forkless upgrades, which allows the protocol to be easily upgradable to stay in line with future technological advances. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Other Comparisons Next Videos Optimistic Rollups Zero-knowledge Rollups Polkadot - Native Shared Security ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Videos about Polkadot General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Build Maintain Learn Videos On this page Videos about Polkadot Visit the Polkadot YouTube Channel For more videos see all playlists on the Polkadot YouTube channel . Ongoing Series ‚Äã Learn about Polkadot and Substrate at a deeper level with these ongoing deep dives, technical seminars, and initiatives within the ecosystem. Technical Explainers Substrate Seminars Polkadot Deep Dives Polkadot Community Calls Polkadot Decoded 2023 Polkadot Blockchain Academy Past Events ‚Äã Polkadot for Beginners ETH Denver 2023 Encode Polkadot Club Sub0 2022 Polkadot Decoded 2022 Polkadot in Davos 2022 Polkadot Webinars Hackathon North America Hackathon Global Series Edit this page Last updated on Jun 30, 2023 by Filippo Previous Layer Two and Rollups Next Future Implementations Ongoing Series Past Events ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Future Implementations General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Videos Future Implementations SPREE Parathreads Archive Build Maintain Learn Future Implementations Future Implementations Implementations that will be deployed on Polkadot. üìÑÔ∏è SPREE Fundamentals of SPREE. üìÑÔ∏è Parathreads An Introductory Guide to Parathreads. Previous Videos Next SPREE ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
SPREE General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Videos Future Implementations SPREE Parathreads Archive Build Maintain Learn Future Implementations SPREE On this page SPREE Shared Protected Runtime Execution Enclaves (SPREE) sometimes referred to as "trust wormholes," are fragments of logic comparable to runtime modules in Substrate, but live on the Polkadot Relay Chain and maybe opted into by parachains. SPREE in brief was described with the following properties and functions: Parachains can opt-in to special runtime logic fragments (like smart contracts). These fragments have their own storage and own XCM endpoint. All instances across parachains have identical logic. It executes alongside parachain logic. Protected: storage can not be altered by parachain logic; messages can not be faked from them by parachains. Origin ‚Äã On 28 March, 2019 u/Tawaren, a member of the Polkadot community, made a post on r/dot called "SmartProtocols Idea" and laid out a proposal for Smart Protocols . The core insight of the post was that XCMP had a complication in that it was difficult to verify and prove code was executed on a parachain without trust. A solution was to install the SmartProtocols in the Relay Chain that would be isolated blobs of code with their own storage per instance that could only be changed through an interface with each parachain. SmartProtocols are the precursor to SPREE. What is a SPREE module? ‚Äã SPREE modules are fragments of logic (in concrete terms they are blobs of WebAssembly code) that are uploaded onto Polkadot through a governance mechanism or by parachains. Once the blob is uploaded to Polkadot, all other parachains can decide to opt-in to the logic. The SPREE module would retain its own storage independent of the parachain, but would be callable through an interface with the parachain. Parachains will send messages to the SPREE module synchronously. SPREE modules are important to the overall XCMP architecture because they give a guarantee to the code that will be executed on destination parachains. While XCMP guarantees the delivery of a message, it does not guarantee what code will be executed, i.e. how the receiving parachain will interpret the message. While XCMP accomplishes trustless message passing, SPREE is the trustless interpretation of the message and a key part of the usefulness of XCMP. SPREE modules are like recipes in cookbooks. For example, if we give an order to a cook to make a souffl√©, and we‚Äôre decently confident in the ability of the cook, we have a vague idea of what will be made but no actual surety of how it will be made. However, let‚Äôs say that a cook has the ‚ÄúSouffl√© Maker‚Äôs Manual‚Äù on their bookshelf and has committed themselves to only make souffles from this book. Now we can also consult the same book that the cook has, and we have a precise understanding of what will happen when we tell the cook to make a souffl√©. In this example, ‚Äúmake a souffl√©‚Äù was the message in XCMP and the cookbook was the SPREE module. In concrete terms, SPREE modules could be useful for various functionality on Polkadot.. One suggested use case of SPREE modules is for a trustless decentralized exchange that is offered as functionality to any parachain without any extra effort from parachain developers. One can imagine this working by having a SPREE module that exposes the interface for the incrementing and decrementing of balances of various assets based on a unique identifier. Why? ‚Äã Sending messages across parachains in XCMP only ensures that the message will be delivered but does not specify the code that will be executed, or how the message will be interpreted by the receiving parachain. There would be ways around this such as requesting a verifiable receipt of the execution from the receiving parachain, but in the naked case, the other parachain would have to be trusted. Having shared code that exists in appendices that the parachain can opt-in to resolves the need for trust and makes the execution of the appendices completely trustless. SPREE would be helpful to ensure that the same logic is shared between parachains in the SPREE modules. An especially relevant use case would revolve around the use of token transfers across parachains in which it is important that the sending and receiving parachains agree about how to change the total supply of tokens and a basic interface. Example ‚Äã The diagram above is a simplification of the Polkadot system. In this diagram, we see that the Wasm code for SPREE module "X" has been uploaded to the Polkadot Relay Chain. The two cylinders "A" and "B" represent two distinct parachains that have both opted-in to this SPREE module creating two distinct instances of it with their own XCMP endpoints "A.X" and "B.X". In the example, we assume that this SPREE module "X" contains the functionality for incrementing or decrementing the balance of a particular asset that is unique to this module. By initiating a transaction at A.X to decrease a particular balance by 1, a message over XCMP can be trustlessly sent to B.X to increase a balance by 1. Collators, represented as the green triangle are responsible for relaying this message from parachain A to parachain B, as well as maintaining the storage for each particular instance of A.X and B.X for their respective parachains. They provide proofs of valid state transitions to the Relay Chain validators represented as blue diamonds. Validators can validate the correct state transitions of SPREE modules A.X and B.X by being provided with the previous state root of the SPREE module instances, the data of the XCMP message between the instances, and the next state root of the instance. They do this validation by checking it against the validate function as provided by the SPREE module API. Collators are expected to be able to provide this information to progress their parachains. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Future Implementations Next Parathreads Origin What is a SPREE module? Why? Example ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Parathreads General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Videos Future Implementations SPREE Parathreads Archive Build Maintain Learn Future Implementations Parathreads On this page Parathreads Parathreads are an idea for parachains to temporarily participate (on a block by block basis) in Polkadot security without needing to lease a dedicated parachain slot. This is done through economically sharing the scarce resource of a parachain slot among several competing resources (parathreads). Chains that otherwise would not be able to acquire a full parachain slot or do not find it economically sensible to do so, are enabled to participate in Polkadot's shared security ‚Äî albeit with an associated fee per executed block. It also offers a graceful off-ramp to parachains that no longer require a dedicated parachain slot, but would like to continue using the Relay Chain. Origin ‚Äã According to this talk in Chengdu, the origin of the idea came from similar notions in the limited resource of memory on early personal computers of the late '80s and '90s. Since computers have a limited amount of physical memory, when an application needs more, the computer can create virtual memory by using swap space on a hard disk. Swap space allows the capacity of a computer's memory to expand and for more processes to run concurrently with the trade-off that some processes will take longer to progress. How do Parathreads Operate? ‚Äã A portion of the parachain slots on the Relay Chain will be designated as part of the parathread pool. In other words, some parachain slots will have no parachain attached to them and rather will be used as a space for which the winner(s) of the block-by-block parathread fee auction can have their block candidate included. Collators will offer a bid designated in DOT for inclusion of a parathread block candidate. The Relay Chain block author is able to select from these bids to include a parathread block. The obvious incentive is for them to accept the block candidate with the highest bid, which would bring them the most profit. The tokens from the parathread bids will likely be split 80-20, meaning that 80% goes into Polkadot treasury and 20% goes to the block author. This is the same split that applies also to transaction fees and, like many other parameters in Polkadot, can be changed through a governance mechanism. Parachain vs. Parathread ‚Äã Parachains and parathreads are very similar from a development perspective. One can imagine that a chain developed with Substrate can at different points in its lifetime assume one of three states: an independent chain with secured bridge, a parachain, or a parathread. It can switch between these last two states with relatively minimal effort since the difference is more of an economic distinction than a technological one. Parathreads have the exact same benefits for connecting to Polkadot that a full parachain has. Namely, it is able to send messages to other para-objects through XCMP and it is secured under the full economic security of Polkadot 's validator set. The difference between parachains and parathreads is economic. Parachains must be registered through a normal means of Polkadot, i.e. governance proposal or parachain slot auction. Parathreads have a fixed fee for registration that would realistically be much lower than the cost of acquiring a parachain slot. Similar to how DOT are locked for the duration of parachain slots and then returned to the winner of the auction, the deposit for a parathread will be returned to the parathread after the conclusion of its term. Registration of the parathread does not guarantee anything more than the registration of the parathread code to the Polkadot Relay Chain. When a parathread progresses by producing a new block, there is a fee that must be paid in order to participate in a per-block auction for inclusion in the verification of the next Relay Chain block. All parathreads that are registered are competing in this auction for their parathread to be included for progression. There are two interesting observations to make about parathreads. Since they compete on a per-block basis, it is similar to how transactions are included in Bitcoin or Ethereum. A similar fee market will likely develop, which means that busier times will drive the price of parathread inclusion up, while times of low activity will require lower fees. Two, this mechanism is markedly different from the parachain mechanism, which guarantees inclusion as long as a parachain slot is held; parathread registration grants no such right to the parathread. Parathread Economics ‚Äã There are two sources of compensation for collators: Assuming a parathread has its own local token system, it pays the collators from the transaction fees in its local token. If the parathread does not implement a local token, or its local token has no value (e.g. it is used only for governance), then it can use DOT to incentivize collators. Parathread protocol subsidy. A parathread can mint new tokens in order to provide additional incentives for the collator. Probably, the amount of local tokens to mint for the parathread would be a function of time, the more time that passes between parathread blocks that are included in the Relay Chain, the more tokens the parathread is willing to subsidize in order to be considered for inclusion. The exact implementation of this minting process could be through local parathread inflation or via a stockpile of funds like a treasury. Collators may be paid in local parathread currency. However, the Relay Chain transacts with the Polkadot native currency only. Collators must then submit block candidates with an associated bid in DOT. Parachain Slot Swaps ‚Äã It will be possible for a parachain that holds a parachain slot to swap this slot with a parathread so that the parathread "upgrades" to a full parachain and the parachain becomes a parathread. The chain can also stop being a chain and continue as a thread without swapping the slot. The slot, if unoccupied, would be auctioned off in the next auction period . This provides a graceful off-ramp for parachains that have reached the end of their lease and do not have sufficient usage to justify renewal; they can remain registered on the Relay Chain but only produce new blocks when they need to. Parathreads help ease the sharp stop of the parachain slot term by allowing parachains that are still doing something useful to produce blocks, even if it is no longer economically viable to rent a parachain slot. Resources ‚Äã Parathreads: Pay-as-you-go Parachains Edit this page Last updated on Jun 30, 2023 by Filippo Previous SPREE Next Archive Origin How do Parathreads Operate? Parachain vs. Parathread Parathread Economics Parachain Slot Swaps Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Archive General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Governance V1 Gov1 Treasury Polkadot Launch Phases Redenomination of DOT Controller Accounts Build Maintain Learn Archive Archive Implementations no longer live on Polkadot. üìÑÔ∏è Governance V1 Polkadot's First Governance Model. üìÑÔ∏è Gov1 Treasury The Polkadot's On-chain Treasury during Gov1. üìÑÔ∏è Polkadot Launch Phases Polkadot's Launch Process and its Individual Phases. üìÑÔ∏è Redenomination of DOT Everything about the DOT Token Redenomination. üìÑÔ∏è Controller Accounts Controller Accounts used in Staking. Previous Parathreads Next Governance V1 ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Governance V1 General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Governance V1 Gov1 Treasury Polkadot Launch Phases Redenomination of DOT Controller Accounts Build Maintain Learn Archive Governance V1 On this page Governance V1 Polkadot uses a sophisticated governance mechanism that allows it to evolve gracefully overtime at the ultimate behest of its assembled stakeholders. The stated goal is to ensure that the majority of the stake can always command the network. Polkadot OpenGov is live on Kusama Network Learn about the upcoming changes to the governance on Polkadot in this Wiki doc on Polkadot OpenGov . Polkadot brings together various novel mechanisms, including an amorphous (abstract) form of state-transition function stored on-chain defined in a platform-agnostic language (i.e. WebAssembly ). It also allows for several on-chain voting mechanisms, such as referenda with the novel concept of Adaptive Quorum Biasing and batch approval voting. All changes to the protocol must be agreed upon by stake-weighted referenda. To make any changes to the network, the idea is to compose active token holders and the council together to administrate a network upgrade decision. No matter whether the proposal is proposed by the public (token holders) or the Council , it finally will have to go through a vote on a referendum to let all holders, weighted by stake, make the decision. Governance Summary ‚Äã The figure below shows an overview of Governance V1 with the key actors and different paths for submitting a proposal that can potentially be voted on as a referendum. The public (i.e. token holders) can submit a proposal that gets added to the proposal queue. Here, proposals are endorsed , and the one that gets the most support will climb to the top of the queue. When it is time, the proposal at the top of the queue will become a Public Referendum . For instance, the proposal with 11 endorsements is shown at the top of the queue in the figure, which is ready to become a referendum. The public can also submit a treasury proposal , which must be evaluated by the Council through a motion. If the Council motion passes, the treasury proposal can be directly executed or go to the external queue, which will be voted on through a Council Referendum . See the figure's green horizontal path from the Public (green) to the Council (yellow). Treasury proposals and Council proposals can be directly executed (horizontal yellow arrows) or go to the external queue, where they will become a referendum Note that the external queue always consists of a single proposal . A proposal in the external queue can be fast-tracked by the Technical Committee (light blue). The fast track can contain as many proposals as possible (also called emergency proposals) that can be voted on simultaneously with with the referenda introduced either by the Council or the Public. See in the figure the yellow circle (i.e. Council Proposal) exiting the external queue, and the yellow circle with a light-blue border also leaving the queue and being fast-tracked by the Technical Committee (TC). Once empty, the external queue can be filled with another Council proposal. The Council can also submit proposals that will end up in the external queue. Voting on Council and Public proposals subject to an alternating timetable , shown in the figure as the "on" and "off" toggles on the external and proposal queues. In this example, the Public proposal will be voted on together with the fast-tracked Council Proposal. Voting on non-fast-tracked Council Proposals will be blocked until the alternating timetable switches the toggles, which stops Public proposals from becoming a referenda. Referenda will follow an adaptive quorum biasing mechanism for deciding whether they get enacted, and if they do, they will be executed after an enactment period . Token holders can delegate their votes (with a conviction multiplier) to another account belonging to a trusted entity voting on their behalf. Proposals ‚Äã Referenda can be started in different ways: Publicly submitted proposals Proposals submitted by the council, either through a majority or unanimously Proposals submitted as part of the enactment of a prior referendum Emergency proposals submitted by the Technical Committee and approved by the Council Starting a proposal in Governance V1 For more information about how to start a proposal, see the dedicated page . Endorsing Proposals ‚Äã Anyone can submit a proposal by depositing the minimum amount of tokens for a certain period (number of blocks). If someone agrees with the proposal, they may deposit the same amount of tokens to support it - this action is called endorsing . The proposal with the highest amount of bonded support will be selected to be a referendum in the next voting cycle based on an alternating voting timetable . Cancelling Proposals ‚Äã A proposal can be canceled if the Technical Committee unanimously agrees to do so or if Root Origin (e.g. sudo) triggers this functionality. A canceled proposal's deposit is burned. Additionally, a two-thirds majority of the council can cancel a referendum. This may function as a last-resort if there is an issue found late in a referendum's proposal, such as a bug in the code of the runtime that the proposal would institute. If the cancellation is controversial enough that the council cannot get a two-thirds majority, then it will be left to the stakeholders en masse to determine the proposal‚Äôs fate. Blacklisting Proposals ‚Äã A proposal can be blacklisted by Root Origin (e.g. sudo). A blacklisted proposal and its related referendum (if any) are immediately canceled . Additionally, a blacklisted proposal's hash cannot re-appear in the proposal queue. Blacklisting is useful when removing erroneous proposals that could be submitted with the same hash. Upon seeing their proposal removed, a submitter who is not properly introduced to the democracy system of Polkadot might be tempted to re-submit the same proposal. That said, this is far from a fool-proof method of preventing invalid proposals from being submitted - a single changed character in a proposal's text will also change the hash of the proposal, rendering the per-hash blacklist invalid. Referenda ‚Äã Referenda are simple, inclusive, stake-based voting schemes. Each referendum has a specific proposal that takes the form of a privileged function call in the runtime. That function includes the most powerful call : set_code , which can switch out the entire runtime code, achieving what would otherwise require a "hard fork". Referenda are discrete events, have a fixed period where voting happens, and then are tallied, and the function call is executed if the vote is approved. Referenda are always binary: your only options in voting are "aye", "nay", or abstaining entirely. Referenda Timeline ‚Äã The structure of the timeline for all referenda is the same regardless of who initiates the proposal, although the timeline length can vary (see below). The figure above provides a summary view of the referenda timeline for Governance V1. In (1), the proposal is submitted, and the Launch Period starts. During this period of indefinite length the voters can endorse proposals by bonding the same amount of tokens used by the depositor. Deposited tokens for endorsement will be returned once the proposal becomes a referendum. During the launch period, the proposal will compete with other proposals, and the one that gets to the top will be selected for a referendum when the next voting period starts. The figure shows that the launch period is shown with a fixed length. Still, it varies depending on who initiated the proposal and how many proposals there are in the pipeline. Council motions will likely have a short launch period when compared to the public referenda which might take longer unless they are the only ones in the pipeline. In (2), the proposal is selected for a referendum. Proposals initiated by the public will become a public referendum , while those initiated by the council will become council referenda . The voting period lasts 28 days, after which, if the proposal is approved, it will go through an enactment period. Rejected proposals will need to start from (1). Note that Governance V1 uses an alternating voting timeline where voters can vote either for a public proposal or a council motion every 28 days. In (3), the proposal is approved and moves through the enactment period that can be of different lengths depending on who initiated the proposal in the first place, with emergency proposals being the fastest ones and the only ones that can be voted simultaneously with other referenda. Public Referenda ‚Äã Public referenda will have a positive turnout bias , meaning that they will require a heavy supermajority of aye votes to pass at low turnouts but as turnout increases towards 100%, it will require a simple majority of aye votes to pass (i.e. 51% wins). Note that the bonded tokens will be released once the proposal is tabled (that is, brought to a vote), and a maximum of public proposals can be in the proposal queue. turnout The total number of voting tokens excluding conviction or voluntary locking . Council Referenda ‚Äã Unanimous Council - When all council members agree on a proposal, it can be moved to a referendum with a negative turnout bias . Briefly, it will require a heavy supermajority of nay votes to reject at low turnouts, but as turnout increases towards 100%, it will require a simple majority of nay votes to fail (i.e. 51% wins). Majority Council - When agreement from only a simple majority of council members occurs, the referendum will need simple majority to pass. Public- vs. Council-initiated Referenda Public referenda must be agreed upon using a positive bias to mitigate attacks by malicious or ill-conceived proposals. Conversely, when a proposal is unanimously voted in favor by the council, it benefits from using the negative bias. We assume low turnout is less problematic if the council proposes a referendum. Also, the council members are elected by the community and have strong technical as well as functional knowledge about the system, and we assume solid justifications back changes proposed by the council. Alternating Voting Timetable ‚Äã All referenda are executed by Root Origin. It follows that multiple referenda cannot be voted upon in the same period, excluding emergency referenda. An emergency referendum occurring at the same time as a regular referendum (either public- or council-proposed) is the only time multiple referenda can be voted on. Every days, a new referendum will come up for a vote, assuming there is at least one proposal in one of the queues. There is a queue for Council-approved proposals and a queue for publicly-submitted proposals. The referendum to be voted upon alternates between the top proposal in the two queues, where the proposals' rank is based on endorsement (i.e. bonded tokens). Adaptive Quorum Biasing ‚Äã Polkadot introduces the concept of Adaptive Quorum Biasing , which is used to alter the effective super-majority required to make it easier or more difficult for a proposal to pass depending on voting power (turnout) and origin (Council or public). Adaptive Quorum Biasing creates three tallying mechanisms: majority carry, super-majority approve, and super-majority against. They all equate to a simple majority-carry system at 100% turnout. Their selection depends on which entity proposed the proposal and whether all Council members voted yes (in the case of Council Referenda). Entity Metric Public Positive Turnout Bias (Super-Majority Approve) Council (Complete agreement) Negative Turnout Bias (Super-Majority Against) Council (Majority agreement) Simple Majority Let's use the image below as an example. If a publicly submitted referendum only has a 25% turnout, the tally of aye votes has to reach 66% for it to pass since we applied Positive Turnout Bias . In contrast, when it has a 75% turnout, the tally of aye votes has to reach 54%, which means that the super-majority required decreases as the turnout increases. A positive turnout bias, whereby a heavy super-majority of aye votes is required to carry at low turnouts. However, as turnout increases towards 100%, it becomes a simple majority carry as below. Where approve is the number of aye votes, against is the number of nay votes, turnout is the total number of voting tokens excluding voluntary locking , and electorate is the total number of tokens issued in the network. When the council proposes a new proposal through unanimous consent, the referendum would be put to the vote using Negative Turnout Bias . Referring to the above image, when a Council referendum only has a 25% turnout, the tally of aye votes has to reach 34% for it to pass, while if the turnout increases to 75%, the tally of aye votes has to reach 46%. A negative turnout bias requires a heavy super-majority of nay votes to reject at low turnouts. However, as turnout increases towards 100%, it becomes a simple majority carry as below. In short, when the turnout rate is low, a super-majority is required to reject the proposal, which means a lower threshold of aye votes must be reached. As turnout increases toward 100%, it becomes a simple majority, a simple comparison of votes. If there are more aye votes than nay , then the proposal is carried, no matter how much stake votes on the proposal. To know more about where these above formulas come from, please read the democracy pallet . Example of Adaptive Quorum Biasing ‚Äã Let's assume we only have 1,500 DOT tokens in total and that this is a public proposal. John: 500 DOT Peter: 100 DOT Lilly: 150 DOT JJ: 150 DOT Ken: 600 DOT John: Votes Yes for a 4 week lock period => 500 x 1 = 500 Votes Peter: Votes Yes for a 4 week lock period => 100 x 1 = 100 Votes JJ: Votes No for a 16 week lock period => 150 x 3 = 450 Votes approve = 600 against = 450 turnout = 750 electorate = 1500 Since the above example is a public referendum, Super-Majority Approve would be used to calculate the result. Super-Majority Approve requires more aye votes to pass the referendum when turnout is low; therefore, based on the above result, the referendum will be rejected. only the winning voter's tokens are locked. If the voters on the losing side of the referendum believe that the outcome will have adverse effects, their tokens are transferrable, so they will not be locked into the decision. Winning proposals are autonomously enacted after the enactment period . Enactment ‚Äã Referenda are considered baked if they are closed and tallied. Assuming a referendum is approved, it will be scheduled for enactment . Referenda are considered unbaked if they are pending an outcome, i.e. being voted on. All referenda are associated with an enactment delay or enactment period . This is the period between a referendum ending and (assuming it was approved) the changes being enacted. For public and Council referenda, the enactment period is a fixed time of . For proposals submitted as part of the enactment of a prior referendum, it can be set as desired. Emergency proposals deal with major problems with the network and need to be "fast-tracked". These will have a shorter enactment period. Voting on a Referendum ‚Äã To vote, a voter generally must lock their tokens up for at least the enactment period beyond the end of the referendum. This is to ensure that some minimal economic buy-in to the result is needed and to dissuade vote selling. Referenda explainer video To learn more about voting on referenda, please check out our technical explainer video . It is possible to vote without locking, but your vote is worth a small fraction of a normal vote, given your stake. At the same time, holding only a small amount of tokens does not mean that the holder cannot influence the referendum result, thanks to time-locking or voluntary locking (see below). Voluntary Locking ‚Äã Polkadot utilizes an idea called voluntary locking that allows token holders to increase their voting power by declaring how long they are willing to lock up their tokens; hence, the number of votes for each token holder will be calculated by the following formula: votes = tokens * conviction_multiplier The conviction multiplier increases the vote multiplier by one every time the number of lock periods double. Lock Periods Vote Multiplier Length in Days 0 0.1 1 1 2 2 4 3 8 4 16 5 32 6 The maximum number of "doublings" of the lock period is set to 6 (and thus 32 lock periods in total), and one lock period equals days. Only doublings are allowed; you cannot lock for 24 periods and increase your conviction by 5.5. For additional information regarding the timeline of governance events, check out the governance section on the Polkadot Parameters page . Staked tokens can be used in governance While a token is locked, you can still use it for voting and staking . You are only prohibited from transferring these tokens to another account. Votes are always "counted" at the same time (at the end of the voting period), no matter for how long the tokens are locked. See below an example that shows how voluntary locking works. Peter: Votes No with 10 DOT for a 128 week lock period => 10 x 6 = 60 Votes Logan: Votes Yes with 20 DOT for a 4 week lock period => 20 x 1 = 20 Votes Kevin: Votes Yes with 15 DOT for a 8 week lock period => 15 x 2 = 30 Votes Even though combined both Logan and Kevin vote with more DOT than Peter, the lock period for both of them is less than Peter, leading to their voting power counting as less. Delegations ‚Äã In Polkadot you can delegate your voting power to another account you trust if you are not willing to stay up-to-date with all referenda. You can also use a governance proxy to vote on behalf of your stash account. The proxy can be yours, or you can authorize a third-party governance proxy to vote with your stash. Learn more from the dedicated page on Proxy Accounts . Council ‚Äã To represent passive stakeholders, Polkadot introduces the idea of a "council". The council is an on-chain entity comprising several actors, each represented as an on-chain account. On Polkadot, the council currently consists of members. Along with controlling the treasury , the council is called upon primarily for three tasks of governance: Proposing sensible referenda Cancelling uncontroversially dangerous or malicious referenda Electing the Technical Committee . For a referendum to be proposed by the council, a strict majority of members must be in favor, with no member exercising a veto. Vetoes may be exercised only once by a member for any single proposal. If the proposal is resubmitted after a cool-down period, they may not veto it a second time. Council motion that pass with a 3/5 (60%) super-majority - but without reaching unanimous support - will move to a public referendum under a neutral, majority-carries voting scheme. In the case that all members of the council that voted are in favor of a motion, the vote is considered unanimous and becomes a referendum with negative turnout bias . Explainer video on the Council For more information, check out our video explainer on Council Prime Members ‚Äã The council, being an instantiation of Substrate's Collective pallet , implements what's called a prime member whose vote acts as the default for other members that fail to vote before the timeout. The prime member is chosen based on a Borda count . The purpose of having a prime council member is to ensure a quorum, even when several members abstain from a vote. Council members might be tempted to vote a "soft rejection" or a "soft approval" by not voting and letting the others vote. The existence of a prime member forces councilors to be explicit in their votes or have their vote counted for whatever is voted on by the prime. Technical Committee ‚Äã The Technical Committee(TC) was introduced in the Kusama rollout and governance post as one of the three chambers of Kusama governance (along with the Council and the Referendum chamber). The TC is composed of the teams that have successfully implemented or specified either a Polkadot runtime or Polkadot Host. Teams are added or removed from the TC via a simple majority vote of the Council . The TC aims to safeguard against malicious referenda, implement bug fixes, reverse faulty runtime updates, or add new but battle-tested features. The TC can fast-track proposals using the Democracy pallet and is the only origin that can trigger the fast-tracking functionality. We can think of the TC as a "unique origin" that cannot generate proposals but fast-track existing ones. Fast-tracked referenda are the only referenda that can be active alongside another active referendum. Thus, with fast-tracked referenda, it is possible to have two active referendums simultaneously. Voting on one does not prevent a user from voting on the other. Frequently Asked Questions ‚Äã How to be a council member? ‚Äã All stakeholders can signal their approval of any of the registered candidates. Council elections are handled by the same Phragm√©n election process that selects validators from the available pool based on nominations. However, token holders' votes for councilors are isolated from any nominations they may have on validators. Council terms last for one week. At the end of each term, Phragm√©n election algorithm runs and the result will choose the new councilors based on the vote configurations of all voters. The election also chooses a set number of runners-up, which is currently 20 that will remain in the queue with their votes intact. As opposed to a "first-past-the-post" electoral system, where voters can only vote for a single candidate from a list, a Phragm√©n election is a more expressive way to include each voter‚Äôs views. Token holders can treat it as a way to support as many candidates as they want. The election algorithm will find a fair subset of the candidates that most closely matches the expressed indications of the electorate as a whole. Let's take a look at the example below. Round 1 Token Holders Candidates A B C D E Peter X X X X Alice X Bob X X X Kelvin X X Total 2 1 3 2 2 The above example shows that candidate C wins the election in round 1, while candidates A, B, D & E keep remaining on the candidates' list for the next round. Round 2 Token Holders Candidates A B D E Peter X X Alice X X Bob X X X X Kelvin X X Total 4 4 1 1 The top-N (say 4 in this example) runners-up can remain, and their votes persist until the next election. After round 2, even though candidates A & B get the same number of votes in this round, candidate A gets elected because after adding the older unused approvals, it is higher than B. How can I appeal to the council to enact a change on my behalf? ‚Äã In some circumstances, you may want to appeal to the on-chain council to enact a change on your behalf. One example of this circumstance is the case of lost or locked funds when the funds were lost due to a human interface error (such as inputting an address for another network). Another example is if you participated in the 2017 Polkadot ICO with a multi-sig address which now does not let you sign a message easily. When these circumstances can be proven beyond a reasonable doubt to be an error, the council may consider a governance motion to correct it. The first step to appeal to the council is to contact the councilors. There is no singular place where you are guaranteed to grab every councilor‚Äôs ear with your message. However, there are a handful of good places to start where you can get the attention of some of them. The Polkadot Direction matrix room is one such place. After creating an account and joining this room, you can post a well-thought-through message here that lays down your case and justifies why you think the council should consider enacting a change to the protocol on your behalf. At some point, you will likely need a place for a longer-form discussion. For this, making a post on Polkassembly is the recommended place to do so. When you write a post on Polkassembly, present all the evidence for your circumstances and state clearly what kind of change you would suggest to the councilors to enact. info Remember, the councilors do not need to make the change, it is your responsibility to make a strong case for why the change should be made. Resources ‚Äã Initial Governance Description Democracy Pallet Governance Demo - Dr. Gavin Wood presents the initial governance structure for Polkadot. (Video) Governance on Polkadot - A webinar explaining how governance works in Polkadot and Kusama. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Archive Next Gov1 Treasury Governance Summary Proposals Endorsing Proposals Cancelling Proposals Blacklisting Proposals Referenda Referenda Timeline Public Referenda Council Referenda Alternating Voting Timetable Adaptive Quorum Biasing Enactment Voting on a Referendum Voluntary Locking Delegations Council Prime Members Technical Committee Frequently Asked Questions How to be a council member? How can I appeal to the council to enact a change on my behalf? Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Governance v1 Treasury General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Governance V1 Gov1 Treasury Polkadot Launch Phases Redenomination of DOT Controller Accounts Build Maintain Learn Archive Gov1 Treasury On this page Governance v1 Treasury Use OpenGov to access treasury funds Governance v1 is deprecated. To access Polkadot treasury funds use OpenGov . For more information about OpenGov Treasury see the dedicated wiki page . The Treasury is a pot of funds collected through a portion of block production rewards, transaction fees, slashing, staking inefficiencies , etc. The Treasury funds are held in a system account not accessible by anyone; only the system internal logic can access it. Funds can be spent by making a spending proposal that, if approved by the Council , will enter a waiting period before distribution. This waiting period is known as the spend period , and its duration is subject to governance , with the current default set to days. The Treasury attempts to spend as many proposals in the queue as it can without running out of funds. Treasury payout is an automatic process: If the Treasury funds run out with approved proposals left to fund, those proposals are kept in the approved queue, and will receive funding in the following spend period. If the Treasury ends a spend period without spending all of its funds, it suffers a burn of a percentage of its funds - thereby causing deflationary pressure. This encourages the spending of the funds in the Treasury by Polkadot's governance system. This percentage is currently at 1% on Polkadot. When a stakeholder wishes to propose a spend from the Treasury, they must reserve a deposit of at least 5% of the proposed spend (see below for variations). This deposit will be slashed if the proposal is rejected, and returned if it is accepted. Proposals may consist of (but are not limited to): Infrastructure deployment and continued operation. Network security operations (monitoring services, continuous auditing). Ecosystem provisions (collaborations with friendly chains). Marketing activities (advertising, paid features, collaborations). Community events and outreach (meetups, pizza parties, hackerspaces). Software development (wallets and wallet integration, clients and client upgrades). The Council governs the Treasury and how the funds are spent is up to their judgment. caution The Council does not approve or deny Treasury Proposals based on the available funds. Proposals are not approved just because there are funds ready to spend but are subject to a burn. Funding the Treasury ‚Äã The Treasury is funded from different sources: Slashing: When a validator is slashed for any reason, the slashed amount is sent to the Treasury with a reward going to the entity that reported the validator (another validator). The reward is taken from the slash amount and varies per offence and number of reporters. Transaction fees: A portion of each block's transaction fees goes to the Treasury, with the remainder going to the block author. Staking inefficiency: Inflation is designed to be 10% in the first year, and the ideal staking ratio is set at 50%, meaning half of all tokens should be locked in staking. Any deviation from this ratio will cause a proportional amount of the inflation to go to the Treasury. In other words, if 50% of all tokens are staked, then 100% of the inflation goes to the validators as reward. If the staking rate is greater than or less than 50%, then the validators will receive less, with the remainder going to the Treasury. Parathreads: Parathreads participate in a per-block auction for block inclusion. Part of this bid goes to the validator that accepts the block and the remainder goes to the Treasury. Tipping ‚Äã Next to the proposals process, a separate system for making tips exists for the Treasury. Tips can be suggested by anyone and are supported by members of the Council. Tips do not have any definite value, and the final value of the tip is decided based on the median of all tips issued by the tippers. Currently, the tippers are the same as the members of the Council. However, being a tipper is not the direct responsibility of the Council, and at some point the Council and the tippers may be different groups of accounts. A tip will enter a closing phase when more than a half plus one of the tipping group have endorsed a tip. During that time frame, the other members of the tipping group can still issue their tips, but do not have to. Once the window closes, anyone can call the close_tip extrinsic, and the tip will be paid out. There are two types of tips: public: A small bond is required to place them. This bond depends on the tip message length, and a fixed bond constant defined on chain, currently . Public tips carry a finder's fee of %, which is paid out from the total amount. tipper-initiated: Tips that a Council member published, do not have a finder's fee or a bond. info For information about how to submit a tip from the Treasury you can read this support article . To better understand the process a tip goes through until it is paid out, let's consider the example below. Example ‚Äã Bob has done something great for Polkadot. Alice has noticed this and decides to report Bob as deserving a tip from the Treasury. The Council is composed of three members Charlie, Dave, and Eve. Alice begins the process by issuing the report_awesome extrinsic. This extrinsic requires two arguments, a reason and the beneficiary. Alice submits Bob's address with the reason being a UTF-8 encoded URL to a post on Polkassembly that explains her reasoning for why Bob deserves the tip. As mentioned above, Alice must also lock up a deposit for making this report. The deposit is the base deposit as set in the chain's parameter list, plus the additional deposit per byte contained in the reason. This is why Alice submitted a URL as the reason instead of the explanation directly: it was cheaper for her to do so. For her trouble, Alice is able to claim the eventual finder's fee if the tip is approved by the tippers. Since the tipper group is the same as the Council, the Council must now collectively (but also independently) decide on the value of the tip that Bob deserves. Charlie, Dave, and Eve all review the report and make tips according to their personal valuation of the benefit Bob has provided to Polkadot. Charlie tips 10 DOT, Dave tips 30 DOT, and Eve tips 100 DOT. The tip could have been closed out with only two of the three tippers. Once more than half of the tippers group have issued tip valuations, the countdown to close the tip will begin. In this case, the third tipper issued their tip before the end of the closing period, so all three were able to make their tip valuations known. The actual tip that will be paid out to Bob is the median of these tips, so Bob will be paid out 30 DOT from the Treasury. In order for Bob to be paid his tip, some account must call the close_tip extrinsic at the end of the closing period for the tip. This extrinsic may be called by anyone. Bounties Spending ‚Äã There are practical limits to Council Members curation capabilities when it comes to treasury proposals: Council members likely do not have the expertise to make a proper assessment of the activities described in all proposals. Even if individual Councillors have that expertise, it is highly unlikely that a majority of members are capable in such diverse topics. Bounties Spending proposals aim to delegate the curation activity of spending proposals to experts called Curators: They can be defined as addresses with agency over a portion of the Treasury with the goal of fixing a bug or vulnerability, developing a strategy, or monitoring a set of tasks related to a specific topic: all for the benefit of the Polkadot ecosystem. A proposer can submit a bounty proposal for the Council to pass, with a curator to be defined later, whose background and expertise is such that they are capable of determining when the task is complete. Curators are selected by the Council after the bounty proposal passes, and need to add an upfront payment to take the position. This deposit can be used to punish them if they act maliciously. However, if they are successful in their task of getting someone to complete the bounty work, they will receive their deposit back and part of the bounty reward. When submitting the value of the bounty, the proposer includes a reward for curators willing to invest their time and expertise in the task: this amount is included in the total value of the bounty. In this sense, the curator's fee can be defined as the result of subtracting the value paid to the bounty rewardee from the total value of the bounty. In general terms, curators are expected to have a well-balanced track record related to the issues the bounty tries to resolve: they should be at least knowledgeable on the topics the bounty touches, and show project management skills or experience. These recommendations ensure an effective use of the mechanism. A Bounty Spending is a reward for a specified body of work - or specified set of objectives - that needs to be executed for a predefined treasury amount to be paid out. The responsibility of assigning a payout address once the specified set of objectives is completed is delegated to the curator. After the Council has activated a bounty, it delegates the work that requires expertise to the curator who gets to close the active bounty. Closing the active bounty enacts a delayed payout to the payout address and a payout of the curator fee. The delay phase allows the Council to act if any issues arise. To minimize storage on chain in the same way as any proposal, bounties don't contain contextual information. When a user submits a bounty spending proposal, they will probably need to find an off-chain way to explain the proposal (any of the available community forums serve this purpose). This template can help as a checklist of all needed information for the Council to make an informed decision. The bounty has a predetermined duration of 90 days with the possibility of being extended by the curator. Aiming to maintain flexibility on the tasks‚Äô curation, the curator will be able to create sub-bounties for more granularity and allocation in the next iteration of the mechanism. Creating a Bounty Proposal ‚Äã Anyone can create a Bounty proposal using Polkadot-JS Apps: Users are able to submit a proposal on the dedicated Bounty section under Governance. The development of a robust user interface to view and manage bounties in the Polkadot Apps is still under development and it will serve Council members, Curators and Beneficiaries of the bounties, as well as all users observing the on-chain treasury governance. For now, the help of a Councillor is needed to open a bounty proposal as a motion to be voted. To submit a bounty, please visit Polkadot-JS Apps and click on the governance tab in the options bar on the top of the site. After, click on 'Bounties' and find the button '+ Add Bounty' on the upper-right side of the interface. Complete the bounty title, the requested allocation (including curator's fee) and confirm the call. After this, a Council member will need to assist you to pass the bounty proposal for vote as a motion. You can contact the Council by joining the Polkadot Direction channel in Element or joining our Polkadot Discord server and publishing a short description of your bounty, with a link to one of the forums for contextual information. A bounty can be cancelled by deleting the earmark for a specific treasury amount or be closed if the tasks have been completed. On the opposite side, the 90 days life of a bounty can be extended by amending the expiry block number of the bounty to stay active. Closing a bounty ‚Äã The curator can close the bounty once they approve the completion of its tasks. The curator should make sure to set up the payout address on the active bounty beforehand. Closing the Active bounty enacts a delayed payout to the payout address and a payout of the curator fee. A bounty can be closed by using the extrinsics tab and selecting the Treasury pallet, then Award_bounty , making sure the right bounty is to be closed and finally sign the transaction. It is important to note that those who received a reward after the bounty is completed, must claim the specific amount of the payout from the payout address, by calling Claim_bounty after the curator closed the allocation. To understand more about Bounties and how this new mechanism works, read this Polkadot Blog post . FAQ ‚Äã What prevents the Treasury from being captured by a majority of the Council? ‚Äã The majority of the Council can decide the outcome of a treasury spend proposal. In an adversarial mindset, we may consider the possibility that the Council may at some point go rogue and attempt to steal all of the treasury funds. It is a possibility that the treasury pot becomes so great, that a large financial incentive would present itself. For one, the Treasury has deflationary pressure due to the burn that is suffered every spend period. The burn aims to incentivize the complete spend of all treasury funds at every burn period, so ideally the treasury pot doesn't have time to accumulate mass amounts of wealth. However, it is the case that the burn on the Treasury could be so little that it does not matter - as is the case currently on Kusama with a 0.2% burn. However, it is the case on Kusama that the Council is composed of mainly well-known members of the community. Remember, the Council is voted in by the token holders, so they must do some campaigning or otherwise be recognized to earn votes. In the scenario of an attack, the Council members would lose their social credibility. Furthermore, members of the Council are usually externally motivated by the proper operation of the chain. This external motivation is either because they run businesses that depend on the chain, or they have direct financial gain (through their holdings) of the token value remaining steady. Concretely, there are a couple on-chain methods that resist this kind of attack. One, the Council majority may not be the token majority of the chain. This means that the token majority could vote to replace the Council if they attempted this attack - or even reverse the treasury spend. They would do this through a normal referendum. Two, there are time delays to treasury spends. They are only enacted every spend period. This means that there will be some time to observe this attack is taking place. The time delay then allows chain participants time to respond. The response may take the form of governance measures or - in the most extreme cases a liquidation of their holdings and a migration to a minority fork. However, the possibility of this scenario is quite low. Further Reading ‚Äã Substrate's Treasury Pallet Documentation of the Rust implementation of the Treasury Edit this page Last updated on Jul 7, 2023 by Radha Previous Governance V1 Next Polkadot Launch Phases Funding the Treasury Tipping Example Bounties Spending Creating a Bounty Proposal Closing a bounty FAQ What prevents the Treasury from being captured by a majority of the Council? Further Reading ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Launch Phases General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Governance V1 Gov1 Treasury Polkadot Launch Phases Redenomination of DOT Controller Accounts Build Maintain Learn Archive Polkadot Launch Phases On this page Polkadot Launch Phases The Polkadot network has a phased roll-out plan, with important milestones toward decentralization marking each phase. Keep up-to-date with the Polkadot's phased roll-out plan at by viewing the roadmap Current Phase: Post-launch Upgrades Claims For the most update-to-date information on DOT claims (if you bought your DOTs before Polkadot went live), check out the following claiming resources and tutorials: Why do I need to claim my DOT tokens, and is there a deadline? I claimed my DOT before Polkadot went live, but still see zero balance! How to claim your DOT - Tutorial How do I know my claim worked? The PoA Launch ‚Äã The Genesis block of the Polkadot network was launched on May 26, 2020, as a Proof of Authority (PoA) network. Governance was restricted to the single Sudo (super-user) key, which was held by Web3 Foundation to issue the commands and upgrades necessary to complete the launch process. During this time, validators started joining the network and signaling their intention to participate in consensus. Nominated Proof of Stake ‚Äã Once Web3 Foundation was confident in the stability of the network and there was a sufficient number of validator intentions, Web3 Foundation used Sudo ‚Äî a superuser account with access to governance functions ‚Äî to initiate the first validator election. Following this election, the network transitioned from PoA into its second phase, Nominated Proof of Stake (NPoS) , on June 18, 2020. Governance ‚Äã After the chain had been running well with the validator set, the Sudo key issued a runtime upgrade that enabled the suite of governance modules in Polkadot; namely, the modules to enable a Council , a Technical Committee , and public referenda . Removal of Sudo ‚Äã The Sudo module was removed by a runtime upgrade on July 20, 2020, transitioning the governance of the chain into the hands of the token (DOT) holders. From this point, the network has been entirely in the hands of the token holders and is no longer under control of any centralized authority. Balance Transfers ‚Äã To enable balance transfers, the community made a public proposal for a runtime upgrade that lifted the restriction on balance transfers. Transfer functionality was subsequently enabled on Polkadot at block number 1_205_128 on August 18, 2020, at 16:39 UTC. Core Functionality ‚Äã After five years of research and development and a multi-stage launch that began in May 2020, Polkadot launch was completed on December 18, 2021, with all auction-winning parachains producing blocks on the network. Check out these resources for further information: Polkadot Network blog . Polkadot A to Z: L for Polkadot Launch . Polkadot 2.0 ‚Äã Researchers are proposing the next version of the Polkadot network. With many questions yet to be answered, as of now, some big areas of research will be in: Economics and Networking (Zero-Knowledge): How will scalability work in Polkadot 2.0? Horizontal vs. Vertical scalability: What is the breaking point of the maximum number of parachains built with horizontal scalability? Nested Relay Chain: How can multiple Relay Chains exist connected through parachains? How many tiers of Relay Chains can be nested? How will validators work together to validate blocks on various Relay Chains? How does XCM work in the nested setup? How is AnV going to work there? Edit this page Last updated on Jun 30, 2023 by Filippo Previous Gov1 Treasury Next Redenomination of DOT The PoA Launch Nominated Proof of Stake Governance Removal of Sudo Balance Transfers Core Functionality Polkadot 2.0 ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Redenomination of DOT General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Governance V1 Gov1 Treasury Polkadot Launch Phases Redenomination of DOT Controller Accounts Build Maintain Learn Archive Redenomination of DOT On this page Redenomination of DOT On August 21, 2020, the redenomination of DOT, the native token on Polkadot, occurred. From this date, one DOT (old) equals 100 new DOT. Denomination Day The DOT redenomination took place on 21 August 2020, known as Denomination Day, at block number 1_248_328. While DOT is the unit of currency on Polkadot that most people use when interacting with the system, the smallest unit of account is the Planck. A Planck's relation to DOT is like the relation of a Satoshi to Bitcoin. Before 21 August, the DOT was denominated as 1e12 Plancks, that is, twelve decimal places. After Denomination Day, DOT is denominated as 1e10 Plancks, as in, ten decimal places. DOT denominated to twelve decimal places is referred to as "DOT (old)" and DOT denominated to ten decimal places is generally referred to as "DOT". When the difference must be made explicit, the current ten-decimal-denominated DOT is referred to as " New DOT ". Redenomination Explainer Check out our technical explainer video that explains more of Redenomination . The change in denomination, henceforth referred to as the redenomination , was voted on by the community of DOT holders. The community decided between four options, to change the DOT denomination by a factor of ten, one hundred, one thousand, or not at all. The end result was to change the denomination by a factor of one hundred. The overall effect of this change was that the number of Polkadot's smallest unit, the Planck, remained constant, while the DOT balance for all holders was increased by a factor of one hundred. As one can see from the example below, the number of Plancks a user has does not change, only the number of Plancks that constitute a single DOT. A user with 1_000_000_000_000 Plancks still has the same number of Plancks but will have 100 DOT under the new denomination, as opposed to one DOT under the old denomination. Before the change the decimal was here v 1.000000000000 DOT 100.0000000000 DOT ^ After the change the decimal is here note There are no state changes with redenomination. There are no transfers. The real change regards the social consensus around where to put the decimal place when we talk about what constitutes a DOT. Origins ‚Äã The initial vote for redenomination occurred as a referendum on the Kusama blockchain. The referendum was summarized as having four effects if approved by KSM holders. Referendum Summary The total allocations of DOT will increase one hundred times from 10 million to 1 billion. DOT allocation balances will increase by a factor of one hundred, such that 1 DOT will be 100 DOT. The distribution of DOT does not change, and holders of DOT still own an equal share of the network as before the change. The precision of DOT will change from 12 decimal places to 10 decimal places. The main benefit of this change is to avoid using small decimals when dealing with DOT and to achieve an easier calculation system. The initial referendum was proposed before the Polkadot genesis block, assuming that making a redenomination would be simpler before the Polkadot chain was live. However, many in the community pointed out the disconnect between the two networks and how it was unfair for holders of DOT to be impacted by a vote by a different token holder set. For this reason, Web3 Foundation decided to make a new vote on Polkadot when it went live, although the Kusama vote ended with a majority in favor of the redenomination change. Web3 Foundation summarized the decision not to change: note However, given the non-negligible amount of opposition, including from some within the ranks of Web3 Foundation and Parity, the Foundation decided that we cannot, in good faith, sponsor the redenomination. The Vote ‚Äã After the genesis block of Polkadot was created and the network was running with a decentralized community of validators securing the network, Web3 Foundation decided to put the redenomination topic up for a vote again. This time, the vote was explicitly binding ‚Äî meaning that it would be executed if voted through. In comparison, the vote on Kusama was non-binding to capture a signal without a direct way to affect the Polkadot chain. Based on the feedback received during the Kusama referendum, the Polkadot vote was held as an approval vote, with four available options. DOT holders could issue votes for any configuration of the four options: no change, a change of 10x, a change of 100x, or a change of 1000x. The voting logic was contained in a specially-built Substrate pallet included in Polkadot's runtime for this poll. Summary of the Vote Any combination of the four options may have been approved by the voter. There was no need to select only one option. Approving all or none of the options was equivalent and did not affect the outcome. All voters could alter their votes any number of times before the close of the poll. No discretionary lock-voting was in place; all DOT used to vote counts the same. Voting was made on a per-account basis; a single account must have voted the same way and could not split its vote. This vote did not affect any economics of the Polkadot platform. As in, staking rewards, inflation, effective market capitalization, and the underlying balances of every account remained completely unchanged. It was ‚Äúmerely‚Äù about what units the network uses to denominate the balances into ‚ÄúDOT‚Äù. With a voting period of two weeks set, the redenomination was now in the hands of the Polkadot community for a final, binding decision. The Outcome ‚Äã After two weeks of voting, the results of the redenomination vote were tallied. About one-third of the total DOT in the network participated in the vote. The redenomination proposal passed with 86% of the voters favoring a 100x factor increase (or two decimal places of precision loss). Polkadot's redenomination then took place on 21 August, now known as Denomination Day, at block #1_248_328. What This Means for the Community ‚Äã If you are a DOT holder or user of the network, then you do not need to take any action. The DOT redenomination was a purely front-end change. You still hold the same amount of Plancks after the change, but now it will appear that you hold 100x more DOT. This change applies proportionally to every account. What This Means for Builders of Tools ‚Äã If you are the builder of a tool that consumes the @polkadot/api package ‚Äî then there should be no real changes to be made in your application. The denomination is technically a cosmetic change, and every value remains a constant amount of Plancks. However ‚Äî if you are a builder of a tool that displays DOT balances to users (e.g. a wallet) or handles DOT balances in an off-chain or custodial way, then you will need to ensure that you display the correct denomination of DOT to users. Please see our Ecosystem Redenomination Guide for recommendations. Please reach out to [email protected] if you need any assistance in making sure your software is compatible with the redenomination. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Polkadot Launch Phases Next Controller Accounts Origins The Vote The Outcome What This Means for the Community What This Means for Builders of Tools ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Controller Accounts General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Basics Advanced Architecture Polkadot Comparisons Videos Future Implementations Archive Governance V1 Gov1 Treasury Polkadot Launch Phases Redenomination of DOT Controller Accounts Build Maintain Learn Archive Controller Accounts On this page Controller Accounts Controller accounts were used for staking and were a "less-powerful" version of staking proxies. Controllers could only sign for unbonding and rebonding funds, nominating and changing the reward destination. The stash account was still used to bond more funds and change the controller. Controller accounts became redundant and added unnecessary complexity to the staking mechanics. With the setup shown above, the stash account was not entirely isolated. More complicated designs to fully isolate the stash account included having both controller and staking proxies (see below). Stash as Controller ‚Äã It was unnecessary to have a controller if you had a staking proxy. In this case the stash was also set to be the controller, and the account security would not have been compromised. The staking proxy was used to sign all staking-relate transactions. Note that you needed to sign with the stash to change the staking proxy. This past situation was similar to the present setup, except that now there is no option to set the stash as controller and that the action of "changing the controller" is missing. From a practical perspective, we need to use only one account and remember one password to sign for all staking-related transactions. From a security perspective, who controls the staking proxy controls our staking actions. Stash not as Controller ‚Äã If the stash and controller were different accounts, the staking proxy was used to bond more funds and change the controller. Thus the staking proxy was used to sign for those transactions that were used less often and usually signed by the stash. From a practical perspective, there were two accounts, and we needed to remember two passwords. From a security perspective, the party who wanted to control our staking actions was required to control two accounts. Edit this page Last updated on Jul 14, 2023 by Filippo Previous Redenomination of DOT Next Builder's Guides Stash as Controller Stash not as Controller ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Builder's Guides General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Integration Guide Tools Resources Maintain Build Builder's Guides On this page Builder's Guides Welcome to the builder's section of the Polkadot Wiki. Here, you will discover many development tools and resources in the Polkadot ecosystem. We are always adding new tools and frameworks as we learn about them so if you are working on something that should be included please reach out to us on Element . This section of the wiki is divided into the following parts: Development Guide ‚Äã Starter's Guide - High level overview on choosing a parachain, parathread, or smart contract for your project and how to get started. Parachain Development - Tools for building parachains and parathreads. Smart Contracts - Tools to deploy contracts to a Polkadot parachain. Tools & Resources ‚Äã Tools - Maintained list of tools. Subkey - Command line utility for generating and inspecting key pairs. JS tools - TypeScript tools for offline signing of transactions, RPC calls, and more. PSP GitHub Repository - Polkadot Standards Proposals (PSPs) are standards for the Polkadot ecosystem. Examples - Example projects sourced from the community -- see what others are building. Grants ‚Äã Grants - Information regarding grants and funding sources available in the Polkadot ecosystem. PSPs ‚Äã Polkadot Standards Proposals (PSPs) are standards for the Polkadot ecosystem. PSP GitHub Repository - Read, create, or discuss standards and proposals. Hackathon ‚Äã Resources For Running a Hackathon Edit this page Last updated on Dec 16, 2022 by Keith Alfaro Previous Controller Accounts Next Builders Starter's Guide Development Guide Tools & Resources Grants PSPs Hackathon ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Builders Starter's Guide General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Builders Starter's Guide Parachain Development Decentralized Storage Smart Contracts Oracles Data Substrate Connect SS58 Registry Opening HRMP Channels Integration Guide Tools Resources Maintain Build Development Guide Builders Starter's Guide On this page Builders Starter's Guide Polkadot is a blockchain protocol with two goals: providing shared security among all connected parachains and allowing all connected chains to interoperate by using XCM . With the advent of PDKs like Substrate and Cumulus , the time it takes to develop and launch a new chain has dropped significantly. While before, it would take years to launch a new chain, now it may only take weeks or even days, depending on your goals. This guide will walk you through the steps you can take today to get started building your vision with Polkadot. It will explain the difference between a parachain and a smart contract in deciding which best fits your architectural needs. This build guide provides four different tracks: Building Parachains Building a Pallet Developing Smart Contracts Developing a dApp info Keep in mind that these tracks are merely suggestive, and there are many ways to utilize Polkadot, Substrate, and their various developmental components. For more inspiration, look at the open source projects featured here in the wiki! Before diving into any one of these tracks, it is encouraged to read about Polkadot and its networks in order to gain context about the application you could make. Development Ecosystem Overview ‚Äã Before diving into the various paths, one can take in developing on Polkadot, it's important to realize and know key terms that make up the following sections. Even before considering what kind of application you want to build, it's prudent to understand what Polkadot is and how relates to it, and how what each developmental component can do for you within the Polkadot ecosystem. Polkadot Ecosystem Networks ‚Äã Mainnet: Polkadot Canary network: Kusama Kusama is a value-bearing canary network that gets features before Polkadot does. Expect Chaos . Official testnets: Westend - Functionality equal to the current Polkadot mainnet, with possible next-generation testing of features from time to time that will eventually migrate onto Polkadot. Perma-testnet (is not reset back to genesis block). Canvas - Wasm-based Smart Contract enabled testnet, primarily for ink! development. Rococo - Parachains and XCM testnet. Occasionally reset (started over again with a new genesis block). Polkadot mainnet has been running since May 2020 and has implementations in various programming languages ranging from Rust to JavaScript. The leading implementation is built in Rust and uses the Substrate framework. Tooling is rapidly evolving to interact with the network; there are many ways to get started! But before you jump head-first into the code, you should consider the kind of decentralized application you want to make and understand the different paradigms available to developers who want to build on Polkadot. Building Parachains ‚Äã Polkadot is canonically referred to as the Relay Chain. It is also considered a layer zero protocol, as it enables the interoperability and shared security of multiple parachains , which are layer one protocols. Parachains connect to a relay chain using the Parachains Protocol . info Throughout this document, you will encounter the term runtime or STF (State Transition Function) . Both refer to the same concept, as they define how exactly a particular system, i.e., a blockchain, should deal with state changes externally and internally. Both of these terms are used extensively in Polkadot, and by association, Substrate contexts. Parachains open possibilities to construct complex runtime , or STF (state transition function) the logic that would be too expensive to execute with smart contracts. However, unlike smart contracts, parachains lack a mandatory gas metering system entirely and could potentially be vulnerable to bugs that cause infinite loops (something that is prevented by design in smart contracts). This vulnerability is mitigated by the weight system that is implemented in Substrate -- although it places more of a burden on the developer of the parachain to perform properly benchmarks. What is a parathread? Parathreads, or "on-demand parachains," are like parachains and enable the developer to have lower-level control of the logic of their application. The main difference between the two is economical since parathreads will be much less expensive to secure than a parachain and is a "pay-as-you-go" model. The lower costs of parathreads are because parathreads will only produce a block when needed, unlike parachains, which have secured a slot to produce a block at every block of the Relay Chain. When building a parathread, you will use the same tools (like PDKs) and get all the benefits of building a parachain without the cost drawback. Constructing FRAME Runtimes with Substrate ‚Äã Polkadot is built using the Substrate framework. Substrate is a highly configurable and dynamic framework for building blockchains. At a lower level, Substrate provides a set of tools and libraries ranging from block production, finality gadgets, to peer-to-peer networking . Both Polkadot and Kusama, as well as most parachains, are built using Substrate. In essence, Substrate can break down a blockchain's development process by providing crucial building blocks of functionality, removing the need for re-engineering complex mechanisms that usually involved when developing a blockchain. Substrate can be used as a basis for a parachain to connect to a relay chain like Polkadot or Kusama, or even as a basis to form a conventional layer one solo chain. Currently, the most streamlined way of utilizing Substrate is FRAME , which conveniently allows for a runtime/STF to be generated from a set of modules (called pallets ). Runtimes in Substrate are built using WebAssembly (Wasm), and represent the state transition function for a network. FRAME allows for a collection of business logic-oriented modules, called pallets , to construct a runtime/STF and define how exactly the blockchain is supposed to behave. Ranging from identity to smart contracts , pallets can be quite extensive in providing on-chain functionality. Even though FRAME is heavily used, it is not the only way to create a valid runtime/STF using Substrate. Substrate can be used to create new paradigms and abstractions that build on the concept of web3. note Although most parachains utilize FRAME and Substrate to build runtime/STFs for connecting to the relay chain, it is not contingent. Building a parachain using other tools is possible, so long as they follow the Parachains Protocol . As a general rule of thumb, Substrate provides the means for this to become possible through comparably minimal effort. Building Parachains with Cumulus ‚Äã Diving further into building parachains, Cumulus is another set of tools that aid in building a parachain-ready blockchain for Polkadot or Kusama. Cumulus utilizes FRAME and Substrate to create an easy way to build your first parachain. It ensures the chain follows the Parachain Protocol. info Cumulus clouds are shaped sort of like dots; together, they form a system that is intricate, beautiful, and functional. For most developers, the best place to start is to get familiar with Substrate independently, followed by FRAME, with Cumulus as the final step to understanding the entire parachain building process. This way, one can view how various paradigms are applied and make decisions on integrating or utilizing Substrate for their particular use case. Please see the parachain development guide for how to get started on building a parachain or parathread. Parachains Benefits ‚Äã Parachains contain their own runtime/STF logic and benefit from the shared security and the cross-chain messaging provided by the Polkadot relay chain. Parachains permit high flexibility and customization but require more effort to create and maintain over time. A production-grade parachain is typically more involved to create due to the complexity involved in blockchain networks' technical and economic aspects. Parachains grant the creators more space to build the monetary system and other chain aspects from the ground up. They will allow for a more concise and efficient execution of complex logic than a smart contract platform could offer. Parachains also offer more flexibility in the form of governance and can perform complete upgrades in a less controversial way than the current process of hard forks. Some examples of features you can have on a parachain or parathread: Custom fee structure (for example, pay a flat transaction fee or pay per byte). Shared security and finalization via the relay chain (Polkadot or Kusama). Custom monetary policy for the native token and local economy. Treasury to be funded through transitions in your state function. A governance mechanism that could manage a DAO that is responsible for allocating your on-chain treasury. Building a Pallet ‚Äã While parachains are highly customizable, they are often complex to develop. If you wish to get familiar with FRAME and Substrate, a good place to start is by building a pallet in a development environment. A pallet is a fully customizable module that allows you to implement layer one logic with relatively minimal development time on a basic level while still allowing the possibility of building advanced functionality into your custom chain. To learn how to build a pallet, follow the Substrate documentation . Developing Smart Contracts ‚Äã Smart contracts are another option that enables for an often simpler developer experience. Below is a quick comparison of how building a smart contract compares to building a parachain: Parachains Smart Contracts Speed of Development - + Ease of Deployment - + Complexity of logic + - Maintenance overhead - + Level of customization + - Strict resource control - + Native chain features + - Scalability + - What's the difference between a smart contract and a pallet? If you recall, a parachain comprises a runtime/STF usually built on Substrate. These runtime/STFs often utilize FRAME, which is subsequently made of pallets. Pallets are part of a Substrate runtime/STF, whereas smart contracts are a product of a pallet (see: pallet_contracts ). Pallets require more engineering and thought, as they can directly affect the chain's state. ink! and EVM-based Smart Contracts ‚Äã At a high level, a smart contract is simply some code that exists at an address on a chain and is callable by external actors. Whether it's EVM-based , or written using ink! , smart contracts sandboxed, executable programs that live on-chain. note The Polkadot relay chain does not support smart contracts. However, several parachains do. See the smart contracts guide for more information about the environments provided. A popular choice for smart contracts is ink! . Other parachains that offer EVM-based contracts written in Solidity alongside ink! are also available . Because smart contracts exist on a single chain at a time, they can have smooth interoperability with other smart contracts on the same chain. However, they will always be constrained and limited by the inherent characteristics of their host chain. As a developer, you will need to consider the storage and complexity of your smart contract to ensure that gas usage stays within reasonable bounds. Consider using the listed options on the decentralized storage page to keep the data and submit only the content address on the chain. Building a smart contract Please see the smart contracts guide for how to get started on building a smart contract. Developing a dApp/uApp ‚Äã If one aims to develop a dApp (Decentralized App) or uApp (Unstoppable App), the Polkadot ecosystem contains various SDKs to tap into the relay chain and parachains. For front-end applications, several options exist for interfacing with Substrate-based chains (parachains, relay chains, etc.) and smart contracts. These often will interact with the RPC of a Substrate node: Polkadot.js Subxt CAPI React Hooks for ink! Promise and RxJS APIs around Polkadot and Substrate based chains via RPC calls. It is dynamically generated based on what the Substrate runtime provides in terms of metadata. Full documentation & examples available. Capi is a framework (based on Typescript) for crafting interactions with Substrate chains. It consists of a development server and fluent API, which facilitates multichain interactions without compromising either performance or ease of use. Submit extrinsics (transactions) to a Substrate node via RPC using Rust. Also referred to as Rust Parity. React hooks for ink! smart contracts For a full list of tools please take a look here: Tools, APIs and Languages Edit this page Last updated on Jun 30, 2023 by bader y Previous Builder's Guides Next Parachain Development Development Ecosystem Overview Polkadot Ecosystem Networks Building Parachains Constructing FRAME Runtimes with Substrate Building a Pallet Developing Smart Contracts ink! and EVM-based Smart Contracts Developing a dApp/uApp ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Parachain Development General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Builders Starter's Guide Parachain Development Decentralized Storage Smart Contracts Oracles Data Substrate Connect SS58 Registry Opening HRMP Channels Integration Guide Tools Resources Maintain Build Development Guide Parachain Development On this page Parachain Development Your Go-To Overview for Developing a Parachain ‚Äã This guide will cover the motivation to build a parachain or parathread, the tools available to facilitate this, the steps to test, and finally, how to launch your network on Polkadot . Why Create a Parachain? ‚Äã Parachains are connected to and secured by the Relay Chain. They benefit from the pooled security , thought-through governance , and overall scalability of the heterogeneous sharding approach of the network. Creating a parachain can be seen as creating a Layer-1 blockchain , which has its own logic and runs in parallel within the Polkadot ecosystem. Developers can focus on creating state-of-the-art chains that take advantage of Polkadot's next-generation approach. Some examples of what a parachain could be are: DeFi (Decentralized Finance) Applications Digital Wallets IoT (Internet of Things) Applications Gaming Web 3.0 Infrastructure and more. Polkadot aims to be a bet against blockchain maximalism, where the success of Polkadot's heterogeneous multi-chain approach will play a key part in the overall advancement of Web 3.0 and decentralized systems. As a result, Polkadot's parachain model was designed with the belief that the internet of the future will have many different types of blockchains working together. What are the Benefits of Deploying a Parachain? ‚Äã The parachain model attempts to alleviate five key build failures of present technology stacks, as described in the Polkadot Whitepaper : Scalability : How much is spent on resources and will the network be subject to bottlenecks? Isolatability : Are the needs of many accounted for under the same framework? Developability : Is the system tooling, system support, and overall system integrity dependable? Governance : Can the network remain flexible to evolve and adapt over time? Can decisions be made with sufficient inclusivity, legitimacy, and transparency to provide effective leadership of a decentralised system? Applicability : Does the technology address a burning need on its own? Is other ‚Äúmiddleware‚Äù required to bridge the gap to actual applications? Shared Security (Pooled Security) ‚Äã Parachains can lease the security of the Polkadot network by bonding DOT for a parachain slot. This means that the social costs of building a community around your project and convincing validators to participate in your network security are reduced. Polkadot has strong security, and decentralised application projects wishing to benefit from this security would want to become a parachain to share in that pooled security. On-Chain Governance (Thought-through Governance) ‚Äã Most governance systems in blockchains use an off-chain governance mechanism. Polkadot's on-chain governance encourages maximum participation of token holders and is frictionless and transparent. It also enables forkless upgrades . Scalability ‚Äã The sharded multichain network approach allows for what is essentially parallel computation (processing power) that can process several transactions in parallel. Isolated blockchains are often faced with the network constraint of processing transactions in sequence, causing bottlenecks. Interoperability ‚Äã Any decentralised application or chain that wants to enable trustless messaging to other parachains already connected to Polkadot would want to become a parachain. Interoperability between sovereign chains involves certain constraints and complex protocols to enable across a wide breadth of chains. With Polkadot, you will get this feature out of the box if you build your application as a parachain. The XCM format allows any parachains to communicate by passing messages between them. Furthermore, as bridges to other chains are connected (such as those to Bitcoin or Ethereum) Polkadot's parachains will be able to communicate with these as well. note Despite the benefits of becoming a parachain, developers should be conscious of the challenges in becoming a parachain, and whether building a blockchain with an end goal of becoming a parachain is a viable one for their project. On Polkadot, you are able to put your blockchain‚Äôs latest block head onto the relay chain. As a parachain, the blocks you submit are verified by validators with a Wasm runtime, which can be stored on the relay chain. You also get the ability to communicate with other parachains using the XCM format: an abstract message passing system. Message passing is tracked on the relay chain - as such, you can prove the delivery of messages and facilitate trustless interactions. As you can place your blockchain‚Äôs latest block head, you can achieve deterministic finalization for your chain. The hard part of reaching finalization for blockchains tends to be the consensus, where, in the parachain model, a blockchain can offload consensus to the overall shared network, and focus on block production. Since the validators have the Wasm runtime for all the parachains, your parachain shares the security of the validator pool with everyone on the relay chain. Any validator in the validator pool can help validate your blockchain. Things to Consider ‚Äã Para-nomics ‚Äã Digital Nation States ‚Äã Parachains can be seen as autonomous agents; networks that act as decentralised digital nation states. Parachains have their own communities, rules, economies, governance, treasuries, and relationships with external chains. As a result, the economic policies within parachain ecosystems are subject to the developers and overall community of that parachain ecosystem; there isn't necessarily a go-to economic model a parachain should follow. Moreover, becoming a parachain has an opportunity cost associated. Ideally, you can increase the value of the network by participating in the parachain selection process, and this should serve as a good return on investment. Connecting Digital Economies ‚Äã Collators act as network maintainers and maintain a full node of a parachain. They can be incentivized with a native token payout from: Transaction fees collected Parathread token sponsorship Blocks are naturally produced when a parathread bid is less than the native token payout. Para-objects ‚Äã The Relay Chain can host arbitrary state machines, not just blockchains. The Polkadot network will encourage the connection and interoperability between different para-objects . Here, para-objects are referring to objects on the network that operate in parallel, generally, parallelizable objects. These could be in the form of: System level chains (permanent chains): leased slots , parathread pool Bridge Hubs Nested Relay Chains: Polkadot 2.0 Migration ‚Äã Projects that are already functioning as "solochains" or in isolated environments may be interested in migrating onto Polkadot as a para-object. While the parachain model has its benefits, it may not be the go-to strategy for some projects. As a path for migration onto Polkadot, it may be more viable to migrate to one of the chains in one of the reserved slots. For instance, there are currently options for smart contract deployment on Kusama through the networks that have secured a slot in the latest slot auctions. Implement a Parachain ‚Äã The Parachain Implementer's Guide is a significant work in progress and maintained by Parity Tech. The live version is built from the source located in the official Polkadot repository . Parachain Development Kit ‚Äã The Parachain Development Kit or PDK is a set of tools that allows developers to easily create a parachain. In practice, the PDK will consist of the following key components: State transition function : a way for your application to move from one state to another state. Collator node network with certain responsibilities regarding parachains. Key Components ‚Äã The state transition function (STF) can be an abstract way for an application to go from one state to another state. The only constraint that Polkadot places on this STF is that it must be easily verifiable -- usually through what we call a witness or proof . It must be so because the Relay Chain validators will need to check that each state it receives from the collator node is correct without actually running through the entire computation. Some examples of these proofs include the Proof-of-Validity blocks or zk-SNARKs, which require less computational resources to verify than they do to generate. The verification asymmetry in the proof generation of the STF is one of the integral insights that allows Polkadot to scale while keeping high-security guarantees. A collator node is one of the types of network maintainers in the protocol. They are responsible for keeping availability of the state of the parachain and the new states returned from the iteration of the state transition function. They must remain online to keep track of the state and also of the XCMP messages that it will route between itself and other parachains. Collator nodes are responsible for passing the succinct proofs to the relay chain's validators and tracking the latest blocks from the relay chain. In essence, a collator node also acts as a light client for the relay chain. For more on collator nodes, see the collator page . What PDKs Exist? ‚Äã Currently, the only PDK is Parity Substrate and Cumulus . Substrate is a blockchain framework that provides the basic building blocks of a blockchain (things like the networking layer, consensus, a Wasm interpreter) while providing an intuitive way to construct your runtime. Substrate is made to ease the process of creating a new chain, but it does not provide support for Polkadot compatibility directly. For this reason, Cumulus , an added library contains all of the Polkadot compatibility glue code. Get started with Substrate The best way to get started with Substrate is to explore the Substrate Documentation maintained by Parity Technologies . Cumulus ‚Äã info Cumulus clouds are shaped sort of like dots. Together, they form an intricate system that is beautiful and functional. Cumulus is an extension to Substrate that makes it easy to make any Substrate-built runtime into a Polkadot-compatible parachain. Cumulus Consensus is a consensus engine for Substrate that follows a Polkadot Relay Chain (i.e., parachains). This runs a Polkadot node internally, and dictates to the client and synchronization algorithms which chain to follow, finalize, and treat as correct. See the Cumulus overview for a more detailed description of Cumulus. Cumulus is still in development, but the idea is that it should be simple to take a Substrate chain and add the parachain code by importing the crates and adding a single line of code. Keep up-to-date with the latest Cumulus developments from the Cumulus section . info Substrate and Cumulus provide a PDK from the abstraction of the blockchain format, but it is not necessary that a parachain even needs to be a blockchain. For example, a parachain just needs to satisfy the two constraints listed above: state transition function and collator node . Everything else is up to the implementer of the PDK. Cumulus handles the network compatibility overhead that any parachain would need to implement to be connected to Polkadot. This includes: Cross-chain message passing (XCMP) Out-of-the-box Collator node setup An embedded full client of the Relay Chain Block authorship compatibility Are you interested in building a PDK? See the future PDKs section for details. How to set up your parachain ‚Äã After creating your chain runtime logic with Substrate, you will be able to compile it down to a Wasm executable. This Wasm code blob will contain the entire state transition function of your chain, and is what you will need to deploy your project to Polkadot as either a parachain or parathread. Validators on Polkadot will use the submitted Wasm code to validate the state transitions of your chain or thread, but doing this requires some additional infrastructure. A validator needs some way to stay up to date with the most recent state transitions, since Polkadot nodes will not be required to also be nodes of your chain. This is where the collator node comes into play. A collator is a maintainer of your parachain and performs the critical action of producing new block candidates for your chain and passing them to Polkadot validators for inclusion in the Polkadot Relay Chain. Substrate comes with its own networking layer built-in but unfortunately only supports solo chains (that is, chains that do not connect to the relay chain). However, there is the Cumulus extension that includes a collator node and allows for your Substrate-built logic to be compatible with Polkadot as either a parachain or parathread. Future PDKs ‚Äã Call to action Do you want to build a Parachain Development Kit from scratch? The Web3 Foundation is giving grants to teams who are doing this, learn more and apply on the W3F grants page . One example of a PDK W3F is interested in supporting is a roll-up kit that allowed developers to create SNARK-based parachains. If we review the roll-up write-up, we see that the system uses two roles: users that update state and an operator that aggregates the state updates into a single on-chain update. It should be straightforward to see how we can translate this to the parachain terms. The state transition function for a roll-up-like parachain would be updating the state (in practice, most likely a Merkle tree, which would be easily verifiable) from the user inputs. The operator would act as the collator node, which would aggregate the state and create the zk-SNARK proof that it would hand to a Relay Chain's validators for verification. If you or your team are interested in developing a PDK feel free to apply for a grant on the W3F Grants Program repository . There may be grants available for this type of work. Testing a Parachain ‚Äã Rococo Testnet ‚Äã Rococo is a testnet built for testing parachains. Rococo utilizes Cumulus and HRMP (Horizontal Relay-routed Message Passing) in order to send transfers and messages between parachains and a Relay Chain. Every message is sent to the Relay Chain, then from the Relay Chain to the desired parachain. Rococo runs a few test system parachains: Rockmine , and Contracts . It also runs several externally developed parachains. What Parachains are on Rococo Now? ‚Äã You can see the list of included parachains here . A list of proposed parachains is available here . Obtaining ROC ‚Äã Follow the instructions here to get ROCs tokens. Build and Register a Rococo Parathread ‚Äã The parachains of Rococo all use the same runtime code. The only difference between them is the parachain ID used for registration with the Relay Chain. You will need to run a Rococo collator. To do so, you need to compile the following binary: cargo build --release --locked -p polkadot-collator Once the executable is built, launch collators for your parachain: ./target/release/polkadot-collator --chain $CHAIN --validator If you are interested in running and launching your own parathread or parachain, Parity Technologies has created parachain tutorials to show you how. Got stuck or need support along the way? Join Substrate Stack Exchange ) to interact with other builders there. How to Connect to a Parachain ‚Äã If you would like to connect to a parachain via Polkadot-JS Apps , you may do so by clicking on the network selection at the top left-hand corner of the navigation and selecting any parachain of choice. For the purpose of these following examples, we will be using the Rococo testnet "Custom Node" underneath "Development", following the parachain tutorials . Parachain Playground ‚Äã You can also take advantage of the account functions offered on Polkadot-JS Apps to test the entire Parachain onboarding process (e.g. crowdloans, auctions, registrations). Start a local node on Westend by running: polkadot --chain = westend-dev --alice Then, connect your local node with Polkadot-JS Apps. Deploy ‚Äã Substrate-based chains, including the Polkadot and Kusama Relay Chains, use an SS58 encoding for their address formats. This page serves as the canonical registry for teams to see which chain corresponds to a given prefix, and which prefixes are available. Parachain ‚Äã To include your parachain into the Polkadot network, you will need to acquire a parachain slot. Parachain slots will be sold in open auctions, the mechanics of which can be found on the parachain auction page of the wiki. Parathread ‚Äã Parathreads will not require a parachain slot, so you will not need to engage in the candle auction mechanism. Instead, you will be able to register your parathread code to a Relay Chain for a fee and from then be able to start participating in the per-block auctions for inclusion of your state transition into a Relay Chain. For more information on how parathread per-block auctions work, see the more detailed parathread page. Resources ‚Äã Parachain tutorials System (Common Good) Parachains The Launch of Parachains Parathreads: Pay-as-you-go Parachains Polkadot Bridges The Path of a Parachain Block The Path of a Parachain Block (Video) Polkadot Parachain Slots How to become a parachain on Polkadot (Video) Trusted Execution Environments and the Polkadot Ecosystem Edit this page Last updated on Jun 30, 2023 by Filippo Previous Builders Starter's Guide Next Decentralized Storage Your Go-To Overview for Developing a Parachain Why Create a Parachain? What are the Benefits of Deploying a Parachain? Things to Consider Para-nomics Para-objects Migration Implement a Parachain Parachain Development Kit How to set up your parachain Future PDKs Testing a Parachain Rococo Testnet What Parachains are on Rococo Now? Obtaining ROC Build and Register a Rococo Parathread How to Connect to a Parachain Parachain Playground Deploy Parachain Parathread Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Decentralized Storage General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Builders Starter's Guide Parachain Development Decentralized Storage Smart Contracts Oracles Data Substrate Connect SS58 Registry Opening HRMP Channels Integration Guide Tools Resources Maintain Build Development Guide Decentralized Storage On this page Decentralized Storage Storage is an integral part of modern computer systems, and the same is true for distributed and decentralized systems like a blockchain. When interacting with the Polkadot ecosystem, it will be helpful if you familiarize yourself with the current Web3 approach towards decentralized storage. DCS (Decentralized Cloud Storage) ‚Äã The key attribute that characterizes centralized cloud storage is the location of data. In decentralized cloud storage, the key attribute becomes the data itself instead of the data's location. This can be viewed as the shift from the centralized location-centric storage approach to the decentralized content-centric approach. IPFS (Interplanetary File System) ‚Äã IPFS is a peer-to-peer distributed file system that seeks to connect all computing devices with the same system of files, by utilizing features such as content-addressing, content-signing, and enhanced security methods through encryption. IPFS aims to address the current hurdles of the HTTP-based Internet. Brief comparison of IPFS & HTTP: ‚Äã IPFS HTTP network: peer-to-peer model (decentralized) network: client-server model (centralized) requests: use a cryptographic hash of that data requests: use the address on which data is hosted accessibility: data is distributed to multiple nodes and can be accessed at any time. Bandwidth is high: nearest peer can serve the data accessibility: data can only be accessed if the server is live and there are no interruptions in transmission. Bandwidth is limited: clients send requests to the same server Crust Storage ‚Äã Crust Network provides a Web3.0 decentralized storage network for the Metaverse. It is designed to realize core values of decentralization, privacy, and assurance. Crust supports multiple storage-layer protocols such as IPFS , and exposes instant accessible on-chain storage functions to users. Crust ºs technical stack is also capable of supporting data manipulating and computing. Crust provides a native cross-chain communication pallet based on XCMP , called xStorage . The protocol also supports most smart contract platforms, including Ethereum, with its cross-chain dStorage solution . Learn more about Crust To learn more about Crust, check out the Crust Network Wiki . Try integrating with Crust by following their Crust Storage 101 guide. Filebase ‚Äã Filebase is the first S3-compatible object storage platform that allows you to store data in a secure, redundant, and performant manner across multiple decentralized storage networks. Filebase offers a geo-redundant IPFS pinning service that allows you to pin files to IPFS across multiple diverse geographic locations. All files uploaded to IPFS through Filebase are automatically pinned to the Filebase infrastructure with 3x replication across the globe. This ensures that your data is globally available and redundant at all times. Filebase acts as an easy on-ramp to IPFS and decentralized storage by offering a user-friendly web console dashboard, making drag-and-dropping files onto Web3 simple and easy. Filebase also provides an S3-compatible API for widespread integrations and configurations in current workflows. Learn more about Filebase To learn more about Filebase, check out the Filebase Documentation , and specifically the documentation on deploying Polkadot dApp on decentralized storage. You can get started with Filebase by signing up here. Using Polkadot-JS Files (IPFS) module ‚Äã Polkadot-JS UI includes a decentralized storage module that allows Substrate-based chain users to upload their files to an IPFS W3Auth Gateway and use the IPFS W3Auth Pinning Service to pin their files on Crust Network. Start by uploading a single file or folder: Choose a Gateway: Sign the message: You should be able to view the file info, as follows: As well as the file status: note The whole files module is decentralized, so your file directory is only cached in browser. The file directory info will not be visible if you switch to a new browser or clear the browser cache. The storage module allows you to export file directory info from the current browser and import it to the new browser. note These above images are taken from this pull request Edit this page Last updated on Jan 23, 2023 by github-actions[bot] Previous Parachain Development Next Smart Contracts DCS (Decentralized Cloud Storage) IPFS (Interplanetary File System) Crust Storage Filebase ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Smart Contracts General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Builders Starter's Guide Parachain Development Decentralized Storage Smart Contracts Oracles Data Substrate Connect SS58 Registry Opening HRMP Channels Integration Guide Tools Resources Maintain Build Development Guide Smart Contracts On this page Smart Contracts The Polkadot Relay Chain which is a layer 0 blockchain, does not support smart contracts natively. However, parachains which are layer 1 blockchains on Polkadot are equipped with the functionality to support smart contracts. The two primary supported smart contract environments are ink! and EVM. There are multiple parachains that support both environments . Difference between developing a smart contract and a parachain ‚Äã Layer of Abstraction ‚Äã When you write a smart contract, you are creating the instructions that associate with and deploy on a specific chain address. In comparison, a runtime module on a parachain is the entire logic of a chain's state transitions (what's called a state transition function). Smart contracts must consciously implement upgradeability while parachains have the ability to swap out their code entirely through a root command or via the governance pallet. When you build a smart contract, it will eventually be deployed to a target chain with its own environment. Parachains allow the developer to declare the environment of their own chain, even allowing others to write smart contracts for it. Gas Fees ‚Äã Smart contracts must find a way to limit their own execution, or else full nodes are vulnerable to DOS attacks. An infinite loop in a smart contract, for example, could consume the computational resources of an entire chain, preventing others from using it. The halting problem shows that even with a powerful enough language, it is impossible to know ahead of time whether or not a program will ever cease execution. Some platforms, such as Bitcoin, get around this constraint by providing a very restricted scripting language. Others, such as Ethereum, "charge" the smart contract "gas" for the rights to execute their code. If a smart contract does get into a state where execution will never halt, it eventually runs out of gas, ceases execution, and any state transition that the smart contract would have made is rolled back. Parachains can implement arbitrarily powerful programming languages and contain no gas notion for their own native logic. This means that some functionality is easier to implement for the developer, but some constructs, such as a loop without a terminating condition, should never be implemented. Leaving certain logic, such as complex loops that could run indefinitely, to a non-smart contract layer, or even trying to eliminate it, will often be a wiser choice. Parachains try to be proactive, while smart contract platforms are event-driven. Polkadot and parachians typically use the weight-fee model and not a gas-metering model . Building a Smart Contract ‚Äã The Polkadot relay chain does not natively support smart contracts. However, since the parachains that connect to Polkadot can support arbitrary state transitions, they support smart contracts. Substrate presently supports smart contracts out-of-the-box in several ways: The EVM pallet offered by Frontier . The Contracts pallet in the FRAME library for Wasm-based contracts. Frontier EVM Contracts ‚Äã Frontier is the suite of tools that enables a Substrate chain to run Ethereum contracts (EVM) natively with the same API/RPC interface, Ethereum exposes on Substrate. Ethereum Addresses can also be mapped directly to and from Substrate's SS58 scheme from existing accounts. Substrate Contracts ‚Äã Substrate offers a built-in contract pallet ; parachains can also support WebAssembly smart contracts. Additionally, there is the EVM Pallet , which allows a parachain to implement the Ethereum Virtual Machine, thereby supporting almost direct ports of Ethereum contracts. A video version of the recap of the smart contract situation is available on the Polkadot YouTube channel . Resources ‚Äã When should I build a Substrate runtime versus a Substrate smart contract ? This post answers the question more technically of when a developer might choose to develop a runtime versus a smart contract. Here is the list of current resources available to developers who want to get started writing smart contracts to deploy on parachains based on Substrate. ink! - Parity's ink to write smart contracts. Substrate ink! Workshop - Walks you through the basics of writing and deploying an ERC-20 token using ink! . Contracts Pallet ‚Äã The experience of deploying to an EVM-based chain may be more familiar to developers that have written smart contracts before. However, the Contracts pallet makes some notable improvements to the design of the EVM: Wasm . The Contracts pallet uses WebAssembly as its compilation target. Any language that compiles to Wasm can potentially be used to write smart contracts. Nevertheless, it is better to have a dedicated domain-specific language, and for that reason Parity offers the ink! language. Deposit . Contracts must hold a deposit (named ContractDeposit ) suitably large enough in order to justify their existence on-chain. A deployer needs to deposit this into the new contract on top of the ExistentialDeposit . Caching . Contracts are cached by default and therefore means they only need to be deployed once and afterward be instantiated as many times as you want. This helps to keep the storage load on the chain down to the minimum. On top of this, when a contract is no longer being used and the existential deposit is drained, the code will be erased from storage (known as reaping). Storage Rent: Deprecated ‚Äã pallet_contracts was initially designed to combat unbounded state growth by charging contracts for the state they consume but has since been deprecated. See the associated pull request for more details. Polkadot Standards Proposals (PSPs) ‚Äã Web3 Foundation supports proposals for Polkadot that define a set standards to fit ecosystem needs. These standards go through several acceptance phases, where the engagement of the whole community is needed to build valuable and future-proof standards. All the teams who will benefit from a standard need to agree on its content. Some of these PSPs are targeting Substrate's contracts pallet: PSP22 - Fungible Token Standard Please visit Polkadot Standards Proposals (PSPs) Github for more information. Ink ‚Äã ink! is a domain specific language for writing smart contracts in Rust and compiles to Wasm code. As it states in its README, it is still in an experimental phase so brave developers should be aware that they might have a bumpy - but workable - development experience. There are some projects that have built projects in ink! with a decent level of complexity such as Plasm's Plasma contracts , so it is mature enough to start building interesting things. For interested developers, they can get started writing smart contracts using ink! by studying the examples that were already written. These can be used as guideposts to writing more complex logic that will be deployable on smart contract parachains. ink! has laid much of the groundwork for a new smart contract stack that is based on a Wasm virtual machine and compatible with Substrate chains. Libraries for Smart Contracts in ink! ‚Äã Collected below are some community examples of smart contracts in ink! . Are you working on a smart contract example? Ask us to add it to this page! OpenBrush : an ink! library providing standard contracts based on PSP with useful contracts and macros for building. Metis : a Wasm contract standard library, developed by Patract Labs . Smart Contract Environments ‚Äã It is still early for smart contracts on Polkadot and the development is only now stabilizing. We are actively producing content to help developers get up to speed and will maintain the Wiki with the latest resources. You should also keep up to date with the following links: Parity Tech ‚Äã ink! Substrate contracts pallet Parachains ‚Äã Moonbeam Astar Acala Phala Darwinia Many smart contract platforms are building to become a parachain in the ecosystem. A community created and maintained list of different smart contract platforms building on Polkadot can be found at PolkaProjects . Moonbeam ‚Äã ink!: Unsupported EVM (Solidity): Supported Moonbeam is another project that is planning to deploy to Polkadot as a parachain and will support Ethereum compatible smart contracts. Since Moonbeam uses Frontier , an interoperability layer with existing Ethereum tooling, it will support all applications that are written to target the EVM environment with little friction. Moonriver , a companion network to Moonbeam, launched as a parachain on Kusama. Parachain functionality is live, and features are being incrementally released. The final phase of the launch will include EVM functionality and balance transfers. Try deploying a smart contract to Moonbeam by following their documentation . Astar ‚Äã ink!/Wasm: Supported EVM (Solidity): Supported Astar Network supports the building of dApps with EVM and WASM smart contracts and offers developers true interoperability. True interoperability with cross-consensus messaging XCM and cross-virtual machine XVM . We are made by developers and for developers. Astar‚Äôs unique Build2Earn model empowers developers to get paid through a dApp staking mechanism for the code they write and dApps they build. Shiden Network is the canary network of Astar Network, live as a parachain on Kusama, and supports the EVM and WASM environment for all developers who want to build out use-cases in a canary network with economic value. Shiden acts as a playground for developers. Try deploying an Ethereum or ink! smart contract by following their documentation . Acala ‚Äã ink!: Unsupported EVM (Solidity): Supported Acala is a decentralized finance consortium and DeFi infrastructure chain delivering a set of protocols to serve as the DeFi hub on Polkadot. Karura , Acala's canary network is live as a parachain on Kusama. Interested teams are now able to deploy DApps and smart contracts on Karura's platform. Acala is also implementing the Acala EVM . Try deploying an Acala EVM smart contract by following their documentation . Phala ‚Äã ink!: Unsupported EVM (Solidity): Unsupported See: Phat Contracts Phala is a privacy-preserving cloud compute platform and aims to provide strong guarantees of confidentiality as a cross-chain platform. As a smart contract platform, Phala will enable the use of confidential smart contracts on Polkadot. Khala is Phala's canary network and is live as a parachain on Kusama. Try deploying a confidential smart contract by following their documentation . Darwinia ‚Äã ink!: Unsupported EVM (Solidity) Support: Supported Darwinia is a cross-chain programmable network that provides two essential solutions: Darwinia Chain and Darwinia Msgport . Darwinia Chain is a smart contract platform compatible with the EVM and shares the robust security of Polkadot. On the other hand, Darwinia Msgport acts as a crucial infrastructure enabling Dapps to seamlessly communicate and exchange messages across different blockchain networks, facilitating efficient interchain operations. Try deploying a smart contract to Darwinia by following their documentation . Keep In Touch ‚Äã Even though the tooling is still maturing, the advantage of being early will be the familiarity and head start on your project, allowing you to innovate and create something truly new. If you have interesting ideas for smart contracts on Polkadot, feel free to drop into the Polkadot Watercooler to talk about them. Developers may be interested in joining the Polkadot Beginners Lounge or Substrate and Polkadot StackExchange to ask their questions. As always, keep up to date with Polkadot and Kusama by following the social channels . Edit this page Last updated on Jun 26, 2023 by bader y Previous Decentralized Storage Next Oracles Difference between developing a smart contract and a parachain Layer of Abstraction Gas Fees Building a Smart Contract Frontier EVM Contracts Substrate Contracts Contracts Pallet Polkadot Standards Proposals (PSPs) Ink Smart Contract Environments Parity Tech Parachains Keep In Touch ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Oracles General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Builders Starter's Guide Parachain Development Decentralized Storage Smart Contracts Oracles Data Substrate Connect SS58 Registry Opening HRMP Channels Integration Guide Tools Resources Maintain Build Development Guide Oracles Oracles In the blockchain context, an oracle is a way to bring real-world data onto the blockchain so that it can be used by a decentralized application. Oracles serve many purposes for application builders. For example: Most stablecoin designs use an oracle to bring in data of the exchange rate of assets, in order to peg their value to a real world currency. Synthetic assets use oracles as price feeds in order to determine if the underlying cryptocurrency can sufficiently collateralize the debt position. Prediction markets use oracles to decide the outcome of real world events and determine the payout of the prediction shares. Decentralized insurance markets use oracles to bring in information about whether a claim is valid or not. Oracle solutions range from centralized and trusted to decentralized and game-theory based. On the centralized end of the spectrum, an oracle could be a single account that has the authority to dictate the real-world data on-chain. On the decentralized end, a complex game of "chicken" can be played among various staked actors who risk getting slashed if they don't submit the same data as everyone else. Solutions such as Chainlink fit somewhere in the middle, where the amount of trust you put into the reporting oracles can be adjusted based on your preferences. A Chainlink Feed Pallet was recently released to allow smart contract applications across Polkadot to access price reference data, made available as a Substrate oracle pallet. Acurast is another solution that enables developers to define their off-chain data and computation requirements and receive the outputs to the Acurast Pallet and EVM or WASM environments. When using an oracle in your application you should be aware of the benefits and risks that are baked into its specific model. As the Polkadot ecosystem develops and oracle parachains begin to appear, this article will be updated with a comparison of the different solutions and the benefits and drawbacks that each provide. Edit this page Last updated on Jan 30, 2023 by github-actions[bot] Previous Smart Contracts Next Data ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Data Analytics General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Builders Starter's Guide Parachain Development Decentralized Storage Smart Contracts Oracles Data Substrate Connect SS58 Registry Opening HRMP Channels Integration Guide Tools Resources Maintain Build Development Guide Data On this page Data Analytics The tools that accentuate and aggregate the data within blockchains are integral for a multi-chain future. Parachains will need robust and secure ways to index and aggregate data, such as a data aggregation layer. Indexing and Querying ‚Äã Subsquid ‚Äã Subsquid is an open-source framework for building tailored GraphQL APIs to query Substrate chain state and history. Subsquid replaces direct gRPC node access with performant Squid archive gateways, allowing quick synchronization of the API with the historical on-chain data. Subsquid-powered APIs support filtering, pagination, union types, interfaces and full-text search out-of-the-box, and can be further extended with custom GraphQL resolvers. SubQuery ‚Äã SubQuery is a fast, flexible, and reliable open-source data indexer that provides you with custom APIs for your Substrate/Polkadot project. We build the best, fully-featured indexer, so you don‚Äôt have to, with: automatic support for any Substrate network support for all Polkadot smart contract frameworks (FrontierEVM, Acala EVM+, and Wasm) automated historical state tracking to enable faster partial re-indexing a lightweight and portable design that doesn't require centralised archives full support for GraphQL filtering, pagination, interfaces, subscriptions, and aggregations a future decentralized network acts as chain-agnostic data aggregation, indexing, and querying layer between blockchains and applications that will not require centralised services. To start building, head to the SubQuery documentation where you'll find quick start guides and walkthrough developer tutorials. Dolpha Analytics ‚Äã Dolpha Analytics is an on-chain data analytics and insights platform for the Polkadot and Kusama ecosystems. Dolpha empowers casual and power users to get a pulse on the ecosystems, perform due diligence, surface signals, and make informed data-driven decisions. Web3Go ‚Äã Web3GO is a Polkadot-based open data analytics platform that provides infrastructure and tooling to help users visualize, curate, share, and analyze on-chain data. The Graph ‚Äã The Graph is a layer-1 indexing tool and querying layer for the decentralized web, who announced an integration plan to bring indexing and querying to Polkadot . The Graph allows developers to build and publish open APIs, called subgraphs. Applications can query subgraphs using GraphQL . Covalent ‚Äã Covalent offers a unified API to understand the data on a blockchain, and is currently live on the Moonbeam network, bringing visibility to billions of blockchain data points to Polkadot via their unified API. The approach to deploy on Moonbeam simplifies the development of Polkadot-based blockchain applications as compared to implementing a full parachain or parathread. Covalent captures the entire history of blockchains and offers a way to zoom into data points recorded in smart contracts that are often not accessible. Polkadot-based Explorers ‚Äã As you can imagine, blockchain explorers also offer data analytics through an interface where users can examine common data points. Some block explorers in the Polkadot ecosystem are listed on the tools page . Edit this page Last updated on Jun 27, 2023 by only25x Previous Oracles Next Substrate Connect Indexing and Querying Subsquid SubQuery Dolpha Analytics Web3Go The Graph Covalent Polkadot-based Explorers ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Substrate Connect General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Builders Starter's Guide Parachain Development Decentralized Storage Smart Contracts Oracles Data Substrate Connect SS58 Registry Opening HRMP Channels Integration Guide Tools Resources Maintain Build Development Guide Substrate Connect On this page Substrate Connect Blockchain User Interfaces are still Centralized ‚Äã The communication between a standard user interface (UI) and a network node is through a JSON RPC protocol. Generally, the UI will showcase the information that is available on the node, and this is done through two main approaches: User-Controlled Nodes : The UI connects to a node client that the user has installed on their machine. These nodes are secure, but installation and maintenance of these nodes tend to be an inconvenience. Publicly-Accessible Nodes : The UI connects to a third-party-owned publicly-accessible node client. While these nodes are more prevalent in their usage as they are convenient to use, they are centralized and insecure. There is now a new paradigm: instead of specifying a centralized RPC node, developers just need to define the blockchain's chain specification for their application to synchronize with the chain. This is possible with Substrate connect. What is Substrate Connect? ‚Äã Replacing RPC node reliance with light clients ‚Äã Substrate connect is a JavaScript library and browser extension that builds on the PolkadotJS API ) to enable developers to build application-specific light clients for Substrate chains. There is no installation required or optional extension with minimal or no maintenance. The node is run by the JavaScript engine. Simply put, Substrate connect is a Substrate client that runs in JavaScript. Application developers no longer need to rely on single RPC nodes to allow end-users to interact with their applications. Substrate (full node) vs. Substrate connect (light client) ‚Äã A light client lets you utilize all basic features of the chain such as fetching data and transferring tokens, but it does not require you to run a full copy of the entire blockchain or having to trust remote peers. Light clients fetch the required data that they need from a Polkadot node with an associated proof to validate the data. Substrate: Full node Substrate connect: Light client full verification of all blocks of the chain only verifies the authenticity of blocks of the chain holds all of the previous block data and the chain's storage in database no database installation, maintenance, and execution tend to be exhaustive and require system administration expertise. no installation; has an optional extension with minimal or no maintenance. Initializes in five to ten seconds How to use Substrate Connect ‚Äã As a JavaScript library ‚Äã Substrate connect provides a PolkadotJS API connected to a bundled node. Through the use of the library, a user can run an actual Substrate-compatible node. A node bundled with its user interface: ready-to-use light clients ‚Äã The UI connects to a node client that is directly integrated: convenient , secure , and decentralized . This is accomplished through Substrate connect using a smoldot Wasm light client to securely connect to the blockchain network without relying on specific third parties. Application developers can now run a Substrate light client in any NodeJS environment ( @substrate/connect ). Currently, Substrate connect supports Polkadot, Kusama, Westend, and Rococo; because light clients are part of the overall Substrate framework, they are available for Substrate-based blockchains. As a browser extension ‚Äã Establishing a sufficient number of peers is difficult due to browser limitations on WebSockets from HTTPS pages, as many nodes need to be available with TLS. The browser extension provided by Substrate connect helps to overcome this limitation and keeps the chains synced in the background, allowing applications to run faster. Bundling light-clients of multiple chains ‚Äã The browser extension allows end-users to interact with applications connected to multiple blockchains or connect their own blockchains to applications that support it. note Substrate Connect will auto-detect whether a user is using the extension. If not, the Wasm light client will be created in-page for them. Resources ‚Äã What is a light client and why you should care? Introducing Substrate Connect: Browser-Based Light Clients for Connecting to Substrate Chains Substrate connect GitHub Repo Edit this page Last updated on Nov 21, 2022 by Keith Alfaro Previous Data Next SS58 Registry Blockchain User Interfaces are still Centralized What is Substrate Connect? Replacing RPC node reliance with light clients Substrate (full node) vs. Substrate connect (light client) How to use Substrate Connect As a JavaScript library As a browser extension Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
SS58 Registry General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Builders Starter's Guide Parachain Development Decentralized Storage Smart Contracts Oracles Data Substrate Connect SS58 Registry Opening HRMP Channels Integration Guide Tools Resources Maintain Build Development Guide SS58 Registry SS58 Registry Substrate-based chains use an SS58 encoding for their address formats. Please see the SS58 registry to see which chain corresponds to a given prefix , and which prefixes are available. Edit this page Last updated on Sep 18, 2022 by Keith Alfaro Previous Substrate Connect Next Opening HRMP Channels ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Opening HRMP Channels General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Builders Starter's Guide Parachain Development Decentralized Storage Smart Contracts Oracles Data Substrate Connect SS58 Registry Opening HRMP Channels Integration Guide Tools Resources Maintain Build Development Guide Opening HRMP Channels On this page Opening HRMP Channels HRMP has the same interface and functionality as XCMP but is much more demanding on resources since it passes all messages via the Relay Chain. When XCMP is implemented on Polkadot, HRMP is planned to be deprecated and phased out in favor of it. Opening HRMP channel: parachain to parachain ‚Äã To open a channel from one parachain to another that are not system chains on Polkadot, the governance of each parachain needs to declare its intent to open a channel on the Relay Chain, and the second chain needs to accept and do the same. In order to do this, the calls to be dispatched from both chains are: hrmp > hrmpInitOpenChannel(recipient, proposedMaxCapacity, proposedMaxMessageSize) : this call initiates opening a channel from a parachain to a given recipient with given channel hrmp > hrmpAcceptOpenChannel(sender) : accepting the channel open request from the given sender. Each parachain is to use its own governance process to execute this. The call will be executed on the Relay Chain. Opening HRMP Channels with System Parachains ‚Äã Opening an HRMP channel with a system parachain requires a referendum. Like all other governance proposals, proposers should follow best practices like opening a discussion on Polkassembly or Subsquare and then submitting the proposal on-chain. Proposals should generally be a batch_all call containing: A force_transfer of the channel deposit from the Treasury to the System parachain's sovereign account. Remember that a bi-direction channel is two channels so will need double the amount. A force_open_hrmp_channel from your chain to the system chain. A force_open_hrmp_channel from the system chain to your chain. caution Please ensure that you use the new force_open_hrmp_channel directly on the Relay Chain, rather than the old two-phase channel request/accept method. Publication on Polkassembly for discussion and feedback ‚Äã The discussions section of Polkassembly is the best place to share the reasoning behind your proposal: make sure to log in with the proposer address (if possible) before publishing yours, and if you can do this also make sure the address has an on-chain identity. Example: Opening an HRMP Channel with the AssetHub ‚Äã The following highlights the steps required to submit a proposal for opening an HRMP channel with the AssetHub. Your submission should contain the following sections to be considered complete for the community to review and ultimately vote on: A request on what the proposal aims to do (opening an HRMP channel with the AssetHub); The use cases this channel will support for your chain; Technical details of the proposal, including proposal parameters and technical details of this call (On Kusama, most proposals were designed as a batchAll calls): A forceTransfer of 20 DOT from the Treasury to AssetHub sovereign account. A forceOpenhrmpchannel from AssetHub (1,000) to ParaID. A forceOpenhrmpChannel from ParaID to AssetHub (1,000). Please note that if governance decides to reduce the HRMP channel deposit on Polkadot to 0 DOT, the first transaction should not be necessary (these guidelines will be updated accordingly). The XCM message to the Asset Hub, which can be decoded on the network; The call data to verify on Polkadot JS Apps Decode tab; The preimage hash to include in the proposal's submission. Note that if the proposer is planning to submit a referendum on the Whitelisted Caller Track , the preimage hash needs to be submitted to and whitelisted by the Technical Fellowship . Below is an example of how teams followed this process on Kusama, as a way to: Proposal to open HRMP channel between Bifrost and the Asset Hub: the motion can be found here . Preimage submission on democracy tab (Polkadot JS Apps) ‚Äã Once the community has given enough feedback (we expect these proposal to be non controversial) please submit the image on the discussion post in the Democracy tab on Polkadot JS Apps, by using the "Submit Preimage" button: On the pop up window, compose the preimage in the discussion post: making sure the proposal hash is the same as in the post. Vote by the community ‚Äã After the community voting period, there will be an enactment period. In general, on Kusama these have been defined in shorter terms than the voting periods. Again, it is very important you encourage the community to vote on this, highlighting the benefits for your chain, use cases enabled with this submission, among other things. After the enactment period is over, both chains will authorize the channel. Edit this page Last updated on Jul 3, 2023 by Filippo Previous SS58 Registry Next Integration Initiation Opening HRMP channel: parachain to parachain Opening HRMP Channels with System Parachains Publication on Polkassembly for discussion and feedback Example: Opening an HRMP Channel with the AssetHub Preimage submission on democracy tab (Polkadot JS Apps) Vote by the community ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Integration Guide General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Integration Guide Integration Initiation Polkadot Protocol Integrating Assets Node Management Node Interaction Transaction Construction Tools Resources Maintain Build Integration Guide Integration Initiation On this page Polkadot Integration Guide Welcome to the Polkadot integration guide. This guide will provide all the information you need to get started on your integration. The target audiences for this guide are wallets and custodians, but it will be useful to any infrastructure provider such as validators or chain indexers. Structure ‚Äã The guide focuses on Polkadot and Kusama, but the principles will apply to parachains and other Substrate-based chains. The guide contains four sections: Protocol Info : The entry point to the guide. Please read it carefully as it provides information about Polkadot that differentiates it from other blockchains. Use this page to check your assumptions. Assets : The guide to integrating assets on Polkadot into your systems. Node Management : This page will guide you to starting and monitoring a node. Node Interaction : This page will teach you to interact with your node via multiple RPC tooling options. Transaction Construction : A guide to transaction construction, signing, decoding, and serialization using several available tools. Recommendation ‚Äã Each page in the guide, especially the Node Interaction and Transaction Construction pages, tries to list several options to accomplish the same thing. We want you to know your options and choose the solution that is best for you. That said, the easiest path to integration is almost always to use Substrate API Sidecar to interact with your node and TxWrapper Core to construct and sign transactions. Parity and Web3 Foundation will be able to provide the best support if you use these tools. If your team would like support, join some of our community channels or contact [email protected] . Edit this page Last updated on Dec 16, 2022 by Keith Alfaro Previous Opening HRMP Channels Next Polkadot Protocol Structure Recommendation ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Protocol Information General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Integration Guide Integration Initiation Polkadot Protocol Integrating Assets Node Management Node Interaction Transaction Construction Tools Resources Maintain Build Integration Guide Polkadot Protocol On this page Polkadot Protocol Information This page serves as a high-level introduction to the Polkadot protocol with terminology that may be specific to Polkadot, notable differences to other chains that you may have worked with, and practical information for dealing with the chain. Tokens ‚Äã Token decimals: Polkadot (DOT): 10 Kusama (KSM): 12 Base unit: "Planck" Balance type: u128 Redenomination ‚Äã Polkadot conducted a poll, which ended on 27 July 2020 (block 888_888), in which the stakeholders decided to redenominate the DOT token. The redenomination does not change the number of base units (called "plancks" in Polkadot) in the network. The only change is that a single DOT token will be 1e10 plancks instead of the original 1e12 plancks. See the Polkadot blog posts explaining the details and the results of the vote. The redenomination took effect 72 hours after transfers were enabled, at block 1_248_326, which occurred at approximately 16:50 UTC on 21 Aug 2020. You can find more information about the redenomination here . Addresses ‚Äã In Polkadot (and most Substrate chains), user accounts are identified by a 32-byte (256-bit) AccountId . This is often, but not always, the public key of a cryptographic key pair. Polkadot (and Substrate) use the SS58 address format. This is a broad "meta-format" designed to handle many different cryptographic schemes and chains. It has much in common with Bitcoin's Base58Check format such as a version prefix, a hash-based checksum suffix, and base-58 encoding. See the SS58 page in the Substrate documentation for encoding information and a more comprehensive list of network prefixes. Do not use regular expressions (regex) to validate addresses Always verify using the prefix and checksum of the address. Substrate API Sidecar provides an accounts/{accountId}/validate path that returns a boolean isValid response for a provided address. Relevant SS58 prefixes for this guide: Polkadot: 0 Kusama: 2 Westend: 42 Cryptography ‚Äã Polkadot supports the following cryptographic key pairs and signing algorithms: Ed25519 Sr25519 - Schnorr signatures on the Ristretto group ECDSA signatures on secp256k1 Note that the address for a secp256k1 key is the SS58 encoding of the hash of the public key in order to reduce the public key from 33 bytes to 32 bytes. Existential Deposit ‚Äã Polkadot, and most Substrate-based chains, use an existential deposit (ED) to prevent dust accounts from bloating chain state. If an account drops below the ED, it will be reaped, i.e. completely removed from storage and the nonce reset. Polkadot's ED is , while Kusama's is . You can always verify the existential deposit by checking the [chain state](https://polkadot.js.org/apps/#/chainstate) for the constant `balances.existentialDeposit`. info For more information about the existential deposit visit the dedicated section in the Accounts page. Likewise, if you send a transfer with value below the ED to a new account, it will fail. Custodial wallets should set a minimum withdrawal amount that is greater than the ED to guarantee successful withdrawals. Wallets and custodians who track account nonces for auditing purposes should take care not to have accounts reaped, as users could refund the address and try making transactions from it. The Balances pallet provides a transfer_keep_alive function that will return an error and abort rather than make the transfer if doing so would result in reaping the sender's account. The existential deposit is a property of the Relay Chain Your account on the Relay Chain has no direct impact on parachains as you have seperate accounts on each parachain. Still, parachains are able to define an existential deposit of their own, but this is seperate to that of the Relay Chain ED. Existential deposit for the Asset Hub The Asset Hub parachain has a lower existential deposit (0.1 DOT) than the Relay Chain (1 DOT) as well as lower transaction fees. It is highly recommended to handle balance transfers on the Asset Hub. Asset Hub integration is discussed in the next page of the guide. Free vs. Reserved vs. Locked vs. Vesting Balance ‚Äã Account balance information is stored in AccountData . Polkadot primarily deals with two types of balances: free and reserved. For most operations, free balance is what you are interested in. It is the "power" of an account in staking and governance, for example. Reserved balance represents funds that have been set aside by some operation and still belong to the account holder, but cannot be used. Locks are an abstraction over free balance that prevent spending for certain purposes. Several locks can operate on the same account, but they overlap rather than add. Locks are automatically added onto accounts when tasks are done on the network (e.g. leasing a parachain slot or voting), these are not customizable. For example, an account could have a free balance of 200 DOT with two locks on it: 150 DOT for Transfer purposes and 100 DOT for Reserve purposes. The account could not make a transfer that brings its free balance below 150 DOT, but an operation could result in reserving DOT such that the free balance is below 150, but above 100 DOT. Bonding tokens for staking and voting in governance referenda both utilize locks. Vesting is another abstraction that uses locks on free balance. Vesting sets a lock that decreases over time until all the funds are transferable. More info: Lockable Currency Lock Withdraw Reasons Vesting Info Extrinsics and Events ‚Äã Block Format ‚Äã A Polkadot block consists of a block header and a block body. The block body is made up of extrinsics representing the generalization of the concept of transactions. Extrinsics can contain any external data the underlying chain wishes to validate and track. The block header is a 5-tuple containing the following elements: parent_hash : a 32-byte Blake2b hash of the SCALE encoded parent block header. number : an integer representing the index of the current block in the chain. It is equal to the number of the ancestor blocks. The genesis state has number 0. state_root : the root of the Merkle tree, used as storage for the system. extrinsics_root : field which is reserved for the Runtime to validate the integrity of the extrinsics composing the block body. digest : field used to store any chain-specific auxiliary data, which could help the light clients interact with the block without the need of accessing the full storage as well as consensus-related data including the block signature. A node creating or receiving a block must gossip that block to the network (i.e. to the other nodes). Other nodes within the network will track this announcement and can request information about the block. Additional details on the process are outlined here in the Polkadot Spec. Extrinsics ‚Äã An extrinsic is a SCALE encoded array consisting of a version number , signature , and varying data types indicating the resulting runtime function to be called, including the parameters required for that function to be executed. Extrinsics constitute information from the outside world and take on three forms: Inherents Signed Transactions Unsigned Transactions As an infrastructure provider, you will deal almost exclusively with signed transactions. You will, however, see other extrinsics within the blocks that you decode. Find more information in the Substrate documentation . Inherent extrinsics are unsigned and contain information that is not provably true, but validators agree on based on some measure of reasonability. For example, a timestamp cannot be proved, but validators can agree that it is within some time difference on their system clock. Inherents are broadcasted as part of the produced blocks rather than being gossiped as individual extrinsics. Signed transactions contain a signature of the account that issued the transaction and stands to pay a fee to have the transaction included on chain. Because the value of including signed transactions on-chain can be recognized prior to execution, they can be gossiped on the network between nodes with a low risk of spam. Signed transactions fit the concept of a transaction in Ethereum or Bitcoin. Some transactions cannot be signed by a fee-paying account and use unsigned transactions. For example, when a user claims their DOT from the Ethereum DOT indicator contract to a new DOT address, the new address doesn't yet have any funds with which to pay fees. The Polkadot Host does not specify or limit the internals of each extrinsics and those are defined and dealt with by the Runtime. Transaction Mortality ‚Äã Extrinsics can be mortal or immortal. The transaction payload includes a block number and block hash checkpoint from which a transaction is valid and a validity period (also called "era" in some places) that represents the number of blocks after the checkpoint for which the transaction is valid. If the extrinsic is not included in a block within this validity window, it will be discarded from the transaction queue. The chain only stores a limited number of prior block hashes as reference. You can query this parameter, called BlockHashCount , from the chain state or metadata. This parameter is set to blocks (about seven hours) at genesis. If the validity period is larger than the number of blocks stored on-chain, then the transaction will only be valid as long as there is a block to check it against, i.e. the minimum value of validity period and block hash count. Setting the block checkpoint to zero, using the genesis hash, and a validity period of zero will make the transaction "immortal". NOTE: If an account is reaped and a user re-funds the account, then they could replay an immortal transaction. Always default to using a mortal extrinsic. Unique Identifiers for Extrinsics ‚Äã Transaction Hash is not a unique identifier The assumption that a transaction's hash is a unique identifier is the number one mistake that indexing services and custodians make. This error will cause major issues for your users. Make sure that you read this section carefully. Many infrastructure providers on existing blockchains, e.g. Ethereum, consider a transaction's hash as a unique identifier. In Substrate-based chains like Polkadot, a transaction's hash only serves as a fingerprint of the information within a transaction, and there are times when two transactions with the same hash are both valid. In the case that one is invalid, the network properly handles the transaction and does not charge a transaction fee to the sender nor consider the transaction in the block's fullness. Imagine this contrived example with a reaped account . The first and last transactions are identical, and both valid. Index Hash Origin Nonce Call Results 0 0x01 Account A 0 Transfer 5 DOT to B Account A reaped 1 0x02 Account B 4 Transfer 7 DOT to A Account A created (nonce = 0) 2 0x01 Account A 0 Transfer 5 DOT to B Successful transaction In addition, not every extrinsic in a Substrate-based chain comes from an account as a public/private key pair; Substrate, rather, has the concept of dispatch ‚Äúorigin‚Äù, which could be created from a public key account, but could also form from other means such as governance. These origins do not have a nonce associated with them the way that an account does. For example, governance might dispatch the same call with the same arguments multiple times, like ‚Äúincrease the validator set by 10%.‚Äù This dispatch information (and therefore its hash) would be the same, and the hash would be a reliable representative of the call, but its execution would have different effects depending on the chain‚Äôs state at the time of dispatch. The correct way to uniquely identify an extrinsic on a Substrate-based chain is to use the block ID (height or hash) and the extrinsic's index. Substrate defines a block as a header and an array of extrinsics; therefore, an index in the array at a canonical height will always uniquely identify a transaction. This methodology is reflected in the Substrate codebase itself, for example to reference a previous transaction from the Multisig pallet. Events ‚Äã While extrinsics represent information from the outside world, events represent information from the chain. Extrinsics can trigger events. For example, the Staking pallet emits a Reward event when claiming staking rewards to tell the user how much the account was credited. If you want to monitor deposits into an address, keep in mind that several transactions can initiate a balance transfer (such as balances.transferKeepAlive and a utility.batch transaction with a transfer inside of it). Only monitoring balances.transfer transactions will not be sufficient. Make sure that you monitor events in each block for events that contain your addresses of interest. Monitor events instead of transaction names to ensure that you can properly credit deposits. Fees ‚Äã Polkadot uses weight-based fees that, unlike gas, are charged pre-dispatch. Users can also add a "tip" to increase transaction priority during congested periods. See the transaction fee page for more info. Encoding ‚Äã Parity's integration tools should allow you to deal with decoded data. If you'd like to bypass them and interact directly with the chain data or implement your own codec, Polkadot encodes block and transaction data using the SCALE codec . Runtime Upgrades ‚Äã Runtime upgrades allow Polkadot to change the logic of the chain without the need for a hard fork. A hard fork would require node operators to manually upgrade their nodes to the latest runtime version. In a distributed system, this is a complex process to coordinate and communicate. Polkadot can upgrade without a hard fork. The existing runtime logic is followed to update the Wasm runtime stored on the blockchain to a new version. The upgrade is then included in the blockchain itself, meaning that all the nodes on the network execute it. Generally there is no need to upgrade your nodes manually before the runtime upgrade as they will automatically start to follow the new logic of the chain. Nodes only need to be updated when the runtime requires new host functions or there is a change in networking or consensus. Transactions constructed for a given runtime version will not work on later versions. Therefore, a transaction constructed based on a runtime version will not be valid in later runtime versions. If you don't think you can submit a transaction before the upgrade, it is better to wait and construct it after the upgrade takes place. Although upgrading your nodes is generally not necessary to follow an upgrade, we recommend following the Polkadot releases and upgrading in a timely manner, especially for high priority or critical releases. Transaction Version Upgrades ‚Äã Apart the runtime_version there is also the transaction_version which denotes how to correctly encode/decode calls for a given runtime (useful for hardware wallets). The reason transaction_version is separate from runtime_version is that it explicitly notes that the call interface is broken/not compatible. The transaction_version is updated in the cases mentioned in the Substrate docs . So when a new transaction version is introduced (during a runtime upgrade), it indicates a breaking change to transaction serialization. In that case, any custom application/tool that constructs & signs transactions should also be updated in order to be compatible with the new transaction version. It is the responsibility of the maintainers of the custom application/tool to keep up with the transaction_version updates. However, if you do not want to keep monitoring these changes yourself, you can also use the txwrapper-core tool that handles these breaking changes for you and allows you to construct transactions using the function names and chain metadata. Smart Contracts ‚Äã The Polkadot Relay Chain does not support smart contracts. Other Networks ‚Äã Besides running a private network, Polkadot has two other networks where you could test infrastructure prior to deploying to the Polkadot mainnet. Kusama Canary Network: Kusama is Polkadot's cutting-edge cousin. Many risky features are deployed to Kusama prior to making their way into Polkadot. Westend Testnet: Westend is Polkadot's testnet and uses the Polkadot runtime. Other F.A.Q. ‚Äã Can an account's balance change without a corresponding, on-chain transaction? No, but not all balance changes are in a transaction, some are in events. You will need to run an archive node and listen for events and transactions to track all account activity. This especially applies to locking operations if you are calculating balance as the spendable balance, i.e. free balance minus the maximum lock. What chain depth is considered "safe"? Polkadot uses a deterministic finality mechanism. Once a block is finalized, it cannot be reverted except by a hard fork. Kusama has had hard forks that had to revert four finalized blocks in order to cancel a runtime upgrade. Using a finalized depth of ten blocks should be safe. Note that block production and finality are isolated processes in Polkadot, and the chain can have a long unfinalized head. Do users need to interact with any smart contracts? No, users interact directly with the chain's logic. Does Polkadot have state rent? No, Polkadot uses the existential deposit to prevent dust accounts and other economic mechanisms like locking or reserving tokens for operations that utilize state. What is an external source to see the current chain height? Polkadot-JS explorer Polkascan block explorer Subscan block explorer Edit this page Last updated on Jun 30, 2023 by Filippo Previous Integration Initiation Next Integrating Assets Tokens Redenomination Addresses Cryptography Existential Deposit Free vs. Reserved vs. Locked vs. Vesting Balance Extrinsics and Events Block Format Extrinsics Transaction Mortality Unique Identifiers for Extrinsics Events Fees Encoding Runtime Upgrades Transaction Version Upgrades Smart Contracts Other Networks Other F.A.Q. ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Assets on Polkadot General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Integration Guide Integration Initiation Polkadot Protocol Integrating Assets Node Management Node Interaction Transaction Construction Tools Resources Maintain Build Integration Guide Integrating Assets On this page Assets on Polkadot The Polkadot Relay Chain does not natively support assets beyond DOT This functionality exists in parachains. On both Polkadot and Kusama, this parachain is called Asset Hub. The Asset Hub provides a first-class interface for creating, managing, and using both fungible and non-fungible assets. The fungible interface is similar to Ethereum's ERC-20 standard. However, the data structures and stateful operations are encoded directly into the chain's runtime, making operations fast and fee-efficient. Beyond merely supporting assets, integrating an Asset Hub into your systems has several benefits for infrastructure providers and users: Support for on-chain assets. Significantly lower transaction fees (about 1/10) than the Relay Chain. Significantly lower deposits (1/10) than the Relay Chain. This includes the existential deposit and deposits for proxy/multisig operations. Ability to pay transaction fees in certain assets. As in, accounts would not need DOT in order to exist on-chain nor to pay fees. The Asset Hub will use DOT as its native currency. Users can transfer DOT from the Relay Chain into the Asset Hub and use it natively. The Relay Chain will also accept DOT transfers from the Asset Hub back to the Relay Chain to use for staking, governance, or any other activity taking place there. Using the Asset Hub for DOT/KSM balance transfers will be much more efficient than the Relay Chain and is highly recommended. Until domain specific parachains are built, the Relay Chain will still need to be used for staking and governance. Assets Basics ‚Äã See the Assets pallet for the most up-to-date info and reference documentation. Assets are stored as a map from an ID to information about the asset, including a management team, total supply, total number of accounts, its sufficiency for account existence, and more. Additionally, the asset owner can register metadata like the name, symbol, and number of decimals for representation. Some assets, as determined by on-chain governance, are regarded as ‚Äúsufficient‚Äù. Sufficiency means that the asset balance is enough to create the account on-chain, with no need for the DOT/KSM existential deposit. Likewise, you cannot send a non-sufficient asset to an account that does not exist. Sufficient assets can be used to pay transaction fees (i.e. there is no need to hold DOT/KSM on the account). Assets do have a minimum balance (set by the creator), and if an account drops below that balance, the dust is lost. Asset Operations ‚Äã The Assets pallet has its own interface for dealing with assets. See the Integration section below for how to fetch information and construct transactions. The main functions you will probably interact with are transfer and transfer_keep_alive . These functions transfer some amount (balance) of an AssetId (a u32 , not a contract address) to another account. The Assets pallet also provides an approve_transfer , cancel_approval , and transfer_approved interface for non-custodial operations. Asset transfers will result in an assets.transferred event. The same instructions for monitoring events and not transactions applies to asset transfers. Note that you can use the same addresses (except pure proxies !) on the Asset Hub that you use on the Relay Chain. The SS58 encodings are the same, only the chain information (genesis hash, etc.) will change on transaction construction. Integration ‚Äã The Asset Hub will come with the same tooling suite that Parity Technologies provides for the Relay Chain, namely API Sidecar and TxWrapper Polkadot . If you have a technical question or issue about how to use one of the integration tools please file a GitHub issue so a developer can help. Parachain Node ‚Äã Using the Asset Hub will require running a parachain node to sync the chain. This is very similar to running a Polkadot node, with the addition of some extra flags. You can follow these guidelines to set up an Asset Hub node. Sidecar ‚Äã API Sidecar is a REST service for Relay Chain and parachain nodes; It comes with endpoints to query info about assets and asset balances on the Asset Hub. Asset lookups will always use the AssetId to refer to an asset class. On-chain metadata is subject to change and thus not suitable as a canonical index. Please refer to docs for full usage information. Details on options like how to make a historical query are not included here. Tx Wrapper Polkadot ‚Äã TxWrapper Polkadot is a library designed to facilitate transaction construction and signing in offline environments; it comes with a set of asset-specific functions to use on the Asset Hub. When constructing parachain transactions, you can use txwrapper-polkadot exactly as on the Relay Chain, but would construct transactions with the appropriate parachain metadata like genesis hash, spec version, and type registry. Edit this page Last updated on Jun 30, 2023 by Filippo Previous Polkadot Protocol Next Node Management Assets Basics Asset Operations Integration Parachain Node Sidecar Tx Wrapper Polkadot ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Node Management General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Integration Guide Integration Initiation Polkadot Protocol Integrating Assets Node Management Node Interaction Transaction Construction Tools Resources Maintain Build Integration Guide Node Management On this page Node Management This page contains basic information about running a Parity Polkadot client. There are a lot of ways to obtain/run a client, e.g. compiling from source, running in Docker, or downloading a binary. This guide will always refer to the executable as polkadot . Always refer to the client's help polkadot --help for the most up-to-date information. note Other client implementation teams: Feel free to make a PR to this page with instructions (or a link to instructions) for your client. If you are trying to run a validator, refer to this tutorial here . Basic Node Operations ‚Äã Selecting a chain Use the --chain <chainspec> option to select the chain. Can be polkadot , kusama , westend , rococo , or a custom chain spec. By default, the client will start Polkadot. Watch How a single codebase can power four different blockchains to learn more about how the chain selection works internally. Archive node An archive node does not prune any block or state data. Use the --pruning archive flag. Certain types of nodes like validators must run in archive mode. Likewise, all events are cleared from state in each block, so if you want to store events then you will need an archive node. Explainer video on upgrading a node To upgrade a node, please refer to this video Exporting blocks To export blocks to a file, use export-blocks . Export in JSON (default) or binary ( --binary true ). polkadot export-blocks --from 0 < output_file > RPC ports Use the --rpc-external flag to expose RPC ports and --ws-external to expose websockets. Not all RPC calls are safe to allow and you should use an RPC proxy to filter unsafe calls. Select ports with the --rpc-port and --ws-port options. To limit the hosts who can access, use the --rpc-cors option. Execution The Parity Polkadot client implements a Polkadot Host and a native runtime. The runtime must compile to WebAssembly and is stored on-chain. If the client's runtime is the same spec as the runtime that is stored on-chain, then the client will execute blocks using the client binary. Otherwise, the client will execute the Wasm runtime from the chain. Therefore, when syncing the chain, the client will execute blocks from past runtimes using their associated Wasm binary. This feature also allows forkless upgrades: the client can execute a new runtime without updating the client. Parity's Polkadot client has two Wasm execution methods, interpreted (default) and compiled. Set the preferred method to use when executing Wasm with --wasm-execution <Interpreted|Compiled> . Compiled execution will run much faster, especially when syncing the chain, but is experimental and may use more memory/CPU. A reasonable tradeoff would be to sync the chain with compiled execution and then restart the node with interpreted execution. File Structure ‚Äã The node stores a number of files in: /home/$USER/.local/share/polkadot/chains/<chain name>/ . You can set a custom path with --base-path <path> . keystore The keystore stores session keys, which are important for validator operations. Polkadot documentation Substrate documentation db The database stores blocks and the state trie. If you are running a validator node, it also stores GRANDPA pre-votes and pre-commits and the offchain-worker DB. Use caution when migrating validator nodes to avoid equivocation. If you want to start a new machine without resyncing, you can stop your node, back up the DB, and move it to a new machine. To delete your DB and re-sync from genesis, run: polkadot purge-chain Validators should sync using the RocksDb backend This is implicit by default, but can be explicit by passing the --database RocksDb flag. In the future, it is recommended to switch to using the faster and more efficient ParityDb option. Switching between database backends will require a resync. If you want to test out ParityDB you can add the flag --database paritydb . Monitoring and Telemetry ‚Äã Node status You can check the node's health via RPC with websocat : echo '{"id":1,"jsonrpc":"2.0","method":"system_health","params":[]}' | websocat -n1 -B 99999999 ws://127.0.0.1:9944 { "jsonrpc" : "2.0" , "result" : { "peers" :50, "isSyncing" :false, "shouldHavePeers" :true } , "id" :1 } Logs The Polkadot client has a number of log targets. The most interesting to users may be: afg (Al's Finality Gadget - GRANDPA consensus) babe telemetry txpool usage Other targets include: db, gossip, peerset, state-db, state-trace, sub-libp2p, trie, wasm-executor, wasm-heap . The log levels, from least to most verbose, are: error warn info debug trace All targets are set to info logging by default. You can adjust individual log levels using the --log (-l short) option, for example -l afg=trace,sync=debug or globally with -ldebug . Telemetry & Metrics The Parity Polkadot client connects to telemetry by default. You can disable it with --no-telemetry , or connect only to specified telemetry servers with the --telemetry-url option (see the help options for instructions). Connecting to public telemetry may expose information that puts your node at higher risk of attack. You can run your own, private telemetry server or deploy a substrate-telemetry instance to a Kubernetes cluster using this Helm chart . The node also exposes a Prometheus endpoint by default (disable with --no-prometheus ). Substrate has a monitor node metrics tutorial which uses this endpoint. Edit this page Last updated on Mar 1, 2023 by Filippo Previous Integrating Assets Next Node Interaction Basic Node Operations File Structure Monitoring and Telemetry ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Node Interaction General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Integration Guide Integration Initiation Polkadot Protocol Integrating Assets Node Management Node Interaction Transaction Construction Tools Resources Maintain Build Integration Guide Node Interaction On this page Node Interaction This page will guide you through some basic interactions with your node. This guide should guide you to the proper tools, not be seen as canonical reference. Always refer to the proper documentation for the tool you are using: Substrate RPC API Polkadot-JS RPC Substrate API Sidecar Polkadot-JS RPC is a JavaScript library for interacting with the Substrate RPC API endpoint, distributed as @polkadot/api Node.js package. Substrate API Sidecar is using the Polkadot-JS RPC to provide separately runnable REST services. Polkadot RPC ‚Äã The Parity Polkadot client exposes HTTP and WS endpoints for RPC connections. The default ports are 9933 for HTTP and 9944 for WS. To get a list of all RPC methods, the node has an RPC endpoint called rpc_methods . For example, using websocat : echo '{"id":1,"jsonrpc":"2.0","method":"rpc_methods","params":[]}' | websocat -n1 -B 99999999 ws://127.0.0.1:9944 { "jsonrpc" : "2.0" , "result" : { "methods" : [ "account_nextIndex" , "author_hasKey" , "author_hasSessionKeys" , "author_insertKey" , "author_pendingExtrinsics" , "author_removeExtrinsic" , "author_rotateKeys" , "author_submitAndWatchExtrinsic" , "author_submitExtrinsic" , "author_unwatchExtrinsic" , "babe_epochAuthorship" , "beefy_getFinalizedHead" , "beefy_subscribeJustifications" , "beefy_unsubscribeJustifications" , "chain_getBlock" , "chain_getBlockHash" , "chain_getFinalisedHead" , "chain_getFinalizedHead" , "chain_getHead" , "chain_getHeader" , "chain_getRuntimeVersion" , "chain_subscribeAllHeads" , "chain_subscribeFinalisedHeads" , "chain_subscribeFinalizedHeads" , "chain_subscribeNewHead" , "chain_subscribeNewHeads" , "chain_subscribeRuntimeVersion" , "chain_unsubscribeAllHeads" , "chain_unsubscribeFinalisedHeads" , "chain_unsubscribeFinalizedHeads" , "chain_unsubscribeNewHead" , "chain_unsubscribeNewHeads" , "chain_unsubscribeRuntimeVersion" , "childstate_getKeys" , "childstate_getKeysPaged" , "childstate_getKeysPagedAt" , "childstate_getStorage" , "childstate_getStorageEntries" , "childstate_getStorageHash" , "childstate_getStorageSize" , "grandpa_proveFinality" , "grandpa_roundState" , "grandpa_subscribeJustifications" , "grandpa_unsubscribeJustifications" , "mmr_generateBatchProof" , "mmr_generateProof" , "offchain_localStorageGet" , "offchain_localStorageSet" , "payment_queryFeeDetails" , "payment_queryInfo" , "state_call" , "state_callAt" , "state_getChildReadProof" , "state_getKeys" , "state_getKeysPaged" , "state_getKeysPagedAt" , "state_getMetadata" , "state_getPairs" , "state_getReadProof" , "state_getRuntimeVersion" , "state_getStorage" , "state_getStorageAt" , "state_getStorageHash" , "state_getStorageHashAt" , "state_getStorageSize" , "state_getStorageSizeAt" , "state_queryStorage" , "state_queryStorageAt" , "state_subscribeRuntimeVersion" , "state_subscribeStorage" , "state_traceBlock" , "state_trieMigrationStatus" , "state_unsubscribeRuntimeVersion" , "state_unsubscribeStorage" , "subscribe_newHead" , "sync_state_genSyncSpec" , "system_accountNextIndex" , "system_addLogFilter" , "system_addReservedPeer" , "system_chain" , "system_chainType" , "system_dryRun" , "system_dryRunAt" , "system_health" , "system_localListenAddresses" , "system_localPeerId" , "system_name" , "system_nodeRoles" , "system_peers" , "system_properties" , "system_removeReservedPeer" , "system_reservedPeers" , "system_resetLogFilter" , "system_syncState" , "system_unstable_networkState" , "system_version" , "unsubscribe_newHead" ] , "version" :1 } , "id" :1 } Note that this call will show even those RPC methods which are disabled by a safety flag like --rpc-methods Safe . This is being worked on . Add parameters in the call, for example get a block by its hash value: echo '{"id":1,"jsonrpc":"2.0","method":"chain_getBlock","params":["0x7d4ef171d483d37aa2339877524f0731af98e367c38f8fa27f133193ed2b5615"]}' | websocat -n1 -B 99999999 ws://127.0.0.1:9944 { "jsonrpc" : "2.0" , "result" : { "block" : { "header" : { "parentHash" : "0xb5e10293122a3c706dfcf5c0e89d5fb90929e7ee580c5167e439afa330fae2c7" , "number" : "0xbb07fe" , "stateRoot" : "0x872dfbb3516a6e3b9becf01bb2192e53a1d77ef6c37e426f03ebf64b33a68ede" , "extrinsicsRoot" : "0xe131e6af57c503ca6c6a151b2e621d05f65ef7be07e24abc2444fa1eb67c444a" , "digest" : { "logs" : [ "0x0642414245b50103b9000000ebdf8810000000002621c85fe312c4b8b9db111b9311a2857e265a62c7bd5a9b08f3e0989e51ea619481408decdc83f0f1322b706b50904f692f3c2dd505e7633dc029ca38a3f40072e7378760cf44e83566ec92ee330042d916684e957399badba91ed342a3270d" , "0x0542414245010190e94b9f1af95ae7645f85dc3d49f4c73dcce31083c9e1f712523a9b132aff798f89e0e6146429a869dde4ee060e7630831890f15942d5889ac4dfa24150368a" ] } } , "extrinsics" : [ "0x280403000bd61300888301" , "..." ] } , "justifications" :null } , "id" :1 } Some return values may not appear meaningful at first glance. Polkadot uses SCALE encoding as a format that is suitable for resource-constrained execution environments. You will need to decode the information and use the chain metadata ( state_getMetadata ) to obtain human-readable information. Tracking the Chain Head ‚Äã Use the RPC endpoint chain_subscribeFinalizedHeads to subscribe to a stream of hashes of finalized headers, or chain_FinalizedHeads to fetch the latest hash of the finalized header. Use chain_getBlock to get the block associated with a given hash. chain_getBlock only accepts block hashes, so if you need to query intermediate blocks, use chain_getBlockHash to get the block hash from a block number. Substrate API Sidecar ‚Äã Parity maintains an RPC client, written in TypeScript, that exposes a limited set of endpoints. It handles the metadata and codec logic so that you are always dealing with decoded information. It also aggregates information that an infrastructure business may need for accounting and auditing, e.g. transaction fees. The sidecar can fetch blocks, get the balance of an address atomically (i.e., with a corresponding block number), get the chain's metadata, get a transaction fee prediction, calculate outstanding staking rewards for an address, submit transactions to a node's transaction queue, and much more . The client runs on an HTTP host. The following examples use python3, but you can query any way you prefer at http://HOST:PORT/ . The default is http://127.0.0.1:8080 . Fetching a Block ‚Äã Fetch a block using the block/number endpoint. To get the chain tip, omit the block number. import requests import json url = 'http://127.0.0.1:8080/blocks/7409038' response = requests . get ( url ) if response . ok : block_info = json . loads ( response . text ) print ( block_info ) This returns a fully decoded block. In the balances.transfer extrinsic, the partialFee item is the transaction fee. It is called "partial fee" because the total fee would include the tip field. Notice that some extrinsics do not have a signature. These are inherents . Tracking transaction fees When tracking transaction fees, the extrinsics.paysFee value is not sufficient for determining if the extrinsic had a fee. This field only means that it would require a fee if submitted as a transaction. In order to charge a fee, a transaction also needs to be signed. So in the following example, the timestamp.set extrinsic does not pay a fee because it is an inherent, put in the block by the block author. { "number" : "7409038" , "hash" : "0x0e9610f3c89fac046ef83aa625ad414d5403031faa026b7ab2a918184e389968" , "parentHash" : "0xba308541eb207bc639f36d392706309a031c21622f883fb07411060389c5ffdd" , "stateRoot" : "0x4426383b64a944ad7222a4019aefd558c749da0c6920cfcdfd587741d54abbe2" , "extrinsicsRoot" : "0x74749e5f5aeb610bc23fd6d8d79fd8bbf5e4b6053f70ba94ea6b3cc271df4b3a" , "authorId" : "Fvvz6Ej1D5ZR5ZTK1vE1dCjBvkbxE1VncptEtmFaecXe4PF" , "logs" : [ { "type" : "PreRuntime" , "index" : "6" , "value" : [ "BABE" , "0x023a0200009c7d191000000000" ] } , { "type" : "Seal" , "index" : "5" , "value" : [ "BABE" , "0x2296a50fa4fea3a46a95ad5b1f09de76d22c6ed3dc6755718c976e2d14c63e4dd3c6257813d9bdc03bb180b1e20393f1558ae1204982e5c7570df393e11f908b" ] } ] , "onInitialize" : { "events" : [ ] } , "extrinsics" : [ { "method" : { "pallet" : "timestamp" , "method" : "set" } , "signature" : null , "nonce" : null , "args" : { "now" : "1620636072000" } , "tip" : null , "hash" : "0x8b853f49b6543e4fcbc796ad3574ea5601d2869d80629e080e501da4cb7b74b4" , "info" : { } , "events" : [ { "method" : { "pallet" : "system" , "method" : "ExtrinsicSuccess" } , "data" : [ { "weight" : "185253000" , "class" : "Mandatory" , "paysFee" : "Yes" } ] } ] , "success" : true , "paysFee" : false } , { "method" : { "pallet" : "balances" , "method" : "transfer" } , "signature" : { "signature" : "0x94b63112648e8e692f0076fa1ccab3a04510c269d1392c1df2560503865e144e3afd578f1e37e98063b64b98a77a89a9cdc8ade579dcac0984e78d90646a052001" , "signer" : { "id" : "Gr5sBB1EgdmQ7FG3Ud2BdECWQTMDXNgGPfdHMMtDsmT4Dj3" } } , "nonce" : "12" , "args" : { "dest" : { "id" : "J6ksma2jVeHRcRoYPZBkJRzRbckys7oSmgvjKLrVbj1U8bE" } , "value" : "100000000" } , "tip" : "0" , "hash" : "0xfbc5e5de75d64abe5aa3ee9272a3112b3ce53710664f6f2b9416b2ffda8799c2" , "info" : { "weight" : "201217000" , "class" : "Normal" , "partialFee" : "2583332634" } , "events" : [ { "method" : { "pallet" : "balances" , "method" : "Transfer" } , "data" : [ "Gr5sBB1EgdmQ7FG3Ud2BdECWQTMDXNgGPfdHMMtDsmT4Dj3" , "J6ksma2jVeHRcRoYPZBkJRzRbckys7oSmgvjKLrVbj1U8bE" , "100000000" ] } , { "method" : { "pallet" : "balances" , "method" : "Deposit" } , "data" : [ "Fvvz6Ej1D5ZR5ZTK1vE1dCjBvkbxE1VncptEtmFaecXe4PF" , "2583332634" ] } , { "method" : { "pallet" : "system" , "method" : "ExtrinsicSuccess" } , "data" : [ { "weight" : "201217000" , "class" : "Normal" , "paysFee" : "Yes" } ] } ] , "success" : true , "paysFee" : true } , { "method" : { "pallet" : "utility" , "method" : "batch" } , "signature" : { "signature" : "0x8aa2fc3f0cff52533745679523705720cff42d0e7258b9797feed193deb0ca73474726e148af0a0b096d44c07f20e5292819ec92279cffb2897e95cc337e638e" , "signer" : { "id" : "F4gmSZGiM9pMYPsKW7xnGktDr4zRmN2jqy5Ze678y9YWR7F" } } , "nonce" : "687" , "args" : { "calls" : [ { "method" : { "pallet" : "staking" , "method" : "payoutStakers" } , "args" : { "validator_stash" : "Cfish3zJiFnTvR9jscCap7imeA9ep3cH1wZfcZwAp2gdZHo" , "era" : "2229" } } , { "method" : { "pallet" : "staking" , "method" : "payoutStakers" } , "args" : { "validator_stash" : "Cfish3zJiFnTvR9jscCap7imeA9ep3cH1wZfcZwAp2gdZHo" , "era" : "2230" } } , { "method" : { "pallet" : "staking" , "method" : "payoutStakers" } , "args" : { "validator_stash" : "Cfish3zJiFnTvR9jscCap7imeA9ep3cH1wZfcZwAp2gdZHo" , "era" : "2231" } } , { "method" : { "pallet" : "staking" , "method" : "payoutStakers" } , "args" : { "validator_stash" : "DifishR4auphofhzxsy2aupgYo4NaUECH7qgt71CgiB2o6P" , "era" : "2231" } } , { "method" : { "pallet" : "staking" , "method" : "payoutStakers" } , "args" : { "validator_stash" : "J1fishfH94nFZLNScHgC2HorWpFD2xdPxd96wtTCHLvKxfa" , "era" : "2231" } } ] } , "tip" : "0" , "hash" : "0x69171ec3f4e5e4dfd27f4d1c5b5dbc884932c5d9a078c84495bb7ab875c8785f" , "info" : { "weight" : "629782467000" , "class" : "Normal" , "partialFee" : "5150837715" } , "events" : [ { "method" : { "pallet" : "staking" , "method" : "Reward" } , "data" : [ "Cfish3zJiFnTvR9jscCap7imeA9ep3cH1wZfcZwAp2gdZHo" , "40730624074" ] } , { "method" : { "pallet" : "staking" , "method" : "Reward" } , "data" : [ "FhLcXuFkTwyc3o9K82VBahpain1YHWyGeNMDTTyeDJKfm5b" , "4296071738" ] } , { "method" : { "pallet" : "staking" , "method" : "Reward" } , "data" : [ "F1NyXFUayqmVMdjNK45hcaTCE3JiqdU83sEGhQ3HQXn2Rpq" , "1770904403" ] } , // . . . { "method" : { "pallet" : "utility" , "method" : "BatchCompleted" } , "data" : [ ] } , { "method" : { "pallet" : "balances" , "method" : "Deposit" } , "data" : [ "Fvvz6Ej1D5ZR5ZTK1vE1dCjBvkbxE1VncptEtmFaecXe4PF" , "5150837715" ] } , { "method" : { "pallet" : "system" , "method" : "ExtrinsicSuccess" } , "data" : [ { "weight" : "629782467000" , "class" : "Normal" , "paysFee" : "Yes" } ] } ] , "success" : true , "paysFee" : true } ] , "onFinalize" : { "events" : [ ] } , "finalized" : true } The JS number type is a 53 bit precision float There is no guarantee that the numerical values in the response will have a numerical type. Any numbers larger than 2**53-1 will have a string type. Submitting a Transaction ‚Äã Submit a serialized transaction using the transaction endpoint with an HTTP POST request. import requests import json url = 'http://127.0.0.1:8080/transaction/' tx_headers = { 'Content-type' : 'application/json' , 'Accept' : 'text/plain' } response = requests . post ( url , data = '{"tx": "0xed0...000"}' , # A serialized tx. headers = tx_headers ) tx_response = json . loads ( response . text ) If successful, this endpoint returns a JSON with the transaction hash. In case of error, it will return an error report, e.g.: { "error": "Failed to parse a tx" | "Failed to submit a tx", "cause": "Upstream error description" } Edit this page Last updated on Oct 12, 2022 by Keith Alfaro Previous Node Management Next Transaction Construction Polkadot RPC Tracking the Chain Head Substrate API Sidecar Fetching a Block Submitting a Transaction ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Transaction Construction and Signing General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Integration Guide Integration Initiation Polkadot Protocol Integrating Assets Node Management Node Interaction Transaction Construction Tools Resources Maintain Build Integration Guide Transaction Construction On this page Transaction Construction and Signing This page will discuss the transaction format in Polkadot and how to create, sign, and broadcast transactions. Like the other pages in this guide, this page demonstrates some of the available tools. Always refer to each tool's documentation when integrating. Transaction Format ‚Äã Polkadot has some basic transaction information that is common to all transactions. Address: The SS58-encoded address of the sending account. Block Hash: The hash of the checkpoint block. Block Number: The number of the checkpoint block. Genesis Hash: The genesis hash of the chain. Metadata: The SCALE-encoded metadata for the runtime when submitted. Nonce: The nonce for this transaction. * Spec Version: The current spec version for the runtime. Transaction Version: The current version for transaction format. Tip: Optional, the tip to increase transaction priority. Era Period: Optional, the number of blocks after the checkpoint for which a transaction is valid. If zero, the transaction is immortal caution There are risks to making a transaction immortal. If an account is reaped and a user re-funds the account, then they could replay an immortal transaction. Always default to using a mortal extrinsic. * The nonce queried from the System module does not account for pending transactions. You must track and increment the nonce manually if you want to submit multiple valid transactions at the same time. Each transaction will have its own (or no) parameters to add. For example, the transferKeepAlive function from the Balances pallet will take: dest : Destination address #[compact] value : Number of tokens (compact encoding) Serialized transaction format Before being submitted, transactions are serialized. Serialized transactions are hex encoded SCALE-encoded bytes. The Polkadot runtimes are upgradable and therefore any interfaces are subject to change, the metadata allows developers to structure any extrinsics or storage entries accordingly. This being said, the serialization format can be described as follows: Compact encoded number of SCALE encoded bytes following this. 1 bit: it is a 0 if no signature is present, or a 1 if it is. 7 bits: the extrinsic version, it is equal to 4 in decimal. 4 bytes: Spec version of the runtime. 4 bytes: Transaction version of the runtime. 32 bytes: Genesis hash of the chain. 32 bytes: Block hash serving as the era reference. If the transaction is immortal, then this would be the genesis hash. If there is a signature: a SCALE encoded sp_runtime::MultiAddress::Id<AccountId32, u32> indicating the signer(s) of the transaction. a SCALE encoded sp_runtime::MultiSignature::{SigningScheme} with the signature * . a SCALE encoded sp_runtime::generic::Era indicating for how long this transaction is valid: If the transaction is immortal, the Era would be simply 0. Otherwise, it would be a Vec[u64, u64] comprising the period and the phase. Compact encoded u32 with the nonce. Compact encoded u128 with the tip paid to the block producer. a SCALE encoded sp_runtime::traits::SignedExtension<Vec<Text>> with the additional data and logic associated with this transaction. The specific transaction parameters or call data, which consists of: 1 byte: the pallet index the transaction is calling into. 1 byte: the function in the pallet the transaction is calling. variable: the SCALE-encoded parameters required by the function being called. The metadata provides you with all of the information required to know how to construct the serialized call data specific to your transaction. You can read more about the metadata, its format and how to get it in the Substrate documentation . * Polkadot supports sr25519, ed25519, and ECDSA as signing schemes. Summary Once you have all the necessary information, you will need to: Construct an unsigned transaction. Create a signing payload. Sign the payload. Serialize the signed payload into a transaction. Submit the serialized transaction. Parity provides the following tools to help perform these steps. Polkadot-JS Tools ‚Äã Polkadot-JS Tools contains a set of command line tools for interacting with a Substrate client, including one called "Signer CLI" to create, sign, and broadcast transactions. This example will use the signer submit command, which will create and submit the transaction. The signer sendOffline command has the exact same API, but will not broadcast the transaction. submit and sendOffline must be connected to a node to fetch the current metadata and construct a valid transaction. Their API has the format: yarn run:signer < submit | sendOffline > --account < from-account-ss5 8 > --ws < endpoint > < module.method > [ param1 ] [ .. . ] [ paramX ] Signing: yarn run:signer sign --account < from-account-ss5 8 > --seed < seed > --type < sr25519 | ed2551 9 > < payload > For example, let's send 0.5 DOT from 121X5bEgTZcGQx5NZjwuTjqqKoiG8B2wEAvrUFjuw24ZGZf2 to 15vrtLsCQFG3qRYUcaEeeEih4JwepocNJHkpsrqojqnZPc2y . yarn run:signer submit --account 121X5bEgTZcGQx5NZjwuTjqqKoiG8B2wEAvrUFjuw24ZGZf2 --ws ws://127.0.0.1:9944 balances.transferKeepAlive 15vrtLsCQFG3qRYUcaEeeEih4JwepocNJHkpsrqojqnZPc2y 5000000000 This will return a payload to sign and an input waiting for a signature. Take this payload and use your normal signing environment (e.g. air gapped machine, VM, etc.). Sign the payload: yarn run:signer sign --account 121X5bEgTZcGQx5NZjwuTjqqKoiG8B2wEAvrUFjuw24ZGZf2 --seed "pulp gaze fuel ... mercy inherit equal" --type sr25519 0x040300ff4a83f1 .. .a8239139ff3ff7c3f6 Save the output and bring it to the machine that you will broadcast from, enter it into submit 's signature field, and send the transaction (or just return the serialized transaction if using sendOffline ). Tx Wrapper ‚Äã If you do not want to use the CLI for signing operations, Parity provides an SDK called TxWrapper Core to generate and sign transactions offline. For Polkadot, Kusama, and select parachains, use the txwrapper-polkadot package. Other Substrate-based chains will have their own txwrapper-{chain} implementations. See the examples for a guide. Import a private key import { importPrivateKey } from '@substrate/txwrapper-polkadot' ; const keypair = importPrivateKey ( ‚Äúpulp gaze fuel ... mercy inherit equal‚Äù ) ; Derive an address from a public key import { deriveAddress } from '@substrate/txwrapper-polkadot' ; // Public key, can be either hex string, or Uint8Array const publicKey = ‚Äú 0x2ca17d26ca376087dc30ed52deb74bf0f64aca96fe78b05ec3e720a72adb1235 ‚Äù ; const address = deriveAddress ( publicKey ) ; Construct a transaction offline import { methods } from "@substrate/txwrapper-polkadot" ; const unsigned = methods . balances . transferKeepAlive ( { dest : "15vrtLsCQFG3qRYUcaEeeEih4JwepocNJHkpsrqojqnZPc2y" , value : 5000000000 , } , { address : "121X5bEgTZcGQx5NZjwuTjqqKoiG8B2wEAvrUFjuw24ZGZf2" , blockHash : "0x1fc7493f3c1e9ac758a183839906475f8363aafb1b1d3e910fe16fab4ae1b582" , blockNumber : 4302222 , genesisHash : "0xe3777fa922cafbff200cadeaea1a76bd7898ad5b89f7848999058b50e715f636" , metadataRpc , // must import from client RPC call state_getMetadata nonce : 2 , specVersion : 1019 , tip : 0 , eraPeriod : 64 , // number of blocks from checkpoint that transaction is valid transactionVersion : 1 , } , { metadataRpc , registry , // Type registry } ) ; Construct a signing payload import { methods , createSigningPayload } from '@substrate/txwrapper-polkadot' ; // See "Construct a transaction offline" for "{...}" const unsigned = methods . balances . transferKeepAlive ( { ... } , { ... } , { ... } ) ; const signingPayload = createSigningPayload ( unsigned , { registry } ) ; Serialize a signed transaction import { createSignedTx } from "@substrate/txwrapper-polkadot" ; // Example code, replace `signWithAlice` with actual remote signer. // An example is given here: // https://github.com/paritytech/txwrapper-core/blob/b213cabf50f18f0fe710817072a81596e1a53cae/packages/txwrapper-core/src/test-helpers/signWithAlice.ts const signature = await signWithAlice ( signingPayload ) ; const signedTx = createSignedTx ( unsigned , signature , { metadataRpc , registry } ) ; Decode payload types You may want to decode payloads to verify their contents prior to submission. import { decode } from "@substrate/txwrapper-polkadot" ; // Decode an unsigned tx const txInfo = decode ( unsigned , { metadataRpc , registry } ) ; // Decode a signing payload const txInfo = decode ( signingPayload , { metadataRpc , registry } ) ; // Decode a signed tx const txInfo = decode ( signedTx , { metadataRpc , registry } ) ; Check a transaction's hash import { getTxHash } from ‚Äò @ substrate / txwrapper - polkadot‚Äô ; const txHash = getTxHash ( signedTx ) ; Submitting a Signed Payload ‚Äã There are several ways to submit a signed payload: Signer CLI ( yarn run:signer submit --tx <signed-transaction> --ws <endpoint> ) Substrate API Sidecar RPC with author_submitExtrinsic or author_submitAndWatchExtrinsic , the latter of which will subscribe you to events to be notified as a transaction gets validated and included in the chain. Notes ‚Äã Some addresses to use in the examples. See Subkey documentation . $ subkey --network polkadot generate Secret phrase ` pulp gaze fuel .. . mercy inherit equal ` is account: Secret seed: 0x57450b3e09ba4598 .. . .. . .. . .. . .. . .. . .. . .. 219756eeba80bb16 Public key ( hex ) : 0x2ca17d26ca376087dc30ed52deb74bf0f64aca96fe78b05ec3e720a72adb1235 Account ID: 0x2ca17d26ca376087dc30ed52deb74bf0f64aca96fe78b05ec3e720a72adb1235 SS58 Address: 121X5bEgTZcGQx5NZjwuTjqqKoiG8B2wEAvrUFjuw24ZGZf2 $ subkey --network polkadot generate Secret phrase ` exercise auction soft .. . obey control easily ` is account: Secret seed: 0x5f4bbb9fbb69261a .. . .. . .. . .. . .. . .. . .. . .. 4691ed7d1130fbbd Public key ( hex ) : 0xda04de6cd781c98acf0693dfb97c11011938ad22fcc476ed0089ac5aec3fe243 Account ID: 0xda04de6cd781c98acf0693dfb97c11011938ad22fcc476ed0089ac5aec3fe243 SS58 Address: 15vrtLsCQFG3qRYUcaEeeEih4JwepocNJHkpsrqojqnZPc2y Edit this page Last updated on Dec 13, 2022 by Keith Alfaro Previous Node Interaction Next Tool Index Transaction Format Polkadot-JS Tools Tx Wrapper Submitting a Signed Payload Notes ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Tool Index General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Integration Guide Tools Tool Index Open Source Stack Resources Maintain Build Tools Tool Index On this page Tool Index Here, we provide a list of tools available for your development needs. They are sorted by context. If you're actively maintaining a tool that might be useful to other Polkadot, Kusama or Substrate developers, feel free to add it in . Wallets ‚Äã Please see the Wallets page. Block Explorers ‚Äã Polkadot-JS Apps Explorer - Polkadot dashboard block explorer. Supports dozens of other networks, including Kusama, Westend, and other remote or local endpoints. Access via IPFS Polkascan - Blockchain explorer for Polkadot, Kusama, and other related chains. Repo . Subscan - Blockchain explorer for Substrate chains. Repo . DotScanner - Polkadot & Kusama Blockchain explorer. 3xpl.com - Fastest ad-free universal block explorer and JSON API with Polkadot support. Blockchair.com - Universal blockchain explorer and search engine with Polkadot support. Polkaholic.io - Polkadot & Kusama Blockchain explorer with API and DeFi support across 40+ parachains. Blockchain Analytics ‚Äã Web3go - An open platform for everyone to play with, curate and visualize multi-blockchain data Polkawatch - Polkadot Decentralization Analytics Network Monitoring & Reporting ‚Äã Polkadot Telemetry Service - Network information including what nodes are running the chain, what software versions they are running, sync status, and location. Polkabot - Polkadot network monitoring and reporting using Matrix (Riot / Element) chat. Users may create custom bot plugins. Blogpost . Ryabina's Telegram Bot - A Telegram bot for monitoring on-chain events of Substrate chains. GitHub Repository PolkaStats - Polkadot network statistics (includes Kusama). Shows network information and staking details from validators and intentions. GitHub Repository . Panic - A node monitoring and alert server for validators. OpenWeb3/Guardian - A CLI tool and JS library to monitor on chain states and events. Clients ‚Äã Polkadot - The original Rust implementation of the Polkadot Host. Kagome - A C++ Polkadot implementation of the Polkadot Host developed by Soramitsu . Gossamer - A Go implementation of the Polkadot Host developed by ChainSafe Systems . TX Wrapper - Helper functions for offline transaction generation. Tools ‚Äã Substrate - Blockchain development platform written in Rust. The Rust version of the Polkadot Host is being built with Substrate. Substrate Docs - Comprehensive documentation and tutorials for building a blockchain using Substrate. Substrate VSCode plugin . Substrate Debug Kit - A collection of debug tools and libraries around substrate chains. Includes tools to calculate NPoS elections offline, disk usage monitoring, test templates against chain state and other pallet-specific helper. Diener - A tool for easy changing of Polkadot or Substrate dependency versions. Polkadot Launch - A tool to easily launch custom local parachain-enabled Polkadot versions. Halva - A Truffle-inspired local development environment for Substrate. Fork-off Substrate - Copies the state of an existing chain into your local version and lets you further experiment on it. srtool - A tool for verifying runtime versions against on-chain proposal hashes. sub-bench - A tool to spam your node with transactions for the sake of benchmarking. substrate-devhub-utils - A set of JavaScript utilities making life with Substrate a little easier. sub-flood - A tool to benchmark Substrate by flooding it with requests. UI ‚Äã Polkadash - VueJS-based starter kit for custom user interfaces for Substrate chains. Polkadot JS Apps UI - Repository of the polkadot.js.org/apps UI. Substrate Front-end Template - ReactJS-based starter UI for custom user interfaces for Substrate chains. Polkadot JS Browser Extension - Key management in a Chrome extension. Libraries ‚Äã Polkadot-JS API ‚Äã The Polkadot-JS API provides various utility functions that are used across all projects in the @polkadot namespace and is split into a number of internal utility packages. The documentation and usage instructions are provided at Polkadot-JS API Documentation . @polkadot/keyring This allows you to create and load accounts in JavaScript. It is helpful for creating wallets or any application that will require the user to write to chain. Examples . @polkadot/util Utility functions like checking if a string is hex-encoded. @polkadot/util-crypto Useful cryptographic utilities for developing with Polkadot. Alternative Libraries ‚Äã The following libraries/SDKs allow for interfacing with a Substrate node in other languages: Python Library by Polkascan Go Library by Centrifuge Java Library by StrategyObject Fearless Utils iOS Swift SDK Nova Wallet iOS/Swift Substrate SDK Kotlin SDK CLI Tools ‚Äã @polkadot/api-cli Command line interface for the polkadot API. Documentation . @polkadot/monitor-rpc An RPC monitor for Polkadot. See the RPC tools below for additional information. @polkadot/signer-cli A Tool to construct, sign, and broadcast transactions. Signing can be done offline. Polkadot API Cpp - A –°++ API for Polkadot, can build clip , a command line tool. Subkey - Command line utility for generating and inspecting key pairs. WASM ‚Äã WebAssembly related tools and projects. ink! - An eDSL to write WebAssembly based smart contracts using the Rust programming language. parity-wasm - Low-level WebAssembly format library. wasm-utils - Collection of WebAssembly utilities used in pwasm-ethereum and substrate contract development. wasmi - A WebAssembly interpreter conceived as a component of parity-ethereum (Ethereum-like contracts in Wasm) and Substrate. RPC and API Tools ‚Äã @polkadot/api/rpc-provider Demonstrates how the JS tools interact with the node over RPC. RPC documentation - Documentation of Substrate RPC methods. Polkadot API Server by SimplyVC - A wrapper around the Polkadot API which makes it easier to make Polkadot API calls from any programming language. Go: Subscan API - Go API for Polkadot. C++ Polkadot API - –°++ API for Polkadot. .NET Toolchain for Polkadot/Substrate API - Toolchain to generate Polkadot API & Service Layer for .NET (usable in Unity). .NET Polkadot API - Polkadot Substrate API for .NET. Python Polkadot API - Polkadot library for Python. GSRPC - Substrate RPC client in Go, a.k.a. GSRPC. Substrate API Sidecar - An HTTP wrapper for Substrate, abstracting some complex RPC calls into simple REST calls. Subxt - A Rust library to submit extrinsics to a Substrate node via RPC. SCALE Codec ‚Äã The SCALE (Simple Concatenated Aggregate Little-Endian) Codec is a lightweight, efficient, binary serialization and deserialization codec. It is designed for high-performance, copy-free encoding and decoding of data in resource-constrained execution contexts, such as the Substrate runtime. It is not self-describing in any way and assumes the decoding context has all type knowledge about the encoded data. It is used in almost all communication to/from Substrate nodes, so implementations in different languages exist: Substrate Awesome maintains a list of SCALE codex implementations. Data Crawling and Conversion ‚Äã The following tools help you extract and structure data from a Substrate node. Polkascan PRE Harvester ( matching explorer for harvested data ) - Transforms raw blockchain data into relational data. Parity's Substrate Archive - Can be run alongside a Substrate node to archive all blocks, state, and extrinsic data into PostgreSQL database. Subsquid Archives - Generate a GraphQL database from a Substrate chain's data with rich filtering and querying capabilities. The data (events, extrinsics, blocks) for most parachains is readily available though public archive GraphQL endpoints, kept in Squid Archive Registry Polka-store - A tool which scans a Substrate chain and stores balance-relevant transactions in an SQLite database. Substrate-graph - A compact indexer for Substrate based nodes providing a GraphQL interface. Edit this page Last updated on Jun 23, 2023 by Filippo Previous Transaction Construction Next Open Source Stack Wallets Block Explorers Blockchain Analytics Network Monitoring & Reporting Clients Tools UI Libraries Polkadot-JS API Alternative Libraries CLI Tools WASM RPC and API Tools SCALE Codec Data Crawling and Conversion ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Open Source Stack General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Integration Guide Tools Tool Index Open Source Stack Resources Maintain Build Tools Open Source Stack On this page Open Source Polkadot Stack Do your research before using open-source tools The tools listed here are open-source and are linked directly to their source code. Before using these tools to build your projects, always do your research and be aware of scams . The goal of this page is to provide an overview of the open-source Polkadot/Kusama Tech Stack. This is a living document and we are relying on our community to contribute to it and help maintain it. Please feel free to make edits and additions via pull requests . We apologize if we missed your project! About Layers of Polkadot Stack User Interface Tools, APIs and Languages ink Smart Contracts Chains and Pallets Host Network Maintenance Tools Signatures Consensus Networking Primitives Contributing About ‚Äã The Polkadot Tech Stack is a subset of the Web 3.0 Tech Stack, which consists of the open-source technologies contributing to and relying on Polkadot , Kusama and Substrate . It is meant to be used for decentralized application (Dapp) development within numerous verticals including DeFi, Gaming, Provenance and many others not pictured below. |------|--------|------------| | DeFi | Gaming | Provenance | |______|________|____________| Dapps |--------------------------/-| | Explorers, Wallets / | |------------------------/---| | Tools, Apis, Languages/ | |----------------------/-----| | 2nd layer protocols / | |--------------------/-------| | Chains / other | |------------------/--- --| | *Polkadot* | tech | |------------------\---------| | P2P, Crypto, Wasm \ | |--------------------\-------| Layers of Polkadot Stack ‚Äã In the below sections you can find a list of different layers of the Polkadot Stack. Maintenance Status : üü¢ Actively maintained üü° Stale (no activity on the main branch for one month) ‚ö™ Unmaintained (no activity on the main branch for more than three months) User Interface ‚Äã Components Existing projects Potentially interesting projects Desktop/Web Wallets polkadot-js/apps üü¢, Talisman Web Application üü¢, mydotwallet üü¢, Sub ID üü¢, AirGap üü¢, Primis ‚ö™, Sakura ‚ö™, Omni desktop ‚ö™, Web3Box ‚ö™, Coong Wallet üü¢ User-friendly Wallet based on the Recovery Pallet , Web wallets focused on user-onboarding (e.g. using localStorage ), Enterprise Wallets Browser Extensions Talisman-Extension üü¢, SubWallet-Extension üü¢, Enkrypt üü¢, Polkadot{.js} üü¢, Polkadot-Js-Plus-Extension ‚ö™, Doter ‚ö™, Speckle OS ‚ö™, Kuma Cross-chain Wallet üü¢ Sign-in with your polkadot, kusama, etc. account. Mobile Wallets SubWallet-Mobile üü°, Lunie ‚ö™, Polkawallet ‚ö™, Parity Signer üü¢, imToken ‚ö™, Fearless Wallet Android üü¢, Fearless Wallet iOS üü¢, Stylo ‚ö™, Nova Wallet üü¢, Fractapp ‚ö™, Interstellar Network üü¢, Hashed Wallet ‚ö™ Burner Wallets/Faucet/Gifts dotdrop üü¢, KodaDot ‚ö™, Astar Faucet Bot üü¢, Generic sybil-resistant faucet ‚ö™ Faucet (a sybil-resistant way to receive free tokens) Wallet Plugins Metamask-Snap by Chainsafe üü¢ CLI Wallet Subwallet ‚ö™, Proxy-hot-wallet ‚ö™ Multisignature Wallets Subscan Multisig UI - React üü¢, Subscan Multisig UI ‚ö™, Dorafactory-Multisig ‚ö™, Multix üü¢ Hardware Wallets Ledger Polkadot üü¢, Ledger Kusama üü¢, Ledger Statemint üü¢, Ledger Statemine üü¢ Trezor Block Explorers Calamar üü¢, Polkaholic üü¢, Polkascan üü°, Polkastats ‚ö™, Subscan üü¢, Statescan ‚ö™, Edgscan ‚ö™, Sirato üü¢, Ink! Explorer API ‚ö™, Substats ‚ö™, Hybrid Block Explorer üü° Mempool focused explorer (including parachain transaction) Validator Dashboards Polkadot Telemetry üü¢, Polkacube ‚ö™, YieldScan ‚ö™, Hubble ‚ö™, Cyclops üü°, Web3Go üü¢ Node Explorers Polkadot Node Explorer ‚ö™ NFT Explorer NFT Explorer for Kusama & Polkadot üü¢ Governance Dashboards Polkadot Delegation Dashboard üü¢, Polkassembly ‚ö™, dotreasury üü¢, Bright Treasury ‚ö™, OpenSquare offchain voting üü¢ UI for the kusama and/or polkadot treasury (see bounty module ), UI for Parachain Lease Offering (PLO) Staking Staking Rewards Collector ‚ö™, Staking Rewards Viewer ‚ö™, Polkadot Staking Site ‚ö™, Polkadot Staking Dashboard üü¢, Polkadot/Kusama Validator Selector ‚ö™, Staking Income CSV Generator ‚ö™ Bridge UI Parity Bridges UI ‚ö™, Donut Interface (Steem - Dot) ‚ö™ Parachain/Crowdloan Parachains.Network ‚ö™, PolkAuction ‚ö™, Crowdloan Front End Template üü°, Slothunter üü¢ Identicon PolkadotWebIdenticon ‚ö™, Polkadot Angular IdentIcon ‚ö™, Bird Identicon ‚ö™ Other KappaSigmaMu Fratority üü¢, Quadratic Funding Webapp ‚ö™, Polkawatch , Bytepay ‚ö™, charging-management-platform ‚ö™, subidentity-webapp ‚ö™, OpenSquare Paid QA üü°, DotPulse ‚ö™, Rubeus Keeper ‚ö™, Polkaflow üü° Portfolio Viewer like Zapper or Zerion Tools, APIs and Languages ‚Äã Components Existing projects Potentially interesting projects Runtime/Parachain frameworks Substrate üü¢, Gosemble üü¢, Subsembly ‚ö™, Parachain utilities ‚ö™, Gantree ‚ö™ Tools to create parachains from frameworks used in other ecosystems Client Libraries Capi - Typescript üü¢, sub-api üü¢, Go üü¢, .Net ‚ö™, .NET Standard 2.0 üü°, C++ ‚ö™, C ‚ö™, Haskell üü°, Javascript üü¢, Substrate API Sidecar - TypeScript üü¢, Python üü¢, Java (+ Android) ‚ö™, Substrate Client Java ‚ö™, Rust SCS üü¢, Rust Parity (subxt) üü¢, Rust pdotc PHP (gmajor-encrypt) ‚ö™, PHP (neha0921) ‚ö™, RPC-Ethereum üü¢, Swift üü¢, Kotlin ‚ö™, substrate-client-kotlin ‚ö™, substrate-client-swift üü°, Dart üü¢, Substrate Core Polywrapper ‚ö™ Substrate Contract clients PatractGo ‚ö™ SCALE Codec Rust üü¢, TypeScript üü¢, Python üü¢, Golang Chainsafe üü¢, Golang Itering üü¢, C ‚ö™, C++ ‚ö™, JavaScript üü¢, AssemblyScript ‚ö™, Haskell üü°, Java ‚ö™, Ruby ‚ö™, Dart ‚ö™, Swift üü¢, scale-codec-swift üü°, scale-codec-kotlin ‚ö™, PHP ‚ö™, JavaScript by Soramitsu ‚ö™, Scale Codec Comparator üü°, ScaleCodec.sol by Darwinia üü°, ScaleCodec.sol by Snowfork üü¢ Easy Runtime Development Subalfred üü¢, substrate-stencil üü°, Play Substrate üü°, substrate-node-template üü°, VS Code Plugin ‚ö™, Atom Code Plugin ‚ö™, Substrate Playground üü°, Substrate Marketplace VS Code Plugin ‚ö™, AssemblyScript Runtime Generation ‚ö™, Substrate Package Manager ‚ö™, Subsembly: Framework for developing AssemblyScript Substrate Runtimes ‚ö™, dependency diener ‚ö™ Easy Smart Contract Development Typechain Polkadot üü¢, ink-playground üü¢, DRink! üü¢, Ink! Remix Plugin ‚ö™, Signac ‚ö™, INK!athon üü¢, ink!-boxes ‚ö™, ink!-smart-contract-wizard ‚ö™, Polkadot Contract Wizard üü¢ Runtime/Pallet Security Substrate Toml Lint ‚ö™, K specifications üü¢, PolPatrol - Polkadot Runtime Checker ‚ö™ Automated Runtime checking tools, economic audit simulator such as gauntlet.network Smart Contract Languages Ask! üü¢, Subscript ‚ö™, Solang üü¢, Ink! üü¢, Move VM Substrate ‚ö™, Move smart contract by Neatcoin ‚ö™, Sol2Ink üü°, eBPF Contracts Hackathon ‚ö™ Functional Programming Languages, other languages with developed toolchains Smart Contract Security ink! Analyzer üü¢, Vanguard ‚ùî, Patron üü¢ Testing Subshell üü°, substrate-simnode üü¢, Halva ‚ö™, Ink Waterfall üü°, Redspot ‚ö™, MixBytes Tank ‚ö™, sub-flood ‚ö™, Substrate debug-kit ‚ö™, Dotscale - SCALE Codec Comparator ‚ö™, Asset CLI tool ‚ö™, sub_crash ‚ö™, subwasm üü¢, subsee ‚ö™, polkadot-lab ‚ö™, RPC-perf ‚ö™ Static Analysis Substrace ‚ö™, Static analyzer for Substrate FRAME's pallets ‚ö™, CoinFabrik Scout üü¢, Ink! Analyzer üü¢ Testnet Zombienet üü¢, Chopsticks üü¢, Polkadot Launch ‚ö™, polkadot-starship ‚ö™, Fork off Substrate ‚ö™, try-runtime-cli üü°, Parachain Launch üü¢ Benchmarking Clockchain ‚ö™, Substrate Graph Benchmarks ‚ö™, ink! & pallet benchmarking template ‚ö™ Blockchain Indexing Engine Substrate Archive ‚ö™, PSQL Indexer ‚ö™, Substrate Graph ‚ö™, Hydra ‚ö™, Subquery üü¢, Polkadot Profit Transformer üü° Blockchain/Event Monitoring Web3 Guardian ‚ö™, Aurras Event Manager üü°, @commonwealth/chain-events ‚ö™, Massbit ‚ö™, Polkadot Basic Notifications ‚ö™ Gaming Crossbow ‚ö™ Amethyst + Substrate No-code Platforms EzCode's Polkadot.js plugin on Bubble.io ‚ö™, Blackprint Visual Programming Polkadot.js module üü¢, SubRelay üü¢ XCM XCM-tools üü¢, XCM-tools Golang üü°, ParaSpell üü°, XBI üü¢, XCM TS/JS SDK ‚ö™ Wallets Talisman Connect ‚ö™, SubWallet-SubConnect üü¢, Metadata Portal üü¢, Tesseract ‚ö™, WalletConnect ‚ö™, BitGoJS üü¢ Other open-web3 JS library ‚ö™, VM-Bridge ‚ö™, srtool üü¢, srtool-cli üü¢, Substrate Tip Bot üü¢, ORI (Onchain Risk Intelligence) ‚ö™, PolkaTools ‚ö™, polkadot-scripts üü°, Sube ‚ö™, data-store-sidecar ‚ö™, SugarFunge üü°, substrate-wasmedge ‚ö™, EightFish üü¢ ink Smart Contracts ‚Äã Components Existing projects Potentially interesting projects Bridges Dante Protocol ‚ö™ DeFi ink_bank üü°, Polkadot AMM ‚ö™, Vera ‚ö™, Nsure Insurance ‚ö™, Everlasting Cash ‚ö™, Coinversation ‚ö™, zenlink-dex-contract ‚ö™, AlgoCash ‚ö™ New seigniorage-style stable coins Gaming Open Emoji Battler üü¢, NewOmega ‚ö™ DAO SyncraDAO üü°, subDAO ‚ö™, RainbowDAO ‚ö™, MangoBox ‚ö™ Identity/DID Dotflow üü¢ Spam Protection Prosopo ‚ö™ Tooling OpenBrush üü¢ Other Candle Auctions ‚ö™, polkasign-contract ‚ö™, OCEX ‚ö™, Roloi ‚ö™, MangoSale ‚ö™, ArtZero ‚ö™ Chains and Pallets ‚Äã Components Existing projects Potentially interesting projects Scalable Transactions Perun channels ‚ö™, CLI demo of Perun ‚ö™, Astar üü¢, Celer ‚ö™, Gunclear ‚ö™ roll-ups, DAG-based consensus mechanisms, side chains Bridges interBTC üü¢, DKG Substrate üü¢, ChainBridge üü¢, EOS by Bifrost ‚ö™, POA - Substrate ‚ö™, Substrate - Ethereum DAI Bridge ‚ö™, Substrate - Substrate Bridge ‚ö™, BTC by ChainX üü¢, Cosmos-Substrate bridge ‚ö™, Substrate IBC Pallet üü°, Polkadot Ethereum Bridge üü¢, Darwinia üü¢, Spacewalk: a Stellar bridge üü¢, Filecoindot ‚ö™ ZCash Privacy ZeroChain ‚ö™, xx network ‚ö™, pLibra (Phala Network) üü¢, Automata Network ‚ö™, zCloak Network ‚ùî, Zero Network üü¢, Silent Data üü° Multi-Asset Shielded Pool (MASP) , Zkay , Zexe ZKP ZeroPool ‚ö™, Megaclite ‚ö™, zkMega ‚ö™, PLONK for Substrate ‚ö™, Webb Anchor Protocol üü¢, zk-SNARKs tutorial ‚ö™, substrate-zk üü° TEE Acurast üü¢, Integritee üü¢, substraTEE ‚ö™ Keysafe Protocol ‚ö™ DeFi PrivaDEX ‚ö™, Fusotao üü¢, Reef ‚ö™, Diora üü°, Pendulum Chain üü¢, Compound Gateway ‚ö™, Parallel Finance üü¢, PINT ‚ö™, Laminar Chain ‚ö™, Acala üü¢, Centrifuge üü¢, Stafi üü°, Definex ‚ö™, OAX Foundation ‚ö™, Cybex ‚ö™, Zenlink ‚ö™, Swaps Pallet ‚ö™, Polkadex üü¢, SubDEX ‚ö™, HydraDX üü¢, Substrate Stablecoin ‚ö™, Standard protocol ‚ö™, Polkaswap üü¢, Curve AMM ‚ö™, Konomi Network ‚ö™, Stable Asset üü¢, Libra Payment ‚ö™, Mangata üü¢, Tidechain üü¢, Basilisk üü¢, Polymesh üü¢ DEX with privacy and confidentiality features such as those found in a dark pool Smart contract chains moonbeam üü¢, Edgeware ‚ö™, ParaState ‚ö™, gear üü¢, CENNZnet ‚ö™, SkyeKiwi ‚ö™, OAK-blockchain üü¢, ICE Blockchain ‚ö™, Polkadot Smart Chain ‚ö™, Madara - Cairo/Starknet üü¢ smart contract chains with novel security approaches, smart contract chains based on existing toolchains Oracle Laminar üü¢, Chainlink-polkadot ‚ö™, Ares Protocol ‚ö™, Kylin Network ‚ö™, interbtc-clients oracle üü¢, Anonima ‚ö™, SaaS3 ‚ö™, Tellor üü°, Bridgestate Oracle üü° Identity/DID Parami ‚ö™, Litentry üü¢, pallet-did ‚ö™, dot-id ‚ö™ IoT Nodle üü¢, MXC/DataHighway ‚ö™, peaq-network-node ‚ö™ Verifiable Claims KILT üü¢, Dock üü¢, Fennel Protocol üü¢ Supply chain DSCP Node üü° Health care Data Availability Avail üü° Social Networking Frequency üü¢, Social Network ‚ö™, SubSocial üü¢, ZeroDAO ‚ö™, Myriad Node üü¢, Wika Network ‚ö™, Project Liberty üü¢, Listen ‚ö™, Tribal Protocol ‚ö™, Five Degrees on Substrate ‚ö™, Acuity Social üü° Private instant messenger that uses on-chain identity Governance/DAO Hashed Network üü°, Sunshine DAO ‚ö™, Governance OS ‚ö™, Idavoll Network ‚ö™, Substrate Moloch ‚ö™, QRUCIAL-DAO üü¢, Societal üü¢, DAOs ‚ö™, Shivarthu üü¢, Faterium üü¢, Supersig üü°, GenesisDAO üü¢, DAO Entrance üü¢, Liberland üü¢ Consul - Open Government and E-Participation Web Software Prediction Markets and Futarchy Zeitgeist üü¢, X Predict Market ‚ö™ Messaging HOPR ‚ö™, Nolik ‚ö™, Uke üü°, Diffy Chat üü¢ File Storage, Cloud Subsocial-Offchain ‚ö™, DatDot ‚ö™, Crust Network üü¢, offchain::ipfs ‚ö™, Canyon Network üü¢, CESS üü¢, CESS Proving Subsystem ‚ö™, Iris ‚ö™, fmd-cess ‚ö™, IPFS Frame V3 ‚ö™, Threefold Chain üü¢, Apron ‚ö™, IPFS Utilities üü° Name Service Substrate Names ‚ö™, ENS on Substrate ‚ö™, PNS-Pallets üü°, Faceless ‚ö™, Anchor üü¢ Gaming Bit.country üü¢, SubGame ‚ö™, subzero ‚ö™, Web3Games ‚ö™, Ajuna Network üü¢, Gafi Network ‚ö™, Asylum üü¢, 3DPass üü¢, Polket ‚ö™ Computation/AI DeepBrain Chain üü°, AI Infrastructure on Blockchain ‚ö™ Enable specific use-cases Robonomics üü¢, UniversalDOT ‚ö™, Evercity Sustainable Finance Protocol ‚ö™, Fennel Protocol üü¢, logion üü¢ NFT ternoa üü°, FRAME Pallet: NFTs for Substrate ‚ö™, Unique NFT Parachain üü°, DNFT ‚ö™, RMRK-Substrate üü°, NT-NFTs ‚ö™, Green Lemon ‚ö™, Basilisk üü¢ Randomness DKG and Randomness Beacon ‚ö™, drand-substrate-client ‚ö™ Licensing Anagolay Network üü° Banking Integration FIAT on-off-ramp üü° Crowdfunding Imbue Network üü¢, Quadratic Funding pallet by Dora ‚ö™, Quadratic Funding pallet by OAK üü¢ Minimum Anti-Collusion Infrastructure (MACI) Collection of Pallets Substrate Open Runtime Module Library üü¢, warehouse üü°, InvArch FRAME Pallet Library üü° Marketplaces Dot Marketplace ‚ö™, Gated Marketplace üü°, Ventur ‚ö™ Carbon Credits BitGreen üü°, Carbon Assets Pallet ‚ö™, Sequester Pallets ‚ö™ UTXO Tuxedo üü° Other Substrate Account Filter ‚ö™, Subtensor üü¢, AdMeta ‚ö™, Chocolate Node ‚ö™, Virto Network üü¢, Substrate Validator Set üü¢, DEIP ‚ö™, DeBio üü¢, MathChain ‚ö™, encointer üü¢, Grassland üü°, Substrate-Tutorials üü°, Fair Squares ‚ö™, Totem Live Accounting ‚ö™, Escrow Pallet ‚ö™, TREX ‚ö™, Relation Graph ‚ö™, Decentralized Invoice ‚ö™, Redstone Network ‚ö™, Access Control Pallet ‚ö™, Omniverse DLT üü¢, ISMP üü¢ Decentralized review/reputation system Host ‚Äã Components Existing projects Potentially interesting projects Rust Substrate üü¢, Cumulus üü¢ C++ Kagome üü¢, Mayon ‚ö™ Go Gossamer üü¢ Java Java Host Research ‚ö™ AssemblyScript Light Client smoldot üü¢, Substrate Connect üü¢, C++ Polkadot Light Client ‚ö™ Testing Polkadot Conformance üü¢ Network Maintenance Tools ‚Äã Components Existing projects Potentially interesting projects Secure validator setup Polkadot Validation Node Ansible Setup ‚ö™, W3F Polkadot Validator Setup ‚ö™, polkadot-ansible ‚ö™ High availability setup Archipel ‚ö™, Polkadot Failover Mechanism ‚ö™, Datagen ‚ö™, High Availability Validator Setup ‚ö™ Load Balanced Endpoints terragrunt-polkadot ‚ö™, Geometry Labs' Substrate Meta repo ‚ö™ Deployment Tools Polkadot Package Manager ‚ö™, PolkaHub ‚ö™, Avado ‚ö™, Polkadot Deployer ‚ö™, Unified Collator Deployment ‚ö™ Validator monitoring ONE-T üü¢, SubVT üü¢, P.A.N.I.C. ‚ö™, Polkalert ‚ö™, B-Harvest ‚ö™, nmonpolkadot ‚ö™, Polkadot-K8s-Monitor ‚ö™, Polkadot-Watcher ‚ö™, 1KV Telegram Bot ‚ö™ Validator payout management Substrate validator auto payout ‚ö™, Polkadot Payouts ‚ö™, staking-payouts CLI üü¢, Payctl ‚ö™, crunch üü¢ Staking Miner Staking Miner v2 üü¢ Nominator Tools Validator Selection ‚ö™, Polkanalyzer üü¢, Polkanalyzer-app üü¢ Signatures ‚Äã Components Existing projects Potentially interesting projects SR25519 rust ‚ö™(contains partial bindings for C, JavaScript, and Python), .Net bindings ‚ö™, C ‚ö™ (old) , C üü° (new) , C/C++ üü°, C# ‚ö™, Go üü°, java ‚ö™, PHP ‚ö™ Signature Aggregation apk-proofs üü° Distributed key generation (DKG) or management keygen.rs ‚ö™, Secure Wallet Origin Distribution (SWORD) ‚ö™ Validator HSMs Zondax Remote Signer ‚ö™ Consensus ‚Äã Components Existing projects Potentially interesting projects PoC Spartan ‚ö™ PoW PoW consensus for Substrate üü¢, RandomX ‚ö™, Sha3 PoW ‚ö™ Block production BABE üü¢, Aura üü¢ Finality GRANDPA üü¢, AlephBFT üü¢ Other Nimbus: Upgradeable consensus framework üü¢ Networking ‚Äã Components Existing projects Potentially interesting projects DHT crawler Go ‚ö™, Kotlin ‚ö™ RPC Tor-like access WhiteNoise ‚ö™ Primitives ‚Äã Components Existing projects Potentially interesting projects Storage Merkle Tree DB üü° Merkle Proofs Solidity Trie Verifier üü¢ Contributing ‚Äã Pull requests, issues, or other contributions from the community are encouraged! You can not only add specific projects, but also potentially interesting fields/areas which are currently missing in the tech stack. ‚ùó All technologies listed above need to be open-source. Ideally, the links lead directly to the code. Note: You will need a GitHub account to suggest changes or open issues. If you do not have one, you may sign up for free . Edit this page Last updated on Jul 17, 2023 by github-actions[bot] Previous Tool Index Next Hackathons About Layers of Polkadot Stack User Interface Tools, APIs and Languages ink Smart Contracts Chains and Pallets Host Network Maintenance Tools Signatures Consensus Networking Primitives Contributing ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Hackathons General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Builder's Guides Development Guide Integration Guide Tools Resources Hackathons Maintain Build Resources Hackathons On this page Hackathons Join a Hackathon ‚Äã Web3 Foundation regularly supports developer focused events! Be the first to know by signing up for the Polkadot newsletter . Host a Hackathon ‚Äã As one of our objectives in the Web3 Foundation is promoting the Web3 technology stack , we would like to support any individual or team who wants to organize a hackathon that gets more developers to understand Polkadot and Substrate in your city or online. We are not only providing technical resources for you, but we can have someone from our team give a hands-on workshop/presentation about the technology to help participants get started hacking on it more quickly. Why you should host a hackathon? ‚Äã Connect with the Web3 Foundation team and larger community Support Web 3.0 education, opportunities, and more Learn the bleeding-edge technology Make lasting impact and gain connections in the community How do hackathons differ from workshops? ‚Äã Hackathons (AKA Buildathons) are aimed at participants who want to deliver their prototype and demonstrate their ideas to reality in a short period (few hours - one day), or online in a few weeks. Workshops are more for getting developers familiar with the tooling and understanding more about what are the resources currently available and how they can make use of it. How can you run a hackathon in your city? ‚Äã Get in touch with the W3F team by email or ask us via our Matrix chat. How we can help ‚Äã Provide technical resources/materials Help sourcing funding for facilitation and prizes. Finding a venue Marketing the event Ensure the hackathon runs smoothly Project & Event Theme Ideas ‚Äã NFTs Bonded tokens New governance pallets with different styles of voting & democracy Upgrade the TCR pallet IPFS integrations Decentralized identity solutions Oracle pallets Implement Generalized State Channel/Plasma Chain Decentralized Exchanges Tools for monitoring validator performance dApps across different parachains Previous Hackathons ‚Äã Here are a few examples of hackathons W3F has facilitated and supported: Polkadot Buildathon: India - This India-focused hackathon was a 9-week online event. Encode Hackathon - an externally facilitated event with a mix of learning challenges and prize pools for dApps as well as stand-alone blockchain project submissions. Hello World! by Polkadot - A collaborative hackathon celebrating the integration of Polkadot and Kusama into the Gitcoin platform. Has many sets of smaller challenges that will reward in either DOT or KSM. Hackusama - A 7-week online hackathon for developers to "Build a Blockchain" and submit your wildest tools, visualizations, and ideas to the "Open Hack" category. Resources ‚Äã Substrate Docs - many introductory learning material lives here, that can be a feature of you hackathon or suggested prerequisite for those joining to use. Support / Contact ‚Äã If you have any questions regarding organizing a hackathon, please free feel to contact us by email or ask us via Element chat. Edit this page Last updated on Mar 6, 2023 by github-actions[bot] Previous Open Source Stack Next Network Maintainers Join a Hackathon Host a Hackathon Why you should host a hackathon? How do hackathons differ from workshops? How can you run a hackathon in your city? How we can help Project & Event Theme Ideas Previous Hackathons Resources Support / Contact ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Network Maintainers General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides Governance Guides Maintain Network Maintainers On this page Network Maintainers Welcome to the network maintainers section of the Polkadot wiki. Here you will find information and guides to set up a node and run the network. Node ‚Äã Networks Guide - A list of the available Polkadot networks that you can connect to with a node. Set up a Full Node - Get up and started by syncing a full node for the Kusama network. The steps in the guide will broadly apply also to any Substrate-based network (like Polkadot). Set up Secure WebSocket - Set up a Secure WebSockets proxy server to safely access your node's RPC server Collator ‚Äã Learn about Collators - High level overview of collators and related links. Nominator ‚Äã Learn about Nominators - High level overview of nominators and related links. Nomination Guide (Polkadot) - Walkthrough on how to nominate on the Polkadot network. Nomination Guide (Kusama) - Walkthrough on how to nominate on the Kusama canary network. How to stop being a Nominator - Guide on how to stop nominating. Validator ‚Äã Learn about Validators - High level overview of validator and related links. Validator Payouts - Overview on how validator rewards are calculated and paid. Validation Guide (Polkadot) - Walkthrough on how to validate on the Polkadot network. Validation Guide (Kusama) - Walkthrough on how to validate on the Kusama canary network. Using systemd for the Validator Node - Configuring systemd with the Validator node. Secure Validator - Best tips and practices for validating. How to upgrade a Validator Node - Guide on upgrading your validator node. How to Chill - Walkthrough on how to chill as a validator. How to Stop Validating - Proper way to stop validating. Governance ‚Äã How to participate in Governance - Walkthrough on how to participate in governance. How to join the Council - Step by step guide for running for the Council. How to vote for a Councillor - Step by step guide for voting for your favorite councillors. Edit this page Last updated on Mar 1, 2023 by Filippo Previous Hackathons Next Parameters Node Collator Nominator Validator Governance ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Parameters General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides Governance Guides Maintain Parameters On this page Polkadot Parameters Many of these parameter values can be updated via on-chain governance. If you require absolute certainty of these parameter values, it is recommended you directly check the constants by looking at the chain state and/or storage . Periods of common actions and attributes ‚Äã NOTE: Polkadot generally runs at ¬º th the speed of Kusama, except in the time slot duration itself. See Kusama Parameters for more details on how Polkadot's parameters differ from Kusama's. Slot: 6 seconds * (generally one block per slot, although see note below) Epoch: 4 hours (2_400 slots x 6 seconds) Session: 4 hours (Session and Epoch lengths are the same) Era: 24 hours (6 sessions per Era, 2_400 slots x 6 epochs x 6 seconds) Polkadot Time Slots * Slot 6 seconds 1 Epoch 4 hours 2_400 Session 4 hours 2_400 Era 24 hours 14_400 * A maximum of one block per slot can be in a canonical chain. Occasionally, a slot will be without a block in the chain. Thus, the times given are estimates . See Consensus for more details. Accounts, Identity and Crowdloans ‚Äã The Existential Deposit is The deposit required to set an Identity is The minimum contribution required to participate in a crowdloan is Governance ‚Äã Democracy Time Slots Description Voting period 28 days 403_200 How long the public can vote on a referendum. Launch period 28 days 403_200 How long the public can select which proposal to hold a referendum on, i.e., every week, the highest-weighted proposal will be selected to have a referendum. Enactment period 28 days 403_200 Time it takes for a successful referendum to be implemented on the network. Council Time Slots Description Term duration 7 days 100_800 The length of a council member's term until the next election round. Voting period 7 days 100_800 The council's voting period for motions. The Polkadot Council consists of up to 13 members and up to 20 runners up. Technical committee Time Slots Description Cool-off period 7 days 100_800 The time a veto from the technical committee lasts before the proposal can be submitted again. Emergency voting period 3 hours 1_800 The voting period after the technical committee expedites voting. Staking, Validating, and Nominating ‚Äã The maximum number of validators that can be nominated by a nominator is . Polkadot Time Slots Description Term duration 1 Day 14_400 The time for which a validator is in the set after being elected. Note, this duration can be shortened in the case that a validator misbehaves. Nomination period 1 Day 14_400 How often a new validator set is elected according to Phragm√©n's method. Bonding duration 28 days 403_200 How long until your funds will be transferrable after unbonding. Note that the bonding duration is defined in eras, not directly by slots. Slash defer duration 28 days 403_200 Prevents overslashing and validators "escaping" and getting their nominators slashed with no repercussions to themselves. Note that the bonding duration is defined in eras, not directly by slots. Parachains ‚Äã Crowdloans and Auctions Time Slots Description Auction duration 7 days 100_800 The total duration of the slot auction, subject to the candle auction mechanism. Opening period 2 days 28_800 The opening period of the slot auction. Ending period 5 days 72_000 The ending period of the slot auction. Parachain Slot Time Slots Description Lease period 12 weeks 1_209_600 The length of one lease period in a parachain slot. Total slot duration 2 years 10_512_000 The maximum duration a parachain can lease by winning a slot auction (8 lease periods). Treasury ‚Äã Treasury Time Slots Description Periods between spends 24 days 345_600 When the treasury can spend again after spending previously. Burn percentage is currently 1.00% . Precision ‚Äã DOT have 10 decimals of precision. In other words, 10 * * 10 (10_000_000_000 or ten billion) Plancks make up a DOT. The denomination of DOT was changed from 12 decimals of precision at block #1,248,328 in an event known as Denomination Day . See Redenomination for details. Edit this page Last updated on Mar 20, 2023 by github-actions[bot] Previous Network Maintainers Next Node Endpoints Periods of common actions and attributes Accounts, Identity and Crowdloans Governance Staking, Validating, and Nominating Parachains Treasury Precision ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Node Endpoints General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides Governance Guides Maintain Node Endpoints On this page Node Endpoints Ideally, one may run their own node when interacting with the Polkadot network via Polkadot-JS Apps or other UIs and programmatic methods. Another option would be to connect to one of the several public endpoints provided by infrastructure and API service providers. For development convenience, Parity Tech maintains archive nodes for Polkadot, Kusama, and their test networks with public endpoints. These endpoints can be used with Polkadot-JS API to interact with their respective chains. The tables below list these endpoints. Network Endpoints ‚Äã Main Networks ‚Äã Network URL Polkadot wss://rpc.polkadot.io Kusama wss://kusama-rpc.polkadot.io Test Networks ‚Äã Network URL Westend wss://westend-rpc.polkadot.io Rococo wss://rococo-rpc.polkadot.io Example usage with Polkadot-JS API ‚Äã To connect to the Parity node, use the endpoint in your JavaScript apps like so: // Using the Polkadot Mainnet Endpoint const { ApiPromise , WsProvider } = require ( '@polkadot/api' ) ; async ( ) => { // Construct a provider with the endpoint URL const provider = new WsProvider ( 'wss://rpc.polkadot.io/' ) ; // Create an API instance for Polkadot const api = await ApiPromise . create ( { provider } ) ; // ... Substrate Connect ‚Äã Substrate connect builds on Polkadot JS so building an app is the same experience as with using a traditional RPC server node. It is a fast, secure, and decentralized way to interact with Polkadot, Kusama, and their parachains right in the browser. info Substrate Connect is still under active development . Third Party Providers ‚Äã There are a number of third-party providers of RPC infrastructure to the Polkadot and Kusama communities, commonly providing access to multiple networks and parachains in a single service. They provide additional services such as higher rate limits, potentially more reliable and scalable service, and additional metrics. OnFinality Dwellir Pinknode Radium Block note The list of third party RPC endpoints above for Polkadot and Kusama is directly fetched from Polkdot-JS UI Edit this page Last updated on Oct 21, 2022 by Keith Alfaro Previous Parameters Next Set up a Full Node Network Endpoints Third Party Providers ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Set up a Full Node General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Set up a Full Node Set up a Boot Node Set up a RPC node Secure the WebSocket Networks Resolving Errors Nominator Guides Validator Guides Governance Guides Maintain Nodes and Dapps Set up a Full Node On this page Set up a Full Node If you're building dApps or products on a Substrate-based chain like Polkadot, Kusama, or a custom Substrate implementation, you want the ability to run a node-as-a-back-end. After all, relying on your infrastructure is always better than a third-party-hosted one in this brave new decentralized world. This guide will show you how to connect to Polkadot network , but the same process applies to any other Substrate -based chain. First, let's clarify the term full node . Types of Nodes ‚Äã A blockchain's growth comes from a genesis block , extrinsics , and events . When a validator seals block 1, it takes the blockchain's state at block 0. It then applies all pending changes on top of it and emits the events resulting from these changes. Later, the chain‚Äôs state at block one is used the same way to build the chain‚Äôs state at block 2, and so on. Once two-thirds of the validators agree on a specific block being valid, it is finalized. An archive node keeps all the past blocks and their states. An archive node makes it convenient to query the past state of the chain at any point in time. Finding out what an account's balance at a particular block was or which extrinsics resulted in a specific state change are fast operations when using an archive node. However, an archive node takes up a lot of disk space - around Kusama's 12 millionth block, this was around 660 GB. tip On the Paranodes or Stakeworld websites, you can find lists of the database sizes of Polkadot and Kusama nodes. Archive nodes are used by utilities that need past information - like block explorers, council scanners, discussion platforms like Polkassembly , and others. They need to be able to look at past on-chain data. A full node prunes historical states: all finalized blocks' states older than a configurable number except the genesis block's state. This is 256 blocks from the last finalized one by default. A pruned node this way requires much less space than an archive node. A full node could eventually rebuild every block's state without additional information and become an archive node. This still needs to be implemented at the time of writing. If you need to query historical blocks' states past what you pruned, you must purge your database and resync your node, starting in archive mode. Alternatively, you can use a backup or snapshot of a trusted source to avoid needing to sync from genesis with the network and only need the states of blocks past that snapshot. Full nodes allow you to read the current state of the chain and to submit and validate extrinsics directly on the network without relying on a centralized infrastructure provider. Another type of node is a light node . A light node has only the runtime and the current state but does not store past blocks and so cannot read historical data without requesting it from a node that has it. Light nodes are useful for resource-restricted devices. An interesting use-case of light nodes is a browser extension, which is a node in its own right, running the runtime in WASM format, as well as a full or light node that is completely encapsulated in WASM and can be integrated into web apps: https://github.com/paritytech/smoldot#wasm-light-node . Substrate Connect Substrate Connect provides a way to interact with substrate-based blockchains in the browser without using an RPC server. It is a light node that runs entirely in Javascript. Substrate Connect uses a smoldot WASM light client to securely connect to the blockchain network without relying on specific 3rd parties. Substrate Connect is available on Chrome and Firefox as a browser extension . Setup Instructions ‚Äã This is not recommended if you're a validator. Please see the secure validator setup if you are running validator. The bash commands that are provided to run against your node use Polkadot as the default chain Use the --chain flag if you follow the setup instructions to setup a Kusama node. For example: ./target/release/polkadot --name "Your Node's Name" --chain kusama macOS Windows Linux (standalone) Linux (package) Install Homebrew within the terminal by running: /bin/bash -c " $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) " Then, run: brew install openssl cmake llvm protobuf Install Rust by running: curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh After Rust is installed, update and add the nightly version: # Ensure the current shell has cargo source ~/.cargo/env # Update the Rust toolchain rustup default stable rustup update # Add the nightly and WebAssembly targets: rustup update nightly rustup target add wasm32-unknown-unknown --toolchain nightly Verify your installation by running the following: rustup show # You should see output similar to: active toolchain ---------------- stable-aarch64-apple-darwin ( default ) rustc 1.68 .1 ( 8460ca823 2023 -03-20 ) rustup +nightly show # You should see output similar to: installed targets for active toolchain -------------------------------------- aarch64-apple-darwin wasm32-unknown-unknown active toolchain ---------------- nightly-aarch64-apple-darwin ( overridden by +toolchain on the command line ) rustc 1.71 .0-nightly ( 9ecda8de8 2023 -04-30 ) Once Rust is configured, run the following command to clone and build the Polkadot code: git clone https://github.com/paritytech/polkadot polkadot cd polkadot ./scripts/init.sh cargo build --release Start your node: ./target/release/polkadot --name "Your Node's Name" Find your node on Telemetry Install WSL . Install Ubuntu (same webpage). Determine the latest version of the Polkadot binary . Download the correct Polkadot binary within Ubuntu by running the following command. Replace *VERSION* with the tag of the latest version from the last step (e.g. v0.8.22 ): curl -sL https://github.com/paritytech/polkadot/releases/download/*VERSION*/polkadot -o polkadot Then, run the following: sudo chmod +x polkadot Start your node: ./target/release/polkadot --name "Your Node's Name" Find your node on Telemetry Determine the latest version of the Polkadot binary . info The nature of pre-built binaries means that they may not work on your particular architecture or Linux distribution. If you see an error like cannot execute binary file: Exec format error it likely means the binary is not compatible with your system. You will either need to compile the source code or use Docker . Download the correct Polkadot binary within Ubuntu by running the following command. Replace *VERSION* with the tag of the latest version from the last step (e.g. v0.8.22 ): curl -sL https://github.com/paritytech/polkadot/releases/download/*VERSION*/polkadot -o polkadot Run the following: sudo chmod +x polkadot Run the following: ./target/release/polkadot --name "Your Node's Name" Find your node on Telemetry You can also install Polkadot from one of our package repositories. Installation from the Debian or rpm repositories will create a systemd service that can be used to run a Polkadot node. The service is disabled by default, and can be started by running systemctl start polkadot on demand (use systemctl enable polkadot to make it auto-start after reboot). By default, it will run as the polkadot user. Command-line flags passed to the binary can be customized by editing /etc/default/polkadot . This file will not be overwritten on updating polkadot. Debian-based (Debian, Ubuntu) ‚Äã Currently supports Debian 10 (Buster) and Ubuntu 20.04 (Focal), and derivatives. Run the following commands as the root user. # Import the [email protected] GPG key gpg --recv-keys --keyserver hkps://keys.mailvelope.com 9D4B2B6EB8F97156D19669A9FF0812D491B96798 gpg --export 9D4B2B6EB8F97156D19669A9FF0812D491B96798 > /usr/share/keyrings/parity.gpg # Add the Parity repository and update the package index echo 'deb [signed-by=/usr/share/keyrings/parity.gpg] https://releases.parity.io/deb release main' > /etc/apt/sources.list.d/parity.list apt update # Install the `parity-keyring` package - This will ensure the GPG key # used by APT remains up-to-date apt install parity-keyring # Install polkadot apt install polkadot If you don't want polkadot package to be automatically updated when you update packages on your server, you can issue the following command: sudo apt-mark hold polkadot RPM-based (Fedora, CentOS) ‚Äã Currently supports Fedora 32 and CentOS 8, and derivatives. # Install dnf-plugins-core (This might already be installed) dnf install dnf-plugins-core # Add the repository and enable it dnf config-manager --add-repo https://releases.parity.io/rpm/polkadot.repo dnf config-manager --set-enabled polkadot # Install polkadot (You may have to confirm the import of the GPG key, which # should have the following fingerprint: 9D4B2B6EB8F97156D19669A9FF0812D491B96798) dnf install polkadot info If you choose to use a custom folder for the polkadot home by passing --base-path '/custom-path' , you will need to issue following command: sudo mkdir /etc/systemd/system/polkadot.service.d And create a new file inside this folder: sudo -e /etc/systemd/system/polkadot.service.d/custom.conf With the following content: [Service] ReadWritePaths=/custom-path And finally issue a reload to have your modifications applied by systemd: systemctl daemon-reload Get Substrate ‚Äã Follow instructions as outlined here - note that Windows users will have their work cut out for them. It's better to use a virtual machine instead. Test if the installation was successful by running cargo --version . Œª cargo --version cargo 1.41 .0 ( 626f0f40e 2019 -12-03 ) Clone and Build ‚Äã The paritytech/polkadot repo's master branch contains the latest Polkadot code. git clone https://github.com/paritytech/polkadot polkadot cd polkadot ./scripts/init.sh cargo build --release Alternatively, if you wish to use a specific release, you can check out a specific tag ( v0.8.3 in the example below): git clone https://github.com/paritytech/polkadot polkadot cd polkadot git checkout tags/v0.8.3 ./scripts/init.sh cargo build --release Run ‚Äã The built binary will be in the target/release folder, called polkadot . Polkadot : ./target/release/polkadot --name "Your Node's Name" Use the --help flag to determine which flags you can use when running the node. For example, if connecting to your node remotely , you'll probably want to use --ws-external and --rpc-cors all . The syncing process will take a while, depending on your capacity, processing power, disk speed and RAM. On a \$10 DigitalOcean droplet, the process can complete in some 36 hours. Congratulations, you're now syncing with Polkadot. Keep in mind that the process is identical when using any other Substrate chain. Running an Archive Node ‚Äã When running as a simple sync node (above), only the state of the past 256 blocks will be kept. It defaults to archive mode when validating. To support the full state, use the --pruning flag: Polkadot : ./target/release/polkadot --name "My node's name" --pruning archive It is possible to almost quadruple synchronization speed by using an additional flag: --wasm-execution Compiled . Note that this uses much more CPU and RAM, so it should be turned off after the node syncs. Using Docker ‚Äã Finally, you can use Docker to run your node in a container. Doing this is more advanced, so it's best left up to those already familiar with docker or who have completed the other set-up instructions in this guide. Be aware that when you run polkadot in docker, the process only listens on localhost by default. If you would like to connect to your node's services (rpc, websockets, and prometheus) you need to ensure that you run you node with the --rpc-external , --ws-external , and --prometheus-external commands. docker run -p 9944:9944 -p 9615:9615 parity/polkadot:v0.9.13 --name "calling_home_from_a_docker_container" --rpc-external --ws-external --prometheus-external Edit this page Last updated on May 11, 2023 by bader y Previous Node Endpoints Next Set up a Boot Node Types of Nodes Setup Instructions Debian-based (Debian, Ubuntu) RPM-based (Fedora, CentOS) Get Substrate Clone and Build Run Running an Archive Node Using Docker ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Set up a Boot Node General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Set up a Full Node Set up a Boot Node Set up a RPC node Secure the WebSocket Networks Resolving Errors Nominator Guides Validator Guides Governance Guides Maintain Nodes and Dapps Set up a Boot Node On this page Set up a Boot Node note When you first start a node, it has to find a way to find other nodes in the network. For that purpose, you need "bootnodes". After the first bootnode is found, it can use that node‚Äôs connections to continue expanding and play its role in the network, like participating as a validator. Accessing the Bootnode ‚Äã The consensus is that bootnodes have to be accessible in three ways: p2p : the p2p port, which can be set by --listen-addr /ip4/0.0.0.0/tcp/<port> . This port is not automatically set on a non-validator node (for example, an archive RPC node). p2p/ws : the WebSocket version, which can be set by --listen-addr /ip4/0.0.0.0/tcp/<port>/ws . p2p/wss : the secure websocket version. An SSL-secured connection to the p2p/ws port must be achieved by a proxy since the node cannot include certificates. It is needed for light clients. See here for info about setting this up. Network Key ‚Äã Starting a node creates its node key in the chains/<chain>/network/secret_ed25519 file. You can also create a node-key by polkadot key generate-node-key and use that node-key in the startup command line. It is essential you backup the node key, especially if it gets included in the polkadot binary because it gets hardcoded in the binary and needs to be recompiled to change. Running the Bootnode ‚Äã Say we are running a polkadot node with polkadot --chain polkadot --name dot-bootnode --listen-addr /ip4/0.0.0.0/tcp/30310 --listen-addr /ip4/0.0.0.0/tcp/30311/ws then we would have the p2p on port 30310 and p2p/ws on port 30311. For the p2p/wss port, we need to set up a proxy, a DNS name, and a corresponding certificate. These concepts and example setups are described here . The following example is for the popular nginx server and enables p2p/wss on port 30312 by proxying the p2p/ws port 30311: /etc/nginx/sites-enabled/dot-bootnode server { listen 30312 ssl http2 default_server; server_name dot-bootnode.stakeworld.io; root /var/www/ ssl_certificate "<your_cert"; ssl_certificate_key "<your_key>"; location / { proxy_buffers 16 4k; proxy_buffer_size 2k; proxy_pass http://localhost:30311; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection "Upgrade"; proxy_set_header Host $host; } } Testing Bootnode Connection ‚Äã If we have the above node running with DNS name dot-bootnode.stakeworld.io , proxied with a valid certificate and node-id 12D3KooWAb5MyC1UJiEQJk4Hg4B2Vi3AJdqSUhTGYUqSnEqCFMFg then the following commands should give you a: "syncing 1 peers". tip You can add -lsub-libp2p=trace on the end to get libp2p trace logging for debugging purposes. p2p : polkadot --chain polkadot --base-path /tmp/node --name "Bootnode testnode" --reserved-only --reserved-nodes "/dns/dot-bootnode.stakeworld.io/tcp/30310/p2p/12D3KooWAb5MyC1UJiEQJk4Hg4B2Vi3AJdqSUhTGYUqSnEqCFMFg" --no-hardware-benchmarks p2p/ws : polkadot --chain polkadot --base-path /tmp/node --name "Bootnode testnode" --reserved-only --reserved-nodes "/dns/dot-bootnode.stakeworld.io/tcp/30311/ws/p2p/12D3KooWAb5MyC1UJiEQJk4Hg4B2Vi3AJdqSUhTGYUqSnEqCFMFg" --no-hardware-benchmarks p2p/wss : polkadot --chain polkadot --base-path /tmp/node --name "Bootnode testnode" --reserved-only --reserved-nodes "/dns/dot-bootnode.stakeworld.io/tcp/30312/wss/p2p/12D3KooWAb5MyC1UJiEQJk4Hg4B2Vi3AJdqSUhTGYUqSnEqCFMFg" --no-hardware-benchmarks Edit this page Last updated on May 9, 2023 by Stakeworld Previous Set up a Full Node Next Set up a RPC node Accessing the Bootnode Network Key Running the Bootnode Testing Bootnode Connection ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Set up a RPC node General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Set up a Full Node Set up a Boot Node Set up a RPC node Secure the WebSocket Networks Resolving Errors Nominator Guides Validator Guides Governance Guides Maintain Nodes and Dapps Set up a RPC node On this page Set up a RPC node The substrate node RPC server can be accessed over the WebSocket protocol, which can be used to access the underlying network and/or validator node. By default, you can access your node's RPC server from localhost (for example, to rotate keys or do other maintenance). To access it from another server or an applications UI (such as Polkadot-JS UI ) it is recommended to enable access to the RPC node over an SSL connection and encrypt the connection between the end user and the RPC server. This can be achieved by setting up a secure proxy. Many browsers, such as Google Chrome, will block non-secure ws endpoints if they come from a different origin. note Enabling remote access to your validator node should not be necessary and is not suggested, as it can often lead to security problems Set up a Node ‚Äã Setting up any Substrate-based node relies on a similar process. For example, by default, they will all share the same WebSocket connection at port 9944 on localhost. In this example, we'll set up a Polkadot sync node on a Debian-flavoured server (such as Ubuntu 22.04). Create a new server on your provider of choice or locally at home. See Set up a Full Node for additional instructions. You can install from the default apt repository or build from scratch. The startup options in the setup process provide various settings that can be modified. A typical setting for an externally accessible polkadot archive RPC node would be: polkadot --chain polkadot --name myrpc --state-pruning archive --blocks-pruning archive --ws-max-connections 100 --rpc-cors all --rpc-methods Safe --ws-port 9944 Or for a Polkadot pruned RPC node: polkadot --chain polkadot --name myrpc --state-pruning 1000 --blocks-pruning archive --ws-max-connections 100 --rpc-cors all --rpc-methods Safe --ws-port 9944 The specified flag options are outlined in greater detail below. Archive Node vs. Pruned Node ‚Äã A pruned node only keeps a limited number of finalized blocks of the network, not its full history. Most frequently required actions can be completed with a pruned node, such as displaying account balances, making transfers, setting up session keys, staking, etc. An archive node has the full history (database) of the network. It can be queried in various ways, such as providing historical information regarding transfers, balance histories, and more advanced queries involving past events. An archive node requires a lot more disk space. At the start of April 2023, Polkadot disk usage was 160 GB for a pruned node and 1 TB for an archive node. This value will increase with time. For an archive node, you need the options --state-pruning archive --blocks-pruning archive in your startup settings. tip Inclusion in the Polkadot.js UI requires an archive node. Secure the RPC server ‚Äã The node startup settings allow you to choose what to expose, how many connections to expose and from where access should be granted through the RPC server. How many : You can set your maximum connections through --ws-max-connections , for example --ws-max-connections 100 From where : by default localhost and the polkadot.js are allowed to access the RPC server; you can change this by setting --rpc-cors , to allow access from everywhere you need --rpc-cors all What : you can limit the methods to use with --rpc-methods , an easy way to set this to a safe mode is --rpc-methods Safe Secure the ws port ‚Äã To safely access your ws connection over an SSL-enabled connection (needed for polkadot.js), you have to convert the ws connection to a secure (wss) connection by using a proxy and an SSL certificate, you can find instructions on securing the ws port here . Connecting to the Node ‚Äã Open Polkadot-JS UI and click the logo in the top left to switch the node. Activate the "Development" toggle and input your node's address - either the domain or the IP address. Remember to prefix with wss:// , and if you're using the 443 port, append :443 like so: wss://example.com:443 . Now you have a secure remote connect setup for your Substrate node. Edit this page Last updated on May 9, 2023 by Stakeworld Previous Set up a Boot Node Next Secure the WebSocket Set up a Node Archive Node vs. Pruned Node Secure the RPC server Secure the ws port Connecting to the Node ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Secure the WebSocket General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Set up a Full Node Set up a Boot Node Set up a RPC node Secure the WebSocket Networks Resolving Errors Nominator Guides Validator Guides Governance Guides Maintain Nodes and Dapps Secure the WebSocket On this page Secure the WebSocket Secure a WS Port ‚Äã A non-secure ws port can be converted to a secure wss port by placing it behind an SSL-enabled proxy. This can be used to secure a bootnode or secure a RPC server . The SSL-enabled apache2/nginx/other proxy server redirects requests to the internal ws and converts it to a secure (wss) connection. For this, you will need an SSL certificate for which you can use a service like letsencrypt or self-signing. Obtaining an SSL Certificate ‚Äã One easy way to get a free SSL certificate can be achieved by following the LetsEncrypt instructions ( nginx / apache ). This will auto-generate an SSL certificate and include it in your configuration. Alternatively, you can generate a self-signed certificate and rely on the raw IP address of your node when connecting to it. This is not preferable since you will have to whitelist the certificate to access it from a browser. sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/selfsigned.key -out /etc/ssl/certs/selfsigned.crt sudo openssl dhparam -out /etc/ssl/certs/dhparam.pem 2048 Installing a Proxy Server ‚Äã There are a lot of different implementations of a WebSocket proxy, some of the more widely used are nginx and apache2 , for which configuration examples provided below. Nginx ‚Äã apt install nginx In an SSL-enabled virtual host add: server { (...) location / { proxy_buffers 16 4k; proxy_buffer_size 2k; proxy_pass http://localhost:9944; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection "Upgrade"; proxy_set_header Host $host; } } Optionally some form of rate limiting can be introduced: http { limit_req_zone "$http_x_forwarded_for" zone=zone:10m rate=2r/s; (...) } location / { limit_req zone=zone burst=5; (...) } Apache2 ‚Äã You can run it in different modes such as prefork, worker, or event. In this example, we use event which works well on higher load environments but other modes are also useful given the requirements. apt install apache2 a2dismod mpm_prefork a2enmod mpm_event proxy proxy_proxy_http proxy_wstunnel rewrite ssl The mod_proxy_wstunnel provides support for the tunneling of web socket connections to a backend websockets server. The connection is automatically upgraded to a WebSocket connection . In an SSL-enabled virtualhost add: (...) SSLProxyEngine on ProxyRequests off ProxyPass / ws://localhost:9944 ProxyPassReverse / ws://localhost:9944 Older versions of mod_proxy_wstunnel do not upgrade the connection automatically and will need the following config added: RewriteEngine on RewriteCond %{HTTP:Upgrade} websocket [NC] RewriteRule /(.*) ws://localhost:9944/$1 [P,L] RewriteRule /(.*) http://localhost:9944/$1 [P,L] Optionally some form of rate limiting can be introduced: apt install libapache2-mod-qos a2enmod qos And edit /etc/apache2/mods-available/qos.conf # allows max 50 connections from a single ip address: QS_SrvMaxConnPerIP 50 Connecting to the Node ‚Äã Open Polkadot-JS UI and click the logo in the top left to switch the node. Activate the "Development" toggle and input your node's address - either the domain or the IP address. Remember to prefix with wss:// and if you're using the 443 port, append :443 , like so: wss://example.com:443 . Now you have a secure remote connect setup for your Substrate node. Edit this page Last updated on May 9, 2023 by Stakeworld Previous Set up a RPC node Next Networks Secure a WS Port Obtaining an SSL Certificate Installing a Proxy Server Nginx Apache2 Connecting to the Node ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Networks General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Set up a Full Node Set up a Boot Node Set up a RPC node Secure the WebSocket Networks Resolving Errors Nominator Guides Validator Guides Governance Guides Maintain Nodes and Dapps Networks On this page Networks Polkadot is built on top of Substrate, a modular framework for blockchains. One feature of Substrate is to allow for connection to different networks using a single executable and configuring it with a start-up flag. Here are some of the networks associated with Polkadot or Substrate that you may want to connect to and join. Main networks ‚Äã To connect to a Polkadot network please follow the instructions for installing the Polkadot executable. Polkadot Mainnet ‚Äã Connecting to the Polkadot network is the default option when starting a node. To start a Polkadot node, run the Polkadot binary: polkadot and you will connect and start syncing to Polkadot. Check your node is connected by viewing it on Telemetry (you can set a custom node name by specifying --name "my-custom-node-name" ) Kusama Canary Network ‚Äã Kusama is a canary network and holds real economic value. Run the Polkadot binary and specify kusama as the chain: polkadot --chain = kusama and you will connect and start syncing to Kusama. Check your node is connected by viewing it on Kusama Telemetry (you can set a custom node name by specifying --name "my-custom-node-name" ) Test Networks ‚Äã Westend Test Network ‚Äã Westend is the primary test network of Polkadot. The tokens on this network are called Westies (WND) and they purposefully hold no economic value. Run the Polkadot binary and specify westend as the chain: polkadot --chain = westend and you will connect and start syncing to Westend. Check that your node is connected by viewing it on Westend Telemetry (you can set a custom node name by specifying --name "my-custom-node-name" ). Westend Faucet ‚Äã Follow the instruction here to get Westies (WND) tokens. Rococo Test Network ‚Äã Rococo is a test network built for parachains. The native token of this network (ROC) holds no economic value. Run the Polkadot binary and specify rococo as the chain: polkadot --chain = rococo and you will connect and start syncing to Rococo. Check that your node is connected by viewing it on Rococo Telemetry (you can set a custom node name by specifying --name "my-custom-node-name" ). Rococo Faucet ‚Äã Follow the instruction here to get ROCs tokens. Wococo Test Network ‚Äã Wococo is a test network of Polkadot built for bridges. The native token of this network (WOOK) holds no economic value. Run the Polkadot binary and specify rococo as the chain: polkadot --chain = wococo and you will connect and start syncing to Wococo. Check that your node is connected by viewing it on Wococo Telemetry (you can set a custom node name by specifying --name "my-custom-node-name" ). Wococo Faucet ‚Äã Follow the instruction here to get WOOKs tokens. Differences ‚Äã Runtime differences (e.g. existential and multisignature deposit sizes) between the different networks can be found by doing a diff between the src/lib.rs of the repositories. For example, to compare the Polkadot and Westend runtimes: git clone https://github.com/paritytech/polkadot && cd polkadot/runtime ls - show the available runtimes diff polkadot/src/lib.rs westend/src/lib.rs You can also paste the runtimes ( Polkadot , Westend ) into a web-based diff tool like Diffchecker if you're not comfortable with the CLI. Telemetry Dashboard ‚Äã If you connect to the public networks, the default configuration for your node will connect it to the public Telemetry service. You can verify that your node is connected by navigating to the correct network on the dashboard and finding the name of your node. There is a built-in search function on the nodes page. Simply start typing keystrokes in the main window to make it available. Edit this page Last updated on Jun 16, 2023 by Pierre Besson Previous Secure the WebSocket Next Resolving Errors Main networks Polkadot Mainnet Kusama Canary Network Test Networks Westend Test Network Rococo Test Network Wococo Test Network Differences Telemetry Dashboard ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Errors and How to Resolve Them General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Set up a Full Node Set up a Boot Node Set up a RPC node Secure the WebSocket Networks Resolving Errors Nominator Guides Validator Guides Governance Guides Maintain Nodes and Dapps Resolving Errors On this page Errors and How to Resolve Them Errors in Substrate-based chains are usually accompanied by descriptive messages. However, to read these messages, a tool parsing the blockchain data needs to request chain metadata from a node. That metadata explains how to read the messages. One such tool with a built-in parser for chain metadata is the Polkadot-JS Apps UI . If this page does not answer your question, try searching for your problem at the Polkadot Knowledge Base for more information on troubleshooting your issue. PolkadotJS Apps Explorer ‚Äã Here's how to find out the detailed error description through Polkadot-JS Apps. A typical failed transactions looks something like this: The image displays only the error name as defined in the code, not its error message. Despite this error being rather self-explanatory, let's find its details. In the explorer tab , find the block in which this failure occurred. Then, expand the system.ExtrinsicFailed frame: Notice how the details field contains a human-readable description of the error. Most errors will have this, if looked up this way. This block is a live example of the above. If you cannot look up the error this way, or there is no message in the details field, consult the table below. Polkascan and Subscan ‚Äã Polkascan and Subscan show the ExtrinsicFailed event when a transaction does not succeed ( example ). This event gives us the error and index indices of the error but does not give us a nice message to understand what it means. We will look up the error in the codebase ourselves to understand what went wrong. First, we should understand that the index number is the index of the pallet in the runtime from which the error originated. The error is likewise the index of that pallet's errors which is the exact one we're looking for. Both of these indices start counting from 0. For example, if index is 5 and error is 3, as in the example linked above, we need to look at the runtime for the fourth error (index 3) in the sixth pallet (index 5). By looking at the runtime code we see that the pallet at index 5 is Balances . Now we will check the Balances pallet's code which is hosted in the Substrate repository, and look for the fourth error in the Error enum . According to its source the error that we got is InsufficientBalance , or in other words, "Balance too low to send value". Common Errors ‚Äã The table below lists the most commonly encountered errors and ways to resolve them. Error Description Solution BadOrigin You are not allowed to do this operation, e.g. trying to create a council motion with a non-council account. Either switch to an account that has the necessary permissions, or check if the operation you're trying to execute is permitted at all (e.g. calling system.setCode to do a runtime upgrade directly, without voting). BadProof The transaction's signature seems invalid. It's possible that the node you're connected to is following an obsolete fork - trying again after it catches up usually resolves the issue. To check for bigger problems, inspect the last finalized and current best block of the node you're connected to and compare the values to chain stats exposed by other nodes - are they in sync? If not, try connecting to a different node. Future Transaction nonce too high, i.e. it's "from the future", see note below . Reduce the nonce to +1 of current nonce. Check current nonce by inspecting the address you're using to send the transaction. Stale Transaction nonce too low. Increase the nonce to +1 of current nonce. Check current nonce by inspecting the address you're using to send the transaction. ExhaustsResources There aren't enough resources left in the current block to submit this transaction. Try again in the next block. Payment Unable to pay for TX fee. You might not have enough free balance to cover the fee this transaction would incur. Temporarily banned The transaction is temporarily banned. The tx is already in pool. Either try on a different node, or wait to see if the initial transaction goes through. Future Error This error will not cause the TX to be discarded immediately. Instead, it will be sent to the futures queue , where it will wait to be executed at the correct place in the nonce sequence OR it will get discarded due to some other error (ex. the validity period expires). * Error Table ‚Äã The below table is a reference to the errors that exists in Polkadot. It is generated from the runtime's metadata. Pallet Error Documentation System (0) InvalidSpecName (0) The name of specification does not match between the current runtime and the new runtime. SpecVersionNeedsToIncrease (1) The specification version is not allowed to decrease between the current runtime and the new runtime. FailedToExtractRuntimeVersion (2) Failed to extract the runtime version from the new runtime. Either calling Core_version or decoding RuntimeVersion failed. NonDefaultComposite (3) Suicide called when the account has non-default composite data. NonZeroRefCount (4) There is a non-zero reference count preventing the account from being purged. Scheduler (1) FailedToSchedule (0) Failed to schedule a call NotFound (1) Cannot find the scheduled call. TargetBlockNumberInPast (2) Given target block number is in the past. RescheduleNoChange (3) Reschedule failed because it does not change scheduled time. Balances (5) VestingBalance (0) Vesting balance too high to send value LiquidityRestrictions (1) Account liquidity restrictions prevent withdrawal Overflow (2) Got an overflow after adding InsufficientBalance (3) Balance too low to send value ExistentialDeposit (4) Value too low to create account due to existential deposit KeepAlive (5) Transfer/payment would kill account ExistingVestingSchedule (6) A vesting schedule already exists for this account DeadAccount (7) Beneficiary account must pre-exist Authorship (6) InvalidUncleParent (0) The uncle parent not in the chain. UnclesAlreadySet (1) Uncles already set in the block. TooManyUncles (2) Too many uncles. GenesisUncle (3) The uncle is genesis. TooHighUncle (4) The uncle is too high in chain. UncleAlreadyIncluded (5) The uncle is already included. OldUncle (6) The uncle isn't recent enough to be included. Staking (7) NotController (0) Not a controller account. NotStash (1) Not a stash account. AlreadyBonded (2) Stash is already bonded. AlreadyPaired (3) Controller is already paired. EmptyTargets (4) Targets cannot be empty. DuplicateIndex (5) Duplicate index. InvalidSlashIndex (6) Slash record index out of bounds. InsufficientValue (7) Can not bond with value less than minimum balance. NoMoreChunks (8) Can not schedule more unlock chunks. NoUnlockChunk (9) Can not rebond without unlocking chunks. FundedTarget (10) Attempting to target a stash that still has funds. InvalidEraToReward (11) Invalid era to reward. InvalidNumberOfNominations (12) Invalid number of nominations. NotSortedAndUnique (13) Items are not sorted and unique. AlreadyClaimed (14) Rewards for this era have already been claimed for this validator. OffchainElectionEarlySubmission (15) The submitted result is received out of the open window. OffchainElectionWeakSubmission (16) The submitted result is not as good as the one stored on chain. SnapshotUnavailable (17) The snapshot data of the current window is missing. OffchainElectionBogusWinnerCount (18) Incorrect number of winners were presented. OffchainElectionBogusWinner (19) One of the submitted winners is not an active candidate on chain (index is out of range in snapshot). OffchainElectionBogusCompact (20) Error while building the assignment type from the compact. This can happen if an index is invalid, or if the weights overflow . OffchainElectionBogusNominator (21) One of the submitted nominators is not an active nominator on chain. OffchainElectionBogusNomination (22) One of the submitted nominators has an edge to which they have not voted on chain. OffchainElectionSlashedNomination (23) One of the submitted nominators has an edge which is submitted before the last non-zero slash of the target. OffchainElectionBogusSelfVote (24) A self vote must only be originated from a validator to ONLY themselves. OffchainElectionBogusEdge (25) The submitted result has unknown edges that are not among the presented winners. OffchainElectionBogusScore (26) The claimed score does not match with the one computed from the data. OffchainElectionBogusElectionSize (27) The election size is invalid. CallNotAllowed (28) The call is not allowed at the given time due to restrictions of election period. IncorrectHistoryDepth (29) Incorrect previous history depth input provided. IncorrectSlashingSpans (30) Incorrect number of slashing spans provided. Session (9) InvalidProof (0) Invalid ownership proof. NoAssociatedValidatorId (1) No associated validator ID for account. DuplicatedKey (2) Registered duplicate key. NoKeys (3) No keys are associated with this account. Grandpa (11) PauseFailed (0) Attempt to signal GRANDPA pause when the authority set isn't live (either paused or already pending pause). ResumeFailed (1) Attempt to signal GRANDPA resume when the authority set isn't paused (either live or already pending resume). ChangePending (2) Attempt to signal GRANDPA change with one already pending. TooSoon (3) Cannot signal forced change so soon after last. InvalidKeyOwnershipProof (4) A key ownership proof provided as part of an equivocation report is invalid. InvalidEquivocationProof (5) An equivocation proof provided as part of an equivocation report is invalid. DuplicateOffenceReport (6) A given equivocation report is valid but already previously reported. ImOnline (12) InvalidKey (0) Non existent public key. DuplicatedHeartbeat (1) Duplicated heartbeat. Democracy (14) ValueLow (0) Value too low ProposalMissing (1) Proposal does not exist BadIndex (2) Unknown index AlreadyCanceled (3) Cannot cancel the same proposal twice DuplicateProposal (4) Proposal already made ProposalBlacklisted (5) Proposal still blacklisted NotSimpleMajority (6) Next external proposal not simple majority InvalidHash (7) Invalid hash NoProposal (8) No external proposal AlreadyVetoed (9) Identity may not veto a proposal twice NotDelegated (10) Not delegated DuplicatePreimage (11) Preimage already noted NotImminent (12) Not imminent TooEarly (13) Too early Imminent (14) Imminent PreimageMissing (15) Preimage not found ReferendumInvalid (16) Vote given for invalid referendum PreimageInvalid (17) Invalid preimage NoneWaiting (18) No proposals waiting NotLocked (19) The target account does not have a lock. NotExpired (20) The lock on the account to be unlocked has not yet expired. NotVoter (21) The given account did not vote on the referendum. NoPermission (22) The actor has no permission to conduct the action. AlreadyDelegating (23) The account is already delegating. Overflow (24) An unexpected integer overflow occurred. Underflow (25) An unexpected integer underflow occurred. InsufficientFunds (26) Too high a balance was provided that the account cannot afford. NotDelegating (27) The account is not currently delegating. VotesExist (28) The account currently has votes attached to it and the operation cannot succeed until these are removed, either through unvote or reap_vote . InstantNotAllowed (29) The instant referendum origin is currently disallowed. Nonsense (30) Delegation to oneself makes no sense. WrongUpperBound (31) Invalid upper bound. MaxVotesReached (32) Maximum number of votes reached. InvalidWitness (33) The provided witness data is wrong. TooManyProposals (34) Maximum number of proposals reached. Council (15) NotMember (0) Account is not a member DuplicateProposal (1) Duplicate proposals not allowed ProposalMissing (2) Proposal must exist WrongIndex (3) Mismatched index DuplicateVote (4) Duplicate vote ignored AlreadyInitialized (5) Members are already initialized! TooEarly (6) The close call was made too early, before the end of the voting. TooManyProposals (7) There can only be a maximum of MaxProposals active proposals. WrongProposalWeight (8) The given weight bound for the proposal was too low. WrongProposalLength (9) The given length bound for the proposal was too low. TechnicalCommittee (16) NotMember (0) Account is not a member DuplicateProposal (1) Duplicate proposals not allowed ProposalMissing (2) Proposal must exist WrongIndex (3) Mismatched index DuplicateVote (4) Duplicate vote ignored AlreadyInitialized (5) Members are already initialized! TooEarly (6) The close call was made too early, before the end of the voting. TooManyProposals (7) There can only be a maximum of MaxProposals active proposals. WrongProposalWeight (8) The given weight bound for the proposal was too low. WrongProposalLength (9) The given length bound for the proposal was too low. ElectionsPhragmen (17) UnableToVote (0) Cannot vote when no candidates or members exist. NoVotes (1) Must vote for at least one candidate. TooManyVotes (2) Cannot vote more than candidates. MaximumVotesExceeded (3) Cannot vote more than maximum allowed. LowBalance (4) Cannot vote with stake less than minimum balance. UnableToPayBond (5) Voter can not pay voting bond. MustBeVoter (6) Must be a voter. ReportSelf (7) Cannot report self. DuplicatedCandidate (8) Duplicated candidate submission. MemberSubmit (9) Member cannot re-submit candidacy. RunnerSubmit (10) Runner cannot re-submit candidacy. InsufficientCandidateFunds (11) Candidate does not have enough funds. NotMember (12) Not a member. InvalidCandidateCount (13) The provided count of number of candidates is incorrect. InvalidVoteCount (14) The provided count of number of votes is incorrect. InvalidRenouncing (15) The renouncing origin presented a wrong Renouncing parameter. InvalidReplacement (16) Prediction regarding replacement after member removal is wrong. Treasury (19) InsufficientProposersBalance (0) Proposer's balance is too low. InvalidIndex (1) No proposal or bounty at that index. ReasonTooBig (2) The reason given is just too big. AlreadyKnown (3) The tip was already found/started. UnknownTip (4) The tip hash is unknown. NotFinder (5) The account attempting to retract the tip is not the finder of the tip. StillOpen (6) The tip cannot be claimed/closed because there are not enough tippers yet. Premature (7) The tip cannot be claimed/closed because it's still in the countdown period. UnexpectedStatus (8) The bounty status is unexpected. RequireCurator (9) Require bounty curator. InvalidValue (10) Invalid bounty value. InvalidFee (11) Invalid bounty fee. PendingPayout (12) A bounty payout is pending. To cancel the bounty, you must unassign and slash the curator. Claims (24) InvalidEthereumSignature (0) Invalid Ethereum signature. SignerHasNoClaim (1) Ethereum address has no claim. SenderHasNoClaim (2) Account ID sending tx has no claim. PotUnderflow (3) There's not enough in the pot to pay out some unvested amount. Generally implies a logic error. InvalidStatement (4) A needed statement was not included. VestedBalanceExists (5) The account already has a vested balance. Vesting (25) NotVesting (0) The account given is not vesting. ExistingVestingSchedule (1) An existing vesting schedule already exists for this account that cannot be clobbered. AmountLow (2) Amount being transferred is too low to create a vesting schedule. Identity (28) TooManySubAccounts (0) Too many subs-accounts. NotFound (1) Account isn't found. NotNamed (2) Account isn't named. EmptyIndex (3) Empty index. FeeChanged (4) Fee is changed. NoIdentity (5) No identity found. StickyJudgement (6) Sticky judgement. JudgementGiven (7) Judgement given. InvalidJudgement (8) Invalid judgement. InvalidIndex (9) The index is invalid. InvalidTarget (10) The target is invalid. TooManyFields (11) Too many additional fields. TooManyRegistrars (12) Maximum amount of registrars reached. Cannot add any more. AlreadyClaimed (13) Account ID is already named. NotSub (14) Sender is not a sub-account. NotOwned (15) Sub-account isn't owned by sender. Proxy (29) TooMany (0) There are too many proxies registered or too many announcements pending. NotFound (1) Proxy registration not found. NotProxy (2) Sender is not a proxy of the account to be proxied. Unproxyable (3) A call which is incompatible with the proxy type's filter was attempted. Duplicate (4) Account is already a proxy. NoPermission (5) Call may not be made by proxy because it may escalate its privileges. Unannounced (6) Announcement, if made at all, was made too recently. Multisig (30) MinimumThreshold (0) Threshold must be 2 or greater. AlreadyApproved (1) Call is already approved by this signatory. NoApprovalsNeeded (2) Call doesn't need any (more) approvals. TooFewSignatories (3) There are too few signatories in the list. TooManySignatories (4) There are too many signatories in the list. SignatoriesOutOfOrder (5) The signatories were provided out of order; they should be ordered. SenderInSignatories (6) The sender was contained in the other signatories; it shouldn't be. NotFound (7) Multisig operation not found when attempting to cancel. NotOwner (8) Only the account that originally created the multisig is able to cancel it. NoTimepoint (9) No timepoint was given, yet the multisig operation is already underway. WrongTimepoint (10) A different timepoint was given to the multisig operation that is underway. UnexpectedTimepoint (11) A timepoint was given, yet no multisig operation is underway. WeightTooLow (12) The maximum weight information provided was too low. AlreadyStored (13) The data to be stored is already stored. Edit this page Last updated on Sep 18, 2022 by Keith Alfaro Previous Networks Next Nominator Guides PolkadotJS Apps Explorer Polkascan and Subscan Common Errors Error Table ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Become a Nominator on Polkadot General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides Governance Guides Maintain Nominator Guides On this page Become a Nominator on Polkadot New to Staking? Start your staking journey or explore more information about staking on Polkadot's Home Page . You can learn how staking works by reading this dedicated page . Discover the new Staking Dashboard that makes staking much easier and check this extensive article list to help you get started. info The following information applies to the Polkadot network. If you want to nominate on Kusama, check out the Kusama guide instead. Nominators are one type of participant in the staking subsystem of Polkadot. They appoint their stake to the validators, the second type of participant. By appointing their stake, they can elect the active set of validators and share in the rewards that are paid out. While the validators are active participants in the network that engage in the block production and finality mechanisms, nominators take a slightly more passive role. Being a nominator does not require running a node of your own or worrying about online uptime. However, a good nominator performs due diligence on the validators that they elect. When looking for validators to nominate, a nominator should pay attention to their own reward percentage for nominating a specific validator - as well as the risk that they bear of being slashed if the validator gets slashed. If you are a beginner, please watch the video below for detailed instructions. Setting up Stash and Staking Proxy Accounts ‚Äã Nominators are recommended to set up separate stash and staking proxy accounts. Explanation and the reasoning for generating distinct accounts for this purpose is elaborated in the keys section of the Wiki. You can generate your stash and staking proxy account via any of the recommended methods, which are detailed on the account generation page. Starting with runtime version v23 natively included in the client version 0.8.23 , payouts can go to any custom address. If you'd like to redirect payments to an account that is neither the staking proxy nor the stash account, set one up. Note that setting an exchange address as the recipient of the staking rewards is extremely unsafe. Using the Polkadot Staking Dashboard ‚Äã Walk-through Video Tutorials Nominating : Stake your tokens, choose your best validators, and start your staking journey on Polkadot. Becoming a Pool Member : Start becoming a part of the Polkadot movement, keep Polkadot secure by staking minimum 1 DOT and receiving staking rewards. Dashboard Walkthrough : Become a Pro using the Staking Dashboard. After Staking : Nominating on Polkadot is not a set-and-forget action, learn what you can do with the dashboard after you started staking. Why am I not receiving Staking Rewards? ‚Äã Bags List & Minimum Active Bond See this video tutorial and read this support article to understand why in some cases you might not receive staking rewards and how to avoid those situations. Using Polkadot-JS UI ‚Äã Using Polkadot-JS UI as a Nominator Here is the list of basic nominator actions that can be performed using the Polkadot-JS UI. How to Bond Tokens and Nominate How to Select Validators How to Stop Nominating & Unbond Tokens How to Rebond Tokens Video Tutorials How to Nominate/Stake Staking with a Ledger and PolkadotJS Apps Staking with a Ledger and Ledger Live Using Command-Line Interface (CLI) ‚Äã Apart from using Polkadot-JS Apps to participate in staking, you can do all these things in CLI instead. The CLI approach allows you to interact with the Polkadot network without going to the Polkadot-JS Apps dashboard. Step 1: Install @polkadot/api-cli ‚Äã We assume you have installed NodeJS with npm . Run the following command to install the @polkadot/api-cli globally: npm install -g @polkadot/api-cli Step 2. Bond your DOT ‚Äã Executing the following command: polkadot-js-api --seed "MNEMONIC_PHRASE" tx.staking.bond CONTROLLER_ADDRESS NUMBER_OF_TOKENS REWARD_DESTINATION --ws WEBSOCKET_ENDPOINT CONTROLLER_ADDRESS : An address you would like to bond to the stash account. Stash and Controller can be the same address but it is not recommended since it defeats the security of the two-account staking model. NUMBER_OF_TOKENS : The number of DOT you would like to stake to the network. Decimal places DOT has ten decimal places and is always represented as an integer with zeros at the end. So 1 DOT = 10_000_000_000 Plancks. REWARD_DESTINATION : Staked - Pay into the stash account, increasing the amount at stake accordingly. Stash - Pay into the stash account, not increasing the amount at stake. Account - Pay into a custom account, like so: Account DMTHrNcmA8QbqRS4rBq8LXn8ipyczFoNMb1X4cY2WD9tdBX . Controller - Pay into the controller account. Example: polkadot-js-api --seed "xxxx xxxxx xxxx xxxxx" tx.staking.bond DMTHrNcmA8QbqRS4rBq8LXn8ipyczFoNMb1X4cY2WD9tdBX 1000000000000 Staked --ws wss://rpc.polkadot.io Result: .. . .. . "status" : { "InBlock" : "0x0ed1ec0ba69564e8f98958d69f826adef895b5617366a32a3aa384290e98514e" } You can check the transaction status by using the value of the InBlock in Polkascan . Also, you can verify the bonding state under the Staking page on the Polkadot-JS Apps Dashboard. Step 3. Nominate a validator ‚Äã To nominate a validator, you can execute the following command: polkadot-js-api --seed "MNEMONIC_PHRASE" tx.staking.nominate '["VALIDATOR_ADDRESS"]' --ws WS_ENDPOINT polkadot-js-api --seed "xxxx xxxxx xxxx xxxxx" tx.staking.nominate '["CmD9vaMYoiKe7HiFnfkftwvhKbxN9bhyjcDrfFRGbifJEG8","E457XaKbj2yTB2URy8N4UuzmyuFRkcdxYs67UvSgVr7HyFb"]' --ws wss://rpc.polkadot.io After a few seconds, you should see the hash of the transaction, and if you would like to verify the nomination status, you can check that on the Polkadot-JS UI as well. Edit this page Last updated on Jul 14, 2023 by Filippo Previous Resolving Errors Next How to run a Validator on Polkadot Setting up Stash and Staking Proxy Accounts Using the Polkadot Staking Dashboard Why am I not receiving Staking Rewards? Using Polkadot-JS UI Using Command-Line Interface (CLI) Step 1: Install @polkadot/api-cli Step 2. Bond your DOT Step 3. Nominate a validator ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Run a Validator (Polkadot) General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides How to run a Validator on Polkadot Validator Payout Overview Using systemd for a Validator Node Secure Validator How to Upgrade Your Validator Monitor your node How to Chill How to Stop Validating Validator Community Overview Validator Best Practices Governance Guides Maintain Validator Guides How to run a Validator on Polkadot On this page Run a Validator (Polkadot) tip If you are a beginner, it is recommended that you start your validator journey on Kusama network. Check the Kusama guide for details on how to get started. Preliminaries ‚Äã Running a validator on a live network is a lot of responsibility! You will be accountable for not only your own stake, but also the stake of your current nominators. If you make a mistake and get slashed, your tokens and your reputation will be at risk. However, running a validator can also be very rewarding, knowing that you contribute to the security of a decentralized network while growing your stash. danger It is highly recommended that you have significant system administration experience before attempting to run your own validator. You must be able to handle technical issues and anomalies with your node which you must be able to tackle yourself. Being a validator involves more than just executing the Polkadot binary. Since security is so important to running a successful validator, you should take a look at the secure validator information to make sure you understand the factors to consider when constructing your infrastructure. As you progress in your journey as a validator, you will likely want to use this repository as a starting point for your own modifications and customizations. If you need help, please reach out on the Polkadot Validator Lounge on Element. The team and other validators are there to help answer questions and provide tips from experience. How many DOT do I need to become an active Validator? ‚Äã You can have a rough estimate on that by using the methods listed here . To be elected into the set, you need a minimum stake behind your validator. This stake can come from yourself or from nominators . This means that as a minimum, you will need enough DOT to set up stash and staking proxy accounts with the existential deposit, plus a little extra for transaction fees. The rest can come from nominators. To understand how validators are elected, check the NPoS Election algorithms page. On-Chain Data for Reference On Polkadot, the minimum stake backing a validator in the active set is in the era . On Kusama, the minimum stake backing a validator in the active set is in the era . Warning: Any DOT that you stake for your validator is liable to be slashed, meaning that an insecure or improper setup may result in loss of DOT tokens! If you are not confident in your ability to run a validator node, it is recommended to nominate your DOT to a trusted validator node instead. Initial Set-up ‚Äã ::: Requirements ‚Äã The most common way for a beginner to run a validator is on a cloud server running Linux. You may choose whatever VPS provider that your prefer. As OS it is best to use a recent Debian Linux. For this guide we will be using Ubuntu 22.04 , but the instructions should be similar for other platforms. Reference Hardware ‚Äã The transaction weights in Polkadot are benchmarked on reference hardware. We ran the benchmark on VM instances of two major cloud providers: Google Cloud Platform (GCP) and Amazon Web Services (AWS). To be specific, we used n2-standard-8 VM instance on GCP and c6i.4xlarge on AWS. It is recommended that the hardware used to run the validators at least matches the specs of the reference hardware in order to ensure they are able to process all blocks in time. If you use subpar hardware you will possibly run into performance issues, get less era points, and potentially even get slashed. CPU x86-64 compatible; Intel Ice Lake, or newer (Xeon or Core series); AMD Zen3, or newer (EPYC or Ryzen); 4 physical cores @ 3.4GHz; Simultaneous multithreading disabled (Hyper-Threading on Intel, SMT on AMD); Prefer single-threaded performance over higher cores count. A comparison of single-threaded performance can be found here . Storage An NVMe SSD of 1 TB (As it should be reasonably sized to deal with blockchain growth). An estimation of current chain snapshot sizes can be found here . In general, the latency is more important than the throughput. Memory 32 GB DDR4 ECC. System Linux Kernel 5.16 or newer. Network The minimum symmetric networking speed is set to 500 Mbit/s (= 62.5 MB/s). This is required to support a large number of parachains and allow for proper congestion control in busy network situations. The specs posted above are not a hard requirement to run a validator, but are considered best practice. Running a validator is a responsible task; using professional hardware is a must in any way. Install & Configure Network Time Protocol (NTP) Client ‚Äã NTP is a networking protocol designed to synchronize the clocks of computers over a network. NTP allows you to synchronize the clocks of all the systems within the network. Currently it is required that validators' local clocks stay reasonably in sync, so you should be running NTP or a similar service. You can check whether you have the NTP client by running: If you are using Ubuntu 18.04 or a newer version, NTP Client should be installed by default. timedatectl If NTP is installed and running, you should see System clock synchronized: yes (or a similar message). If you do not see it, you can install it by executing: sudo apt-get install ntp ntpd will be started automatically after install. You can query ntpd for status information to verify that everything is working: sudo ntpq -p danger Skipping this can result in the validator node missing block authorship opportunities. If the clock is out of sync (even by a small amount), the blocks the validator produces may not get accepted by the network. This will result in ImOnline heartbeats making it on chain, but zero allocated blocks making it on chain. Make Sure Landlock is Enabled ‚Äã Landlock is a Linux security feature used in Polkadot: Landlock empowers any process, including unprivileged ones, to securely restrict themselves. To make use of landlock, make sure you are on the reference kernel version or newer. Most Linux distributions should already have landlock enabled, but you can check by running the following as root: dmesg | grep landlock || journalctl -kg landlock If it is not enabled, please see the official docs ("Kernel support") if you would like to build Linux with landlock enabled. Installing the polkadot binary ‚Äã Installation from official releases ‚Äã The official polkadot binaries can be downloaded from the Github Releases . You should download the latest available version. You can also download the binary by using the following direct link (replace X.Y.Z by the appropriate version): https://github.com/paritytech/polkadot/releases/download/vX.Y.Z/polkadot Optional: Installation with Package Managers ‚Äã The Polkadot Binary in included in Debian derivatives (i.e. Debian , Ubuntu ) and RPM-based distros (i.e. Fedora , CentOS ). Debian-based (Debian, Ubuntu) ‚Äã Run the following commands as the root user: # Import the [email protected] GPG key gpg --recv-keys --keyserver hkps://keys.mailvelope.com 9D4B2B6EB8F97156D19669A9FF0812D491B96798 gpg --export 9D4B2B6EB8F97156D19669A9FF0812D491B96798 > /usr/share/keyrings/parity.gpg # Add the Parity repository and update the package index echo 'deb [signed-by=/usr/share/keyrings/parity.gpg] https://releases.parity.io/deb release main' > /etc/apt/sources.list.d/parity.list apt update # Install the `parity-keyring` package - This will ensure the GPG key # used by APT remains up-to-date apt install parity-keyring # Install polkadot apt install polkadot RPM-based (Fedora, CentOS) ‚Äã Run the following commands as the root user: # Install dnf-plugins-core (This might already be installed) dnf install dnf-plugins-core # Add the repository and enable it dnf config-manager --add-repo https://releases.parity.io/rpm/polkadot.repo dnf config-manager --set-enabled polkadot # Install polkadot (You may have to confirm the import of the GPG key, which # should have the following fingerprint: 9D4B2B6EB8F97156D19669A9FF0812D491B96798) dnf install polkadot After installing Polkadot, you can verify the installation by running which polkadot It should return /usr/bin/polkadot By default, the Polkadot systemd service is disabled To start the service, run: sudo systemctl start polkadot.service Optional: Installation with Ansible ‚Äã To manage Polkadot installation with Ansible, you can use the Substrate node role distributed on the Parity chain operations Ansible collection Optional: Installation with Docker ‚Äã To run Polkadot on a Docker or an OCI compatible container runtime, you can use the official parity/polkadot docker image , available on Docker Hub (replace X.Y.Z by the appropriate version): docker.io/parity/polkadot:vX.Y.Z Building the polkadot binary from sources ‚Äã Prerequisites: Install Rust and Dependencies ‚Äã If you have never installed Rust, you should do this first. If you have already installed Rust, run the following command to make sure you are using the latest version. rustup update If not, this command will fetch the latest version of Rust and install it. curl https://sh.rustup.rs -sSf | sh -s -- -y note If you do not have "curl" installed, run: sudo apt install curl It will also be valuable to have "websocat" (Netcat, curl and socat for WebSockets) installed for RPC interactions. Installation instructions for various operating systems can be found here . To configure your shell, run the following command. source $HOME/.cargo/env Verify your installation. rustc --version Finally, run this command to install the necessary dependencies for compiling and running the Polkadot node software. sudo apt install make clang pkg-config libssl-dev build-essential Note - if you are using OSX and you have Homebrew installed, you can issue the following equivalent command INSTEAD of the previous one: brew install cmake pkg-config openssl git llvm Build the Polkadot Binary from sources ‚Äã You can build the polkadot binary from the paritytech/polkadot repository on GitHub. You should generally use the latest X.Y.Z tag. You should either review the output from the "git tag" command or view the Polkadot Github tags to see a list of all the available release versions. You should replace VERSION below with the latest build (i.e., the highest number). note If you prefer to use SSH rather than HTTPS, you can replace the first line of the below with git clone [email protected] :paritytech/polkadot.git git clone https://github.com/paritytech/polkadot.git cd polkadot Run the following command to find the latest version. git tag -l | sort -V | grep -v -- '-rc' Find the latest version; replace "VERSION" in the commmand below and run to change your branch. git checkout VERSION ./scripts/init.sh Build native code with the production profile. cargo build --profile production This step will take a while (generally 10 - 40 minutes, depending on your hardware). Compilation Errors If you run into compile errors, you may have to pin the version of Rust compiler to the one that was used to build the release. Check out Rust compiler versions section in the release notes. This can be done by running: rustup install nightly-2022-05-18 rustup target add wasm32-unknown-unknown --toolchain nightly-2022-05-18 cargo +nightly-2022-05-18 build --release You may also need to run the build more than once. If you would like to execute the tests, run the following command: cargo test --all If you are interested in generating keys locally, you can also install subkey from the same directory. You may then take the generated subkey executable and transfer it to an air-gapped machine for extra security. cargo install --force --git https://github.com/paritytech/substrate subkey Synchronize Chain Data ‚Äã You can begin syncing your node by running the following command if you do not want to start in validator mode right away: polkadot info If you want to run a validator on Kusama, you have an option to specify the chain. With no specification, this would default to Polkadot. polkadot --chain=kusama 2021-06-17 03:07:07 Parity Polkadot 2021-06-17 03:07:07 ‚úåÔ∏è version 0.9.5-95f6aa201-x86_64-linux-gnu 2021-06-17 03:07:07 ‚ù§Ô∏è by Parity Technologies < [email protected] >, 2017-2021 2021-06-17 03:07:07 üìã Chain specification: Polkadot 2021-06-17 03:07:07 üè∑ Node name: boiling-pet-7554 2021-06-17 03:07:07 üë§ Role: FULL 2021-06-17 03:07:07 üíæ Database: RocksDb at /root/.local/share/polkadot/chains/polkadot/db 2021-06-17 03:07:07 ‚õì Native runtime: polkadot-9050 (parity-polkadot-0.tx7.au0) 2021-06-17 03:07:10 üè∑ Local node identity is: 12D3KooWLtXFWf1oGrnxMGmPKPW54xWCHAXHbFh4Eap6KXmxoi9u 2021-06-17 03:07:10 üì¶ Highest known block at #17914 2021-06-17 03:07:10 „ÄΩÔ∏è Prometheus server started at 127.0.0.1:9615 2021-06-17 03:07:10 Listening for new connections on 127.0.0.1:9944. Example of node sync 2021-06-17 03:07:39 üîç Discovered new external address for our node: /ip4/10.26.16.1/tcp/30333/ws/p2p/12D3KooWLtXFWf1oGrnxMGmPKPW54xWCHAXHbFh4Eap6KXmxoi9u 2021-06-17 03:07:40 ‚öôÔ∏è Syncing 218.8 bps, target=#5553764 (17 peers), best: #24034 (0x08af‚Ä¶dcf5), finalized #23552 (0xd4f0‚Ä¶2642), ‚¨á 173.5kiB/s ‚¨Ü 12.7kiB/s 2021-06-17 03:07:45 ‚öôÔ∏è Syncing 214.8 bps, target=#5553765 (20 peers), best: #25108 (0xb272‚Ä¶e800), finalized #25088 (0x94e6‚Ä¶8a9f), ‚¨á 134.3kiB/s ‚¨Ü 7.4kiB/s 2021-06-17 03:07:50 ‚öôÔ∏è Syncing 214.8 bps, target=#5553766 (21 peers), best: #26182 (0xe7a5‚Ä¶01a2), finalized #26112 (0xcc29‚Ä¶b1a9), ‚¨á 5.0kiB/s ‚¨Ü 1.1kiB/s 2021-06-17 03:07:55 ‚öôÔ∏è Syncing 138.4 bps, target=#5553767 (21 peers), best: #26874 (0xcf4b‚Ä¶6553), finalized #26624 (0x9dd9‚Ä¶27f8), ‚¨á 18.9kiB/s ‚¨Ü 2.0kiB/s 2021-06-17 03:08:00 ‚öôÔ∏è Syncing 37.0 bps, target=#5553768 (22 peers), best: #27059 (0x5b73‚Ä¶6fc9), finalized #26624 (0x9dd9‚Ä¶27f8), ‚¨á 14.3kiB/s ‚¨Ü 4.4kiB/s Use Warp sync for faster syncing By default, the node performs full sync, which downloads and validates the full blockchain history. Full sync works by listening to announced blocks and requesting the blocks from the announcing peers, or just the block headers in case of light clients. Fast sync is another option that works by downloading the block header history and validating the authority set changes in order to arrive at a specific (usually the most recent) header. After the desired header has been reached and verified, the state can be downloaded and imported. Once this process has been completed, the node can proceed with a full sync. polkadot --sync warp Warp sync initially downloads and validates the finality proofs from GRANDPA and then downloads the state of the latest finalized block. After the warp sync, the node is ready to import the latest blocks from the network and can be used as a Validator. The blocks from genesis will be downloaded in the background. Check this discussion for more information about the different sync options available. Validators should sync using the RocksDb backend This is implicit by default, but can be explicit by passing the --database RocksDb flag. In the future, it is recommended to switch to the faster and more efficient ParityDB option. Note that ParityDB is still experimental and should not be used in production. If you want to test out ParityDB, you can add the flag --database paritydb . Switching between database backends will require a resync. Depending on the size of the chain when you do this, this step may take anywhere from a few minutes to a few hours. If you are interested in determining how much longer you have to go, your server logs (printed to STDOUT from the polkadot process) will tell you the latest block your node has processed and verified. You can then compare that to the current highest block via Telemetry or the PolkadotJS Block Explorer . Database Snapshot Services ‚Äã If you start a node for the first time, it will start building from the genesis block. This process can take a while depending on the database size. To make this process faster, snapshots can be used. Snapshots are compressed backups of the database directory of Polkadot/Kusama nodes, containing the whole chain (or a pruned version of it, with states only from the latest 1000 or 256 blocks). Listed below are a few public snapshot providers for Polkadot and Kusama. Stakeworld Polkachu Polkashots caution For the security of the network, it is recommended that you sync from scratch, even if you are running your node in pruning mode for validation. The reason is that if these snapshots get corrupted and a majority of nodes run based on these snapshots, the network could end up running on a non-canonical chain. Bond DOT ‚Äã It is highly recommended that you set a stash account and a staking proxy. For this, you will create two accounts and make sure each of them have at least enough funds to pay the fees for making transactions. Keep most of your funds in the stash account since it is meant to be the custodian of your staking funds. Make sure not to bond all your DOT balance since you will be unable to pay transaction fees from your bonded balance. It is now time to set up our validator. We will do the following: Bond the DOT of the Stash account. These DOT will be put at stake for the security of the network and can be slashed. Select the staking proxy. This is the account that will decide when to start or stop validating. First, go to the Staking section. Click on "Account Actions", and then the "+ Stash" button. Stash account - Select your Stash account. In this example, we will bond 1 DOT, where the minimum bonding amount is 1. Make sure that your Stash account contains at least this much. You can, of course, stake more than this. Staking proxy account - Select the staking proxy account created earlier. This account will also need a small amount of DOT in order to start and stop validating. Value bonded - How much DOT from the Stash account you want to bond/stake. Note that you do not need to bond all of the DOT in that account. Also note that you can always bond more DOT later. However, withdrawing any bonded amount requires the duration of the unbonding period. On Kusama, the unbonding period is 7 days. On Polkadot, the planned unbonding period is 28 days. Payment destination - The account where the rewards from validating are sent. More info here . Starting with runtime version v23 natively included in client version 0.9.3 , payouts can go to any custom address. If you'd like to redirect payments to an account that is neither the staking proxy nor the stash account, set one up. Note that it is extremely unsafe to set an exchange address as the recipient of the staking rewards. Once everything is filled in properly, click Bond and sign the transaction with your Stash account. After a few seconds, you should see an ExtrinsicSuccess message. Your bonded account will available under Stashes . You should now see a new card with all your accounts (note: you may need to refresh the screen). The bonded amount on the right corresponds to the funds bonded by the Stash account. Set Session Keys ‚Äã Session keys are consensus critical If you are not sure if your node has the current session keys that you made the setKeys transaction then you can use one of the two available RPC methods to query your node: hasKey to check for a specific key or hasSessionKeys to check the full session key public key string. Once your node is fully synced, stop the process by pressing Ctrl-C. At your terminal prompt, you will now start running the node. polkadot --validator --name "name on telemetry" Similarly: 2021-06-17 03:12:08 Parity Polkadot 2021-06-17 03:12:08 ‚úåÔ∏è version 0.9.5-95f6aa201-x86_64-linux-gnu 2021-06-17 03:12:08 ‚ù§Ô∏è by Parity Technologies < [email protected] >, 2017-2021 2021-06-17 03:12:08 üìã Chain specification: Polkadot 2021-06-17 03:12:08 üè∑ Node name: nateched-test 2021-06-17 03:12:08 üë§ Role: AUTHORITY 2021-06-17 03:12:08 üíæ Database: RocksDb at /root/.local/share/polkadot/chains/polkadot/db 2021-06-17 03:12:08 ‚õì Native runtime: polkadot-9050 (parity-polkadot-0.tx7.au0) 2021-06-17 03:12:12 üè∑ Local node identity is: 12D3KooWLtXFWf1oGrnxMGmPKPW54xWCHAXHbFh4Eap6KXmxoi9u 2021-06-17 03:12:12 üì¶ Highest known block at #64673 2021-06-17 03:12:12 „ÄΩÔ∏è Prometheus server started at 127.0.0.1:9615 2021-06-17 03:12:12 Listening for new connections on 127.0.0.1:9944. 2021-06-17 03:12:12 üë∂ Starting BABE Authorship worker 2021-06-17 03:12:16 üîç Discovered new external address for our node: /ip4/10.26.11.1/tcp/30333/p2p/12D3KooWLtXFWf1oGrnxMGmPKPW54xWCHAXHbFh4Eap6KXmxoi9u 2021-06-17 03:12:17 ‚öôÔ∏è Syncing, target=#5553810 (14 peers), best: #65068 (0x6da5‚Ä¶0662), finalized #65024 (0x4e84‚Ä¶d170), ‚¨á 352.2kiB/s ‚¨Ü 75.6kiB/s You can give your validator any name that you like, but note that others will be able to see it, and it will be included in the list of all servers using the same telemetry server. Since numerous people are using telemetry, it is recommended that you choose something likely to be unique. Generating the Session Keys ‚Äã You need to tell the chain your Session keys by signing and submitting an extrinsic. This is what associates your validator node with your stash account on Polkadot. Option 1: PolkadotJS-APPS ‚Äã You can generate your Session keys in the client via the apps RPC. If you are doing this, make sure that you have the PolkadotJS-Apps explorer attached to your validator node. You can configure the apps dashboard to connect to the endpoint of your validator in the Settings tab. If you are connected to a default endpoint hosted by Parity of Web3 Foundation, you will not be able to use this method since making RPC requests to this node would effect the local keystore hosted on a public node and you want to make sure you are interacting with the keystore for your node . Once ensuring that you have connected to your node, the easiest way to set session keys for your node is by calling the author_rotateKeys RPC request to create new keys in your validator's keystore. Navigate to Toolbox tab and select RPC Calls then select the author > rotateKeys() option and remember to save the output that you get back for a later step. Option 2: CLI ‚Äã If you are on a remote server, it is easier to run this command on the same machine (while the node is running with the default WS RPC port configured): echo '{"id":1,"jsonrpc":"2.0","method":"author_rotateKeys","params":[]}' | websocat -n1 -B 99999999 ws://127.0.0.1:9944 The output will have a hex-encoded "result" field. The result is the concatenation of the four public keys. Save this result for a later step. You can restart your node at this point. Submitting the setKeys Transaction ‚Äã You need to tell the chain your Session keys by signing and submitting an extrinsic. This is what associates your validator with your staking proxy. Go to Staking > Account Actions , and click "Set Session Key" on the bonding account you generated earlier. Enter the output from author_rotateKeys in the field and click "Set Session Key". Submit this extrinsic and you are now ready to start validating. Validate ‚Äã To verify that your node is live and synchronized, head to Telemetry and find your node. Note that this will show all nodes on the Polkadot network, which is why it is important to select a unique name! In this example, we used the name techedtest and have successfully located it upon searching: Setup via Validator Tab ‚Äã Here you will need to input the Keys from rotateKeys , which is the Hex output from author_rotateKeys . The keys will show as pending until applied at the start of a new session. The "reward commission percentage" is the commission percentage that you can declare against your validator's rewards. This is the rate that your validator will be commissioned with. Payment preferences - You can specify the percentage of the rewards that will get paid to you. The remaining will be split among your nominators. Setting a commission rate of 100% suggests that you do not want your validator to receive nominations You can also determine if you would like to receive nominations with the "allows new nominations" option. Click "Bond & Validate". If you go to the "Staking" tab, you will see a list of active validators currently running on the network. At the top of the page, it shows the number of validator slots that are available as well as the number of nodes that have signaled their intention to be a validator. You can go to the "Waiting" tab to double check to see whether your node is listed there. The validator set is refreshed every era. In the next era, if there is a slot available and your node is selected to join the validator set, your node will become an active validator. Until then, it will remain in the waiting queue. If your validator is not selected to become part of the validator set, it will remain in the waiting queue until it is. There is no need to re-start if you are not selected for the validator set in a particular era. However, it may be necessary to increase the number of DOT staked or seek out nominators for your validator in order to join the validator set. Congratulations! If you have followed all of these steps, and been selected to be a part of the validator set, you are now running a Polkadot validator! If you need help, reach out on the Polkadot Validator chat . Thousand Validators Programme ‚Äã The Thousand Validators Programme is a joint initiative by Web3 Foundation and Parity Technologies to provide support for community validators. If you are interested in applying for the programme, you can find more information on the wiki page . Running a validator on a testnet ‚Äã To verify your validator set up, it is possible to run it against a PoS test network such as Westend or Wococo. However, validator slots are intentionally limited on Westend to ensure the stability and availability of the testnet for the Polkadot release process. As such it is advised for node operators wishing to run testnet validators to join the Wococo network. You can obtain WOOK tokens here . Here is a small comparison of each network characteristics as relevant to validators: Network Polkadot Westend Wococo epoch 4h 1h 10m era 1d 6h 1h token DOT WND (test) WOOK (test) active validators ~300 ~20 10<x<100 FAQ ‚Äã Why am I unable to synchronize the chain with 0 peers? ‚Äã Make sure to enable 30333 libp2p port. Eventually, it will take a little bit of time to discover other peers over the network. How do I clear all my chain data? ‚Äã polkadot purge-chain info Check out the Substrate StackExchange to quickly get the answers you need. Note about VPS ‚Äã VPS providers are very popular for running servers of any kind. Extensive benchmarking was conducted to ensure that VPS servers are able to keep up with the work load in general. note Before you run a live Validator, please verify if the advertised performance is actually delivered consistently by the VPS provider. that this is not an endorsement in any way: GCP's c2 and c2d machine families AWS's c6id machine family The following additional configurations were applied to the instances to tune their performance: Disable SMT ‚Äã As critical path of Substrate is single-threaded we need to optimize for single-core CPU performance. The node still profits from multiple cores when doing networking and other non-runtime operations. It is therefore still necessary to run it on at least the minimum required number of cores. Disabling SMT improves the performance as each vCPU becomes mapped to a physical CPU core rather than being presented to the OS as two logical cores. SMT implementation is called Hyper-Threading on Intel and 2-way SMT on AMD Zen. To disable SMT in runtime: for cpunum in $( cat /sys/devices/system/cpu/cpu*/topology/thread_siblings_list | cut -s -d, -f2- | tr ',' '\n' | sort -un ) do echo 0 > /sys/devices/system/cpu/cpu $cpunum /online done It will disable every other (vCPU) core. To save changes permanently add nosmt=force as kernel parameter. Edit /etc/default/grub and add nosmt=force to GRUB_CMDLINE_LINUX_DEFAULT variable and run sudo update-grub . After the reboot you should see half of the cores are offline. Run lscpu --extended to confirm. Disable automatic NUMA balancing ‚Äã If you have multiple physical CPUs (CPU0 and CPU1) in the system each with its own memory bank (MB0 and MB1), then it is usually slower for a CPU0 to access MB1 due to the slower interconnection. To prevent the OS from automatically moving the running Substrate process from one CPU to another and thus causing an increased latency, it is recommended to disable automatic NUMA balancing. With automatic NUMA balancing disabled, an OS will always run a process on the same NUMA node where it was initially scheduled. To disable NUMA balancing in runtime: sysctl kernel.numa_balancing = 0 To save changes permanently, update startup options and reconfigure GRUB. Edit /etc/default/grub and add numa_balancing=disable to GRUB_CMDLINE_LINUX_DEFAULT variable and run sudo update-grub . After reboot you can confirm the change by running sysctl -a | grep 'kernel.numa_balancing' and checking if the parameter is set to 0 Configure Spectre/Meltdown Mitigations ‚Äã Spectre and Meltdown are vulnerabilities discovered in modern CPUs a few years ago. Mitigations were made to the Linux kernel to cope with the multiple variations of these attacks. Check out https://meltdownattack.com/ for more info. Initially those mitigations added ~20% penalty to the performance of the workloads. As CPU manufacturers started to roll-out mitigations implemented in hardware, the performance gap narrowed down . As the benchmark demonstrates, the performance penalty got reduced to ~7% on Intel 10th Gen CPUs. This is true for the workloads running on both bare-metal and VMs. But the penalty remains high for the containerized workloads in some cases. As demonstrated in Yusuke Endoh's article , a performance penalty for containerized workloads can be as high as 100%. This is due to SECCOMP profile being overprotective about applying Spectre/Meltdown mitigations without providing real security. A longer explanation is a available in the kernel patch discussion . Linux 5.16 loosened the protections applied to SECCOMP threads by default. Containers running on kernel 5.16 and later now don't suffer from the performance penalty implied by using a SECCOMP profile in container runtimes. For Linux >= 5.16 ‚Äã You are all set. The performance of containerized workloads is on par with non-containerized ones. You don't have to do anything. For Linux < 5.16 ‚Äã You'll need to disable mitigations for Spectre V2 for user-space tasks as well as Speculative Store Bypass Disable (SSBD) for Spectre V4. This patch message describes the reasoning for this default change in more detail: Ultimately setting SSBD and STIBP by default for all seccomp jails is a bad sweet spot and bad default with more cons than pros that end up reducing security in the public cloud (by giving an huge incentive to not expose SPEC_CTRL which would be needed to get full security with IBPB after setting nosmt in the guest) and by excessively hurting performance to more secure apps using seccomp that end up having to opt out with SECCOMP_FILTER_FLAG_SPEC_ALLOW. To disable the mitigations edit /etc/default/grub and add spec_store_bypass_disable=prctl spectre_v2_user=prctl to GRUB_CMDLINE_LINUX_DEFAULT variable, run sudo update-grub , then reboot. Note that mitigations are not disabled completely. You can fully disable all the available kernel mitigations by setting mitigations=off . But we don't recommend doing this unless you run a fully trusted code on the host. VPS List ‚Äã Google Cloud Amazon AWS OVH Digital Ocean Vultr Linode Scaleway OnFinality Beware of the Terms and Conditions and Acceptable Use Policies for each VPS provider You may be locked out of your account and your server shut down if you come in violation. For instance, Digital Ocean lists "Mining of Cryptocurrencies" under the Network Abuse section of their Acceptable Use Policy and requires explicit permission to do so. This may extend to other cryptocurrency activity. Edit this page Last updated on Jul 14, 2023 by Filippo Previous Nominator Guides Next Validator Payout Overview Preliminaries How many DOT do I need to become an active Validator? Initial Set-up Requirements Install & Configure Network Time Protocol (NTP) Client Make Sure Landlock is Enabled Installing the polkadot binary Building the polkadot binary from sources Build the Polkadot Binary from sources Synchronize Chain Data Bond DOT Set Session Keys Generating the Session Keys Submitting the setKeys Transaction Validate Setup via Validator Tab Thousand Validators Programme Running a validator on a testnet FAQ Why am I unable to synchronize the chain with 0 peers? How do I clear all my chain data? Note about VPS Disable SMT Disable automatic NUMA balancing Configure Spectre/Meltdown Mitigations VPS List ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Validator Payout Overview General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides How to run a Validator on Polkadot Validator Payout Overview Using systemd for a Validator Node Secure Validator How to Upgrade Your Validator Monitor your node How to Chill How to Stop Validating Validator Community Overview Validator Best Practices Governance Guides Maintain Validator Guides Validator Payout Overview On this page Validator Payout Overview Era Points ‚Äã For every era (a period of time approximately 6 hours in length in Kusama, and 24 hours in Polkadot), validators are paid proportionally to the amount of era points they have collected. Era points are reward points earned for payable actions like: issuing validity statements for parachain blocks. producing a non-uncle block in the Relay Chain. producing a reference to a previously unreferenced uncle block. producing a referenced uncle block. note An uncle block is a Relay Chain block that is valid in every regard, but which failed to become canonical. This can happen when two or more validators are block producers in a single slot, and the block produced by one validator reaches the next block producer before the others. We call the lagging blocks uncle blocks. Payments occur at the end of every era. Era points create a probabilistic component for staking rewards. If the mean of staking rewards is the average rewards per era, then the variance is the variability from the average staking rewards. The exact DOT value of each era point is not known in advance since it depends on the total number of points earned by all validators in a given era. This is designed this way so that the total payout per era depends on Polkadot's inflation model , and not on the number of payable actions (f.e., authoring a new block) executed. For more information, check this stackexchange post . With parachains now on Polkadot, a large percentage of era points will come from parachain validation, as a subset of validators are selected to para-validate for all parachains each epoch, and those para-validators can generate more era points as a result. Para-validators are rewarded 20 era points each for each parachain block that they validate. In this case, analyzing the expected value of staking rewards will paint a better picture as the weight of era points of validators and para-validators in the reward average are taken into consideration. High-level breakdown of reward variance This should only serve as a high-level overview of the probabilistic nature for staking rewards. Let: pe = para-validator era points, ne = non-para-validator era points, EV = expected value of staking rewards, Then, EV(pe) has more influence on the EV than EV(ne) . Since EV(pe) has a more weighted probability on the EV , the increase in variance against the EV becomes apparent between the different validator pools (aka. validators in the active set and the ones chosen to para-validate). Also, let: v = the variance of staking rewards, p = number of para-validators, w = number validators in the active set, e = era, Then, v ‚Üë if w ‚Üë , as this reduces p : w , with respect to e . Increased v is expected, and initially keeping p ‚Üì using the same para-validator set for all parachains ensures availability and approval voting . In addition, despite v ‚Üë on an e to e basis, over time, the amount of rewards each validator receives will equal out based on the continuous selection of para-validators. There are plans to scale the active para-validation set in the future Payout Scheme ‚Äã No matter how much total stake is behind a validator, all validators split the block authoring payout essentially equally. The payout of a specific validator, however, may differ based on era points , as described above. Although there is a probabilistic component to receiving era points, and they may be impacted slightly depending on factors such as network connectivity, well-behaving validators should generally average out to having similar era point totals over a large number of eras. Validators may also receive "tips" from senders as an incentive to include transactions in their produced blocks. Validators will receive 100% of these tips directly. Validators will receive staking rewards in the form of the native token of that chain (KSM for Kusama and DOT for Polkadot). For simplicity, the examples below will assume all validators have the same amount of era points, and received no tips. Validator Set Size (v): 4 Validator 1 Stake (v1): 18 tokens Validator 2 Stake (v2): 9 tokens Validator 3 Stake (v3): 8 tokens Validator 4 Stake (v4): 7 tokens Payout (p): 8 DOT Payout for each validator (v1 - v4): p / v = 8 / 4 = 2 tokens Note that this is different than most other Proof-of-Stake systems such as Cosmos. As long as a validator is in the validator set, it will receive the same block reward as every other validator. Validator v1 , who had 18 tokens staked, received the same reward (2 tokens) in this era as v4 who had only 7 tokens staked. Running Multiple Validators ‚Äã It is possible for a single entity to run multiple validators. Running multiple validators may provide a better risk/reward ratio. Assuming you have enough DOT, or enough stake nominates your validator, to ensure that your validators remain in the validator set, running multiple validators will result in a higher return than running a single validator. For the following example, assume you have 18 DOT to stake. For simplicity's sake, we will ignore nominators. Running a single validator, as in the example above, would net you 2 DOT in this era. Note that while DOT is used as an example, this same formula would apply to KSM when running a validator on Kusama. Validator Set Size (v): 4 Validator 1 Stake (v1): 18 DOT <- Your validator Validator 2 Stake (v2): 9 DOT Validator 3 Stake (v3): 8 DOT Validator 4 Stake (v4): 7 DOT Payout (p): 8 DOT Your payout = (p / v) * 1 = (8 / 4) * 1 = 2 Running two validators, and splitting the stake equally, would result in the original validator v4 to be kicked out of the validator set, as only the top v validators (as measured by stake) are selected to be in the validator set. More important, it would also double the reward that you get from each era. Validator Set Size (v): 4 Validator 1 Stake (v1): 9 DOT <- Your first validator Validator 2 Stake (v2): 9 DOT <- Your second validator Validator 3 Stake (v3): 9 DOT Validator 4 Stake (v4): 8 DOT Payout (p): 8 DOT Your payout = (p / v) * 2 = (8 / 4) * 2 = 4 With enough stake, you could run more than two validators. However, each validator must have enough stake behind it to be in the validator set. The incentives of the system favor equally-staked validators. This works out to be a dynamic, rather than static, equilibrium. Potential validators will run different numbers of validators and apply different amounts of stake to them as time goes on, and in response to the actions of other validators on the network. Slashing ‚Äã Although rewards are paid equally, slashes are relative to a validator's stake. Therefore, if you do have enough DOT to run multiple validators, it is in your best interest to do so. A slash of 30% will, of course, be more DOT for a validator with 18 DOT staked than one with 9 DOT staked. Running multiple validators does not absolve you of the consequences of misbehavior. Polkadot punishes attacks that appear coordinated more severely than individual attacks. You should not, for example, run multiple validators hosted on the same infrastructure. A proper multi-validator configuration would ensure that they do not fail simultaneously. Nominators have the incentive to nominate the lowest-staked validator, as this will result in the lowest risk and highest reward. This is due to the fact that while their vulnerability to slashing remains the same (since it is percentage-based), their rewards are higher since they will be a higher proportion of the total stake allocated to that validator. To clarify this, let us imagine two validators, v1 and v2 . Assume both are in the active set, have commission set to 0%, and are well-behaved. The only difference is that v1 has 90 DOT nominating it and v2 only has 10. If you nominate v1 , it now has 90 + 10 = 100 DOT, and you will get 10% of the staking rewards for the next era. If you nominate v2 , it now has 10 + 10 = 20 DOT nominating it, and you will get 50% of the staking rewards for the next era. In actuality, it would be quite rare to see such a large difference between the stake of validators, but the same principle holds even for smaller differences. If there is a 10% slash of either validator, then you will lose 1 DOT in each case. caution If a validator is oversubscribed in an era, staking rewards are distributed only to the the top nominators and the rest of the nominators do not receive any rewards. This is not the case for slashing! Every active nominator of the validator committing slashable offence will be slashed. Nominators and Validator Payments ‚Äã Nominated stake allows you to "vote" for validators and share in the rewards (and slashing) without running a validator node yourself. Validators can choose to keep a percentage of the rewards due to their validator to "reimburse" themselves for the cost of running a validator node. Other than that, all rewards are shared based on the stake behind each validator. This includes the stake of the validator itself, plus any stake bonded by nominators. info Validators set their preference as a percentage of the block reward, not an absolute number of DOT. Polkadot's block reward is based on the total amount at stake, with the reward peaking when the amount staked is at 50% of the total supply. The commission is set as the amount taken by the validator; that is, 0% commission means that the validator does not receive any proportion of the rewards besides that owed to it from self-stake, and 100% commission means that the validator operator gets all rewards and gives none to its nominators. In the following examples, we can see the results of several different validator payment schemes and split between nominator and validator stake. We will assume a single nominator for each validator. However, there can be numerous nominators for each validator. Rewards are still distributed proportionally - for example, if the total rewards to be given to nominators is 2 DOT, and there are four nominators with equal stake bonded, each will receive 0.5 DOT. Note also that a single nominator may stake different validators. Each validator in the example has selected a different validator payment (that is, a percentage of the reward set aside directly for the validator before sharing with all bonded stake). The validator's payment percentage (in DOT, although the same calculations work for KSM) is listed in brackets ( [] ) next to each validator. Note that since the validator payment is public knowledge, having a low or non-existent validator payment may attract more stake from nominators, since they know they will receive a larger reward. Validator Set Size (v): 4 Validator 1 Stake (v1) [20% commission]: 18 DOT (9 validator, 9 nominator) Validator 2 Stake (v2) [40% commission]: 9 DOT (3 validator, 6 nominator) Validator 3 Stake (v3) [10% commission]: 8 DOT (4 validator, 4 nominator) Validator 4 Stake (v4) [ 0% commission]: 6 DOT (1 validator, 5 nominator) Payout (p): 8 DOT Payout for each validator (v1 - v4): p / v = 8 / 4 = 2 DOT v1: (0.2 * 2) = 0.4 DOT -> validator payment (2 - 0.4) = 1.6 -> shared between all stake (9 / 18) * 1.6 = 0.8 -> validator stake share (9 / 18) * 1.6 = 0.8 -> nominator stake share v1 validator total reward: 0.4 + 0.8 = 1.2 DOT v1 nominator reward: 0.8 DOT v2: (0.4 * 2) = 0.8 DOT -> validator payment (2 - 0.8) = 1.2 -> shared between all stake (3 / 9) * 1.2 = 0.4 -> validator stake share (6 / 9) * 1.2 = 0.8 -> nominator stake share v2 validator total reward: 0.8 + 0.4 = 1.2 DOT v2 nominator reward: 0.8 DOT v3: (0.1 * 2) = 0.2 DOT -> validator payment (2 - 0.2) = 1.8 -> shared between all stake (4 / 8) * 1.8 = 0.9 -> validator stake share (4 / 8) * 1.8 = 0.9 -> nominator stake share v3 validator total reward: 0.2 + 0.9 DOT = 1.1 DOT v3 nominator reward: 0.9 DOT v4: (0 * 2) = 0 DOT -> validator payment (2 - 0) = 2.0 -> shared between all stake (1 / 6) * 2 = 0.33 -> validator stake share (5 / 6) * 2 = 1.67 -> nominator stake share v4 validator total reward: 0 + 0.33 DOT = 0.33 DOT v4 nominator reward: 1.67 DOT Edit this page Last updated on May 11, 2023 by Filippo Previous How to run a Validator on Polkadot Next Using systemd for a Validator Node Era Points Payout Scheme Running Multiple Validators Slashing Nominators and Validator Payments ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Using systemd for a Validator Node General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides How to run a Validator on Polkadot Validator Payout Overview Using systemd for a Validator Node Secure Validator How to Upgrade Your Validator Monitor your node How to Chill How to Stop Validating Validator Community Overview Validator Best Practices Governance Guides Maintain Validator Guides Using systemd for a Validator Node Using systemd for a Validator Node You can run your validator as a systemd process so that it will automatically restart on server reboots or crashes (and helps to avoid getting slashed!). Before following this guide you should have already set up your validator by following the How to validate article. First create a new unit file called polkadot-validator.service in /etc/systemd/system/ . touch /etc/systemd/system/polkadot-validator.service In this unit file you will write the commands that you want to run on server boot / restart. [Unit] Description=Polkadot Validator [Service] ExecStart=PATH_TO_POLKADOT_BIN --validator --name SHOW_ON_TELEMETRY Restart=always RestartSec=120 [Install] WantedBy=multi-user.target danger It is recommended to delay the restart of a node with RestartSec in the case of node crashes. It's possible that when a node crashes, consensus votes in GRANDPA aren't persisted to disk. In this case, there is potential to equivocate when immediately restarting. What can happen is the node will not recognize votes that didn't make it to disk, and will then cast conflicting votes. Delaying the restart will allow the network to progress past potentially conflicting votes, at which point other nodes will not accept them. To enable this to autostart on bootup run: systemctl enable polkadot-validator.service Start it manually with: systemctl start polkadot-validator.service You can check that it's working with: systemctl status polkadot-validator.service You can tail the logs with journalctl like so: journalctl -f -u polkadot-validator Edit this page Last updated on Sep 18, 2022 by Keith Alfaro Previous Validator Payout Overview Next Secure Validator ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Secure Validator General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides How to run a Validator on Polkadot Validator Payout Overview Using systemd for a Validator Node Secure Validator How to Upgrade Your Validator Monitor your node How to Chill How to Stop Validating Validator Community Overview Validator Best Practices Governance Guides Maintain Validator Guides Secure Validator On this page Secure Validator Validators in a Proof of Stake network are responsible for keeping the network in consensus and verifying state transitions. As the number of validators is limited, validators in the set have the responsibility to be online and faithfully execute their tasks. This primarily means that validators: Must be high availability. Must have infrastructure that protects the validator's signing keys so that an attacker cannot take control and commit slashable behavior. High Availability ‚Äã High availability set-ups that involve redundant validator nodes may seem attractive at first. However, they can be very dangerous if they are not set up perfectly. The reason for this is that the session keys used by a validator should always be isolated to just a single node. Replicating session keys across multiple nodes could lead to equivocation slashes or parachain validity slashes which can make you lose 100% of your staked funds . The good news is that 100% uptime of your validator is not really needed, as it has some buffer within eras in order to go offline for a little while and upgrade. For this reason, we advise that you only attempt a high availability set-up if you're confident you know exactly what you're doing. Many expert validators have made mistakes in the past due to the handling of session keys. Remember, even if your validator goes offline for some time, the offline slash is much more forgiving than the equivocation or parachain validity slashing. Key Management ‚Äã See the Polkadot Keys guide for more information on keys. The keys that are of primary concern for validator infrastructure are the Session keys. These keys sign messages related to consensus and parachains. Although Session keys are not account keys and therefore cannot transfer funds, an attacker could use them to commit slashable behavior. Session keys are generated inside the node via RPC call. See the How to Validate guide for instructions on setting Session keys. These should be generated and kept within your client. When you generate new Session keys, you must submit an extrinsic (a Session certificate) from your staking proxy key telling the chain your new Session keys. Generating session keys Session keys can also be generated outside the client and inserted into the client's keystore via RPC. For most users, we recommend using the key generation functionality within the client. Signing Outside the Client ‚Äã In the future, Polkadot will support signing payloads outside the client so that keys can be stored on another device, e.g. a hardware security module (HSM) or secure enclave. For the time being, however, Session key signatures are performed within the client. HSMs are not a panacea They do not incorporate any logic and will just sign and return whatever payload they receive. Therefore, an attacker who gains access to your validator node could still commit slashable behavior. Monitoring Tools ‚Äã Telemetry This tracks your node details including the version you are running, block height, CPU & memory usage, block propagation time, etc. Prometheus -based monitoring stack, including Grafana for dashboards and log aggregation. It includes alerting, querying, visualization, and monitoring features and works for both cloud and on-premise systems. The data from substrate-telemetry can be made available to Prometheus through exporters like this . Linux Best Practices ‚Äã Never use the root user. Always update the security patches for your OS. Enable and set up a firewall. Never allow password-based SSH, only use key-based access. Disable non-essential SSH subsystems (banner, motd, scp, X11 forwarding) and harden your SSH configuration ( reasonable guide to begin with ). Back up your storage regularly. Conclusions ‚Äã At the moment, Polkadot/Substrate can't interact with HSM/SGX, so we need to provide the signing key seeds to the validator machine. This key is kept in memory for signing operations and persisted to disk (encrypted with a password). Given that HA setups would always be at risk of double-signing and there's currently no built-in mechanism to prevent it, we propose having a single instance of the validator to avoid slashing. Slashing penalties for being offline are much less than those for equivocation. Validators ‚Äã Validators should only run the Polkadot binary, and they should not listen on any port other than the configured p2p port. Validators should run on bare-metal machines, as opposed to VMs. This will prevent some of the availability issues with cloud providers, along with potential attacks from other VMs on the same hardware. The provisioning of the validator machine should be automated and defined in code. This code should be kept in private version control, reviewed, audited, and tested. Session keys should be generated and provided in a secure way. Polkadot should be started at boot and restarted if stopped for any reason (supervisor process). Polkadot should run as a non-root user. Monitoring ‚Äã There should be an on-call rotation for managing the alerts. There should be a clear protocol with actions to perform for each level of each alert and an escalation policy. Resources ‚Äã Figment Network's Full Disclosure of Cosmos Validator Infrastructure Certus One's Knowledge Base EOS Block Producer Security List HSM Policies and the Important of Validator Security Edit this page Last updated on Jul 14, 2023 by Filippo Previous Using systemd for a Validator Node Next How to Upgrade Your Validator High Availability Key Management Signing Outside the Client Monitoring Tools Linux Best Practices Conclusions Validators Monitoring Resources ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
How to Upgrade Your Validator General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides How to run a Validator on Polkadot Validator Payout Overview Using systemd for a Validator Node Secure Validator How to Upgrade Your Validator Monitor your node How to Chill How to Stop Validating Validator Community Overview Validator Best Practices Governance Guides Maintain Validator Guides How to Upgrade Your Validator On this page How to Upgrade Your Validator Validators perform critical functions for the network, and as such, have strict uptime requirements. Validators may have to go offline for short-periods of time to upgrade client software or to upgrade the host machine. Usually, standard client upgrades will only require you to stop the service, replace the binary and restart the service. This operation can be executed within a session and if performed correctly will not produce a slashable event. Validators may also need to perform long-lead maintenance tasks that will span more than one session. Under these circumstances, an active validator may chill their stash and be removed from the active validator set. Alternatively, the validator may substitute the active validator server with another allowing the former to undergo maintenance activities. This guide will provide an option for validators to seamlessly substitute an active validator server to allow for maintenance operations. The process can take several hours, so make sure you understand the instructions first and plan accordingly. Keep an eye out on new releases from the community Upgrade or downgrade accordingly. Key Components ‚Äã Session Keys ‚Äã Session keys are stored in the client and used to sign validator operations. They are what link your validator node to your staking proxy. If you change them within a session you will have to wait for the current session to finish and a further two sessions to elapse before they are applied. More info about keys in Polkadot. Keystore ‚Äã Each validator server contains essential private keys in a folder called the keystore . These keys are used by a validator to sign transactions at the network level. If two or more validators sign certain transactions using the same keys, it can lead to an equivocation slash . For this reason, it is advised that validators DO NOT CLONE or COPY the keystore folder and instead generate session keys for each new validator instance. Default keystore path - /home/polkadot/.local/share/polkadot/chains/<chain>/keystore Steps ‚Äã The following steps require a second validator which will be referred to as Validator B ; the original server that is in the active set will be referred to as Validator A . Session N ‚Äã Start a second node. Once it is synced, use the --validator flag. This is now "Validator B." Generate Session keys for Validator B . Submit a set_key extrinsic from your staking proxy with the session key generated from Validator B . Take note of the Session that this extrinsic was executed in. Allow the current session to elapse and then wait for two full sessions. It is imperative that you keep Validator A running during this time. set_key does not have an immediate effect and requires two full sessions to elapse before it does. If you do switch off Validator A too early you may risk being chilled and face a fault within the Thousand Validator Programme. Session N+3 ‚Äã Validator B is now acting as your validator - you can safely perform operations on Validator A . When you are ready to restore Validator A : Start Validator A , sync the database and ensure that it is operating with the --validator flag. Generate new Session keys for Validator A . Submit a set_key extrinsic from your staking proxy with the session key generated from Validator A . Take note of the Session that this extrinsic was executed in. Again, it is imperative that Validator B is kept running until the current session finishes and two further full sessions have elapsed. Once this time has elapsed, Validator A will take over. You can safely stop Validator B. NOTE: To verify that the Session has changed, make sure that a block in the new Session is finalized. You should see log messages like the ones below to confirm the change: 2019-10-28 21:44:13 Applying authority set change scheduled at block #450092 2019-10-28 21:44:13 Applying GRANDPA set change to new set with 20 authorities Edit this page Last updated on Jul 14, 2023 by Filippo Previous Secure Validator Next Monitor your node Key Components Session Keys Keystore Steps Session N Session N+3 ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Monitor your node General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides How to run a Validator on Polkadot Validator Payout Overview Using systemd for a Validator Node Secure Validator How to Upgrade Your Validator Monitor your node How to Chill How to Stop Validating Validator Community Overview Validator Best Practices Governance Guides Maintain Validator Guides Monitor your node On this page Monitor your node This guide will walk you through how to set up Prometheus with Grafana to monitor your node using Ubuntu 18.04 or 20.04. A Substrate-based chain exposes data such as the height of the chain, the number of connected peers to your node, CPU, memory usage of your machine, and more. To monitor this data, Prometheus is used to collect metrics and Grafana allows for displaying them on the dashboard. Preparation ‚Äã First, create a user for Prometheus by adding the --no-create-home flag to disallow prometheus from logging in. sudo useradd --no-create-home --shell /usr/sbin/nologin prometheus Create the directories required to store the configuration and executable files. sudo mkdir /etc/prometheus sudo mkdir /var/lib/prometheus Change the ownership of these directories to prometheus so that only prometheus can access them. sudo chown -R prometheus:prometheus /etc/prometheus sudo chown -R prometheus:prometheus /var/lib/prometheus Installing and Configuring Prometheus ‚Äã After setting up the environment, update your OS, and install the latest Prometheus. You can check the latest release by going to their GitHub repository under the releases page. sudo apt-get update && apt-get upgrade wget https://github.com/prometheus/prometheus/releases/download/v2.26.0/prometheus-2.26.0.linux-amd64.tar.gz tar xfz prometheus-*.tar.gz cd prometheus-2.26.0.linux-amd64 The following two binaries are in the directory: prometheus - Prometheus main binary file promtool The following two directories (which contain the web interface, configuration files examples and the license) are in the directory: consoles console_libraries Copy the executable files to the /usr/local/bin/ directory. sudo cp ./prometheus /usr/local/bin/ sudo cp ./promtool /usr/local/bin/ Change the ownership of these files to the prometheus user. sudo chown prometheus:prometheus /usr/local/bin/prometheus sudo chown prometheus:prometheus /usr/local/bin/promtool Copy the consoles and console_libraries directories to /etc/prometheus sudo cp -r ./consoles /etc/prometheus sudo cp -r ./console_libraries /etc/prometheus Change the ownership of these directories to the prometheus user. sudo chown -R prometheus:prometheus /etc/prometheus/consoles sudo chown -R prometheus:prometheus /etc/prometheus/console_libraries Once everything is done, run this command to remove prometheus directory. cd .. && rm -rf prometheus* Before using Prometheus, it needs some configuration. Create a YAML configuration file named prometheus.yml by running the command below. sudo nano /etc/prometheus/prometheus.yml The configuration file is divided into three parts which are global , rule_files , and scrape_configs . scrape_interval defines how often Prometheus scrapes targets, while evaluation_interval controls how often the software will evaluate rules. rule_files block contains information of the location of any rules we want the Prometheus server to load. scrape_configs contains the information which resources Prometheus monitors. The configuration file should look like this below: global : scrape_interval : 15s evaluation_interval : 15s rule_files : # - "first.rules" # - "second.rules" scrape_configs : - job_name : "prometheus" scrape_interval : 5s static_configs : - targets : [ "localhost:9090" ] - job_name : "substrate_node" scrape_interval : 5s static_configs : - targets : [ "localhost:9615" ] With the above configuration file, the first exporter is the one that Prometheus exports to monitor itself. As we want to have more precise information about the state of the Prometheus server we reduced the scrape_interval to 5 seconds for this job. The parameters static_configs and targets determine where the exporters are running. The second exporter is capturing the data from your node, and the port by default is 9615 . You can check the validity of this configuration file by running promtool check config /etc/prometheus/prometheus.yml . Save the configuration file and change the ownership of the file to prometheus user. sudo chown prometheus:prometheus /etc/prometheus/prometheus.yml Starting Prometheus ‚Äã To test that Prometheus is set up properly, execute the following command to start it as the prometheus user. sudo -u prometheus /usr/local/bin/prometheus --config.file /etc/prometheus/prometheus.yml --storage.tsdb.path /var/lib/prometheus/ --web.console.templates = /etc/prometheus/consoles --web.console.libraries = /etc/prometheus/console_libraries The following messages indicate the status of the server. If you see the following messages, your server is set up properly. level = info ts = 2021 -04-16T19:02:20.167Z caller = main.go:380 msg = "No time or size retention was set so using the default time retention" duration = 15d level = info ts = 2021 -04-16T19:02:20.167Z caller = main.go:418 msg = "Starting Prometheus" version = "(version=2.26.0, branch=HEAD, revision=3cafc58827d1ebd1a67749f88be4218f0bab3d8d)" level = info ts = 2021 -04-16T19:02:20.167Z caller = main.go:423 build_context = "(go=go1.16.2, [email protected] , date=20210331-11:56:23)" level = info ts = 2021 -04-16T19:02:20.167Z caller = main.go:424 host_details = "(Linux 5.4.0-42-generic #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020 x86_64 ubuntu2004 (none))" level = info ts = 2021 -04-16T19:02:20.167Z caller = main.go:425 fd_limits = "(soft=1024, hard=1048576)" level = info ts = 2021 -04-16T19:02:20.167Z caller = main.go:426 vm_limits = "(soft=unlimited, hard=unlimited)" level = info ts = 2021 -04-16T19:02:20.169Z caller = web.go:540 component = web msg = "Start listening for connections" address = 0.0 .0.0:9090 level = info ts = 2021 -04-16T19:02:20.170Z caller = main.go:795 msg = "Starting TSDB ..." level = info ts = 2021 -04-16T19:02:20.171Z caller = tls_config.go:191 component = web msg = "TLS is disabled." http2 = false level = info ts = 2021 -04-16T19:02:20.174Z caller = head.go:696 component = tsdb msg = "Replaying on-disk memory mappable chunks if any" level = info ts = 2021 -04-16T19:02:20.175Z caller = head.go:710 component = tsdb msg = "On-disk memory mappable chunks replay completed" duration = 1 .391446ms level = info ts = 2021 -04-16T19:02:20.175Z caller = head.go:716 component = tsdb msg = "Replaying WAL, this may take a while" level = info ts = 2021 -04-16T19:02:20.178Z caller = head.go:768 component = tsdb msg = "WAL segment loaded" segment = 0 maxSegment = 4 level = info ts = 2021 -04-16T19:02:20.193Z caller = head.go:768 component = tsdb msg = "WAL segment loaded" segment = 1 maxSegment = 4 level = info ts = 2021 -04-16T19:02:20.221Z caller = head.go:768 component = tsdb msg = "WAL segment loaded" segment = 2 maxSegment = 4 level = info ts = 2021 -04-16T19:02:20.224Z caller = head.go:768 component = tsdb msg = "WAL segment loaded" segment = 3 maxSegment = 4 level = info ts = 2021 -04-16T19:02:20.229Z caller = head.go:768 component = tsdb msg = "WAL segment loaded" segment = 4 maxSegment = 4 level = info ts = 2021 -04-16T19:02:20.229Z caller = head.go:773 component = tsdb msg = "WAL replay completed" checkpoint_replay_duration = 43.716 ¬µs wal_replay_duration = 53 .973285ms total_replay_duration = 55 .445308ms level = info ts = 2021 -04-16T19:02:20.233Z caller = main.go:815 fs_type = EXT4_SUPER_MAGIC level = info ts = 2021 -04-16T19:02:20.233Z caller = main.go:818 msg = "TSDB started" level = info ts = 2021 -04-16T19:02:20.233Z caller = main.go:944 msg = "Loading configuration file" filename = /etc/prometheus/prometheus.yml level = info ts = 2021 -04-16T19:02:20.234Z caller = main.go:975 msg = "Completed loading of configuration file" filename = /etc/prometheus/prometheus.yml totalDuration = 824.115 ¬µs remote_storage = 3.131 ¬µs web_handler = 401ns query_engine = 1.056 ¬µs scrape = 236.454 ¬µs scrape_sd = 45.432 ¬µs notify = 723ns notify_sd = 2.61 ¬µs rules = 956ns level = info ts = 2021 -04-16T19:02:20.234Z caller = main.go:767 msg = "Server is ready to receive web requests." Go to http://SERVER_IP_ADDRESS:9090/graph to check whether you are able to access the Prometheus interface or not. If it is working, exit the process by pressing on CTRL + C . Next, we would like to automatically start the server during the boot process, so we have to create a new systemd configuration file with the following config. sudo nano /etc/systemd/system/prometheus.service [ Unit ] Description = Prometheus Monitoring Wants = network-online.target After = network-online.target [ Service ] User = prometheus Group = prometheus Type = simple ExecStart = /usr/local/bin/prometheus \ --config.file /etc/prometheus/prometheus.yml \ --storage.tsdb.path /var/lib/prometheus/ \ --web.console.templates = /etc/prometheus/consoles \ --web.console.libraries = /etc/prometheus/console_libraries ExecReload = /bin/kill -HUP $MAINPID [ Install ] WantedBy = multi-user.target Once the file is saved, execute the command below to reload systemd and enable the service so that it will be loaded automatically during the operating system's startup. sudo systemctl daemon-reload && systemctl enable prometheus && systemctl start prometheus Prometheus should be running now, and you should be able to access its front again end by re-visiting IP_ADDRESS:9090/ . Installing Grafana ‚Äã In order to visualize your node metrics, you can use Grafana to query the Prometheus server. Run the following commands to install it first. sudo apt-get install -y adduser libfontconfig1 wget https://dl.grafana.com/oss/release/grafana_7.5.4_amd64.deb sudo dpkg -i grafana_7.5.4_amd64.deb If everything is fine, configure Grafana to auto-start on boot and then start the service. sudo systemctl daemon-reload sudo systemctl enable grafana-server sudo systemctl start grafana-server You can now access it by going to the http://SERVER_IP_ADDRESS:3000/login . The default user and password is admin/admin. note If you want to change the port on which Grafana runs (3000 is a popular port), edit the file /usr/share/grafana/conf/defaults.ini with a command like sudo vim /usr/share/grafana/conf/defaults.ini and change the http_port value to something else. Then restart grafana with sudo systemctl restart grafana-server . In order to visualize the node metrics, click settings to configure the Data Sources first. Click Add data source to choose where the data is coming from. Select Prometheus . The only thing you need to input is the URL that is https://localhost:9090 and then click Save & Test . If you see Data source is working , your connection is configured correctly. Next, import the dashboard that lets you visualize your node data. Go to the menu bar on the left and mouse hover "+" then select Import . Import via grafana.com - It allows you to use a dashboard that someone else has created and made public. You can check what other dashboards are available via https://grafana.com/grafana/dashboards . In this guide, we use "My Polkadot Metrics" , so input "12425" under the id field and click Load . Once it has been loaded, make sure to select "Prometheus" in the Prometheus dropdown list. Then click Import . In the meantime, start your Polkadot node by running ./polkadot . If everything is done correctly, you should be able to monitor your node's performance such as the current block height, CPU, memory usage, etc. on the Grafana dashboard. Installing and Configuring Alertmanager (Optional) ‚Äã In this section, let's configure the Alertmanager that helps to predict the potential problem or notify you of the current problem in your server. Alerts can be sent in Slack, Email, Matrix, or others. In this guide, we will show you how to configure the email notifications using Gmail if your node goes down. First, download the latest binary of AlertManager and unzip it by running the command below: wget https://github.com/prometheus/alertmanager/releases/download/v0.21.0/alertmanager-0.21.0.linux-amd64.tar.gz tar -xvzf alertmanager-0.21.0.linux-amd64.tar.gz mv alertmanager-0.21.0.linux-amd64/alertmanager /usr/local/bin/ Gmail Setup ‚Äã To allow AlertManager to send an email to you, you will need to generate something called an app password in your Gmail account. For details, click here to follow the whole setup. You should see something like below: Copy and save it somewhere else first. AlertManager Configuration ‚Äã There is a configuration file named alertmanager.yml inside the directory that you just extracted in the previous command, but that is not of our use. We will create our alertmanager.yml file under /etc/alertmanager with the following config. note Ensure to change the ownership of "/etc/alertmanager" to prometheus by executing sudo chown -R prometheus:prometheus /etc/alertmanager global: resolve_timeout: 1m route: receiver: 'gmail-notifications' receivers: - name: 'gmail-notifications' email_configs: - to: YOUR_EMAIL from: YOUR_EMAIL smarthost: smtp.gmail.com:587 auth_username: YOUR_EMAIL auth_identity: YOUR_EMAIL auth_password: YOUR_APP_PASSWORD send_resolved: true With the above configuration, alerts will be sent using the the email you set above. Remember to change YOUR_EMAIL to your email and paste the app password you just saved earlier to the YOUR_APP_PASSWORD . Next, create another systemd configuration file named alertmanager.service by running the command sudo nano /etc/systemd/system/alertmanager.service with the following config. SERVER_IP Change to your host IP address and make sure port 9093 is opened. [Unit] Description=AlertManager Server Service Wants=network-online.target After=network-online.target [Service] User=root Group=root Type=simple ExecStart=/usr/local/bin/alertmanager --config.file /etc/alertmanager/alertmanager.yml --web.external-url=http://SERVER_IP:9093 --cluster.advertise-address='0.0.0.0:9093' [Install] WantedBy=multi-user.target To the start the Alertmanager, run the following commands: sudo systemctl daemon-reload && sudo systemctl enable alertmanager && sudo systemctl start alertmanager && sudo systemctl status alertmanager ‚óè alertmanager.service - AlertManager Server Service Loaded: loaded (/etc/systemd/system/alertmanager.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2020-08-20 22:01:21 CEST; 3 days ago Main PID: 20592 (alertmanager) Tasks: 70 (limit: 9830) CGroup: /system.slice/alertmanager.service You should see the process status is "active (running)" if you have configured properly. There is a Alertmanager plugin in Grafana that can help you to monitor the alert information. To install it, execute the command below: sudo grafana-cli plugins install camptocamp-prometheus-alertmanager-datasource And restart Grafana once the plugin is successfully installed. sudo systemctl restart grafana-server Now go to your Grafana dashboard SERVER_IP:3000 and configure the Alertmanager datasource. Go to Configuration -> Data Sources, search "Prometheus AlertManger" if you cannot find it at the top. Fill in the URL to your server location followed by the port number used in the Alertmanager. Then click "Save & Test" at the bottom to test the connection. To monitor the alerts, let's import dashboard " 8010 " that is used for Alertmanager. And make sure to select the "Prometheus AlertManager" in the last column. Then click "Import". You will end up having the following: AlertManager Integration ‚Äã To let the Prometheus server be able to talk to the AlertManager, we will need to add the following config in the etc/prometheus/prometheus.yml . rule_files: - 'rules.yml' alerting: alertmanagers: - static_configs: - targets: - localhost:9093 That is the updated etc/prometheus/prometheus.yml . global: scrape_interval: 15s evaluation_interval: 15s rule_files: - 'rules.yml' alerting: alertmanagers: - static_configs: - targets: - localhost:9093 scrape_configs: - job_name: 'prometheus' scrape_interval: 5s static_configs: - targets: ['localhost:9090'] - job_name: 'substrate_node' scrape_interval: 5s static_configs: - targets: ['localhost:9615'] We will need to create a new file called "rules.yml" under /etc/prometheus/ that is defined all the rules we would like to detect. If any of the rules defined in this file is fulfilled, an alert will be triggered. The rule below checks whether the instance is down. If it is down for more than 5 minutes, an email notification will be sent. If you would like to learn more about the details of the rule defining, go here . There are other interesting alerts you may find useful here . groups: - name: alert_rules rules: - alert: InstanceDown expr: up == 0 for: 5m labels: severity: critical annotations: summary: "Instance [{{ $labels.instance }}] down" description: "[{{ $labels.instance }}] of job [{{ $labels.job }}] has been down for more than 1 minute." Change the ownership of this file to prometheus instead of root by running: sudo chown prometheus:prometheus rules.yml To check the rules defined in the "rules.yml" is syntactically correct, run the following command: sudo -u prometheus promtool check rules rules.yml Finally, restart everything by running: sudo systemctl restart prometheus && sudo systemctl restart alertmanager Now if one of your target instances down, you will receive an alert on the AlertManager and Gmail like below. Edit this page Last updated on Dec 28, 2022 by Keith Alfaro Previous How to Upgrade Your Validator Next How to Chill Preparation Installing and Configuring Prometheus Starting Prometheus Installing Grafana Installing and Configuring Alertmanager (Optional) Gmail Setup AlertManager Configuration AlertManager Integration ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
How to Chill General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides How to run a Validator on Polkadot Validator Payout Overview Using systemd for a Validator Node Secure Validator How to Upgrade Your Validator Monitor your node How to Chill How to Stop Validating Validator Community Overview Validator Best Practices Governance Guides Maintain Validator Guides How to Chill On this page How to Chill Staking bonds can be in any of the three states: validating, nominating, or chilled (neither validating nor nominating). When a staker wants to temporarily pause their active engagement in staking but does not want to unbond their funds, they can choose to "chill" their involvement and keep their funds bonded. An account can step back from participating in active staking by clicking "Stop" under the Network > Staking > Account actions page in PolkadotJS Apps or by calling the chill extrinsic in the staking pallet . When an account chooses to chill, it becomes inactive in the next era. The call must be signed by the staking proxy account, not the stash . Primer on stash and staking proxy accounts If you need a refresher on the different responsibilities of the stash and staking proxy account when staking, take a look at the accounts section in the general staking guide. Consideration for Staking Election ‚Äã A bond that is actively participating in staking but chilled would continue to participate in staking for the rest of the current era. If the bond was chilled in sessions 1 through 4 and continues to be chilled for the rest of the era, it would NOT be selected for election in the next era. If a bond was chilled for the entire session 5, it would not be considered in the next election. If the bond was chilled in session 6, its participation in the next era's election would depend on its state in session 5. Chilling as a Nominator ‚Äã When you chill after being a nominator, your nominations will be reset. This means that when you decide to start nominating again you will need to select validators to nominate once again. These can be the same validators if you prefer, or, a completely new set. Just be aware - your nominations will not persist across chills. Your nominator will remain bonded when it is chilled. When you are ready to nominate again, you will not need to go through the whole process of bonding again, rather, you will issue a new nominate call that specifies the new validators to nominate. Chilling as a Validator ‚Äã When you voluntarily chill after being a validator, your nominators will remain. As long as your nominators make no action, you will still have the nominations when you choose to become an active validator once again. You bond however would not be listed as a nominable validator thus any nominators issuing new or revisions to existing nominations would not be able to select your bond. When you become an active validator, you will also need to reset your validator preferences (commission, etc.). These can be configured as the same values set previously or something different. Involuntary Chills ‚Äã If a validator was unresponsive for an entire session, the validator bond would be chilled in a process known as involuntary chilling. When a validator has been involuntarily chilled, it may restrict the validator from being selected in the next election depending on the session in which it was chilled (see considerations above). A chilled validator may re-declare the intent to validate at any time. However, it is recommended that the validator attempts to determine the source of the chill before doing so. Slashing may also result in an involuntary chill. However, in that scenario, the validator would also lose their nominations. By this action, even if the validator re-declares its intent to validate before session 5, there wouldn't be sufficient nominations to re-elect the node into the active set. Nominators have the option to renominate a slashed validator using a display row in Polkadot-JS UI. This row is displayed in the "Account Actions" tab for the nominator under a heading that says "Renomination required". Chill Other ‚Äã An unbounded and unlimited number of nominators and validators in Polkadot's NPoS is not possible due to constraints in the runtime. As a result, multiple checks are incorporated to keep the size of staking system manageable, like mandating minimum active bond requirements for both nominators and validators. When these requirements are modified through on-chain governance, they can be enforced only on the accounts that newly call nominate or validate after the update. The changes to the bonding parameters would not automatically chill the active accounts on-chain which do not meet the requirements. Chill Threshold ChillThreshold defines how close to the max nominators or validators we must reach before users can start chilling one-another. This value is currently set to % For instance, let us consider a scenario where the minimum staking requirement for nominators is changed from 80 DOTs to 120 DOTs. An account that was actively nominating with 80 DOTs before this update would still keep receiving staking rewards. To handle this corner case, the chillOther extrinsic was incorporated which also helps to keep things backwards compatible and safe. The chillOther extrinsic is permissionless and any third party user can target it on an account where the minimum active bond is not satisfied, and chill that account. The list of addresses of all the active validators and their nominators can be viewed by running validator stats script. Chill Other on Polkadot Network Through Referendum 90 , maxNominatorCount on Polkadot is set to None eliminating the upper bound on the number of nominators on the network. Due to this, the chillOther extrinsic on Polkadot network has no effect as the chill threshold will never be met. Edit this page Last updated on Jul 14, 2023 by Filippo Previous Monitor your node Next How to Stop Validating Consideration for Staking Election Chilling as a Nominator Chilling as a Validator Involuntary Chills Chill Other ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
How to Stop Validating General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides How to run a Validator on Polkadot Validator Payout Overview Using systemd for a Validator Node Secure Validator How to Upgrade Your Validator Monitor your node How to Chill How to Stop Validating Validator Community Overview Validator Best Practices Governance Guides Maintain Validator Guides How to Stop Validating On this page How to Stop Validating If you wish to remain a validator or nominator (e.g. you're only stopping for planned downtime or server maintenance), submitting the chill extrinsic in the staking pallet should suffice. It is only if you wish to unbond funds or reap an account that you should continue with the following. To ensure a smooth stop to validation, make sure you should do the following actions: Chill your validator Purge validator session keys Unbond your tokens These can all be done with PolkadotJS Apps interface or with extrinsics. Chill Validator ‚Äã To chill your validator or nominator, call the staking.chill() extrinsic. See the How to Chill page for more information. You can also claim your rewards at this time. Purge validator session keys ‚Äã Purging the validator's session keys removes the key reference. This can be done through the session.purgeKeys() extrinsic. The key reference exists on the account that originally called the session.set_keys() extrinsic, which could be the stash or the staking proxy (at the time the keys were set). Purge keys using the same account that set the keys Make sure to call the session.purge_keys() extrinsic from the same account that set the keys in the first place in order for the correct reference to be removed. Calling the session.purge_keys() from the wrong account, although it may succeed, will result in a reference on the other account that cannot be removed, and as a result that account cannot be reaped. caution If you skip this step, you will not be able to reap your stash account , and you will also need to rebond, purge the session keys, unbond, and wait the unbonding period again before being able to transfer your tokens. See Unbonding and Rebonding for more details. Unbond your tokens ‚Äã Unbonding your tokens can be done through the Network > Staking > Account actions page in PolkadotJS Apps by clicking the corresponding stash account dropdown and selecting "Unbond funds". This can also be done through the staking.unbond() extrinsic with the staking proxy account. Edit this page Last updated on Jul 14, 2023 by Filippo Previous How to Chill Next Validator Community Overview Chill Validator Purge validator session keys Unbond your tokens ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Validator Community Overview General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides How to run a Validator on Polkadot Validator Payout Overview Using systemd for a Validator Node Secure Validator How to Upgrade Your Validator Monitor your node How to Chill How to Stop Validating Validator Community Overview Validator Best Practices Governance Guides Maintain Validator Guides Validator Community Overview On this page Validator Community Overview Building a Community and Attracting Nominations ‚Äã After setting up a validator , nominations will only come in with extra work. The community of nominators will need to know about the validator to trust staking with them, and thus the validator must distinguish themselves to attract nominations. The following gives some general guidance on different approaches to building a community and attracting nominations. Being a high-quality validator entails effectively running nodes and building a brand, reputation, and community around validation services. The responsibilities of a quality validator additionally include marketing oneself and participating in the greater community. Becoming a known participant throughout the ecosystem is a great way to attract nominations and solidify longevity and sustainability as a validator. One thing to remember is that there is a risk involved in staking for both validators and nominators, as both can lose up to 100% of their funds if a validator gets slashed. This means it is paramount for nominators only to nominate validators that they trust, as well as for validators to do their best to instill confidence in their ability to provide validation services. Validators should do their best to build a reputation through many different means, as this is one of the most important factors in how nominators should pick whom they stake with. Gaining Visibility ‚Äã Nominators should be able to know whom they are staking with. If nominators stake with a bunch of pseudo-anonymous addresses because it seems profitable, they expose themselves to more risks than nominating validators that follow best practices to whom they know the addresses belong. Establishing a clear identity in multiple places can help gain visibility across the ecosystem. This includes setting an on-chain identity and making a known presence throughout various community channels. Setting Identity ‚Äã All validators should set an on-chain identity and get a judgement on the identity so that nominators can find nodes when browsing through various dashboards and UIs. When someone interacts with the chain, it ensures that an address they may come across belongs to the validator, and actions of that identity throughout various parts of the ecosystem (staking, governance, block explorers, etc.) form a cohesive representation of their participation. When running multiple validator nodes, the best way to scale an identity is to use multiple sub-identities from a single verified identity It's recommended to fill out as many fields in the identity as possible so Nominators have ample means of reaching out. Nominators may wish to know more about the Validator, the particular setup, future staking plans, tooling used, or several additional topics. Having a dedicated website additionally to provide this sort of information is ideal. Ledger app on Nano S doesn't support the extrinsic for setting identity yet Website ‚Äã One strategy for helping gain additional visibility is to set up a dedicated site for your validator, which includes the networks that one is a validator for and validator details such as addresses, commission, and so forth. Including all suggestions from this page is potential content to include on the site. After setting up a website, a validator should add this website to the corresponding field in their identity so nominators can find it easily. Transparency & Establishing Trust ‚Äã Considering the risks involved for both Validators and Nominators, establishing trust is one of the most essential factors in running quality validator services. Self Stake ‚Äã Validators should have skin in the game in their operations in the form of a stake that is self-bonded to their validator stash. Slashing applies to the total stake of a Validator, therefore having a high self-stake shows confidence in the operations. This helps show commitment from the Validator as they have skin in the game and can be penalized for negative actions or poor maintenance. mess up. Having very little self-stake can signal to nominators that they have nothing to lose in the case of failures. Additionally, it can help nominators to get a sense of how validators manage their stakes. Defining a self-allocation strategy is also helpful in seeing how efficiently a validator's stake can be utilized. Commission & Rewards ‚Äã Commission ‚Äã What does your validator charge as commission, and how did you reach this number? It can be helpful to be transparent about the long-term plans around the business models of running a validator, including the costs for infrastructure and person-hours involved in maintaining operations. As many validators will charge low commissions that often do not cover costs, outlining what commission is charged and why can help justify higher commission rates. Besides the current commission, it would be helpful to describe the range of commission charged, as nominators can know what to expect if the rate goes up or down. Nominators may want to nominate a validator with a very narrow commission percent range, as this signals stability in a validator's operations and business plans. Many validators will charge 0% or near 0% commission to bootstrap themselves at first, with plans to raise that over time. It can be helpful to elaborate on these plans in the future. For example " after x amount of months in the active set with 0% commission, we plan to increase it to 1% ." Rewards ‚Äã Another factor to consider is that claiming rewards for both the validator and the nominator is not automatic. Rewards must be claimed manually or set up in an automated way. Validators are suggested to claim rewards on behalf of their nominators and be transparent about how often claiming will happen. A nominator may be more likely to stake with a validator that claims rewards daily instead of one that doesn't claim rewards at all. The following are some tools for automating reward claiming: staking-payouts substrate-payctl Validator Experience ‚Äã Architecture ‚Äã One aspect of building trust is being transparent about your validator infrastructure. If nominators know that you are running a tight ship that is focused on security, they are more likely to trust you compared to those that do not disclose their infrastructure. Some factors of architecture to highlight might include: Servers ‚Äã Outlining how a validator runs its servers helps nominators understand how diversified a validator is. Does the Validator run in the cloud, on dedicated machines, in a co-located datacenter, or in a home residential setup? Do they run multiple nodes on the same machine? If every validator is hosted in AWS, there is a risk of potential outages that cause large amounts of nodes to go offline, causing slashing for unresponsiveness. Nominators may want to choose validators that have thoroughly diversified the providers they use or the facilities they operate in. Additionally, how does a Validator contribute to decentralization? It can be helpful to outline these efforts so that the values of a Nominator and Validator are aligned. It's also helpful to outline what kind of OS is used on these servers and what is the updating policy for the software on that OS. For example, are LTS versions used? Do they use NiXOS, distro-packaged libraries? Any server hardening practices, etc. Specs ‚Äã Are you running the recommended Standard Hardware for Polkadot? Can you ensure that machines have enough processing power, memory, file storage, and network connectivity? It's helpful for nominators to know the specs of the machines a validator uses to assess how they may perform in the network. If a validator is running underpowered machines, they may not want to nominate them, as these can result in fewer blocks produced and fewer overall rewards. In certain circumstances, more powerful machines can result in higher rewards for both the Validator and their Nominators. Automation and orchestration approaches (Terraform, Ansible, Chef, Puppet, Kubernetes, etc.) ‚Äã What kind of approach is taken for spinning up and provisioning nodes? How might you automate spinning up large clusters of nodes and upgrading them? Elaborating on what type of automation (or lack thereof) can help get a sense of how robust a validator setup is. Many everyday actions or routine maintenance needs to be done, and automating this type of thing often helps mitigate human errors. Network Topology ‚Äã Does the Validator node have protection against Denial of Service attacks, and if so, how is that done? Outlining a desired network topology for a Validators infrastructure design will help Nominators understand how resilient their operations are to attacks. Some things to highlight are the usage of firewalls, VPNs, network segmentation, proxies, or other layers separation. Upgrading ‚Äã Both Polkadot and Kusama releases are published. here . Validators are expected to upgrade their nodes as soon as a new release comes. Although not every release is mandatory to upgrade, each new release usually has bug fixes, optimizations, new features, or other beneficial changes. It's in the best interest of the entire network that validators update their nodes in a timely fashion. This signals to nominators that a validator is timely care about their operations and is quick to adapt to necessary circumstances. It can also be helpful for Nominators to know how the Validator runs software and where they get new binaries. How do they get alerted for new releases? Do they receive updates from the matrix chatrooms? Do they have alerts for particular GitHub activities? Do they use the Debian/RPM packages? Do they use the Parity-provided GitHub binaries? Do they use Parity Docker images? Do they make their own Docker images? Do they build the binaries themselves? Validators often have their own build server for making binaries. If they take the extra steps to make these and do not rely on external parties, this can be seen as a plus from nominators, as it helps contribute to decentralization. Logging, metrics, monitoring, and observability ‚Äã Good node operators keep tabs on how their systems are running. Observability is one of the most critical aspects of understanding the performance and behavior of a node. One should be able to outline the efforts taken in building out monitoring and observability practices. Are Prometheus and Grafana set up? What types of metrics are collected and looked at? How is this done across multiple nodes? A quality validator may make these metrics and graphs public so that Nominators can see how these nodes are running. Health checks and alerting conditions ‚Äã Similar to the last point, it can be helpful for nominators to know what kind of health checks and alerting conditions are in place for validator nodes. What conditions are not typical and may need to be looked at? If conditions are not specific, how is the node operator alerted to this? Are there any public Telegram, SMS, or email alerts? Nominators will want to know that a Validator can respond to abnormal conditions promptly, as their tokens are on the line of potentially being lost. Scenario runbooks ‚Äã Many scenarios happen routinely, such as upgrading nodes, restoring backups, or moving servers. Creating runbooks and sharing the procedures and precautions taken around these can instill confidence in nominators that various scenarios are thought out and planned for. Which regions nodes are in ‚Äã A diverse network of nodes in varying different regions helps strengthen decentralized networks. Outlining what regions nodes are in gives clarity to this facet of networks. Nominators may want to promote validators that actively try to decentralize networks operating in regions in which others do not run nodes. Security / Key handling policies ‚Äã It is paramount that session keys and stash/staking proxy keys are stored and handled with the utmost care. If compromised, both the validator and nominator can be slashed. Outlining how keys are handled, how they are stored, who has access to them, and the overall policies and procedures around them is a great point of reference for nominators to gauge how comfortable they are with the security a validator takes. Robust Communication ‚Äã The relationship between Validators and Nominators is one built on trust, and as such, having direct lines of communication with Nominators is a great way to build and reinforce that trust. This could mean setting up dedicated Telegram / Matrix / Discord channels or hosting a reoccurring call where anyone can join. Creating inclusive environments with direct connections between parties is going the extra mile to ensure that nominators know they're in good hands. Many updates can be given, such as nodes being updated to a new version, rewards being paid out, servers being migrated, new features or tools being built, or just checking in to say hello. These kinds of gestures can be much appreciated in putting words and a person behind the name of someone running a server. Actively Participating in the Community ‚Äã Participating in the community goes hand in hand with building a reputation. This is not only for Nominators, but for other Validators, builders, developers, governance participants, and general enthusiasts. Being helpful or contributing to discussions can go a long way in building a trusted brand and reputation. There are many communities to participate in, from validator, developer, and governance communities, to local communities dedicated to specific regions. For example, one can be pretty active in the South American communities and building camaraderie among those who speak the same language or can attend the same meetups in an area. One absolute best way to build trust is meeting people in person. Participating in Governance ‚Äã Another way to show that one cares about the network is by actively participating in governance. Whether by voting on-chain, or by discussing off-chain, or proposing new things, active participation in the direction of the chain is an excellent signal that a validator is there for the network‚Äôs good. There are many ways to participate in different governance aspects, such as voting for council members, weighing in on treasury proposals, voting on public referenda, proposing tips, and more. See the section on governance for additional details. Producing Educational Content ‚Äã With a fast-moving ecosystem, there often are gaps in educational content where there are new features, changes, deprecations, or just a slow-moving process for putting out information about very complex concepts. Putting out educational content in the form of blog posts, videos, tutorials, development guides, and more (especially if it's geared toward nominators) provides tangible value to the ecosystem. It shows that one has a good grasp of how things work, and disseminating this knowledge to others can give some credence to one's brand and reputation as a competent entity in the space. Furthermore, one might get tips from the treasury if the community finds something beneficial. Building Tooling ‚Äã Building public tooling is a great way to support the ecosystem. This provides tangible value to those that use this tooling and gives visibility to the validator for their contributions. A nominator might be more likely to nominate a validator for the utilities they provide the ecosystem since the validator then can build a reputation around the quality of their work outside their validation services. Some potential building categories are block explorers, deployment scripts, monitoring, observability services, staking dashboards, wallets, command-line utilities, or porting implementations to other languages. Additionally, this may also be eligible to be funded via a Web3 Foundation Grant . Edit this page Last updated on Jul 14, 2023 by Filippo Previous How to Stop Validating Next Validator Best Practices Building a Community and Attracting Nominations Gaining Visibility Setting Identity ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Validator Best Practices General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides How to run a Validator on Polkadot Validator Payout Overview Using systemd for a Validator Node Secure Validator How to Upgrade Your Validator Monitor your node How to Chill How to Stop Validating Validator Community Overview Validator Best Practices Governance Guides Maintain Validator Guides Validator Best Practices On this page Validator Best Practices Best practices to prevent slashing ‚Äã Slashing is implemented as a deterrent for validators to misbehave. Slashes are applied to a validator‚Äôs total stake (own + nominated) and can range from as little as 0.01% or rise to 100%. In all instances, slashes are accompanied by a loss of nominators. A slash may occur under four circumstances: Unresponsiveness ‚Äì Slashing starts when 10% of the active validators set are offline and increases in a linear manner until 44% of the validator set is offline; at this point, the slash is held at 7% Equivocation ‚Äì A slash of 0.01% is applied with as little as a single evocation. The slashed amount increases to 100% incrementally as more validators also equivocate. Malicious action ‚Äì This may result from a validator trying to represent the contents of a block falsely. Slashing penalties of 100% may apply. Application related (bug or otherwise) ‚Äì The amount is unknown and may manifest as scenarios 1, 2, and 3 above. This article provides some best practices to prevent slashing based on lessons learned from previous slashes. It provides comments and guidance for all circumstances except for malicious action by the node operator. Unresponsiveness ‚Äã An offline event occurs when a validator does not produce a BLOCK or IMONLINE message within an EPOCH. Isolated offline events do not result in a slash; however, the validator would not earn any era points while offline. A slash for unresponsiveness occurs when 10% or more of the active validators are offline at the same time. Check the Wiki section on slashing due to unresponsiveness to learn more about its specifics. The following are recommendations to validators to avoid slashing under liveliness for servers that have historically functioned: Utilize systems to host your validator instance. Systemd should be configured to auto reboot the service with a minimum 60-second delay. This configuration should aid with re-establishing the instance under isolated failures with the binary. A validator instance can demonstrate un-lively behaviour if it cannot sync new blocks. This may result from insufficient disk space or a corrupt database. Monitoring should be implemented that allows node operators to monitor connectivity network connectivity to the peer-to-peer port of the validator instance. Monitoring should also be implemented to ensure that there is <50 Block ‚Äòdrift‚Äô between the target and best blocks. If either event produces a failure, the node operator should be notified. The following are recommendations to validators to avoid liveliness for new servers / migrated servers: Ensure that the --validator flag is used when starting the validator instance If switching keys, ensure that the correct session keys are applied If migrating using a two-server approach, ensure that you don‚Äôt switch off the original server too soon. Ensure that the database on the new server is fully synchronized. It is highly recommended to avoid hosting on providers that other validators may also utilize. If the provider fails, there is a probability that one or more other validators would also fail due to liveliness building to a slash. There is a precedent that a slash may be forgiven if a single validator faces an offline event when a larger operator also faces multiple offline events, resulting in a slash. Equivocation ‚Äã Equivocation events can occur when a validator produces two or more of the same block; under this condition, it is referred to as a BABE equivocation. Equivocation may also happen when a validator signs two or more of the same consensus vote; under this condition, it is referred to as a GRANDPA Equivocation. Equivocations usually occur when duplicate signing keys reside on the validator host. If keys are never duplicated, the probability of an equivocation slash decreases to near 0. Check the Wiki section on Equivocation to learn more about its specifics. The following are scenarios that build towards slashes under equivocation: Cloning a server, i.e., copying all contents when migrating to new hardware. This action should be avoided. If an image is desired, it should be taken before keys are generated. High Availability (HA) Systems ‚Äì Equivocation can occur if there are any concurrent operations, either when a failed server restarts or if false positive event results in both servers being online simultaneously. HA systems are to be treated with extreme caution and are not advised. The keystore folder is copied when attempting to copy a database from one instance to another. It is important to note that equivocation slashes occur with a single incident. This can happen if duplicated keystores are used for only a few seconds. A slash can result in losing nominators, and funds, removal from the Thousand Validator Programme, and reputational damage. An offline event results in losing some funds but the retention of nominators and a fault under the Thousand Validator Programme. Application Related ‚Äã In the past, there have been releases with bugs that lead to slashes; these issues are not as prevalent in current releases. The following are advised to node operators to ensure that they obtain pristine binaries or source code and to ensure the security of their node: Always download either source files or binaries from the official Parity repository Verify the hash of downloaded files. Use the W3F secure validator setup or adhere to its principles Ensure essential security items are checked, use a firewall, manage user access, use SSH certificates Avoid using your server as a general-purpose system. Hosting a validator on your workstation or one that hosts other services increases the risk of maleficence. Examples ‚Äã Network Era Event Type Details Action Taken Polkadot 774 Small Equivocation The validator migrated servers and cloned the keystore folder. The on-chain event can be viewed here . The validator did not submit a request for the slash to be canceled. Kusama 3329 Small Equivocation The validator operated a test machine with cloned keys; the test machine was online at the same time as the primary, which resulted in a slash. Details can be found here . The validator requested a cancellation of the slash, but the council declined. Kusama 3995 Small Equivocation The validator noticed several errors, after which the client crashed, and a slash was applied. The validator recorded all events and opened GitHub issues to allow for technical opinions to be shared. Details can be found here . The validator requested to cancel the slash. The council approved the request as they believed the error was not operator related. Kusama 4543 Medium Unresponsiveness A large amount of disputes flooded the network resulting in an application fault. The fault caused the client software to hang and as a result ~197 unique validators become unresponsive. Further details can be found here . The pending slash was cancelled and with runtime 9350 all lost nominations were restored. The application bug was addressed with client version 0.9.36 Edit this page Last updated on Jan 16, 2023 by github-actions[bot] Previous Validator Community Overview Next Participate in Democracy Best practices to prevent slashing Unresponsiveness Equivocation Application Related Examples ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Participate in Democracy General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides Governance Guides Participate in Democracy Participate in Polkadot OpenGov Join the Council Voting for Councillors Maintain Governance Guides Participate in Democracy On this page Participate in Democracy Polkadot OpenGov is Live! This document will soon be archived. Polkadot OpenGov: Polkadot‚Äôs next generation of decentralized governance is live on Polkadot and Kusama. Check the Polkadot OpenGov page for a detailed explanation of the latest features of Polkadot governance. The democracy pallet (Gov1) will soon be deprecated. The public referenda chamber is one of the three bodies of on-chain governance as it's instantiated in Polkadot and Kusama. The other two bodies are the council and the technical committee . Public referenda can be proposed and voted on by any token holder in the system as long as they provide a bond. After a proposal is made, others can agree with it by endorsing it and putting up tokens equal to the original bond. During every launch period, the most endorsed proposal will be moved to the public referenda table where it can be voted upon. Voters who are willing to lock up their tokens for a greater duration of time can do so and get their votes amplified. For more details on the governance system please see here . This guide will instruct token holders how to propose and vote on public referenda using the Democracy module. Below are a few links to stay informed and directly engage with the community. Polkadot Direction - a place to discuss governance and the future of Polkadot. Kusama Direction - a place to discuss governance and the future of Kusama. Polkadot and Kusama Polkassembly - for current referenda, latest proposals, motions, treasury proposals, tips, bounties, and more. Polkadot Daily Digest - News about what is happening in the Polkadot ecosystem, published every weekday except holidays. Important Parameters ‚Äã The important parameters to be aware of when voting using the Democracy module are as follows: Launch Period - How often new public referenda are launched. Voting Period - How often votes for referenda are tallied. Emergency Voting Period - The minimum voting period for a fast-tracked emergency referendum. Minimum Deposit - The minimum amount to be used as a deposit for a public referendum proposal. Enactment Period - The minimum period for locking funds and the period between a proposal being approved and enacted. Cooloff Period - The period in blocks where a proposal may not be re-submitted after being vetoed. Proposing an Action ‚Äã Proposing an action to be taken requires you to bond some tokens. In order to ensure you have enough tokens to make the minimum deposit, you can check the parameter in the chain state. Navigate to Developer > Chain State > Constants > selected constant query: democracy > minimumDeposit: u128 and then click on the plus button. The bonded tokens will only be released once the proposal is tabled (that is, brought to a vote); there is no way for the user to "revoke" their proposal and get the bond back before it has become a referendum. Since it is essentially impossible to predict definitely when a proposal may become a referendum (if ever), this means that any tokens bonded will be locked for an indeterminate amount of time. Proposals cannot be revoked by the proposer, even if they never turn into a referendum It is important to realize that there is no guarantee that DOT you use for proposing or endorsing a proposal will be returned to that account in any given timeframe. order to submit a proposal, you will need to submit what's called the preimage hash. The preimage hash is simply the hash of the proposal to be enacted. The easiest way to get the preimage hash is by clicking on the "Submit preimage" button and configuring the action that you are proposing. The example below demonstrates the creation of a proposal on Kusama ( the same procedure applies to Polkadot ). If you wanted to propose that a remark "Expect Chaos!" is added, the preimage hash would be 0x8ac3f722caf7677254e25ca4ad77d533ca893c7d2ad4009e258d749f2004ef94 . You can copy this preimage hash and save it for the next step. There is no need to click Submit Preimage at this point, though you could. We'll go over that in the next section. Now you will click on the "Submit proposal" button and enter the preimage hash in the input titled "preimage hash" and at least the minimum deposit into the "locked balance" field. Click on the "Submit proposal" button and confirm the transaction. You should now see your proposal appear in the "proposals" column on the page. Now your proposal is visible to anyone who accesses the chain and others can endorse it or submit a preimage. However, it's hard to tell what exactly this proposal does since it shows the hash of the action. Other holders will not be able to make a judgement on whether they endorse it or not until someone submits the actual preimage for this proposal. In the next step, you will submit the preimage. Submitting a Preimage ‚Äã The act of making a proposal is split from submitting the preimage for the proposal since the storage cost of submitting a large preimage could be pretty expensive. Allowing for the preimage submission to come as a separate transaction means that another account could submit the preimage for you if you don't have the funds to do so. It also means that you don't have to pay so many funds right away as you can prove the preimage hash out-of-band. However, at some point before the proposal passes you will need to submit the preimage or else the proposal cannot be enacted. The guide will now show you how to do this. Click on the blue "Submit preimage" button and configure it to be the same as what you did before to acquire the preimage hash. This time, instead of copying the hash to another tab, you will follow through and click "Submit preimage" and confirm the transaction. Once the transaction is included you should see the UI update with the information for your already submitted proposal. Endorsing a Proposal ‚Äã Endorsing a proposal means that you are agreeing with the proposal and backing it with an equal amount of deposit as was originally locked. The bonded tokens will be released once the proposal is tabled (that is, brought to a vote), just like the original proposer's bond. By endorsing a proposal you will move it higher up the rank of proposals. The most endorsed proposal ‚Äî in value, not the number of supporters ‚Äî will be brought to a referendum every launch period. It is important to note that there is no way to stop or cancel endorsing a proposal once it has been done. Therefore, the DOT that was endorsed will be reserved until the proposal is tabled as a referendum. This is an indeterminate amount of time, since there is no guarantee that a proposal will become a referendum for a given period, as other proposals may be proposed and tabled before it. Note that it is possible for a single account to endorse a proposal multiple times. This is by design; it is the value, not the number of endorsements per se , that counts in terms of weighting. If there were a limit of one endorsement per account, it would be trivial for a user with, for example, 1000 DOT to create ten accounts with 100 DOT instead of a single account with 1000 DOT. Thus, no restrictions are made on the number of times a single account can endorse a proposal. To endorse a proposal, navigate to the proposal you want to endorse and click on the "Endorse" button. You will be prompted with the full details of the proposal (if the preimage has been submitted!) and can then broadcast the transaction by clicking the blue "Endorse" button. Once successful, you will see your endorsement appear in the dropdown in the proposal details. Voting on a Proposal ‚Äã At the end of each launch period, the most endorsed proposal will move to a referendum. During this time you can cast a vote for or against the proposal. You may also lock up your tokens for a greater length of time to weigh your vote more strongly. During the time your tokens are locked, you are unable to transfer them, however they can still be used for further votes. Locks are layered on top of each other, so an eight-week lock does not become a 15-week lock if you vote again a week later, rather another eight-week lock is placed to extend the lock just one extra week. Voting and staking locks can overlap A user can use staked funds to vote on referenda (and/or to vote for Councillors). A user is only prohibited from transferring these funds to another account. To vote on a referendum, navigate to the "Democracy" tab of Polkadot Apps . Any active referendum will show in the "referenda" column. Click the blue button "Vote" to cast a vote for the referendum. If you would like to cast your vote for the proposal select the "Aye, I approve" option. If you would like to cast your vote against the proposal in the referendum you will select "Nay, I do not approve" option. The endorsement option is to select your conviction for this vote. The longer you are willing to lock your tokens, the stronger your vote will be weighted. The timeline for the conviction starts after the voting period ends; tokens used for voting will always be locked until the end of the voting period, no matter what conviction you vote with. Unwillingness to lock your tokens means that your vote only counts for 10% of the tokens that you hold, while the maximum lock-up of 896 days means you can make your vote count for 600% of the tokens that you hold. It is possible to vote with tokens already locked by staking. When you are comfortable with the decision you have made, click the blue "Vote" button to submit your transaction and wait for it to be included in a block. Unlocking Locked Tokens ‚Äã Like vesting , the tokens that are locked in democracy are unlocked lazily. This means that you, the user, must explicitly call an unlock extrinsic to make your funds available again after the lock expires. Unbonding is another term you hear a lot in Polkadot, it means withdrawing your DOT that was used in staking. To know more about it, please see here . You can do this from the "Accounts" page in Polkadot-JS Apps , unless you use Ledger (see below). First, check that your account has a "democracy" lock by opening the details on your balance. In the example below the account has 150 KSM locked in democracy. Now you can click the menu button (the three dots) and find the option that says "Clear expired democracy locks". After selecting this option you may confirm the transaction and your locks will be cleared when successful. With a Ledger hardware wallet or Unlocking Very Old Locks ‚Äã If you do not see an option to clear expired democracy votes, it may be that the lock is very old. Or, if you are using the Ledger hardware wallet, you will not be able to issue the batch Unlock action from the UI. Instead, you must clear the lock by directly issuing the correct extrinsics. Navigate to the Extrinsics page and submit the following extrinsic: democracy.removeVote(index) using the account that you voted with. For the index number (ReferendumIndex), enter the number of the referendum for which you voted ("12" in the image below). The number of the referendum for which you voted is visible in an explorer such as Polkascan. You need to press the "Submit Transaction" button to submit the extrinsic. Now submit the following extrinsic: `democracy.unlock(target), where the target is your account address. If you return to the Accounts page , you should see that the democracy lock has been released. Note that this applies only to locked DOT that were used for voting on referenda. In order to unlock DOT locked by voting for members of the Polkadot Council, you need to go to the Council page, click "Vote", and then click on "Unvote All". Delegate a Vote ‚Äã If you are too busy to keep up and vote on upcoming referenda, there is an option to delegate your vote to another account whose opinion you trust. When you delegate to another account, that account gets the added voting power of your tokens along with the conviction that you set. The conviction for delegation works just like the conviction for regular voting, except your tokens may be locked longer than they would normally since locking resets when you undelegate your vote. The account that is being delegated to does not make any special action once the delegation is in place. They can continue to vote on referenda how they see fit. The difference is now when the Democracy system tallies votes, the delegated tokens now are added to whatever vote the delegatee has made. You can delegate your vote to another account and even attach a "Conviction" to the delegation. Navigate to the "Accounts" tab on Polkadot-JS UI and click on the three vertical dots and select "Delegate democracy votes" option for the account of your choice. Then you would be presented with a pop-up window which lets you enter the account details of the delegated account, delegated amount and conviction. You can enter the relevant details and click on the delegate button and then sign and submit the transaction. Your delegation will count toward whatever account you delegated for votes on until you explicitly undelegate your vote. Query the chain state for an account's delegation preferences It is possible to query the delegation preferences of any actively delegating account on the network through democracy.votingOf extrinsic. Undelegate a Vote ‚Äã caution If there is an existing lock due to a previous delegation change or undelegation, any new change or undelegation will restart the lock period for the larger DOT amount and the longest conviction period between the existing and the new lock. This will only matter to accounts with conviction, as the accounts with no conviction don't have any lock period. Examples: Delegate 500 DOT with 1x conviction, then change delegation to 1000 DOT with 1x conviction, the lock period will reset for 1000 DOT with 1x conviction. Delegate 500 DOT with 3x conviction, then change the delegation to 1000 DOT with 1x conviction, the lock period will reset for 1000 DOT with 3x conviction. Delegate 500 DOT with 1x conviction, then change the delegation to 200 DOT with 1x conviction, the lock period will reset for 500 DOT with 1x conviction. To understand this in further detail checkout this stackexchange post. You may decide at some point in the future to remove your delegation to a target account. In this case, your tokens will be locked for the maximum amount of time in accordance with the conviction you set at the beginning of the delegation. For example, if you chose "2x" delegation for four weeks of lock-up time, your tokens will be locked for 4 weeks after sending the undelegate transaction. Once your vote has been undelegated, you are in control of making votes with it once again. You can start to vote directly, or choose a different account to act as your delegate. The undelegate transaction must be sent from the account that you wish to clear of its delegation. For example, if Alice has delegated her tokens to Bob, Alice would need to be the one to call the undelegate transaction to clear her delegation. The easiest way to do this is from the "Accounts" tab on Polkadot-JS UI where you can manage the delegation of your account. When you click on "manage delegation", you would be presented with a pre-populated pop-up window with that account's delegation preferences. Here, you have an option to undelegate or save the modifications made to the account's delegation preferences. Voting with a Governance Proxy ‚Äã Making a vote on behalf of a stash requires a "proxy" transaction from the Proxy pallet. When you choose this transaction from the "Extrinsics" tab, it will let you select "vote" from the Democracy pallet, and you will specify the index of the referendum that is being voted, the judgement (i.e. "Aye" for approval or "Nay" for rejection), and the conviction, just like a normal vote. For more material on adding and removing Governance proxies, as well as other types, please see the Proxy page . Interpreting On-Chain Voting Data ‚Äã Consider the following example showcasing how votes would be displayed on a block explorer. Nay 0.1x => 0 Nay 1x => 1 Nay 2x => 2 Nay 3x => 3 Nay 4x => 4 Nay 5x => 5 Nay 6x => 6 Aye 0.1x => 128 Aye 1x => 129 Aye 2x => 130 Aye 3x => 131 Aye 4x => 132 Aye 5x => 133 Aye 6x => 134 At first glance, it may be difficult to interpret what you voted on. We need to take a step back and consider the "voting data" at the binary level. The vote is stored as a byte using a bitfield data structure and displayed on the block explorer as a decimal integer. The bitfield stores both the conviction and aye/nay boolean, where the boolean is represented using the MSB of the byte. This would mean that the grouping of the 7 remaining bits is used to store the conviction. Edit this page Last updated on Jun 21, 2023 by Filippo Previous Validator Best Practices Next Participate in Polkadot OpenGov Important Parameters Proposing an Action Submitting a Preimage Endorsing a Proposal Voting on a Proposal Unlocking Locked Tokens Delegate a Vote Undelegate a Vote Voting with a Governance Proxy Interpreting On-Chain Voting Data ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Participate in Polkadot OpenGov General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides Governance Guides Participate in Democracy Participate in Polkadot OpenGov Join the Council Voting for Councillors Maintain Governance Guides Participate in Polkadot OpenGov On this page Participate in Polkadot OpenGov Learn more about Polkadot OpenGov For background information about Polkadot OpenGov, please refer to this dedicated Wiki document . This guide will instruct token holders how to propose and vote on public referenda using the Referenda module (OpenGov). Below are a few links to stay informed and directly engage with the community. Polkadot Direction - a place to discuss governance and the future of Polkadot. Kusama Direction - a place to discuss governance and the future of Kusama. Polkadot and Kusama Polkassembly - for current referenda, latest proposals, motions, treasury proposals, tips, bounties, and more. Polkadot Daily Digest - News about what is happening in the Polkadot ecosystem, published every weekday except holidays. Polkadot OpenGov Terminology and Parameters ‚Äã The important parameters to be aware of when voting using the Referenda module are as follows: Origin - Each origin has a fixed set of privileges. When making a proposal, it is important to choose the origin that has the privilege to execute the referenda. Track - Each track has its own dispatch origin and a preset configuration that governs the voting process and parameters. Submission Deposit - The minimum amount to be used as a (refundable) deposit to submit a public referendum proposal. Prepare Period - The minimum time the referendum needs to wait before it can progress to the next phase after submission. Voting is enabled, but the votes do not count toward the outcome of the referendum yet. Decision Deposit - This deposit is required for a referendum to progress to the decision phase after the end of prepare period. Decision Period - Amount of time a decision may take to be approved to move to the confirming period. If the proposal is not approved by the end of the decision period, it gets rejected. Max Deciding - The maximum number of referenda that can be in the decision period of a track all at once. Conviction : A multiplier to increase voting power. Approval : the share of the approval vote-weight after adjustments for conviction against the total number of vote-weight for both approval and rejection Support : The total number of votes in approval (ignoring adjustments for conviction) compared to the total possible amount of votes that could be made in the system. Support also takes into account abstained votes. Min Approval - The threshold of approval (along with the min support) needed for a proposal to meet the requirements of the confirm period. Min Support - The threshold of support (along with the min approval) needed for a proposal to meet the requirements of the confirm period. Confirmation Period - The total time the referenda must meet both the min approval and support criteria during the decision period in order to pass and enter the enactment period. Min Enactment Period - Minimum time that an approved proposal must be in the dispatch queue after approval. The proposer has the option to set the enactment period to be of any value greater than the min enactment period. Origins and Tracks Info ‚Äã info For every referendum in each of these tracks, the Polkadot-JS UI displays interactive graphs of the support and approval. Root ‚Äã The origin with the highest level of privileges. This track requires extremely high levels of approval and support for early passing. The prepare and enactment periods are also large. For instance, a referendum proposed in this track needs to amass 48.2% support (total network issuance) by the end of the first day with over 93.5% approval to be considered to be part of the confirm period. The support curve drops linearly to 25% by the end of day 14 and almost to 0% by the end of day 28. This ensures that the token holders receive ample time to vote on the proposal during the decision period. Whitelisted Caller ‚Äã Origin commanded by the Fellowship whitelist some hash of a call and allow the call to be dispatched with the root origin (after the referendum passes). This track allows for a shorter voting turnaround, safe in the knowledge through an open and transparent process for time-critical proposals. For instance, a referendum proposed in this track needs to amass 20% support (much lesser than the root) by the end of the first day with over 93.5% approval to be considered to be part of the confirm period. Staking Admin ‚Äã The origin for canceling slashes. This origin has the privilege to execute calls from the staking pallet and the Election Provider Multiphase Pallet. Treasurer ‚Äã The origin for spending funds from the treasury (up to 10M DOT). This origin has the privilege to execute calls from the Treasury pallet. Lease Admin ‚Äã Origin can force slot leases. This origin has the privilege to execute calls from the Slots pallet. Fellowship Admin ‚Äã The origin for managing the composition of the fellowship. General Admin ‚Äã The origin for managing the registrar. This origin has the privilege of executing calls from the Identity pallet. Auction Admin ‚Äã The origin for starting auctions. This origin can execute calls from the Auctions pallet and the Scheduler Pallet. Referendum Canceller ‚Äã The origin can cancel referenda. This track has a low lead time and approval/support curves with slightly sharper reductions in their thresholds for passing. Referendum Killer ‚Äã The origin can cancel an ongoing referendum and slash the deposits. This track also has a low lead-time and approval/support curves with slightly sharper reductions in their thresholds for passing. Small Tipper ‚Äã Origin able to spend up to 250 DOT from the treasury at once. Big Tipper ‚Äã Origin able to spend up to 1000 DOT from the treasury at once. Small Spender ‚Äã Origin able to spend up to 10000 DOT from the treasury at once. Medium Spender ‚Äã Origin able to spend up to 100000 DOT from the treasury at once. Big Spender ‚Äã Origin able to spend up to 1000000 DOT from the treasury at once. Create a Referenda Proposal using Polkadot-JS UI ‚Äã Submitting a Preimage ‚Äã The act of creating a proposal is split from submitting the preimage for the proposal since the storage cost of submitting a large preimage could be expensive. Allowing the preimage submission to come as a separate transaction means that another account could submit the preimage for you and pay the fee. The example below demonstrates the creation of a preimage on Kusama ( the same procedure applies to Polkadot ). To propose that a remark "Expect Chaos!" be added to the blockchain, the preimage hash would be 0x8ac3f722caf7677254e25ca4ad77d533ca893c7d2ad4009e258d749f2004ef94 . After the preimage is submitted successfuly on-chain, Polkadot-JS UI lists it under the tab of Governance > Preimages. Submitting a Proposal ‚Äã Submitting a proposal requires you to bond some tokens. On Polkadot-JS UI, you can navigate to the Governance -> Referenda to make a new proposal. In order to submit a proposal, you will need to submit what's called the preimage hash. The preimage hash is simply the hash of the proposal to be enacted. The easiest way to get the preimage hash is by clicking on the "Submit preimage" button as shown in the previous section. The proposal will be registered from the account selected and the balance lock will be applied to it. An appropriate origin must be chosen, as each origin has different privileges, and acceptance criteria. After entering the hash of the preimage for the proposal, the preimage length field is automatically populated. The enactment delay can be specified either as a block number, or as a specific number of blocks after the referendum is approved. The deposit for this proposal will be locked for the referendum duration. Submitting a Referendum on the Whitelisted Caller Track ‚Äã Let's consider increasing the number of validators participating in parachain consensus. You could submit a preimage with the call that sets the number of validators to 1,000 and submit a referendum to the Root track directly. However, this requires a large decision deposit and has very conservative passing parameters such that it will probably need the entire 28-day voting period to pass. Operations that are deemed safe or time critical by the Polkadot Technical Fellowship can use the Whitelisted Caller track. This track requires less turnout in the first half of the decision period so that it can pass more quickly. This track is typically used for more neutral, technical proposals like runtime upgrades or changing the system's parachain validation configuration. Using the Whitelisted Caller track requires some special calls. Submitting a referendum in the same form as other tracks will not work. Namely, rather than voting on a particular proposal , the Whitelisted Caller track requires a vote to dispatch the proposal via the Whitelist pallet. Before opening a referendum on this track, you should also attempt to get a positive signal from the Fellowship that they will whitelist the proposal. If they do not, then even if the public referendum passes, it will not execute. Below are the steps to follow when submitting a proposal to the Whitelist track. Submit a preimage with the call to dispatch the proposal ( call ) you want to submit -- whitelist.dispatchWhitelistedCallWithPreimage(call) -- and obtain the preimage hash. This is the preimage for the public referendum on the Whitelisted Caller track. Obtain the hash of call . The Polkadot Fellowship needs to start a Fellowship referendum to whitelist the call with whitelist.whitelistCall(callHash) . The Fellowship referendum gets voted on by the Polkadot Fellowship members only. The public now votes on the referendum. Someone must place a decision deposit to go into the deciding phase. Once passed, it gets enacted successfully as long as the call has been whitelisted by the Fellowship. Note that the public referendum and Fellowship referendum can happen simultaneously. However, if the Fellowship does not whitelist the call, you must submit it directly to the Root origin. Voting on Referenda ‚Äã As Polkadot OpenGov takes both the approval and support into account, there are four options to choose from when voting on a referendum: Aye Nay Split Abstain Also, you have to specify the conviction multiplier for this vote. The longer you are willing to lock your tokens, the stronger your vote will be weighted. Unwillingness to lock your tokens means that your vote only counts for 10% of the tokens that you hold. For detailed instructions on how to vote on Polkadot OpenGov referenda, check this support guide. Polkadot OpenGov uses Conviction Voting Pallet (Not Democracy Pallet) Use convictionVoting.vote for voting on Referenda in Polkadot OpenGov instead of democracy.vote (which only works for the old version of governance). Removing expired voting locks ‚Äã To remove the lock from votes, you first need to call removeVote and then unlock through the convictionVoting pallet. For detailed instructions, check this support guide. Delegating Voting Power ‚Äã For an overview of how delegation works in Polkadot OpenGov, check out the Multirole Delegation section on the Learn Polkadot OpenGov page. Delegation Dashboard ‚Äã To make multi-role delegation easy and intuitive, Delegation Dashboard provides an interactive interface that displays the list of delegates and their details. The video tutorial below walks through the features of the Delegation Dashboard and shows how to perform multi-role delegation. For detailed instructions on how to delegate your voting power using dashboard, check this support guide. If you like to use Polkadot-JS UI for performing multi-role delegation, the instructions are available here . Edit this page Last updated on Jun 28, 2023 by Filippo Previous Participate in Democracy Next Join the Council Polkadot OpenGov Terminology and Parameters Origins and Tracks Info Root Whitelisted Caller Staking Admin Treasurer Lease Admin Fellowship Admin General Admin Auction Admin Referendum Canceller Referendum Killer Small Tipper Big Tipper Small Spender Medium Spender Big Spender Create a Referenda Proposal using Polkadot-JS UI Submitting a Preimage Submitting a Proposal Submitting a Referendum on the Whitelisted Caller Track Voting on Referenda Removing expired voting locks Delegating Voting Power Delegation Dashboard ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Join the Council General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides Governance Guides Participate in Democracy Participate in Polkadot OpenGov Join the Council Voting for Councillors Maintain Governance Guides Join the Council On this page Join the Council The council is an elected body of on-chain accounts that are intended to represent the passive stakeholders of Polkadot and/or Kusama. The council has two major tasks in governance: proposing referenda and vetoing dangerous or malicious referenda. For more information on the council, see the governance page . This guide will walk you through entering your candidacy to the council. Submit Candidacy ‚Äã Submitting your candidacy for the council requires a small bond of DOT / KSM. Unless your candidacy wins, the bond will be forfeited. You can receive your bond back if you manually renounce your candidacy before losing. Runners-up are selected after every round and are reserved members in case one of the winners gets forcefully removed. note Currently the bond for submitting a council candidacy on Polkadot is 100 DOT, and 0.0033 KSM on Kusama. It is a good idea to announce your council intention before submitting your candidacy so that your supporters will know when they can start to vote for you. You can also vote for yourself in case no one else does. Go to Polkadot Apps Dashboard and navigate to the "Council" tab. Click the button on the right that says "Submit Candidacy." After making the transaction, you will see your account appear underneath the row "Candidates." It is a good idea now to lead by example and give yourself a vote. Voting on Candidates ‚Äã Next to the button to submit candidacy is another button titled "Vote." You will click this button to make a vote for yourself (optional). The council uses Phragm√©n approval voting, which is also used in the validator elections. This means that you can choose up to 16 distinct candidates to vote for and your stake will equalize between them. For this guide, choose to approve your own candidacy by clicking on the switch next to your account and changing it to say "Aye." Winning ‚Äã If you are one of the lucky ones to win a council election you will see your account move underneath the row "Members". Now you are able to participate on the council by making motions or voting proposals. To join in on the active discussions, join the Polkadot Direction channel . Edit this page Last updated on May 16, 2023 by Radha Previous Participate in Polkadot OpenGov Next Voting for Councillors Submit Candidacy Voting on Candidates Winning ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Voting for Councillors General Learn Build Maintain Kusama Search Contribute English English Help us translate General Learn Build Maintain Network Maintainers Parameters Node Endpoints Nodes and Dapps Nominator Guides Validator Guides Governance Guides Participate in Democracy Participate in Polkadot OpenGov Join the Council Voting for Councillors Maintain Governance Guides Voting for Councillors On this page Voting for Councillors The council is an elected body of on-chain accounts that are intended to represent the passive stakeholders of Polkadot and/or Kusama. The council has two major tasks in governance: proposing referenda and vetoing dangerous or malicious referenda. For more information on the council, see the governance page . This guide will walk you through voting for councillors in the elections. Voting for Councillors ‚Äã Voting for councillors requires you to reserve as a base amount and an amount of per vote. You can then bond whatever amount you wish to put behind your vote. See the democracy guide for more information. Voting and staking locks can overlap A user can use staked funds to vote for Councillors (and/or use those funds in referenda). A user is only prohibited from transferring these funds to another account. danger If your balance is vesting, you cannot use unvested tokens for this lock. You will have to wait until you have at least that many free tokens to vote. Like the validator elections, you can approve up to 16 different councillors and your vote will be equalized among the chosen group. Unlike validator elections, there is no unbonding period for your reserved tokens. Once you remove your vote, your tokens will be liquid again. danger It is your responsibility not to put your entire balance into the reserved value when you make a vote for councillors. It's best to keep at least enough DOT/KSM to pay for transaction fees. Go to the Polkadot-JS Apps Dashboard and click on the "Council" tab. On the right side of the window there are two blue buttons, click on the one that says "Vote." Since the council uses approval voting, when you vote you signal which of the candidates you approve of and your voted tokens will be equalized among the selected candidates. Select up to 16 council candidates by moving the slider to "Aye" for each one that you want to be elected. When you've made the proper configuration submit your transaction. You should see your vote appear in the interface immediately after your transaction is included. Removing your Vote ‚Äã In order to get your reserved tokens back, you will need to remove your vote. Only remove your vote when you're done participating in elections and you no longer want your reserved tokens to count for the councillors that you approve. Go to the "Governance" > "Council" tab on the Polkadot-JS Apps Dashboard . Under the "Council overview" tab, click on "Vote". Issue the "Unvote all" option. When the transaction is included in a block you should have your reserved tokens made liquid again and your vote will no longer be counting for any councillors in the elections starting in the next term. Edit this page Last updated on Dec 28, 2022 by Keith Alfaro Previous Join the Council Voting for Councillors Removing your Vote ¬© 2023 Web3 Foundation Legal Disclosures Disclaimer Privacy 
Polkadot Protocol | Polkadot Protocol Specification Protocol Polkadot Protocol caution This specification is Work-In-Progress and any content, structure, design and/or hyper/anchor-link is subject to change . Formally, Polkadot is a replicated sharded state machine designed to resolve the scalability and interoperability among blockchains. In Polkadot vocabulary, shards are called parachains and Polkadot relay chain is part of the protocol ensuring global consensus among all the parachains. The Polkadot relay chain protocol, henceforward called Polkadot protocol , can itself be considered as a replicated state machine on its own. As such, the protocol can be specified by identifying the state machine and the replication strategy. From a more technical point of view, the Polkadot protocol has been divided into two parts, the Polkadot Runtime and the Polkadot Host . The Runtime comprises the state transition logic for the Polkadot protocol and is designed and be upgradable via the consensus engine without requiring hard forks of the blockchain. The Polkadot Host provides the necessary functionality for the Runtime to execute its state transition logic, such as an execution environment, I/O, consensus and network interoperability between parachains. The Polkadot Host is planned to be stable and mostly static for the lifetime duration of the Polkadot protocol, the goal being that most changes to the protocol are primarily conducted by applying Runtime updates and not having to coordinate with network participants on manual software updates. Next Polkadot Host 
Polkadot Host | Polkadot Protocol Specification Host Polkadot Host With the current document, we aim to specify the Polkadot Host part of the Polkadot protocol as a replicated state machine. After defining the different types of hosts in Chapter 1 , we proceed to specify the representation of a valid state of the Protocol in Chapter 2 . We also identify the protocol states by explaining the Polkadot state transition and discussing the detail based on which the Polkadot Host interacts with the state transition function, i.e., Runtime, in the same chapter. Following, we specify the input messages triggering the state transition and the system behavior. In Chapter 4 , we specify the communication protocols and network messages required for the Polkadot Host to communicate with other nodes in the network, such as exchanging blocks and consensus messages. In Chapter 5 and Chapter 6 , we specify the consensus protocol, which is responsible for keeping all the replicas in the same state. Finally, the initial state of the machine is identified and discussed in Section A.3.3. . A Polkadot Host implementation that conforms with this part of the specification should successfully be able to sync its states with the Polkadot network. üìÑÔ∏è 1. Overview The Polkadot Protocol differentiates between different classes of Polkadot Hosts. Each class differs in its trust roots and how active or passively they interact with the network. üìÑÔ∏è 2. States and Transitions 2.1. Introduction üìÑÔ∏è 3. Synchronization Many applications that interact with the Polkadot network, to some extent, must be able to retrieve certain information about the network. Depending on the utility, this includes validators that interact with Polkadot‚Äôs consensus and need access to the full state, either from the past or just the most up-to-date state, or light clients that are only interested in the minimum information required in order to verify some claims about the state of the network, such as the balance of a specific account. To allow implementations to quickly retrieve the required information, different types of synchronization protocols are available, respectively Full, Fast, and Warp sync suited for different needs. üìÑÔ∏è 4. Networking This chapter in its current form is incomplete and considered work in progress. Authors appreciate receiving request for clarification or any reports regarding deviation from the current Polkadot network protocol. This can be done through filing an issue in Polkadot Specification repository. üìÑÔ∏è 5. Block Production 5.1. Introduction üìÑÔ∏è 6. Finality 6.1. Introduction üìÑÔ∏è 7. Light Clients 7.1. Requirements for Light Clients üìÑÔ∏è 8. Availability & Validity Polkadot serves as a replicated shared-state machine designed to resolve scalability issues and interoperability among blockchains. The validators of Polkadot execute transactions and participate in the consensus of Polkadots primary chain, the so-called relay chain. Parachains are independent networks that maintain their own state and are connected to the relay chain. Those parachains can take advantage of the relay chain consensus mechanism, including sending and receiving messages to and from other parachains. Parachain nodes that send parachain blocks, known as candidates, to the validators in order to be included in relay chain are referred to as collators. Previous Polkadot Protocol Next 1. Overview 
1. Overview | Polkadot Protocol Specification Host 1. Overview On this page 1. Overview The Polkadot Protocol differentiates between different classes of Polkadot Hosts. Each class differs in its trust roots and how active or passively they interact with the network. 1.1. Light Client ‚Äã The light client is a mostly passive participant in the protocol. Light clients are designed to work in resource-constrained environments like browsers, mobile devices, or even on-chain. Its main objective is to follow the chain, make queries to the full node on specific information on the recent state of the blockchain, and add extrinsics (transactions). It does not maintain the full state, but rather queries the full node on the latest finalized state and verifies the authenticity of the responses trustlessly. Details of specifications focused on Light Clients can be found in Chapter 7 . 1.2. Full Node ‚Äã While the full node is still a mostly passive participant of the protocol, they follow the chain by receiving and verifying every block in the chain. It maintains a full state of the blockchain by executing the extrinsics in blocks. Their role in the consesus mechanism is limited to following the chain and not producing the blocks. Functional Requirements: The node must populate the state storage with the official genesis state, elaborated further in Section A.3.3. . The node should maintain a set of around 50 active peers at any time. New peers can be found using the discovery protocols ( Section 4.4. ) The node should open and maintain the various required streams ( Section 4.7. ) with each of its active peers. Furthermore, the node should send block requests ( Section 4.8.2. ) to these peers to receive all blocks in the chain and execute each of them. The node should exchange neighbor packets ( Section 4.8.6.1. ). 1.3. Authoring Node ‚Äã The authoring node covers all the features of the full node, but instead of just passively following the protocol, it is an active participant, producing blocks and voting in Grandpa. Functional Requirements: Verify that the Host‚Äôs session key is included in the current Epoch‚Äôs authority set ( Section 3.3.1. ). Run the BABE lottery ( Chapter 5 ) and wait for the next assigned slot in order to produce a block. Gossip any produced blocks to all connected peers ( Section 4.8.1. ). Run the catch-up protocol ( Section 6.6.1. ) to make sure that the node is participating in the current round and not a past round. Run the GRANDPA rounds protocol ( Chapter 6 ). 1.4. Relaying Node ‚Äã The relaying node covers all the features of the authoring node but also participants in the availability and validity process to process new parachain blocks as described in Chapter 8 . Previous Polkadot Host Next 2. States and Transitions 1.1. Light Client 1.2. Full Node 1.3. Authoring Node 1.4. Relaying Node 
2. States and Transitions | Polkadot Protocol Specification Host 2. States and Transitions On this page 2. States and Transitions 2.1. Introduction ‚Äã Definition 1. Discrete State Machine (DSM) ‚Äã A Discrete State Machine (DSM) is a state transition system that admits a starting state and whose set of states and set of transitions are countable. Formally, it is a tuple of ( Œ£ , S , s 0 , Œ¥ ) (\Sigma, S, s_0, \delta) ( Œ£ , S , s 0 ‚Äã , Œ¥ ) where Œ£ \Sigma Œ£ is the countable set of all possible inputs. S {S} S is a countable set of all possible states. s 0 ‚àà S {s}_{{0}}\in{S} s 0 ‚Äã ‚àà S is the initial state. Œ¥ \delta Œ¥ is the state-transition function, known as Runtime in the Polkadot vocabulary, such that Œ¥ : S √ó Œ£ ‚Üí S \delta : S \times \Sigma \rightarrow S Œ¥ : S √ó Œ£ ‚Üí S Definition 2. Path Graph ‚Äã A path graph or a path of n {n} n nodes, formally referred to as P n {P}_{{n}} P n ‚Äã , is a tree with two nodes of vertex degree 1 and the other n-2 nodes of vertex degree 2. Therefore, P n {P}_{{n}} P n ‚Äã can be represented by sequences of ( v 1 , ‚Ä¶ , v n ) {\left({v}_{{1}},\ldots,{v}_{{n}}\right)} ( v 1 ‚Äã , ‚Ä¶ , v n ‚Äã ) where e i = ( v i , v i + 1 ) {e}_{{i}}={\left({v}_{{i}},{v}_{{{i}+{1}}}\right)} e i ‚Äã = ( v i ‚Äã , v i + 1 ‚Äã ) for 1 ‚â§ i ‚â§ n ‚àí 1 {1}\le{i}\le{n}-{1} 1 ‚â§ i ‚â§ n ‚àí 1 is the edge which connect v i {v}_{{i}} v i ‚Äã and v i + 1 {v}_{{{i}+{1}}} v i + 1 ‚Äã . Definition 3. Blockchain ‚Äã A blockchain C {C} C is a directed path graph . Each node of the graph is called Block and indicated by B {B} B . The unique sink of C {C} C is called Genesis Block , and the source is called the Head \text{Head} Head of C {C} C . For any vertex ( B 1 , B 2 ) {\left({B}_{{1}},{B}_{{2}}\right)} ( B 1 ‚Äã , B 2 ‚Äã ) where B 1 ‚Üí B 2 {B}_{{1}}\rightarrow{B}_{{2}} B 1 ‚Äã ‚Üí B 2 ‚Äã we say B 2 {B}_{{2}} B 2 ‚Äã is the parent of B 1 {B}_{{1}} B 1 ‚Äã , which is the child of B 2 {B}_{{2}} B 2 ‚Äã , respectively. We indicate that by: B 2 : = P ( B 1 ) B_2 := P(B_1) B 2 ‚Äã := P ( B 1 ‚Äã ) The parent refers to the child by its hash value ( Definition 10 ), making the path graph tamper-proof since any modifications to the child would result in its hash value being changed. info The term "blockchain" can also be used as a way to refer to the network or system that interacts or maintains the directed path graph. 2.1.1. Block Tree ‚Äã In the course of formation of a (distributed) blockchain, it is possible that the chain forks into multiple subchains in various block positions. We refer to this structure as a block tree : Definition 4. Block ‚Äã The block tree of a blockchain, denoted by B T {B}{T} B T is the union of all different versions of the blockchain observed by the Polkadot Host such that every block is a node in the graph and B 1 {B}_{{1}} B 1 ‚Äã is connected to B 2 {B}_{{2}} B 2 ‚Äã if B 1 {B}_{{1}} B 1 ‚Äã is a parent of B 2 {B}_{{2}} B 2 ‚Äã . When a block in the block tree gets finalized, there is an opportunity to prune the block tree to free up resources into branches of blocks that do not contain all of the finalized blocks or those that can never be finalized in the blockchain ( Chapter 6 ). Definition 5. Pruned Block Tree ‚Äã By Pruned Block Tree , denoted by PBT \text{PBT} PBT , we refer to a subtree of the block tree obtained by eliminating all branches which do not contain the most recent finalized blocks ( Definition 85 ). By pruning , we refer to the procedure of B T ‚Üê PBT {B}{T}\leftarrow\text{PBT} B T ‚Üê PBT . When there is no risk of ambiguity and it is safe to prune BT, we use BT \text{BT} BT to refer to PBT \text{PBT} PBT . Definition 6 gives the means to highlight various branches of the block tree. Definition 6. Subchain ‚Äã Let G {G} G be the root of the block tree and B {B} B be one of its nodes. By Chain ( B ) \text{Chain}{\left({B}\right)} Chain ( B ) , we refer to the path graph from G {G} G to B {B} B in BT \text{BT} BT . Conversely, for a chain C = Chain ( B ) {C}=\text{Chain}{\left({B}\right)} C = Chain ( B ) , we define the head of C {C} C to be B {B} B , formally noted as B = C ‚Äæ {B}\:=\overline{{C}} B = C . We define ‚à£ C ‚à£ {\left|{C}\right|} ‚à£ C ‚à£ , the length of C {C} C as a path graph. If B ‚Ä≤ {B}' B ‚Ä≤ is another node on Chain ( B ) \text{Chain}{\left({B}\right)} Chain ( B ) , then by SubChain ( B ‚Ä≤ , B ) \text{SubChain}{\left({B}',{B}\right)} SubChain ( B ‚Ä≤ , B ) we refer to the subgraph of Chain ( B ) \text{Chain}{\left({B}\right)} Chain ( B ) path graph which contains B {B} B and ends at B ‚Ä≤ {B}' B ‚Ä≤ and by ‚à£ SubChain ( B ‚Ä≤ , B ) ‚à£ {\left|\text{SubChain}{\left({B}',{B}\right)}\right|} ‚à£ SubChain ( B ‚Ä≤ , B ) ‚à£ we refer to its length. Accordingly, C B ‚Ä≤ ( B T ) {\mathbb{{C}}}_{{{B}'}}{\left({B}{T}\right)} C B ‚Ä≤ ‚Äã ( B T ) is the set of all subchains of B T {B}{T} B T rooted at B ‚Ä≤ {B}' B ‚Ä≤ . The set of all chains of B T {B}{T} B T , C G ( B T ) {\mathbb{{C}}}_{{G}}{\left({B}{T}\right)} C G ‚Äã ( B T ) is denoted by C ( B T ) {\mathbb{{C}}}{\left({B}{T}\right)} C ( B T ) or simply C {\mathbb{{C}}} C , for the sake of brevity. Definition 7. Longest Chain ‚Äã We define the following complete order over C {\mathbb{{C}}} C as follows. For chains C 1 , C 2 ‚àà C {C}_{{1}},{C}_{{2}}\in{\mathbb{{C}}} C 1 ‚Äã , C 2 ‚Äã ‚àà C we have that C 1 > C 2 {C}_{{1}}>{C}_{{2}} C 1 ‚Äã > C 2 ‚Äã if either ‚à£ C 1 ‚à£ > ‚à£ C 2 ‚à£ {\left|{C}_{{1}}\right|}>{\left|{C}_{{2}}\right|} ‚à£ C 1 ‚Äã ‚à£ > ‚à£ C 2 ‚Äã ‚à£ or ‚à£ C 1 ‚à£ = ‚à£ C 2 ‚à£ {\left|{C}_{{1}}\right|}={\left|{C}_{{2}}\right|} ‚à£ C 1 ‚Äã ‚à£ = ‚à£ C 2 ‚Äã ‚à£ . If ‚à£ C 1 ‚à£ = ‚à£ C 2 ‚à£ {\left|{C}_{{1}}\right|}={\left|{C}_{{2}}\right|} ‚à£ C 1 ‚Äã ‚à£ = ‚à£ C 2 ‚Äã ‚à£ we say C 1 > C 2 {C}_{{1}}>{C}_{{2}} C 1 ‚Äã > C 2 ‚Äã if and only if the block arrival time ( Definition 63 ) of C ‚Äæ 1 \overline{{C}}_{{1}} C 1 ‚Äã is less than the block arrival time of C ‚Äæ 2 \overline{{C}}_{{2}} C 2 ‚Äã , from the subjective perspective of the Host. We define the Longest-Chain ( B T ) \text{Longest-Chain}{\left({B}{T}\right)} Longest-Chain ( B T ) to be the maximum chain given by this order. Definition 8. Longest Path ‚Äã Longest-Path ( B T ) \text{Longest-Path}{\left({B}{T}\right)} Longest-Path ( B T ) returns the path graph of B T {B}{T} B T which is the longest among all paths in B T {B}{T} B T and has the earliest block arrival time ( Definition 63 ). Deepest-Leaf ( B T ) \text{Deepest-Leaf}{\left({B}{T}\right)} Deepest-Leaf ( B T ) returns the head of Longest-Path ( B T ) \text{Longest-Path}{\left({B}{T}\right)} Longest-Path ( B T ) chain. Because every block in the blockchain contains a reference to its parent, it is easy to see that the block tree is de facto a tree. A block tree naturally imposes partial order relationships on the blocks as follows: Definition 9. Descendant and Ancestor ‚Äã We say B {B} B is descendant of B ‚Ä≤ {B}' B ‚Ä≤ , formally noted as B > B ‚Ä≤ {B}>{B}' B > B ‚Ä≤ , if ( ‚à£ B ‚à£ > ‚à£ B ‚Ä≤ ‚à£ ) ‚àà C {\left({\left|{B}\right|}>{\left|{B}'\right|}\right)}\in{C} ( ‚à£ B ‚à£ > ‚à£ B ‚Ä≤ ‚à£ ) ‚àà C . Respectively, we say that B ‚Ä≤ {B}' B ‚Ä≤ is an ancestor of B {B} B , formally noted as B < B ‚Ä≤ {B}<{B}' B < B ‚Ä≤ , if ( ‚à£ B ‚à£ < ‚à£ B ‚Ä≤ ‚à£ ) ‚àà C {\left({\left|{B}\right|}<{\left|{B}'\right|}\right)}\in{C} ( ‚à£ B ‚à£ < ‚à£ B ‚Ä≤ ‚à£ ) ‚àà C . 2.2. State Replication ‚Äã Polkadot nodes replicate each other‚Äôs state by syncing the history of the extrinsics. This, however, is only practical if a large set of transactions are batched and synced at the time. The structure in which the transactions are journaled and propagated is known as a block of extrinsics ( Section 2.2.1. ). Like any other replicated state machine, state inconsistency can occur between Polkadot replicas. Section 2.4.5. gives an overview of how a Polkadot Host node manages multiple variants of the state. 2.2.1. Block Format ‚Äã A Polkadot block consists a block header ( Definition 10 ) and a block body ( Definition 13 ). The block body , in turn, is made up out of extrinsics , which represent the generalization of the concept of transactions . Extrinsics can contain any set of external data the underlying chain wishes to validate and track. Image 1. Block ‚Äã cluster__block Block block__seq pos size type id 0 ... BlockHeader header ... ... BlockBody body block_header__seq BlockHeader block__seq:header_type->block_header__seq block_body__seq BlockBody block__seq:body_type->block_body__seq Definition 10. Block Header ‚Äã The header of block B , H h ( B ) {H}_{{h}}{\left({B}\right)} H h ‚Äã ( B ) , is a 5-tuple containing the following elements: parent_hash: formally indicated as H p {H}_{{p}} H p ‚Äã , is the 32-byte Blake2b hash ( Section A.1.1.1. ) of the SCALE encoded parent block header ( Definition 12 ). number: formally indicated as H i {H}_{{i}} H i ‚Äã , is an integer, which represents the index of the current block in the chain. It is equal to the number of the ancestor blocks. The genesis state has the number 0. state_root: formally indicated as H r {H}_{{r}} H r ‚Äã , is the root of the Merkle trie, whose leaves implement the storage for the system. extrinsics _ root: is the field which is reserved for the Runtime to validate the integrity of the extrinsics composing the block body. For example, it can hold the root hash of the Merkle trie which stores an ordered list of the extrinsics being validated in this block. The extrinsics _ root is set by the runtime and its value is opaque to the Polkadot Host. This element is formally referred to as H e {H}_{{e}} H e ‚Äã . digest: this field is used to store any chain-specific auxiliary data, which could help the light clients interact with the block without the need of accessing the full storage as well as consensus-related data including the block signature. This field is indicated as H d {H}_{{d}} H d ‚Äã ( Definition 11 ). Image 2. Block Header ‚Äã cluster__block_header BlockHeader block_header__seq pos size type id 0 32 parent_hash 32 ... Scale::CompactInt number ... 32 state_root ... 32 extrinsic_root ... ... Scale::CompactInt num_digests ... ... Digest digests repeat num_digests.value times digest__seq Digest block_header__seq:digests_type->digest__seq Definition 11. Header Digest ‚Äã The header digest of block B {B} B formally referred to by H d ( B ) {H}_{{d}}{\left({B}\right)} H d ‚Äã ( B ) is an array of digest items H d i {{H}_{{d}}^{{i}}} H d i ‚Äã ‚Äôs, known as digest items of varying data type ( Definition 178 ) such that: H d ( B ) : = H d 1 , . . . , H d n H_d(B) := H_d^1, ..., H_d^n H d ‚Äã ( B ) := H d 1 ‚Äã , ... , H d n ‚Äã where each digest item can hold one of the following type identifiers: H d i = { 4 ‚Üí ( t , id , m ) 5 ‚Üí ( t , id , m ) 6 ‚Üí ( t , id , m ) 8 ‚Üí ( t ) H_d^i = \begin{cases} 4 \text{ } \rarr \text{ } (t, \text{id}, m) \\ 5 \text{ } \rarr \text{ } (t, \text{id}, m) \\ 6 \text{ } \rarr \text{ } (t, \text{id}, m) \\ 8 \text{ } \rarr \text{ } (t) \end{cases} H d i ‚Äã = ‚é© ‚é® ‚éß ‚Äã 4 ‚Üí ( t , id , m ) 5 ‚Üí ( t , id , m ) 6 ‚Üí ( t , id , m ) 8 ‚Üí ( t ) ‚Äã where id \text{id} id is a 4-byte ASCII encoded consensus engine identifier m \text{m} m is a SCALE-encoded byte array containing the message payload t = 4 t = 4 t = 4 Consensus Message , contains scale-encoded message m m m from the Runtime to the consensus engine. The receiving engine is determined by the id identifier: id = BABE: a message to BABE engine ( Definition 54 ) id = FRNK: a message to GRANDPA engine ( Definition 82 ) t = 5 t = 5 t = 5 Seal , is produced by the consensus engine and proves the authorship of the block producer. The engine used for this is provided through id (at the moment, BABE ), while m m m contains the scale-encoded signature ( Definition 66 ) of the block producer. In particular, the Seal digest item must be the last item in the digest array and must be stripped off by the Polkadot Host before the block is submitted to any Runtime function, including for validation. The Seal must be added back to the digest afterward. t = 6 t = 6 t = 6 Pre-Runtime digest , contains messages from the consensus engines to the runtime. Currently only used by BABE to pass the scale encoded BABE Header ( Definition 65 ) in m m m with id = BABE . t = 8 t = 8 t = 8 Runtime Environment Updated digest , indicates that changes regarding the Runtime code or heap pages ( Section 2.6.3.1. ) occurred. No additional data is provided. Image 3. Digest ‚Äã cluster__digest Digest cluster__pre_runtime Digest::PreRuntime cluster__post_runtime Digest::PostRuntime cluster__seal Digest::Seal cluster__empty Digest::Empty digest__seq pos size type id 0 1 u1‚ÜíTypeId type 1 ... switch (type) value digest__seq:type_type->digest__seq:value_type digest__seq_value_switch case type :type_id_pre_runtime PreRuntime :type_id_post_runtime PostRuntime :type_id_seal Seal :type_id_runtime_updated Empty digest__seq:value_type->digest__seq_value_switch pre_runtime__seq pos size type id 0 4 str(ASCII) engine 4 ... Scale::Bytes payload digest__seq_value_switch:case0->pre_runtime__seq post_runtime__seq pos size type id 0 4 str(ASCII) engine 4 ... Scale::Bytes payload digest__seq_value_switch:case1->post_runtime__seq seal__seq pos size type id 0 4 str(ASCII) engine 4 ... Scale::Bytes payload digest__seq_value_switch:case2->seal__seq empty__seq pos size type id digest__seq_value_switch:case3->empty__seq Definition 12. Header Hash ‚Äã The block header hash of block B {B} B , H h ( B ) {H}_{{h}}{\left({B}\right)} H h ‚Äã ( B ) , is the hash of the header of block B {B} B encoded by simple codec: H h ( B ) = Blake2b ( Enc S C ( Head ( B ) ) ) \displaystyle{H}_{{h}}{\left({B}\right)}\:=\text{Blake2b}{\left(\text{Enc}_{{{S}{C}}}{\left(\text{Head}{\left({B}\right)}\right)}\right)} H h ‚Äã ( B ) = Blake2b ( Enc S C ‚Äã ( Head ( B ) ) ) Definition 13. Block Body ‚Äã The block body consists of a sequence of extrinsics, each encoded as a byte array. The content of an extrinsic is completely opaque to the Polkadot Host. As such, from the point of the Polkadot Host, and is simply a SCALE encoded array of byte arrays. The body of Block B {B} B represented as Body ( B ) \text{Body}{\left({B}\right)} Body ( B ) is defined to be: Body ( B ) : = Enc S C ( E 1 , . . . , E n ) \text{Body}(B) := \text{Enc}_{SC}(E_1,...,E_n) Body ( B ) := Enc SC ‚Äã ( E 1 ‚Äã , ... , E n ‚Äã ) Where each E i ‚àà B {E}_{{i}}\in{\mathbb{{B}}} E i ‚Äã ‚àà B is a SCALE encoded extrinsic. Image 4. Block Body ‚Äã cluster__block_body BlockBody cluster__transaction BlockBody::Transaction block_body__seq pos size type id 0 ... Scale::CompactInt num_transactions ... ... Transaction transactions repeat num_transactions.value times transaction__seq pos size type id 0 ... Scale::CompactInt len_data ... len_data.value data block_body__seq:transactions_type->transaction__seq 2.3. Extrinsics ‚Äã The block body consists of an array of extrinsics. In a broad sense, extrinsics are data from outside of the state which can trigger state transitions. This section describes extrinsics and their inclusion into blocks. 2.3.1. Preliminaries ‚Äã The extrinsics are divided into two main categories defined as follows: Transaction extrinsics are extrinsics which are signed using either of the key types ( Section A.1.4. ) and broadcasted between the nodes. Inherent extrinsics are unsigned extrinsics that are generated by Polkadot Host and only included in the blocks produced by the node itself. They are broadcasted as part of the produced blocks rather than being gossiped as individual extrinsics. The Polkadot Host does not specify or limit the internals of each extrinsics and those are defined and dealt with by the Runtime ( Definition 1 ). From the Polkadot Host point of view, each extrinsics is simply a SCALE-encoded blob ( Section A.2.2. ). 2.3.2. Transactions ‚Äã Transaction are submitted and exchanged through Transactions network messages ( Section 4.8.5. ). Upon receiving a Transactions message, the Polkadot Host decodes the SCALE-encoded blob and splits it into individually SCALE-encoded transactions. Alternatively, transactions can be submitted to the host by off-chain worker through the Host API ( Section B.6.2. ). Any new transaction should be submitted to the Runtime ( Section C.7.1. ). This will allow the Polkadot Host to check the validity of the received transaction against the current state and if it should be gossiped to other peers. If it considers the submitted transaction as valid, the Polkadot Host should store it for inclusion in future blocks. The whole process of handling new transactions is described in more detail by Validate-Transactions-and-Store . Additionally, valid transactions that are supposed to be gossiped are propagated to connected peers of the Polkadot Host. While doing so the Polkadot Host should keep track of peers already aware of each transaction. This includes peers which have already gossiped the transaction to the node as well as those to whom the transaction has already been sent. This behavior is mandated to avoid resending duplicates and unnecessarily overloading the network. To that aim, the Polkadot Host should keep a transaction pool and a transaction queue defined as follows: Definition 14. Transaction Queue ‚Äã The Transaction Queue of a block producer node, formally referred to as T Q {T}{Q} T Q is a data structure which stores the transactions ready to be included in a block sorted according to their priorities ( Section 4.8.5. ). The Transaction Pool , formally referred to as T P {T}{P} T P , is a hash table in which the Polkadot Host keeps the list of all valid transactions not in the transaction queue. Furthermore, Validate-Transactions-and-Store updates the transaction pool and the transaction queue according to the received message: Algorithm 1. Validate Transactions and Store ‚Äã \begin{algorithm} \caption{Validate-Transactions-and-Store} \begin{algorithmic} \state $L \leftarrow Dec_{SC}(M_T)$ \forall{$\{T \in L \mid T \notin TQ \mid T \notin TP\}$} \state $B_d \leftarrow$ \call{Head}{\call{Longest-Chain}{$BT$}} \state $N \leftarrow H_n(B_d)$ \state $R \leftarrow$ \call{Call-Runtime-Entry}{$\texttt{TaggedTransactionQueue\_validate\_transaction}, N, T$} \if{\call{Valid}{$R$}} \if{\call{Requires}{$R$}$ \subset \bigcup_{\forall T \in (TQ~\cup~B_i \mid \exists i_{\mid d > i})}$ \call{Provided-Tags}{$T$}} \state \call{Insert-At}{$TQ, T, $\call{Requires}{$R$}$, $\call{Priority}{$R$}} \else \state \call{Add-To}{$TP,T$} \endif \state \call{Maintain-Transaction-Pool}{} \if{\call{ShouldPropagate}{$R$}} \state \call{Propagate}{$T$} \endif \endif \endfor \end{algorithmic} \end{algorithm} where M T {M}_{{T}} M T ‚Äã is the transaction message (offchain transactions?) Dec S C \text{Dec}_{{{S}{C}}} Dec S C ‚Äã decodes the SCALE encoded message. Longest-Chain \text{Longest-Chain} Longest-Chain is defined in Definition 7 . T a g g e d T r a n s a c t i o n Q u e u e _ v a l i d a t e _ t r a n s a c t i o n {\tt{TaggedTransactionQueue\_validate\_transaction}} TaggedTransactionQueue_validate_transaction is a Runtime entrypoint specified in Section C.7.1. and R e q u i r e s ( R ) {Requires}{\left({R}\right)} R e q u i res ( R ) , P r i o r i t y ( R ) {Priority}{\left({R}\right)} P r i or i t y ( R ) and P r o p a g a t e ( R ) {Propagate}{\left({R}\right)} P ro p a g a t e ( R ) refer to the corresponding fields in the tuple returned by the entrypoint when it deems that T {T} T is valid. Provided-Tags ( T ) \text{Provided-Tags}{\left({T}\right)} Provided-Tags ( T ) is the list of tags that transaction T {T} T provides. The Polkadot Host needs to keep track of tags that transaction T {T} T provides as well as requires after validating it. Insert-At ( T Q , T , Requires ( R ) , Priority ( R ) ) \text{Insert-At}{\left({T}{Q},{T},\text{Requires}{\left({R}\right)},\text{Priority}{\left({R}\right)}\right)} Insert-At ( T Q , T , Requires ( R ) , Priority ( R ) ) places T {T} T into T Q {T}{Q} T Q approperietly such that the transactions providing the tags which T {T} T requires or have higher priority than T {T} T are ahead of T {T} T . Maintain-Transaction-Pool \text{Maintain-Transaction-Pool} Maintain-Transaction-Pool is described in Maintain-Transaction-Pool . ShouldPropagate \text{ShouldPropagate} ShouldPropagate indicates whether the transaction should be propagated based on the Propagate field in the ValidTransaction type as defined in Definition 218 , which is returned by TaggedTransactionQueue_validate_transaction {\mathtt{\text{TaggedTransactionQueue\_validate\_transaction}}} TaggedTransactionQueue_validate_transaction . Propagate ( T ) \text{Propagate}{\left({T}\right)} Propagate ( T ) sends T {T} T to all connected peers of the Polkadot Host who are not already aware of T {T} T . Algorithm 2. Maintain Transaction Pool ‚Äã \begin{algorithm} \caption{Maintain-Transaction-Pool} \begin{algorithmic} \state Scan the pool for ready transactions \state Move them to the transaction queue \state Drop invalid transactions \end{algorithmic} \end{algorithm} info This has not been defined yet. 2.3.3. Inherents ‚Äã Inherents are unsigned extrinsics inserted into a block by the block author and as a result are not stored in the transaction pool or gossiped across the network. Instead, they are generated by the Polkadot Host by passing the required inherent data, as listed in Table 1 , to the Runtime method BlockBuilder_inherent_extrinsics {\mathtt{\text{BlockBuilder\_inherent\_extrinsics}}} BlockBuilder_inherent_extrinsics ( Section C.6.3. ). Then the returned extrinsics should be included in the current block as explained in Build-Block . Table 1. Inherent Data ‚Äã Identifier Value Type Description timstap0 Unsigned 64-bit integer Unix epoch time ( Definition 171 ) babeslot Unsigned 64-bit integer The babe slot ( DEPRECATED ) ( Definition 50 ) parachn0 Parachain inherent data ( Definition 93 ) Parachain candidate inclusion ( Section 8.2.2. ) Definition 15. Inherent Data ‚Äã Inherent-Data is a hashtable ( Definition 182 ), an array of key-value pairs consisting of the inherent 8-byte identifier and its value, representing the totality of inherent extrinsics included in each block. The entries of this hash table which are listed in Table 1 are collected or generated by the Polkadot Host and then handed to the Runtime for inclusion ( Build-Block ). 2.4. State Storage Trie ‚Äã For storing the state of the system, Polkadot Host implements a hash table storage where the keys are used to access each data entry. There is no assumption either on the size of the key nor on the size of the data stored under them, besides the fact that they are byte arrays with specific upper limits on their length. The limit is imposed by the encoding algorithms to store the key and the value in the storage trie ( Section A.2.2.1. ). 2.4.1. Accessing System Storage ‚Äã The Polkadot Host implements various functions to facilitate access to the system storage for the Runtime ( Section 2.6.1. ). Here we formalize the access to the storage when it is being directly accessed by the Polkadot Host (in contrast to Polkadot runtime). Definition 16. Stored Value ‚Äã The StoredValue {\mathsf{\text{StoredValue}}} StoredValue function retrieves the value stored under a specific key in the state storage and is formally defined as: StoredValue: K ‚Üí V \sf \text{StoredValue: } \mathcal K \rarr \mathcal V StoredValue: K ‚Üí V k ‚Üí { v if ( k , v ) exists in state storage œï otherwise k \rarr \begin{cases} v \text{ if } (k,v) \text{ exists in state storage} \\ \phi \text{ otherwise} \end{cases} k ‚Üí { v if ( k , v ) exists in state storage œï otherwise ‚Äã where K ‚äÇ B {\mathcal{{K}}}\subset{\mathbb{{B}}} K ‚äÇ B and V ‚äÇ B {\mathcal{{V}}}\subset{\mathbb{{B}}} V ‚äÇ B are respectively the set of all keys and values stored in the state storage. V {\mathcal{{V}}} V can be an empty value. 2.4.2. General Structure ‚Äã In order to ensure the integrity of the state of the system, the stored data needs to be re-arranged and hashed in a radix tree , which hereafter we refer to as the State Trie or just Trie . This rearrangement is necessary to be able to compute the Merkle hash of the whole or part of the state storage, consistently and efficiently at any given time. The trie is used to compute the Merkle root ( Section 2.4.4. ) of the state, H r {H}_{{r}} H r ‚Äã ( Definition 10 ), whose purpose is to authenticate the validity of the state database. Thus, the Polkadot Host follows a rigorous encoding algorithm to compute the values stored in the trie nodes to ensure that the computed Merkle hash, H r {H}_{{r}} H r ‚Äã , matches across the Polkadot Host implementations. The trie is a radix-16 tree ( Definition 17 ). Each key value identifies a unique node in the tree. However, a node in a tree might or might not be associated with a key in the storage. Definition 17. Radix-r Tree ‚Äã A Radix-r tree is a variant of a trie in which: Every node has at most r {r} r children where r = 2 x {r}={2}^{{x}} r = 2 x for some x {x} x ; Each node that is the only child of a parent, which does not represent a valid key is merged with its parent. As a result, in a radix tree, any path whose interior vertices all have only one child and does not represent a valid key in the data set, is compressed into a single edge. This improves space efficiency when the key space is sparse. When traversing the trie to a specific node, its key can be reconstructed by concatenating the subsequences of the keys which are stored either explicitly in the nodes on the path or implicitly in their position as a child of their parent. To identify the node corresponding to a key value, k {k} k , first, we need to encode k {k} k in a way consistent with the trie structure. Because each node in the trie has at most 16 children, we represent the key as a sequence of 4-bit nibbles: Definition 18. Key Encode ‚Äã For the purpose of labeling the branches of the trie, the key k {k} k is encoded to k enc {k}_{{\text{enc}}} k enc ‚Äã using KeyEncode {\mathsf{\text{KeyEncode}}} KeyEncode functions: k enc = ( k enc 1 , ‚Ä¶ , k enc 2 n ) = KeyEncode ( k ) {k}_{{\text{enc}}}\:={\left({k}_{{\text{enc}_{{1}}}},\ldots,{k}_{{\text{enc}_{{{2}{n}}}}}\right)}\:={\mathsf{\text{KeyEncode}}}{\left({k}\right)} k enc ‚Äã = ( k enc 1 ‚Äã ‚Äã , ‚Ä¶ , k enc 2 n ‚Äã ‚Äã ) = KeyEncode ( k ) such that: KeyEncode : B ‚Üí Nibbles 4 {\mathsf{\text{KeyEncode}}}:{\mathbb{{B}}}\rightarrow\text{Nibbles}^{{4}} KeyEncode : B ‚Üí Nibbles 4 k ‚üº ( k enc 1 , ‚Ä¶ , k enc 2 n ) {k} \longmapsto{\left({k}_{{\text{enc}_{{1}}}},\ldots,{k}_{{\text{enc}_{{{2}{n}}}}}\right)} k ‚üº ( k enc 1 ‚Äã ‚Äã , ‚Ä¶ , k enc 2 n ‚Äã ‚Äã ) ( b 1 , ‚Ä¶ , b n ) ‚üº ( b 1 1 , b 1 2 , b 2 1 , b 2 2 , ‚Ä¶ , b n 1 , b n 2 ) {\left({b}_{{1}},\ldots,{b}_{{n}}\right)} \longmapsto{\left({{b}_{{1}}^{{{1}}}},{{b}_{{1}}^{{2}}},{{b}_{{2}}^{{1}}},{{b}_{{2}}^{{2}}},\ldots,{{b}_{{n}}^{{1}}},{{b}_{{n}}^{{2}}}\right)} ( b 1 ‚Äã , ‚Ä¶ , b n ‚Äã ) ‚üº ( b 1 1 ‚Äã , b 1 2 ‚Äã , b 2 1 ‚Äã , b 2 2 ‚Äã , ‚Ä¶ , b n 1 ‚Äã , b n 2 ‚Äã ) where Nibble 4 \text{Nibble}^{{4}} Nibble 4 is the set of all nibbles of 4-bit arrays and b i 1 {{b}_{{i}}^{{1}}} b i 1 ‚Äã and b i 2 {{b}_{{i}}^{{2}}} b i 2 ‚Äã are 4-bit nibbles, which are the big endian representations of b i {b}_{{i}} b i ‚Äã : k enc i = ( b i 1 , b i 2 ) = ( b i √∑ 16 , b i mod 16 ) {k}_{{\text{enc}_{{i}}}}\:={\left({{b}_{{i}}^{{1}}},{{b}_{{i}}^{{2}}}\right)}\:={\left({b}_{{i}}\div{16},{b}_{{i}}\text{mod}{16}\right)} k enc i ‚Äã ‚Äã = ( b i 1 ‚Äã , b i 2 ‚Äã ) = ( b i ‚Äã √∑ 16 , b i ‚Äã mod 16 ) where mod \text{mod} mod is the remainder and √∑ \div √∑ is the integer division operators. By looking at k enc {k}_{{\text{enc}}} k enc ‚Äã as a sequence of nibbles, one can walk the radix tree to reach the node identifying the storage value of k {k} k . 2.4.3. Trie Structure ‚Äã In this subsection, we specify the structure of the nodes in the trie as well as the trie structure: Definition 19. Set of Nodes ‚Äã We refer to the set of the nodes of Polkadot state trie by N {\mathcal{{N}}} N . By N ‚àà N {N}\in{\mathcal{{N}}} N ‚àà N to refer to an individual node in the trie. Definition 20. State Trie ‚Äã The state trie is a radix-16 tree ( Definition 17 ). Each node in the trie is identified with a unique key k N {k}_{{N}} k N ‚Äã such that: k N {k}_{{N}} k N ‚Äã is the shared prefix of the key of all the descendants of N {N} N in the trie. and at least one of the following statements holds: ( k N , v ) {\left({k}_{{N}},{v}\right)} ( k N ‚Äã , v ) corresponds to an existing entry in the State Storage. N {N} N has more than one child. Conversely, if ( k , v ) {\left({k},{v}\right)} ( k , v ) is an entry in the state trie then there is a node N ‚àà N {N}\in{\mathcal{{N}}} N ‚àà N such that k N = k {k}_{{N}}={k} k N ‚Äã = k . Definition 21. Branch ‚Äã A branch node N b ‚àà N b {N}_{{b}}\in{\mathcal{{N}}}_{{b}} N b ‚Äã ‚àà N b ‚Äã is a node which has one child or more. A branch node can have at most 16 children. A leaf node N l ‚àà N l {N}_{{l}}\in{\mathcal{{N}}}_{{l}} N l ‚Äã ‚àà N l ‚Äã is a childless node. Accordingly: N b = { N b ‚àà N ‚à£ N b is a branch node } {\mathcal{{N}}}_{{b}}\:={\left\lbrace{N}_{{b}}\in{\mathcal{{N}}}{\mid}{N}_{{b}}\ \text{ is a branch node}\right\rbrace} N b ‚Äã = { N b ‚Äã ‚àà N ‚à£ N b ‚Äã is a branch node } N l = { N l ‚àà N ‚à£ N l is a leaf node } {\mathcal{{N}}}_{{l}}\:={\left\lbrace{N}_{{l}}\in{\mathcal{{N}}}{\mid}{N}_{{l}}\ \text{ is a leaf node}\right\rbrace} N l ‚Äã = { N l ‚Äã ‚àà N ‚à£ N l ‚Äã is a leaf node } For each node, part of k N {k}_{{N}} k N ‚Äã is built while the trie is traversed from the root to N {N} N and another part of k N {k}_{{N}} k N ‚Äã is stored in N {N} N ( Definition 22 ). Definition 22. Aggregated Prefix Key ‚Äã For any N ‚àà N {N}\in{\mathcal{{N}}} N ‚àà N , its key k N {k}_{{N}} k N ‚Äã is divided into an aggregated prefix key, pk N Agr {\text{pk}_{{N}}^{{\text{Agr}}}} pk N Agr ‚Äã , aggregated by Aggregate-Key and a partial key , pk N \text{pk}_{{N}} pk N ‚Äã of length 0 ‚â§ l pk N {0}\le{l}_{{\text{pk}_{{N}}}} 0 ‚â§ l pk N ‚Äã ‚Äã in nibbles such that: pk N = ( k enc i , ‚Ä¶ , k enc i + l pk N ) \text{pk}_{{N}}\:={\left({k}_{{\text{enc}_{{i}}}},\ldots,{k}_{{\text{enc}_{{{i}+{l}_{{\text{pk}_{{N}}}}}}}}\right)} pk N ‚Äã = ( k enc i ‚Äã ‚Äã , ‚Ä¶ , k enc i + l pk N ‚Äã ‚Äã ‚Äã ‚Äã ) where pk N Agr {\text{pk}_{{N}}^{{\text{Agr}}}} pk N Agr ‚Äã is a prefix subsequence of k N {k}_{{N}} k N ‚Äã ; i {i} i is the length of pk N Agr {\text{pk}_{{N}}^{{\text{Agr}}}} pk N Agr ‚Äã in nibbles and so we have: KeyEncode ( k N ) = pk N Agr ‚à£ ‚à£ pk N = ( k enc 1 , ‚Ä¶ , k enc i ‚àí 1 , k enc i , k enc i + l pk N ) {\mathsf{\text{KeyEncode}}}{\left({k}_{{N}}\right)}={\text{pk}_{{N}}^{{\text{Agr}}}}{\mid}{\mid}\text{pk}_{{N}}={\left({k}_{{\text{enc}_{{1}}}},\ldots,{k}_{{\text{enc}_{{{i}-{1}}}}},{k}_{{\text{enc}_{{i}}}},{k}_{{\text{enc}_{{{i}+{l}_{{\text{pk}_{{N}}}}}}}}\right)} KeyEncode ( k N ‚Äã ) = pk N Agr ‚Äã ‚à£ ‚à£ pk N ‚Äã = ( k enc 1 ‚Äã ‚Äã , ‚Ä¶ , k enc i ‚àí 1 ‚Äã ‚Äã , k enc i ‚Äã ‚Äã , k enc i + l pk N ‚Äã ‚Äã ‚Äã ‚Äã ) Part of pk N Agr {\text{pk}_{{N}}^{{\text{Agr}}}} pk N Agr ‚Äã is explicitly stored in N {N} N ‚Äôs ancestors. Additionally, for each ancestor, a single nibble is implicitly derived while traversing from the ancestor to its child included in the traversal path using the Index N \text{Index}_{{N}} Index N ‚Äã function ( Definition 23 ). Definition 23. Index ‚Äã For N ‚àà N b {N}\in{\mathcal{{N}}}_{{b}} N ‚àà N b ‚Äã and N c {N}_{{c}} N c ‚Äã child of N {N} N , we define Index N {\mathsf{\text{Index}}}_{{N}} Index N ‚Äã function as: Index N : { N C ‚àà c c ( N ) ‚à£ N c is a child of N } ‚Üí Nibbles 1 4 N c ‚Üí i \textsf{Index}_N: \{N_C \in cc(N) \mid N_c \text{ is a child of } N\} \rightarrow \text{Nibbles}_1^4 \\ N_c \rightarrow i Index N ‚Äã : { N C ‚Äã ‚àà cc ( N ) ‚à£ N c ‚Äã is a child of N } ‚Üí Nibbles 1 4 ‚Äã N c ‚Äã ‚Üí i such that k N c = k N ‚à£ ‚à£ i ‚à£ ‚à£ pk N c {k}_{{{N}_{{c}}}}={k}_{{N}}{\mid}{\left|{i}{\mid}\right|}\text{pk}_{{{N}_{{c}}}} k N c ‚Äã ‚Äã = k N ‚Äã ‚à£ ‚à£ i ‚à£ ‚à£ pk N c ‚Äã ‚Äã Algorithm 3. Aggregate-Key ‚Äã \begin{algorithm} \caption{Aggregate-Key} \begin{algorithmic} \require{$P_N \coloneqq ($\textsc{TrieRoot}$ = N_1, \dots, N_j = N)$} \state $pk^{Agr}_N \leftarrow \phi$ \state $i \leftarrow 1$ \forall{$N_i \in P_N$} \state $pk^{Agr}_N \leftarrow pk^{Agr}_N || pk_{N_i} || \textrm{Index}_{N_i}(N_{i + 1})$ \endfor \state $pk^{Agr}_N \leftarrow pk^{Agr}_N || pk_{N}$ \return $pk^{Agr}_N$ \end{algorithmic} \end{algorithm} Assuming that P N {P}_{{N}} P N ‚Äã is the path ( Definition 2 ) from the trie root to node N {N} N , Aggregate-Key rigorously demonstrates how to build pk N Agr {\text{pk}_{{N}}^{{\text{Agr}}}} pk N Agr ‚Äã while traversing P N {P}_{{N}} P N ‚Äã . Definition 24. Node Value ‚Äã A node N ‚àà N {N}\in{\mathcal{{N}}} N ‚àà N stores the node value , v N {v}_{{N}} v N ‚Äã , which consists of the following concatenated data: Node Header ‚à£ ‚à£ Partial Key ‚à£ ‚à£ Node Subvalue \text{Node Header}{\left|{\left|\text{Partial Key}\right|}\right|}\text{Node Subvalue} Node Header ‚à£ ‚à£ Partial Key ‚à£ ‚à£ Node Subvalue Formally noted as: v N = Head N ‚à£ ‚à£ Enc HE ( p k N ) ‚à£ ‚à£ s v N {v}_{{N}}\:=\text{Head}_{{N}}{\left|{\left|\text{Enc}_{\text{HE}}{\left({p}{k}_{{N}}\right)}\right|}\right|}{s}{v}_{{N}} v N ‚Äã = Head N ‚Äã ‚à£ ‚à£ Enc HE ‚Äã ( p k N ‚Äã ) ‚à£ ‚à£ s v N ‚Äã where Head N \text{Head}_{{N}} Head N ‚Äã is the node header from Definition 25 p k N {p}{k}_{{N}} p k N ‚Äã is the partial key from Definition 22 Enc HE \text{Enc}_{\text{HE}} Enc HE ‚Äã is hex encoding ( Definition 189 ) s v N {s}{v}_{{N}} s v N ‚Äã is the node subvalue from Definition 27 Definition 25. Node Header ‚Äã The node header , consisting of ‚â• 1 \ge{1} ‚â• 1 bytes, N 1 ‚Ä¶ N n {N}_{{1}}\ldots{N}_{{n}} N 1 ‚Äã ‚Ä¶ N n ‚Äã , specifies the node variant and the partial key length ( Definition 22 ). Both pieces of information can be represented in bits within a single byte, N 1 {N}_{{1}} N 1 ‚Äã , where the amount of bits of the variant, v {v} v , and the bits of the partial key length, p l {p}_{{l}} p l ‚Äã varies. v = { 01 Leaf p l = 2 6 10 Branch Node with k N ‚àâ K p l = 2 6 11 Branch Node with k N ‚àà K p l = 2 6 001 Leaf containing a hashed subvalue p l = 2 5 0001 Branch containing a hashed subvalue p l = 2 4 00000000 Empty p l = 0 00000001 Reserved for compact encoding {v}={\left\lbrace\begin{matrix}{01}&\text{Leaf}&{p}_{{l}}={2}^{{6}}\\{10}&\text{Branch Node with }\ {k}_{{N}}\notin{\mathcal{{K}}}&{p}_{{l}}={2}^{{6}}\\{11}&\text{Branch Node with }\ {k}_{{N}}\in{\mathcal{{K}}}&{p}_{{l}}={2}^{{6}}\\{001}&\text{Leaf containing a hashed subvalue}&{p}_{{l}}={2}^{{5}}\\{0001}&\text{Branch containing a hashed subvalue}&{p}_{{l}}={2}^{{4}}\\{0000}{0000}&\text{Empty}&{p}_{{l}}={0}\\{0000}{0001}&\text{Reserved for compact encoding}&\end{matrix}\right.} v = ‚é© ‚é® ‚éß ‚Äã 01 10 11 001 0001 0000 0000 0000 0001 ‚Äã Leaf Branch Node with k N ‚Äã ‚àà / K Branch Node with k N ‚Äã ‚àà K Leaf containing a hashed subvalue Branch containing a hashed subvalue Empty Reserved for compact encoding ‚Äã p l ‚Äã = 2 6 p l ‚Äã = 2 6 p l ‚Äã = 2 6 p l ‚Äã = 2 5 p l ‚Äã = 2 4 p l ‚Äã = 0 ‚Äã If the value of p l {p}_{{l}} p l ‚Äã is equal to the maximum possible value the bits can hold, such as 63 ( 2 6 ‚àí 1 {2}^{{6}}-{1} 2 6 ‚àí 1 ) in case of the 01 {01} 01 variant, then the value of the next 8 bits ( N 2 {N}_{{2}} N 2 ‚Äã ) are added the length. This process is repeated for every N n {N}_{{n}} N n ‚Äã where N n = 2 8 ‚àí 1 {N}_{{n}}={2}^{{8}}-{1} N n ‚Äã = 2 8 ‚àí 1 . Any value smaller than the maximum possible value of N n {N}_{{n}} N n ‚Äã implies that the next value of N n + 1 {N}_{{{n}+{1}}} N n + 1 ‚Äã should not be added to the length. The hashed subvalue for variants 001 {001} 001 and 0001 {0001} 0001 is described in Definition 28 . Formally, the length of the partial key, pk N l {\text{pk}_{{N}}^{{l}}} pk N l ‚Äã , is defined as: pk N l = p l + N n + N n + x + ‚Ä¶ + N n + x + y {\text{pk}_{{N}}^{{l}}}={p}_{{l}}+{N}_{{n}}+{N}_{{{n}+{x}}}+\ldots+{N}_{{{n}+{x}+{y}}} pk N l ‚Äã = p l ‚Äã + N n ‚Äã + N n + x ‚Äã + ‚Ä¶ + N n + x + y ‚Äã as long as p l = m {p}_{{l}}={m} p l ‚Äã = m , N n + x = 2 8 ‚àí 1 {N}_{{{n}+{x}}}={2}^{{8}}-{1} N n + x ‚Äã = 2 8 ‚àí 1 and N n + x + y < 2 8 ‚àí 1 {N}_{{{n}+{x}+{y}}}<{2}^{{8}}-{1} N n + x + y ‚Äã < 2 8 ‚àí 1 , where m {m} m is the maximum possible value that p l {p}_{{l}} p l ‚Äã can hold. 2.4.4. Merkle Proof ‚Äã To prove the consistency of the state storage across the network and its modifications both efficiently and effectively, the trie implements a Merkle tree structure. The hash value corresponding to each node needs to be computed rigorously to make the inter-implementation data integrity possible. The Merkle value of each node should depend on the Merkle value of all its children as well as on its corresponding data in the state storage. This recursive dependency is encompassed into the subvalue part of the node value, which recursively depends on the Merkle value of its children. Additionally, as Section 2.5.1. clarifies, the Merkle proof of each child trie must be updated first before the final Polkadot state root can be calculated. We use the auxiliary function introduced in Definition 26 to encode and decode the information stored in a branch node. Definition 26. Children Bitmap ‚Äã Suppose N b , N c ‚àà N {N}_{{b}},{N}_{{c}}\in{\mathcal{{N}}} N b ‚Äã , N c ‚Äã ‚àà N and N c {N}_{{c}} N c ‚Äã is a child of N b {N}_{{b}} N b ‚Äã . We define bit b i : = 1 {b}_{{i}}:={1} b i ‚Äã := 1 if and only if N b {N}_{{b}} N b ‚Äã has a child with index i {i} i , therefore we define ChildrenBitmap functions as follows: ChildrenBitmap: \text{ChildrenBitmap:} ChildrenBitmap: N b ‚Üí B 2 {\mathcal{{N}}}_{{b}}\rightarrow{\mathbb{{B}}}_{{2}} N b ‚Äã ‚Üí B 2 ‚Äã N b ‚Üí ( b 15 , ‚Ä¶ , b 8 , b 7 , ‚Ä¶ , b 0 ) 2 {N}_{{b}}\rightarrow{\left({b}_{{{15}}},\ldots,{b}_{{8}},{b}_{{7}},\ldots,{b}_{{0}}\right)}_{{2}} N b ‚Äã ‚Üí ( b 15 ‚Äã , ‚Ä¶ , b 8 ‚Äã , b 7 ‚Äã , ‚Ä¶ , b 0 ‚Äã ) 2 ‚Äã where b i = { 1 ‚àÉ N c ‚àà N : k N c = k N b ‚à£ ‚à£ i ‚à£ ‚à£ p k N c 0 otherwise {b}_{{i}}\:={\left\lbrace\begin{matrix}{1}&\exists{N}_{{c}}\in{\mathcal{{N}}}:{k}_{{{N}_{{c}}}}={k}_{{{N}_{{b}}}}{\left|{\left|{i}\right|}\right|}{p}{k}_{{{N}_{{c}}}}\\{0}&\text{otherwise}\end{matrix}\right.} b i ‚Äã = { 1 0 ‚Äã ‚àÉ N c ‚Äã ‚àà N : k N c ‚Äã ‚Äã = k N b ‚Äã ‚Äã ‚à£ ‚à£ i ‚à£ ‚à£ p k N c ‚Äã ‚Äã otherwise ‚Äã Definition 27. Subvalue ‚Äã For a given node N {N} N , the subvalue of N {N} N , formally referred to as s v N {s}{v}_{{N}} s v N ‚Äã , is determined as follows: s v N = { StoredValue SC Enc SC ( ChildrenBitmap ( N ) ‚à£ ‚à£ StoredValue SC ‚à£ ‚à£ Enc SC ( H ( N C 1 ) ) , ‚Ä¶ , Enc SC ( H ( N C n ) ) ) {s}{v}_{{N}}\:={\left\lbrace\begin{matrix}\text{StoredValue}_{{\text{SC}}}\\\text{Enc}_{{\text{SC}}}{\left(\text{ChildrenBitmap}{\left({N}\right)}{\left|{\left|\text{StoredValue}_{{\text{SC}}}\right|}\right|}\text{Enc}_{{\text{SC}}}{\left({H}{\left({N}_{{{C}_{{1}}}}\right)}\right)},\ldots,\text{Enc}_{{\text{SC}}}{\left({H}{\left({N}_{{{C}_{{n}}}}\right)}\right)}\right)}\end{matrix}\right.} s v N ‚Äã = { StoredValue SC ‚Äã Enc SC ‚Äã ( ChildrenBitmap ( N ) ‚à£ ‚à£ StoredValue SC ‚Äã ‚à£ ‚à£ Enc SC ‚Äã ( H ( N C 1 ‚Äã ‚Äã ) ) , ‚Ä¶ , Enc SC ‚Äã ( H ( N C n ‚Äã ‚Äã ) ) ) ‚Äã where the first variant is a leaf node and the second variant is a branch node. StoredValue SC = { Enc SC ( StoredValue ( k N ) ) if StoredValue ( k N ) = v œï if StoredValue ( k N ) = œï \text{StoredValue}_{{\text{SC}}}\:={\left\lbrace\begin{matrix}\text{Enc}_{{\text{SC}}}{\left(\text{StoredValue}{\left({k}_{{N}}\right)}\right)}&\text{if StoredValue}{\left({k}_{{N}}\right)}={v}\\\phi&\text{if StoredValue}{\left({k}_{{N}}\right)}=\phi\end{matrix}\right.} StoredValue SC ‚Äã = { Enc SC ‚Äã ( StoredValue ( k N ‚Äã ) ) œï ‚Äã if StoredValue ( k N ‚Äã ) = v if StoredValue ( k N ‚Äã ) = œï ‚Äã N C 1 ‚Ä¶ N C n {N}_{{{C}_{{1}}}}\ldots{N}_{{{C}_{{n}}}} N C 1 ‚Äã ‚Äã ‚Ä¶ N C n ‚Äã ‚Äã with n ‚â§ 16 {n}\le{16} n ‚â§ 16 are the children nodes of the branch node N {N} N . Enc SC \text{Enc}_{{\text{SC}}} Enc SC ‚Äã is defined in Section A.2.2. . StoredValue \text{StoredValue} StoredValue , where v {v} v can be empty, is defined in Definition 16 . H {H} H is defined in Definition 29 . ChildrenBitmap ( N ) \text{ChildrenBitmap}{\left({N}\right)} ChildrenBitmap ( N ) is defined in Definition 26 . The trie deviates from a traditional Merkle tree in that the node value ( Definition 24 ), v N {v}_{{N}} v N ‚Äã , is presented instead of its hash if it occupies less space than its hash. Definition 28. Hashed Subvalue ‚Äã To increase performance, a Merkle proof can be generated by inserting the hash of a value into the trie rather than the value itself (which can be quite large). If Merkle proof computation with node hashing is explicitly executed via the Host API ( Section B.2.8.2. ), then any value larger than 32 bytes is hashed, resulting in that hash being used as the subvalue ( Definition 27 ) under the corresponding key. The node header must specify the variant 001 {001} 001 and 0001 {0001} 0001 respectively for leaves containing a hash as their subvalue and for branches containing a hash as their subvalue ( Definition 25 ). Definition 29. Merkle Value ‚Äã For a given node N {N} N , the Merkle value of N {N} N , denoted by H ( N ) {H}{\left({N}\right)} H ( N ) is defined as follows: H : B ‚Üí U i ‚Üí 0 32 B 32 {H}:{\mathbb{{B}}}\rightarrow{{U}_{{{i}\rightarrow{0}}}^{{{32}}}}{\mathbb{{B}}}_{{32}} H : B ‚Üí U i ‚Üí 0 32 ‚Äã B 32 ‚Äã H ( N ) : { v N ‚à£ ‚à£ v N ‚à£ ‚à£ < 32 and N ‚â† R Blake2b ( v n ) ‚à£ ‚à£ v N ‚à£ ‚à£ ‚â• 32 or N = R {H}{\left({N}\right)}:{\left\lbrace\begin{matrix}{v}_{{N}}&{\left|{\left|{v}_{{N}}\right|}\right|}<{32}\ \text{ and }\ {N}\ne{R}\\\text{Blake2b}{\left({v}_{{n}}\right)}&{\left|{\left|{v}_{{N}}\right|}\right|}\ge{32}\ \text{ or }\ {N}={R}\end{matrix}\right.} H ( N ) : { v N ‚Äã Blake2b ( v n ‚Äã ) ‚Äã ‚à£ ‚à£ v N ‚Äã ‚à£ ‚à£ < 32 and N ÓÄ† = R ‚à£ ‚à£ v N ‚Äã ‚à£ ‚à£ ‚â• 32 or N = R ‚Äã Where v N {v}_{{N}} v N ‚Äã is the node value of N {N} N ( Definition 24 ) and R {R} R is the root of the trie. The Merkle hash of the trie is defined to be H ( R ) {H}{\left({R}\right)} H ( R ) . 2.4.5. Managing Multiple Variants of State ‚Äã Unless a node is committed to only updating its state according to the finalized block ( Definition 85 ), it is inevitable for the node to store multiple variants of the state (one for each block). This is, for example, necessary for nodes participating in the block production and finalization. While the state trie structure ( Section 2.4.3. ) facilitates and optimizes storing and switching between multiple variants of the state storage, the Polkadot Host does not specify how a node is required to accomplish this task. Instead, the Polkadot Host is required to implement Set-State-At \text{Set-State-At} Set-State-At ( Definition 30 ): Definition 30. Set State At Block ‚Äã The function: Set-State-At ( B ) \text{Set-State-At}{\left({B}\right)} Set-State-At ( B ) in which B {B} B is a block in the block tree ( Definition 4 ), sets the content of state storage equal to the resulting state of executing all extrinsics contained in the branch of the block tree from genesis till block B including those recorded in Block B {B} B . For the definition of the state storage see Section 2.4. . 2.5. Child Storage ‚Äã As clarified in Section 2.4. , the Polkadot state storage implements a hash table for inserting and reading key-value entries. The child storage works the same way but is stored in a separate and isolated environment. Entries in the child storage are not directly accessible via querying the main state storage. The Polkadot Host supports as many child storages as required by Runtime and identifies each separate child storage by its unique identifying key. Child storages are usually used in situations where Runtime deals with multiple instances of a certain type of objects such as Parachains or Smart Contracts. In such cases, the execution of the Runtime entrypoint might result in generating repeated keys across multiple instances of certain objects. Even with repeated keys, all such instances of key-value pairs must be able to be stored within the Polkadot state. In these situations, the child storage can be used to provide the isolation necessary to prevent any undesired interference between the state of separated instances. The Polkadot Host makes no assumptions about how child storages are used, but provides the functionality for it via the Host API ( Section B.3. ). 2.5.1. Child Tries ‚Äã The child trie specification is the same as the one described in Section 2.4.3. . Child tries have their own isolated environment. Nonetheless, the main Polkadot state trie depends on them by storing a node ( K N , V N {K}_{{N}},{V}_{{N}} K N ‚Äã , V N ‚Äã ) which corresponds to an individual child trie. Here, K N {K}_{{N}} K N ‚Äã is the child storage key associated to the child trie, and V N {V}_{{N}} V N ‚Äã is the Merkle value of its corresponding child trie computed according to the procedure described in Section 2.4.4. . The Polkadot Host API ( Section B.3. ) allows the Runtime to provide the key K N {K}_{{N}} K N ‚Äã in order to identify the child trie, followed by a second key in order to identify the value within that child trie. Every time a child trie is modified, the Merkle proof V N {V}_{{N}} V N ‚Äã of the child trie stored in the Polkadot state must be updated first. After that, the final Merkle proof of the Polkadot state can be computed. This mechanism provides a proof of the full Polkadot state including all its child states. 2.6. Runtime Interactions ‚Äã Like any transaction-based transition system, Polkadot‚Äôs state is changed by executing an ordered set of instructions. These instructions are known as extrinsics . In Polkadot, the execution logic of the state transition function is encapsulated in a Runtime ( Definition 1 ). For easy upgradability, this Runtime is presented as a Wasm blob. Nonetheless, the Polkadot Host needs to be in constant interaction with the Runtime ( Section 2.6.1. ). In Section 2.3. , we specify the procedure of the process where the extrinsics are submitted, pre-processed, and validated by Runtime and queued to be applied to the current state. To make state replication feasible, Polkadot journals and batches a series of its extrinsics together into a structure known as a block , before propagating them to other nodes, similar to most other prominent distributed ledger systems. The specification of the Polkadot block as well as the process of verifying its validity, are both explained in Section 2.2. . 2.6.1. Interacting with the Runtime ‚Äã The Runtime ( Definition 1 ) is the code implementing the logic of the chain. This code is decoupled from the Polkadot Host to make the logic of the chain easily upgradable without the need to upgrade the Polkadot Host itself. The general procedure to interact with the Runtime is described by Interact-With-Runtime . Algorithm 4. Interact With Runtime ‚Äã \begin{algorithm} \caption{Interact-With-Runtime} \begin{algorithmic} \require $F, H_b(B),(A_1,\ldots,A_n)$ \state $\mathcal{S}_B \leftarrow$ \call{Set-State-At}{$H_b(B)$} \state $A \leftarrow Enc_{SC}((A_1, \ldots, A_n))$ \state \call{Call-Runtime-Entrypoint}{$R_B, \mathcal{RE}_B, F, A, A_{len}$} \end{algorithmic} \end{algorithm} where F {F} F is the runtime entry point call. H b ( B ) {H}_{{b}}{\left({B}\right)} H b ‚Äã ( B ) is the block hash indicating the state at the end of B {B} B . A 1 , ‚Ä¶ , A n {A}_{{1}},\ldots,{A}_{{n}} A 1 ‚Äã , ‚Ä¶ , A n ‚Äã are arguments to be passed to the runtime entrypoint. In this section, we describe the details upon which the Polkadot Host is interacting with the Runtime. In particular, Set-State-At \text{Set-State-At} Set-State-At and Call-Runtime-Entrypoint \text{Call-Runtime-Entrypoint} Call-Runtime-Entrypoint procedures called by Interact-With-Runtime are explained in Definition 32 and Definition 30 respectively. R B {R}_{{B}} R B ‚Äã is the Runtime code loaded from S B {S}_{{B}} S B ‚Äã , as described in Definition 31 , and R E B {R}{E}_{{B}} R E B ‚Äã is the Polkadot Host API, as described in Definition 194 . 2.6.2. Loading the Runtime Code ‚Äã The Polkadot Host expects to receive the code for the Runtime of the chain as a compiled WebAssembly (Wasm) Blob. The current runtime is stored in the state database under the key represented as a byte array: b = 3A,63,6F,64,65 {b}\:=\text{3A,63,6F,64,65} b = 3A,63,6F,64,65 which is the ASCII byte representation of the string :code ( Section A.3.3. ). As a result of storing the Runtime as part of the state, the Runtime code itself becomes state sensitive and calls to Runtime can change the Runtime code itself. Therefore the Polkadot Host needs to always make sure to provide the Runtime corresponding to the state in which the entry point has been called. Accordingly, we define R B {R}_{{B}} R B ‚Äã ( Definition 31 ). The initial Runtime code of the chain is provided as part of the genesis state ( Section A.3.3. ) and subsequent calls to the Runtime have the ability to, in turn, upgrade the Runtime by replacing this Wasm blob with the help of the storage API ( Section B.2. ). Therefore, the executor must always load the latest Runtime from storage - or preferably detect Runtime upgrades ( Definition 11 ) - either based on the parent block when importing blocks or the best/highest block when creating new blocks. Definition 31. Runtime Code at State ‚Äã By R B {R}_{{B}} R B ‚Äã , we refer to the Runtime code stored in the state storage at the end of the execution of block B {B} B . The WASM blobs may be compressed using zstd . In such cases, there is an 8-byte magic identifier at the head of the blob, indicating that it should be decompressed with zstd compression. The magic identifier prefix ZSTD_PREFIX = [82, 188, 83, 118, 70, 219, 142, 5] is different from the WASM magic bytes . The decompression has to be applied on the blob excluding the ZSTD-PREFIX and has a Bomb Limit of CODE_BLOB_BOMB_LIMIT = 50 * 1024 * 1024 to mitigate compression bomb attacks. 2.6.3. Code Executor ‚Äã The Polkadot Host executes the calls of Runtime entrypoints inside a Wasm Virtual Machine (VM), which in turn provides the Runtime with access to the Polkadot Host API. This part of the Polkadot Host is referred to as the Executor . Definition 32 introduces the notation for calling the runtime entrypoint which is used whenever an algorithm of the Polkadot Host needs to access the runtime. It is acceptable behavior that the Runtime panics during execution of a function in order to indicate an error. The Polkadot Host must be able to catch that panic and recover from it. In this section, we specify the general setup for an Executor that calls into the Runtime. In Appendix C we specify the parameters and return values for each Runtime entrypoint separately. Definition 32. Call Runtime Entrypoint ‚Äã By Call-Runtime-Entrypoint ( R , R E , Runtime-Entrypoint , A , A ‚â§ n ) \text{Call-Runtime-Entrypoint}{\left({R},{R}{E},\text{Runtime-Entrypoint},{A},{A}_{\le}{n}\right)} Call-Runtime-Entrypoint ( R , R E , Runtime-Entrypoint , A , A ‚â§ ‚Äã n ) we refer to the task using the executor to invoke the while passing an A 1 , ‚Ä¶ , A n {A}_{{1}},\ldots,{A}_{{n}} A 1 ‚Äã , ‚Ä¶ , A n ‚Äã argument to it and using the encoding described in Section 2.6.3.2. . 2.6.3.1. Memory Management ‚Äã The Polkadot Host is responsible for managing the WASM heap memory starting at the exported symbol as a part of implementing the allocator Host API ( Section B.10. ) and the same allocator should be used for any other heap allocation to be used by the Polkadot Runtime. The size of the provided WASM memory should be based on the value of the storage key (an unsigned 64-bit integer), where each page has a size of 64KB. This memory should be made available to the Polkadot Runtime for import under the symbol name memory . 2.6.3.2. Sending Data to a Runtime Entrypoint ‚Äã In general, all data exchanged between the Polkadot Host and the Runtime is encoded using the SCALE codec described in Section A.2.2. . Therefore all runtime entrypoints have the following identical Wasm function signatures: (func $runtime_entrypoint (param $data i32) (param $len i32) (result i64)) In each invocation of a Runtime entrypoints, the argument(s) which are supposed to be sent to the entrypoint, need to be SCALE encoded into a byte array B {B} B ( Section A.2.2. ) and copied into a section of Wasm shared memory managed by the shared allocator described in Section 2.6.3.1. . When the Wasm method, corresponding to the entrypoint, is invoked, two integers are passed as arguments. The first argument is set to the memory address of the byte array B {B} B in Wasm memory. The second argument sets the length of the encoded data stored in B {B} B . 2.6.3.3. Receiving Data from a Runtime Entrypoint ‚Äã The value which is returned from the invocation is an integer, representing two consecutive integers in which the least significant one indicates the pointer to the offset of the result returned by the entrypoint encoded in SCALE codec in the memory buffer. The most significant one provides the size of the blob. 2.6.3.4. Runtime Version Custom Section ‚Äã For newer Runtimes, the Runtime version ( Section C.4.1. ) can be read directly from the Wasm custom section with the name runtime_version. The content is a SCALE encoded structure as described in Section C.4.1. . Retrieving the Runtime version this way is preferred over calling the Core_version entrypoint since it involves significantly less overhead. Previous 1. Overview Next 3. Synchronization 2.1. Introduction 2.1.1. Block Tree 2.2. State Replication 2.2.1. Block Format 2.3. Extrinsics 2.3.1. Preliminaries 2.3.2. Transactions 2.3.3. Inherents 2.4. State Storage Trie 2.4.1. Accessing System Storage 2.4.2. General Structure 2.4.3. Trie Structure 2.4.4. Merkle Proof 2.4.5. Managing Multiple Variants of State 2.5. Child Storage 2.5.1. Child Tries 2.6. Runtime Interactions 2.6.1. Interacting with the Runtime 2.6.2. Loading the Runtime Code 2.6.3. Code Executor 
3. Synchronization | Polkadot Protocol Specification Host 3. Synchronization On this page 3. Synchronization Many applications that interact with the Polkadot network, to some extent, must be able to retrieve certain information about the network. Depending on the utility, this includes validators that interact with Polkadot‚Äôs consensus and need access to the full state, either from the past or just the most up-to-date state, or light clients that are only interested in the minimum information required in order to verify some claims about the state of the network, such as the balance of a specific account. To allow implementations to quickly retrieve the required information, different types of synchronization protocols are available, respectively Full, Fast, and Warp sync suited for different needs. The associated network messages are specified in Section 4.8. . 3.1. Warp Sync ‚Äã Warp sync ( Section 4.8.4. ) only downloads the block headers where authority set changes occurred, so-called fragments ( Definition 41 ), and by verifying the GRANDPA justifications ( Definition 45 ). This protocol allows nodes to arrive at the desired state much faster than fast sync. 3.2. Fast Sync ‚Äã Fast sync works by downloading the block header history and validating the authority set changes ( Section 3.3.1. ) in order to arrive at a specific (usually the most recent) header. After the desired header has been reached and verified, the state can be downloaded and imported ( Section 4.8.3. ). Once this process has been completed, the node can proceed with a full sync. 3.3. Full Sync ‚Äã The full sync protocol is the "default" protocol that‚Äôs suited for many types of implementations, such as archive nodes (nodes that store everything), validators that participate in Polkadots consensus and light clients that only verify claims about the state of the network. Full sync works by listening to announced blocks ( Section 4.8.1. ) and requesting the blocks from the announcing peers or just the block headers in case of light clients. The full sync protocol usually downloads the entire chain, but no such requirements must be met. If an implementation only wants the latest, finalized state, it can combine it with protocols such as fast sync ( Section 3.2. ) and/or warp sync ( Section 3.1. ) to make synchronization as fast as possible. 3.3.1. Consensus Authority Set ‚Äã Because Polkadot is a proof-of-stake protocol, each of its consensus engines has its own set of nodes represented by known public keys, which have the authority to influence the protocol in pre-defined ways explained in this Section. To verify the validity of each block, the Polkadot node must track the current list of authorities ( Definition 33 ) for that block. Definition 33. Authority List ‚Äã The authority list of block B {B} B for consensus engine C {C} C noted as Auth C ( B ) \text{Auth}_{{C}}{\left({B}\right)} Auth C ‚Äã ( B ) is an array that contains the following pair of types for each of its authorities A ‚àà Auth C ( B ) {A}\in\text{Auth}_{{C}}{\left({B}\right)} A ‚àà Auth C ‚Äã ( B ) : ( p k A , w A ) {\left({p}{k}_{{A}},{w}_{{A}}\right)} ( p k A ‚Äã , w A ‚Äã ) p k A {p}{k}_{{A}} p k A ‚Äã is the session public key ( Definition 170 ) of authority A {A} A . And w A {w}_{{A}} w A ‚Äã is an unsigned 64-bit integer indicating the authority weight. The value of Auth C ( B ) \text{Auth}_{{C}}{\left({B}\right)} Auth C ‚Äã ( B ) is part of the Polkadot state. The value for Auth C ( B 0 ) \text{Auth}_{{C}}{\left({B}_{{0}}\right)} Auth C ‚Äã ( B 0 ‚Äã ) is set in the genesis state ( Section A.3.3. ) and can be retrieved using a runtime entrypoint corresponding to consensus engine C {C} C . The authorities and their corresponding weights can be retrieved from the Runtime ( Section C.10.1. ). info In Polkadot, the authorities are unweighted, i.e., the weights for all authorities are set to 1. The proportionality in terms of stakes is managed by the NPOS (Nominated Proof-of-Stake) algorithm in Polkadot. Once validators are elected for an era using the NPOS algorithm, they are considered equal in the BABE and GRANDPA consensus algorithms. 3.3.2. Runtime-to-Consensus Engine Message ‚Äã The authority list ( Definition 33 ) is part of the Polkadot state, and the Runtime has the authority to update this list in the course of any state transitions. The Runtime informs the corresponding consensus engine about the changes in the authority set by adding the appropriate consensus message in the form of a digest item ( Definition 11 ) to the block header of block B {B} B which caused the transition in the authority set. The Polkadot Host must inspect the digest header of each block and delegate consensus messages to their consensus engines. The BABE and GRANDPA consensus engine must react based on the type of consensus messages it receives. The active GRANDPA authorities can only vote for blocks that occurred after the finalized block in which they were selected. Any votes for blocks before they came into effect would get rejected. 3.4. Importing and Validating Block ‚Äã Block validation is the process by which a node asserts that a block is fit to be added to the blockchain. This means that the block is consistent with the current state of the system and transitions to a new valid state. New blocks can be received by the Polkadot Host via other peers ( Section 4.8.2. ) or from the Host‚Äôs own consensus engine ( Chapter 5 ). Both the Runtime and the Polkadot Host then need to work together to assure block validity. A block is deemed valid if the block author had authorship rights for the slot in which the block was produced as well as if the transactions in the block constitute a valid transition of states. The former criterion is validated by the Polkadot Host according to the block production consensus protocol. The latter can be verified by the Polkadot Host invoking entry into the Runtime as ( Section C.4.2. ) as a part of the validation process. Any state changes created by this function on successful execution are persisted. The Polkadot Host implements Import-and-Validate-Block to assure the validity of the block. Algorithm 5. Import-and-Validate-Block ‚Äã \begin{algorithm} \caption{Import-and-Validate-Block} \begin{algorithmic} \require $B, \text{Just}(B)$ \state \call{Set-Storage-State-At}{$P(B)$} \if{$\text{Just}(B) \neq \emptyset$} \state \call{Verify-Block-Justification}{$B, \text{Just}(B)$} \if{$B~\textbf{is}~\text{Finalized}~\textbf{and}~P(B)~\textbf{is not}~\text{Finalized}$} \state \call{Mark-as-Final}{$P(B)$} \endif \endif \if{$H_p(B) \notin PBT$} \return \endif \state \call{Verify-Authorship-Right}{$\text{Head}(B)$} \state $B \leftarrow$ \call{Remove-Seal}{$B$} \state $R \leftarrow$ \call{Call-Runtime-Entry}{$\texttt{Core\_execute\_block}, B$} \state $B \leftarrow$ \call{Add-Seal}{$B$} \if{$R =$ \textsc{True}} \state \call{Persist-State}{} \endif \end{algorithmic} \end{algorithm} where Remove-Seal \text{Remove-Seal} Remove-Seal removes the Seal digest from the block ( Definition 11 ) before submitting it to the Runtime. Add-Seal \text{Add-Seal} Add-Seal adds the Seal digest back to the block ( Definition 11 ) for later propagation. Persist-State \text{Persist-State} Persist-State implies the persistence of any state changes created by Core_execute_block {\mathtt{\text{Core\_execute\_block}}} Core_execute_block ( Section C.4.2. ) on successful execution. PBT \text{PBT} PBT is the pruned block tree ( Definition 4 ). Verify-Authorship-Right \text{Verify-Authorship-Right} Verify-Authorship-Right is part of the block production consensus protocol and is described in Verify-Authorship-Right . Finalized block and finality are defined in Chapter 6 . Previous 2. States and Transitions Next 4. Networking 3.1. Warp Sync 3.2. Fast Sync 3.3. Full Sync 3.3.1. Consensus Authority Set 3.3.2. Runtime-to-Consensus Engine Message 3.4. Importing and Validating Block 
4. Networking | Polkadot Protocol Specification Host 4. Networking On this page 4. Networking info This chapter in its current form is incomplete and considered work in progress. Authors appreciate receiving request for clarification or any reports regarding deviation from the current Polkadot network protocol. This can be done through filing an issue in Polkadot Specification repository . 4.1. Introduction ‚Äã The Polkadot network is decentralized and does not rely on any central authority or entity for achieving its fullest potential of provided functionality. The networking protocol is based on a family of open protocols, including protocol implemented libp2p e.g. the distributed Kademlia hash table which is used for peer discovery. This chapter walks through the behavior of the networking implementation of the Polkadot Host and defines the network messages. The implementation details of the libp2p protocols used are specified in external sources as described in Section 4.2. 4.2. External Documentation ‚Äã Complete specification of the Polkadot networking protocol relies on the following external protocols: libp2p - libp2p is a modular peer-to-peer networking stack composed of many modules and different parts. includes the multiplexing protocols and . libp2p addressing - The Polkadot Host uses the libp2p addressing system to identify and connect to peers. Kademlia - Kademlia is a distributed hash table for decentralized peer-to-peer networks. The Polkadot Host uses Kademlia for peer discovery. Noise - The Noise protocol is a framework for building cryptographic protocols. The Polkadot Host uses Noise to establish the encryption layer to remote peers. yamux - yamux is a multiplexing protocol developed by HashiCorp. It is the de-facto standard for the Polkadot Host. Section 4.7. describes the subprotocol in more detail. Protocol Buffers - Protocol Buffers is a language-neutral, platform-neutral mechanism for serializing structured data and is developed by Google. The Polkadot Host uses Protocol Buffers to serialize specific messages, as clarified in Section 4.8. . 4.3. Node Identities ‚Äã Each Polkadot Host node maintains an ED25519 key pair which is used to identify the node. The public key is shared with the rest of the network allowing the nodes to establish secure communication channels. Each node must have its own unique ED25519 key pair. If two or more nodes use the same key, the network will interpret those nodes as a single node, which will result in unspecified behavior. Furthermore, the node‚Äôs PeerId as defined in Definition 34 is derived from its public key. PeerId is used to identify each node when they are discovered in the course of the discovery mechanism described in Section 4.4. . Definition 34. PeerId ‚Äã The Polkadot node‚Äôs PeerId, formally referred to as P i d {P}_{{{i}{d}}} P i d ‚Äã , is derived from the ED25519 public key and is structured based on the libp2p specification , but does not fully conform to the specification. Specifically, it does not support CID and the only supported key type is ED25519. The byte representation of the PeerId is always of the following bytes in this exact order: b 0 = 0 {b}_{{0}}={0} b 0 ‚Äã = 0 b 1 = 36 {b}_{{1}}={36} b 1 ‚Äã = 36 b 2 = 8 {b}_{{2}}={8} b 2 ‚Äã = 8 b 3 = 1 {b}_{{3}}={1} b 3 ‚Äã = 1 b 4 = 18 {b}_{{4}}={18} b 4 ‚Äã = 18 b 5 = 32 {b}_{{5}}={32} b 5 ‚Äã = 32 b 6. . 37 = ‚Ä¶ {b}_{{{6}.{.37}}}=\ldots b 6 . .37 ‚Äã = ‚Ä¶ where b 0 {b}_{{0}} b 0 ‚Äã is the multihash prefix of value 0 {0} 0 (implying no hashing is used). b 1 {b}_{{1}} b 1 ‚Äã the length of the PeerId (remaining bytes). b 2 {b}_{{2}} b 2 ‚Äã and b 3 {b}_{{3}} b 3 ‚Äã are a protobuf encoded field-value pair indicating the used key type (field 1 {1} 1 of value 1 {1} 1 implies ED25519 ). b 4 {b}_{{4}} b 4 ‚Äã , b 5 {b}_{{5}} b 5 ‚Äã and b 6. . 37 {b}_{{{6}.{.37}}} b 6 . .37 ‚Äã are a protobuf encoded field-value pair where b 5 {b}_{{5}} b 5 ‚Äã indicates the length of the public key followed by the the raw ED25519 public key itself, which varies for each Polkadot Host and is always 32 bytes (field 2 {2} 2 contains the public key, which has a field value length prefix). 4.4. Discovery mechanism ‚Äã The Polkadot Host uses various mechanisms to find peers within the network, to establish and maintain a list of peers and to share that list with other peers from the network as follows: Bootstrap nodes are hard-coded node identities and addresses provided by the genesis state ( Section A.3.3. ). mDNS is a protocol that performs a broadcast to the local network. Nodes that might be listening can respond to the broadcast. The libp2p mDNS specification defines this process in more detail. This protocol is an optional implementation detail for Polkadot Host implementers and is not required to participate in the Polkadot network. Kademlia requests invoking Kademlia requests, where nodes respond with their list of available peers. Kademlia requests are performed on a specific substream as described in Section 4.7. . 4.5. Connection establishment ‚Äã Polkadot nodes connect to peers by establishing a TCP connection. Once established, the node initiates a handshake with the remote peers on the encryption layer. An additional layer on top of the encryption layer, known as the multiplexing layer, allows a connection to be split into substreams, as described by the yamux specification , either by the local or remote node. The Polkadot node supports two types of substream protocols. Section 4.7. describes the usage of each type in more detail: Request-Response substreams : After the protocol is negotiated by the multiplexing layer, the initiator sends a single message containing a request. The responder then sends a response, after which the substream is then immediately closed. The requests and responses are prefixed with their LEB128 encoded length. Notification substreams . After the protocol is negotiated, the initiator sends a single handshake message. The responder can then either accept the substream by sending its own handshake or reject it by closing the substream. After the substream has been accepted, the initiator can send an unbound number of individual messages. The responder keeps its sending side of the substream open, despite not sending anything anymore, and can later close it in order to signal to the initiator that it no longer wishes to communicate. Handshakes and messages are prefixed with their LEB128 encoded lengths. A handshake can be empty, in which case the length prefix would be 0 . Connections are established by using the following protocols: /noise - a protocol that is announced when a connection to a peer is established. /multistream/1.0.0 - a protocol that is announced when negotiating an encryption protocol or a substream. /yamux/1.0.0 - a protocol used during yamux negotiation. See Section 4.7. for more information. The Polkadot Host can establish a connection with any peer of which it knows the address. The Polkadot Host supports multiple networking protocols: TCP/IP with addresses in the form of /ip4/1.2.3.4/tcp/30333 to establish a TCP connection and negotiate encryption and a multiplexing layer. WebSocket with addresses in the form of /ip4/1.2.3.4/tcp/30333/ws to establish a TCP connection and negotiate the WebSocket protocol within the connection. Additionally, encryption and multiplexing layer is negotiated within the WebSocket connection. DNS addresses in form of /dns/example.com/tcp/30333 and /dns/example.com/tcp/30333/ws . The addressing system is described in the libp2p addressing specification. After a base-layer protocol is established, the Polkadot Host will apply the Noise protocol to establish the encryption layer as described in Section 4.6. . 4.6. Encryption Layer ‚Äã Polkadot protocol uses the libp2p Noise framework to build an encryption protocol. The Noise protocol is a framework for building encryption protocols. libp2p utilizes that protocol for establishing encrypted communication channels. Refer to the libp2p Secure Channel Handshake specification for a detailed description. Polkadot nodes use the XX handshake pattern to establish a connection between peers. The three following steps are required to complete the handshake process: The initiator generates a keypair and sends the public key to the responder. The Noise specification and the libp2p PeerId specification describe keypairs in more detail. The responder generates its own key pair and sends its public key back to the initiator. After that, the responder derives a shared secret and uses it to encrypt all further communication. The responder now sends its static Noise public key (which may change anytime and does not need to be persisted on disk), its libp2p public key and a signature of the static Noise public key signed with the libp2p public key. The initiator derives a shared secret and uses it to encrypt all further communication. It also sends its static Noise public key, libp2p public key and signature to the responder. After these three steps, both the initiator and responder derive a new shared secret using the static and session-defined Noise keys, which are used to encrypt all further communication. 4.7. Protocols and Substreams ‚Äã After the node establishes a connection with a peer, the use of multiplexing allows the Polkadot Host to open substreams. libp2p uses the yamux protocol to manage substreams and to allow the negotiation of application-specific protocols , where each protocol serves a specific utility. The Polkadot Host uses multiple substreams whose usage depends on a specific purpose. Each substream is either a Request-Response substream or a Notification substream , as described in Section 4.5. . info The prefixes on those substreams are known as protocol identifiers and are used to segregate communications to specific networks. This prevents any interference with other networks. dot is used exclusively for Polkadot. Kusama, for example, uses the protocol identifier ksmcc3 . /ipfs/ping/1.0.0 - Open a standardized substream libp2p to a peer and initialize a ping to verify if a connection is still alive. If the peer does not respond, the connection is dropped. This is a Request-Response substream . Further specification and reference implementation are available in the libp2p documentation . /ipfs/id/1.0.0 - Open a standardized libp2p substream to a peer to ask for information about that peer. This is a Request-Response substream , but the initiator does not send any message to the responder and only waits for the response. Further specification and reference implementation are available in the libp2p documentation . /dot/kad - Open a standardized substream for Kademlia FIND_NODE requests. This is a Request-Response substream , as defined by the libp2p standard. Further specification and reference implementation are available on Wikipedia respectively the golang Github repository . /91b171bb158e2d3848fa23a9f1c25182fb8e20313b2c1eb49219da7a70ce90c3/light/2 - a request and response protocol that allows a light client to request information about the state. This is a Request-Response substream . The messages are specified in Section 7.4. . info For backwards compatibility reasons, /dot/light/2 is also a valid substream for those messages. /91b171bb158e2d3848fa23a9f1c25182fb8e20313b2c1eb49219da7a70ce90c3/block-announces/1 - a substream/notification protocol which sends blocks to connected peers. This is a Notification substream . The messages are specified in Section 4.8.1. . info For backwards compatibility reasons, /dot/block-announces/1 is also a valid substream for those messages. /91b171bb158e2d3848fa23a9f1c25182fb8e20313b2c1eb49219da7a70ce90c3/sync/2 - a request and response protocol that allows the Polkadot Host to request information about blocks. This is a Request-Response substream . The messages are specified in Section 4.8.2. . info For backwards compatibility reasons, /dot/sync/2 is also a valid substream for those messages. /91b171bb158e2d3848fa23a9f1c25182fb8e20313b2c1eb49219da7a70ce90c3/sync/warp - a request and response protocol that allows the Polkadot Host to perform a warp sync request. This is a Request-Response substream . The messages are specified in Section 4.8.4. . info For backwards compatibility reasons, /dot/sync/warp is also a valid substream for those messages. /91b171bb158e2d3848fa23a9f1c25182fb8e20313b2c1eb49219da7a70ce90c3/transactions/1 - a substream/notification protocol which sends transactions to connected peers. This is a Notification substream . The messages are specified in Section 4.8.5. . info For backwards compatibility reasons, /dot/transactions/1 is also a valid substream for those messages. /91b171bb158e2d3848fa23a9f1c25182fb8e20313b2c1eb49219da7a70ce90c3/grandpa/1 - a substream/notification protocol that sends GRANDPA votes to connected peers. This is a Notification substream . The messages are specified in Section 4.8.6. . info For backwards compatibility reasons, /paritytech/grandpa/1 is also a valid substream for those messages. 4.8. Network Messages ‚Äã The Polkadot Host must actively communicate with the network in order to participate in the validation process or act as a full node. info The Polkadot network originally only used SCALE encoding for all message formats. Meanwhile, Protobuf has been adopted for certain messages. The encoding of each listed message is always SCALE encoded unless Protobuf is explicitly mentioned. Encoding and message formats are subject to change. 4.8.1. Announcing blocks ‚Äã When the node creates or receives a new block, it must be announced to the network. Other nodes within the network will track this announcement and can request information about this block. The mechanism for tracking announcements and requesting the required data is implementation-specific. Block announcements, requests and responses are sent over the substream as described in Definition 35 . Definition 35. Block Announce Handshake ‚Äã The BlockAnnounceHandshake initializes a substream to a remote peer. Once established, all BlockAnounce messages ( Definition 36 ) created by the node are sent to the /dot/block-announces/1 substream. The BlockAnnounceHandshake is a structure of the following format: B A h = Enc SC ( R , N B , h B , h G ) {B}{A}_{{h}}=\text{Enc}_{{\text{SC}}}{\left({R},{N}_{{B}},{h}_{{B}},{h}_{{G}}\right)} B A h ‚Äã = Enc SC ‚Äã ( R , N B ‚Äã , h B ‚Äã , h G ‚Äã ) where R = { 1 The node is a full node 2 The node is a light client 4 The node is a validator {R}={\left\lbrace\begin{matrix}{1}&\text{The node is a full node}\\{2}&\text{The node is a light client}\\{4}&\text{The node is a validator}\end{matrix}\right.} R = ‚é© ‚é® ‚éß ‚Äã 1 2 4 ‚Äã The node is a full node The node is a light client The node is a validator ‚Äã N B = Best block number according to the node {N}_{{B}}=\text{Best block number according to the node} N B ‚Äã = Best block number according to the node h B = Best block hash according to the node {h}_{{B}}=\text{Best block hash according to the node} h B ‚Äã = Best block hash according to the node h G = Genesis block hash according to the node {h}_{{G}}=\text{Genesis block hash according to the node} h G ‚Äã = Genesis block hash according to the node Definition 36. Block Announce ‚Äã The BlockAnnounce message is sent to the specified substream and indicates to remote peers that the node has either created or received a new block. The message is a structure of the following format: B A = Enc SC ( Head ( B ) , b ) {B}{A}=\text{Enc}_{{\text{SC}}}{\left(\text{Head}{\left({B}\right)},{b}\right)} B A = Enc SC ‚Äã ( Head ( B ) , b ) where Head ( B ) = Header of the announced block \text{Head}{\left({B}\right)}=\text{Header of the announced block} Head ( B ) = Header of the announced block b = { 0 Is not part of the best chain 1 Is the best block according to the node {b}={\left\lbrace\begin{matrix}{0}&\text{Is not part of the best chain}\\{1}&\text{Is the best block according to the node}\end{matrix}\right.} b = { 0 1 ‚Äã Is not part of the best chain Is the best block according to the node ‚Äã 4.8.2. Requesting Blocks ‚Äã Block requests can be used to retrieve a range of blocks from peers. Those messages are sent over the /dot/sync/2 substream. Definition 37. Block Request ‚Äã The BlockRequest message is a Protobuf serialized structure of the following format: Type Id Description Value uint32 1 Bits of block data to request B f {B}_{{f}} B f ‚Äã oneof Start from this block B s {B}_{{s}} B s ‚Äã Direction 5 Sequence direction, interpreted as Id 0 (ascending) if missing. uint32 6 Maximum amount ( optional ) B m {B}_{{m}} B m ‚Äã where B f {B}_{{f}} B f ‚Äã indicates all the fields that should be included in the request. its big-endian encoded bitmask that applies to all desired fields with bitwise OR operations. For example, the B f {B}_{{f}} B f ‚Äã value to request Header and Justification is 0001 0001 (17). Field Value Header 0000 0001 Body 0000 0010 Justification 0001 0000 B s {B}_{{s}} B s ‚Äã is a Protobuf structure indicating a varying data type (enum) of the following values: Type Id Description bytes 2 The block hash bytes 3 The block number Direction is a Protobuf structure indicating the sequence direction of the requested blocks. The structure is a varying data type (enum) of the following format: Id Description 0 Enumerate in ascending order (from child to parent) 1 Enumerate in descending order (from parent to canonical child) B m {B}_{{m}} B m ‚Äã is the number of blocks to be returned. An implementation defined maximum is used when unspecified. Definition 38. Block Response ‚Äã The BlockResponse message is received after sending a BlockRequest message to a peer. The message is a Protobuf serialized structure of the following format: Type Id Description Repeated BlockData 1 Block data for the requested sequence where BlockData is a Protobuf structure containing the requested blocks. Do note that the optional values are either present or absent depending on the requested fields (bitmask value). The structure has the following format: Type Id Description Value bytes 1 Block header hash Definition 12 bytes 2 Block header (optional) Definition 10 repeated bytes 3 Block body (optional) Definition 13 bytes 4 Block receipt (optional) bytes 5 Block message queue (optional) bytes 6 Justification (optional) Definition 74 bool 7 Indicates whether the justification is empty (i.e. should be ignored) 4.8.3. Requesting States ‚Äã The Polkadot Host can request the state in form of a key/value list at a specified block. When receiving state entries from the state response messages ( Definition 40 ), the node can verify the entries with the entry proof (id 1 in KeyValueStorage ) against the merkle root in the block header (of the block specified in Definition 39 ). Once the state response message claims that all entries have been sent (id 3 in KeyValueStorage ), the node can use all collected entry proofs and validate it against the merkle root to confirm that claim. See the the synchronization chapter for more information ( Chapter 3 ). Definition 39. State Request ‚Äã A state request is sent to a peer to request the state at a specified block. The message is a single 32-byte Blake2 hash which indicates the block from which the sync should start. Depending on what substream is used, he remote peer either sends back a state response ( Definition 40 ) on the /dot/sync/2 substream or a warp sync proof ( Definition 41 ) on the /dot/sync/warp . Definition 40. State Response ‚Äã The state response is sent to the peer that initialized the state request ( Definition 39 ) and contains a list of key/value entries with an associated proof. This response is sent continuously until all key/value pairs have been submitted. Type Id Description repeated KeyValueStateEntry 1 State entries bytes 2 State proof where KeyValueStateEntry is of the following format: Type Id Description bytes 1 Root of the entry, empty if top level repeated StateEntry 2 Collection of key/values bool 3 Equal 'true' if there are no more keys to return. and StateEntry : Type Id Description bytes 1 The key of the entry bytes 2 The value of the entry 4.8.4. Warp Sync ‚Äã The warp sync protocols allows nodes to retrieve blocks from remote peers where authority set changes occurred. This can be used to speed up synchronization to the latest state. See the the synchronization chapter for more information ( Chapter 3 ). Definition 41. Warp Sync Proof ‚Äã The warp sync proof message, P {P} P , is sent to the peer that initialized the state request ( Definition 39 ) on the /dot/sync/warp substream and contains accumulated proof of multiple authority set changes ( Section 3.3.2. ). It‚Äôs a datastructure of the following format: P = ( f x ‚Ä¶ f y , c ) {P}={\left({{f}_{{x}}\ldots}{{f}_{{y}},}{c}\right)} P = ( f x ‚Äã ‚Ä¶ f y ‚Äã , c ) f x ‚Ä¶ f y {{f}_{{x}}\ldots}{{f}_{{y}}} f x ‚Äã ‚Ä¶ f y ‚Äã is an array consisting of warp sync fragments of the following format: f x = ( B h , J r , stage ( B ) ) {{f}_{{x}}=}{\left({B}_{{h}},{J}^{{{r},\text{stage}}}{\left({B}\right)}\right)} f x ‚Äã = ( B h ‚Äã , J r , stage ( B ) ) where B h {B}_{{h}} B h ‚Äã is the last block header containing a digest item ( Definition 11 ) signaling an authority set change from which the next authority set change can be fetched from. J r , stage ( B ) {J}^{{{r},\text{stage}}}{\left({B}\right)} J r , stage ( B ) is the GRANDPA justification ( Definition 74 ) and c {c} c is a boolean that indicates whether the warp sync has been completed. 4.8.5. Transactions ‚Äã Transactions ( Section 2.3. ) are sent directly to peers with which the Polkadot Host has an open transaction substream ( Definition 42 ). Polkadot Host implementers should implement a mechanism that only sends a transaction once to each peer and avoids sending duplicates. Sending duplicate transactions might result in undefined consequences such as being blocked for bad behavior by peers. The mechanism for managing transactions is further described in Section Section 2.3. . Definition 42. Transaction Message ‚Äã The transactions message is the structure of how the transactions are sent over the network. It is represented by M T {M}_{{T}} M T ‚Äã and is defined as follows: M T = Enc SC ( C 1 , ‚Ä¶ , C n ) {M}_{{T}}\:=\text{Enc}_{{\text{SC}}}{\left({C}_{{1}},\ldots,{C}_{{n}}\right)} M T ‚Äã = Enc SC ‚Äã ( C 1 ‚Äã , ‚Ä¶ , C n ‚Äã ) in which C i = Enc SC ( E i ) {C}_{{i}}\:=\text{Enc}_{{\text{SC}}}{\left({E}_{{i}}\right)} C i ‚Äã = Enc SC ‚Äã ( E i ‚Äã ) Where each E i {E}_{{i}} E i ‚Äã is a byte array and represents a separate extrinsic. The Polkadot Host is agnostic about the content of an extrinsic and treats it as a blob of data. Transactions are sent over the /dot/transactions/1 substream. 4.8.6. GRANDPA Messages ‚Äã The exchange of GRANDPA messages is conducted on the substream. The process for the creation and distributing these messages is described in Chapter 6 . The underlying messages are specified in this section. Definition 43. Grandpa Gossip Message ‚Äã A GRANDPA gossip message , M {M} M , is a varying datatype ( Definition 178 ) which identifies the message type that is cast by a voter followed by the message itself. M = { 0 Vote message V m 1 Commit message C m 2 Neighbor message N m 3 Catch-up request message R m 4 Catch-up message U m {M}={\left\lbrace\begin{matrix}{0}&\text{Vote message}&{V}_{{m}}\\{1}&\text{Commit message}&{C}_{{m}}\\{2}&\text{Neighbor message}&{N}_{{m}}\\{3}&\text{Catch-up request message}&{R}_{{m}}\\{4}&\text{Catch-up message}&{U}_{{m}}\end{matrix}\right.} M = ‚é© ‚é® ‚éß ‚Äã 0 1 2 3 4 ‚Äã Vote message Commit message Neighbor message Catch-up request message Catch-up message ‚Äã V m ‚Äã C m ‚Äã N m ‚Äã R m ‚Äã U m ‚Äã ‚Äã where V m {V}_{{m}} V m ‚Äã is defined in Definition 44 . C m {C}_{{m}} C m ‚Äã is defined in Definition 46 . N m {N}_{{m}} N m ‚Äã is defined in Definition 47 . R m {R}_{{m}} R m ‚Äã is defined in Definition 48 . U M {U}_{{M}} U M ‚Äã is defined in Definition 49 . Definition 44. GRANDPA Vote Messages ‚Äã A GRANDPA vote message by voter v {v} v , M v r , stage {{M}_{{v}}^{{{r},\text{stage}}}} M v r , stage ‚Äã , is gossip to the network by voter v {v} v with the following structure: M v r , stage ( B ) = Enc SC ( r , id V , SigMsg ) {{M}_{{v}}^{{{r},\text{stage}}}}{\left({B}\right)}\:=\text{Enc}_{{\text{SC}}}{\left({r},\text{id}_{{{\mathbb{{V}}}}},\text{SigMsg}\right)} M v r , stage ‚Äã ( B ) = Enc SC ‚Äã ( r , id V ‚Äã , SigMsg ) SigMsg = ( msg , Sig v i r , stage , v id ) \text{SigMsg}\:={\left(\text{msg},{\text{Sig}_{{{v}_{{i}}}}^{{{r},\text{stage}}}},{v}_{{\text{id}}}\right)} SigMsg = ( msg , Sig v i ‚Äã r , stage ‚Äã , v id ‚Äã ) msg = Enc SC ( stage , V v r , stage ( B ) ) \text{msg}\:=\text{Enc}_{{\text{SC}}}{\left(\text{stage},{{V}_{{v}}^{{{r},\text{stage}}}}{\left({B}\right)}\right)} msg = Enc SC ‚Äã ( stage , V v r , stage ‚Äã ( B ) ) where r {r} r is an unsigned 64-bit integer indicating the Grandpa round number ( Definition 72 ). id V \text{id}_{{{\mathbb{{V}}}}} id V ‚Äã is an unsigned 64-bit integer indicating the authority Set Id ( Definition 69 ). Sig v i r , stage {\text{Sig}_{{{v}_{{i}}}}^{{{r},\text{stage}}}} Sig v i ‚Äã r , stage ‚Äã is a 512-bit byte array containing the signature of the authority ( Definition 73 ). v i d {v}_{{{i}{d}}} v i d ‚Äã is a 256-bit byte array containing the ed25519 public key of the authority. stage \text{stage} stage is a 8-bit integer of value 0 if it‚Äôs a pre-vote sub-round, 1 if it‚Äôs a pre-commit sub-round or 2 if it‚Äôs a primary proposal message. V v r , stage ( B ) {{V}_{{v}}^{{{r},\text{stage}}}}{\left({B}\right)} V v r , stage ‚Äã ( B ) is the GRANDPA vote for block B {B} B ( Definition 72 ). This message is the sub-component of the GRANDPA gossip message ( Definition 43 ) of type Id 0. Definition 45. GRANDPA Compact Justification Format ‚Äã The GRANDPA compact justification format is an optimized data structure to store a collection of pre-commits and their signatures to be submitted as part of a commit message. Instead of storing an array of justifications, it uses the following format: J v 0 , ‚Ä¶ n r , comp = ( { V v 0 r , p c , ‚Ä¶ V v n r , p c } , { ( Sig v 0 r , p c , v id 0 ) , ‚Ä¶ ( Sig v n r , p c , v id n ) } ) {{J}_{{{v}_{{{0},\ldots{n}}}}}^{{{r},\text{comp}}}}\:={\left({\left\lbrace{{V}_{{{v}_{{0}}}}^{{{r},{p}{c}}}},\ldots{{V}_{{{v}_{{n}}}}^{{{r},{p}{c}}}}\right\rbrace},{\left\lbrace{\left({\text{Sig}_{{{v}_{{0}}}}^{{{r},{p}{c}}}},{v}_{{\text{id}_{{0}}}}\right)},\ldots{\left({\text{Sig}_{{{v}_{{n}}}}^{{{r},{p}{c}}}},{v}_{{\text{id}_{{n}}}}\right)}\right\rbrace}\right)} J v 0 , ‚Ä¶ n ‚Äã r , comp ‚Äã = ( { V v 0 ‚Äã r , p c ‚Äã , ‚Ä¶ V v n ‚Äã r , p c ‚Äã } , { ( Sig v 0 ‚Äã r , p c ‚Äã , v id 0 ‚Äã ‚Äã ) , ‚Ä¶ ( Sig v n ‚Äã r , p c ‚Äã , v id n ‚Äã ‚Äã ) } ) where V v i r , p c {{V}_{{{v}_{{i}}}}^{{{r},{p}{c}}}} V v i ‚Äã r , p c ‚Äã is a 256-bit byte array containing the pre-commit vote of authority v i {v}_{{i}} v i ‚Äã ( Definition 72 ). Sig v i r , p c {\text{Sig}_{{{v}_{{i}}}}^{{{r},{p}{c}}}} Sig v i ‚Äã r , p c ‚Äã is a 512-bit byte array containing the pre-commit signature of authority v i {v}_{{i}} v i ‚Äã ( Definition 73 ). v id n {v}_{{\text{id}_{{n}}}} v id n ‚Äã ‚Äã is a 256-bit byte array containing the public key of authority v i {v}_{{i}} v i ‚Äã . Definition 46. GRANDPA Commit Message ‚Äã A GRANDPA commit message for block B {B} B in round r {r} r , M v r , Fin ( B ) {{M}_{{v}}^{{{r},\text{Fin}}}}{\left({B}\right)} M v r , Fin ‚Äã ( B ) , is a message broadcasted by voter v {v} v to the network indicating that voter v {v} v has finalized block B {B} B in round r {r} r . It has the following structure: M v r , Fin ( B ) = Enc SC ( r , id V , V v r ( B ) , J v 0 , ‚Ä¶ n r , comp ) {{M}_{{v}}^{{{r},\text{Fin}}}}{\left({B}\right)}\:=\text{Enc}_{{\text{SC}}}{\left({r},\text{id}_{{{\mathbb{{V}}}}},{{V}_{{v}}^{{r}}}{\left({B}\right)},{{J}_{{{v}_{{{0},\ldots{n}}}}}^{{{r},\text{comp}}}}\right)} M v r , Fin ‚Äã ( B ) = Enc SC ‚Äã ( r , id V ‚Äã , V v r ‚Äã ( B ) , J v 0 , ‚Ä¶ n ‚Äã r , comp ‚Äã ) where r {r} r is an unsigned 64-bit integer indicating the round number ( Definition 72 ). i d V {id}_{{{\mathbb{{V}}}}} i d V ‚Äã is the authority set Id ( Definition 69 ). V v r ( B ) {{V}_{{v}}^{{r}}}{\left({B}\right)} V v r ‚Äã ( B ) is a 256-bit array containing the GRANDPA vote for block B {B} B ( Definition 71 ). J v 0 , ‚Ä¶ n r , comp {{J}_{{{v}_{{{0},\ldots{n}}}}}^{{{r},\text{comp}}}} J v 0 , ‚Ä¶ n ‚Äã r , comp ‚Äã is the compacted GRANDPA justification containing observed pre-commit of authorities v 0 {v}_{{0}} v 0 ‚Äã to v n {v}_{{n}} v n ‚Äã ( Definition 45 ). This message is the sub-component of the GRANDPA gossip message ( Definition 43 ) of type Id 1 . 4.8.6.1. GRANDPA Neighbor Messages ‚Äã Neighbor messages are sent to all connected peers but they are not repropagated on reception. A message should be send whenever the messages values change and at least every 5 minutes. The sender should take the recipients state into account and avoid sending messages to peers that are using a different voter sets or are in a different round. Messages received from a future voter set or round can be dropped and ignored. Definition 47. GRANDPA Neighbor Message ‚Äã A GRANDPA Neighbor Message is defined as: M neigh = Enc SC ( v , r , id V , H i ( B last ) ) {M}^{{\text{neigh}}}\:=\text{Enc}_{{\text{SC}}}{\left({v},{r},\text{id}_{{{\mathbb{{V}}}}},{H}_{{i}}{\left({B}_{{\text{last}}}\right)}\right)} M neigh = Enc SC ‚Äã ( v , r , id V ‚Äã , H i ‚Äã ( B last ‚Äã ) ) where v {v} v is an unsigned 8-bit integer indicating the version of the neighbor message, currently 1 . r {r} r is an unsigned 64-bit integer indicating the round number ( Definition 72 ). id V \text{id}_{{{\mathbb{{V}}}}} id V ‚Äã is an unsigned 64-bit integer indicating the authority Id ( Definition 69 ). H i ( B last ) {H}_{{i}}{\left({B}_{{\text{last}}}\right)} H i ‚Äã ( B last ‚Äã ) is an unsigned 32-bit integer indicating the block number of the last finalized block B last {B}_{{\text{last}}} B last ‚Äã . This message is the sub-component of the GRANDPA gossip message ( Definition 43 ) of type Id 2 . 4.8.6.2. GRANDPA Catch-up Messages ‚Äã Whenever a Polkadot node detects that it is lagging behind the finality procedure, it needs to initiate a catch-up procedure. GRANDPA Neighbor messages ( Definition 47 ) reveal the round number for the last finalized GRANDPA round which the node‚Äôs peers have observed. This provides the means to identify a discrepancy in the latest finalized round number observed among the peers. If such a discrepancy is observed, the node needs to initiate the catch-up procedure explained in Section 6.6.1. ). In particular, this procedure involves sending a catch-up request and processing catch-up response messages. Definition 48. Catch-Up Request Message ‚Äã A GRANDPA catch-up request message for round r {r} r , M i , v Cat ‚àí q ( id V , r ) {{M}_{{{i},{v}}}^{{\text{Cat}-{q}}}}{\left(\text{id}_{{{\mathbb{{V}}}}},{r}\right)} M i , v Cat ‚àí q ‚Äã ( id V ‚Äã , r ) , is a message sent from node i {i} i to its voting peer node v {v} v requesting the latest status of a GRANDPA round r ‚Ä≤ > r {r}'>{r} r ‚Ä≤ > r of the authority set V id {\mathbb{{V}}}_{{\text{id}}} V id ‚Äã along with the justification of the status and has the following structure: M i , v r , Cat ‚àí q = Enc SC ( r , id V ) {{M}_{{{i},{v}}}^{{{r},\text{Cat}-{q}}}}\:=\text{Enc}_{{\text{SC}}}{\left({r},\text{id}_{{{\mathbb{{V}}}}}\right)} M i , v r , Cat ‚àí q ‚Äã = Enc SC ‚Äã ( r , id V ‚Äã ) This message is the sub-component of the GRANDPA Gossip message ( Definition 43 ) of type Id 3 . Definition 49. Catch-Up Response Message ‚Äã A GRANDPA catch-up response message for round r {r} r , M v , i Cat ‚àí s ( id V , r ) {{M}_{{{v},{i}}}^{{\text{Cat}-{s}}}}{\left(\text{id}_{{{\mathbb{{V}}}}},{r}\right)} M v , i Cat ‚àí s ‚Äã ( id V ‚Äã , r ) , is a message sent by a node v {v} v to node i {i} i in response of a catch-up request M v , i Cat ‚àí q ( id V , r ‚Ä≤ ) {{M}_{{{v},{i}}}^{{\text{Cat}-{q}}}}{\left(\text{id}_{{{\mathbb{{V}}}}},{r}'\right)} M v , i Cat ‚àí q ‚Äã ( id V ‚Äã , r ‚Ä≤ ) in which r ‚â• r ‚Ä≤ {r}\ge{r}' r ‚â• r ‚Ä≤ is the latest GRANDPA round which v has prove of its finalization and has the following structure: M v , i Cat ‚àí s = Enc SC ( id V , r , J 0 , ‚Ä¶ n r , pv ( B ) , J 0 , ‚Ä¶ m r , pc ( B ) , H h ( B ‚Ä≤ ) , H i ( B ‚Ä≤ ) ) {{M}_{{{v},{i}}}^{{\text{Cat}-{s}}}}\:=\text{Enc}_{{\text{SC}}}{\left(\text{id}_{{{\mathbb{{V}}}}},{r},{{J}_{{{0},\ldots{n}}}^{{{r},\text{pv}}}}{\left({B}\right)},{{J}_{{{0},\ldots{m}}}^{{{r},\text{pc}}}}{\left({B}\right)},{H}_{{h}}{\left({B}'\right)},{H}_{{i}}{\left({B}'\right)}\right)} M v , i Cat ‚àí s ‚Äã = Enc SC ‚Äã ( id V ‚Äã , r , J 0 , ‚Ä¶ n r , pv ‚Äã ( B ) , J 0 , ‚Ä¶ m r , pc ‚Äã ( B ) , H h ‚Äã ( B ‚Ä≤ ) , H i ‚Äã ( B ‚Ä≤ ) ) Where B {B} B is the highest block which v {v} v believes to be finalized in round r {r} r ( Definition 72 ). B ‚Ä≤ {B}' B ‚Ä≤ is the highest ancestor of all blocks voted on in the arrays of justifications J 0 , ‚Ä¶ n r , pv ( B ) {{J}_{{{0},\ldots{n}}}^{{{r},\text{pv}}}}{\left({B}\right)} J 0 , ‚Ä¶ n r , pv ‚Äã ( B ) and J 0 , ‚Ä¶ m r , pc ( B ) {{J}_{{{0},\ldots{m}}}^{{{r},\text{pc}}}}{\left({B}\right)} J 0 , ‚Ä¶ m r , pc ‚Äã ( B ) ( Definition 74 ) with the exception of the equivocatory votes. This message is the sub-component of the GRANDPA Gossip message ( Definition 43 ) of type Id 4 . Previous 3. Synchronization Next 5. Block Production 4.1. Introduction 4.2. External Documentation 4.3. Node Identities 4.4. Discovery mechanism 4.5. Connection establishment 4.6. Encryption Layer 4.7. Protocols and Substreams 4.8. Network Messages 4.8.1. Announcing blocks 4.8.2. Requesting Blocks 4.8.3. Requesting States 4.8.4. Warp Sync 4.8.5. Transactions 4.8.6. GRANDPA Messages 
5. Block Production | Polkadot Protocol Specification Host 5. Block Production On this page 5. Block Production 5.1. Introduction ‚Äã The Polkadot Host uses BABE protocol for block production. It is designed based on Ouroboros praos. BABE execution happens in sequential non-overlapping phases known as an epoch . Each epoch is divided into a predefined number of slots. All slots in each epoch are sequentially indexed starting from 0. At the beginning of each epoch, the BABE node needs to run Block-Production-Lottery to find out in which slots it should produce a block and gossip to the other block producers. In turn, the block producer node should keep a copy of the block tree and grow it as it receives valid blocks from other block producers. A block producer prunes the tree in parallel by eliminating branches that do not include the most recently finalized blocks ( Definition 5 ). 5.1.1. Block Producer ‚Äã A block producer , noted by P j {\mathcal{{P}}}_{{j}} P j ‚Äã , is a node running the Polkadot Host, which is authorized to keep a transaction queue and which it gets a turn in producing blocks. 5.1.2. Block Authoring Session Key Pair ‚Äã Block authoring session key pair ( s k j s , p k j s ) {\left({s}{{k}_{{j}}^{{s}}},{p}{{k}_{{j}}^{{s}}}\right)} ( s k j s ‚Äã , p k j s ‚Äã ) is an SR25519 key pair which the block producer P j {\mathcal{{P}}}_{{j}} P j ‚Äã signs by their account key ( Definition 167 ) and is used to sign the produced block as well as to compute its lottery values in Block-Production-Lottery . Definition 50. Epoch and Slot ‚Äã A block production epoch , formally referred to as E {\mathcal{{E}}} E , is a period with a pre-known starting time and fixed-length during which the set of block producers stays constant. Epochs are indexed sequentially, and we refer to the n t h {n}^{{{t}{h}}} n t h epoch since genesis by E n {\mathcal{{E}}}_{{n}} E n ‚Äã . Each epoch is divided into equal-length periods known as block production slots , sequentially indexed in each epoch. The index of each slot is called a slot number . The equal length duration of each slot is called the slot duration and indicated by T {\mathcal{{T}}} T . Each slot is awarded to a subset of block producers during which they are allowed to generate a block. info Substrate refers to an epoch as a "session" in some places. However, epoch should be the preferred and official name for these periods. | Definition 51. Epoch and Slot Duration ‚Äã We refer to the number of slots in epoch E n {\mathcal{{E}}}_{{n}} E n ‚Äã by s c n {s}{c}_{{n}} s c n ‚Äã . s c n {s}{c}_{{n}} s c n ‚Äã is set to the duration field in the returned data from the call of the Runtime entry BabeApi_configuration ( Section C.11.1. ) at genesis. For a given block B {B} B , we use the notation s B {s}_{{B}} s B ‚Äã to refer to the slot during which B {B} B has been produced. Conversely, for slot s {s} s , B c {\mathcal{{B}}}_{{c}} B c ‚Äã is the set of Blocks generated at slot s {s} s . Definition 52 provides an iterator over the blocks produced during a specific epoch. Definition 52. Epoch Subchain ‚Äã By SubChain ( E n ) {\text{SubChain}{\left({\mathcal{{E}}}_{{n}}\right)}} SubChain ( E n ‚Äã ) for epoch E n {\mathcal{{E}}}_{{n}} E n ‚Äã , we refer to the path graph of B T {B}{T} B T containing all the blocks generated during the slots of epoch E n {\mathcal{{E}}}_{{n}} E n ‚Äã . When there is more than one block generated at a slot, we choose the one which is also on Longest-Chain ( B T ) \text{Longest-Chain}{\left({B}{T}\right)} Longest-Chain ( B T ) . Definition 53. Equivocation ‚Äã A block producer equivocates if they produce more than one block at the same slot. The proof of equivocation is the given distinct headers that were signed by the validator and which include the slot number. The Polkadot Host must detect equivocations committed by other validators and submit those to the Runtime as described in Section C.11.6. . Definition 54. BABE Consensus Message ‚Äã CM b \text{CM}_{{b}} CM b ‚Äã , the consensus message for BABE, is of the following format: CM b = { 1 ( Auth C , r ) 2 A i 3 D \text{CM}_{{b}}={\left\lbrace\begin{matrix}{1}&{\left(\text{Auth}_{{C}},{r}\right)}\\{2}&{A}_{{i}}\\{3}&{D}\end{matrix}\right.} CM b ‚Äã = ‚é© ‚é® ‚éß ‚Äã 1 2 3 ‚Äã ( Auth C ‚Äã , r ) A i ‚Äã D ‚Äã where 1 implies next epoch data : The Runtime issues this message on every first block of an epoch. The supplied authority set Definition 33 , Auth C {\text{Auth}_C} Auth C ‚Äã , and randomness Definition 67 , r {r} r , are used in the next epoch E n + 1 \mathcal E_n + 1 E n ‚Äã + 1 . 2 implies on disabled : A 32-bit integer, A i {A_i} A i ‚Äã , indicating the individual authority in the current authority list that should be immediately disabled until the next authority set changes. This message's initial intention was to cause an immediate suspension of all authority functionality with the specified authority. 3 implies next epoch descriptor : These messages are only issued on configuration change and in the first block of an epoch. The supplied configuration data are intended to be used from the next epoch onwards. D D D is a varying datatype of the following format: D = { 1 , ( c , 2 nd ) } D = \{1, (c,2_{\text{nd}})\} D = { 1 , ( c , 2 nd ‚Äã )} where c {c} c is the probability that a slot will not be empty Definition 55 . It is encoded as a tuple of two unsigned 64-bit integers c n o m i n a t o r , c d e n o m i n a t o r {c_{nominator},c_{denominator}} c n o mina t or ‚Äã , c d e n o mina t or ‚Äã which are used to compute the rational c = c n o m i n a t o r c d e n o m i n a t o r {c = \frac{c_{nominator}}{c_{denominator}}} c = c d e n o mina t or ‚Äã c n o mina t or ‚Äã ‚Äã . 2 nd {2_{\text{nd}}} 2 nd ‚Äã describes what secondary slot Definition 57 , if any, is to be used. It is encoded as one-byte varying datatype: s 2nd = { 0 ‚Üí no secondary slot 1 ‚Üí plain secondary slot 2 ‚Üí secondary slot with VRF output s_{\text{2nd}} = \begin{cases} 0 \rightarrow \text{no secondary slot} \\ 1 \rightarrow \text{plain secondary slot} \\ 2 \rightarrow \text{secondary slot with VRF output} \end{cases} s 2nd ‚Äã = ‚é© ‚é® ‚éß ‚Äã 0 ‚Üí no secondary slot 1 ‚Üí plain secondary slot 2 ‚Üí secondary slot with VRF output ‚Äã 5.2. Block Production Lottery ‚Äã The babe constant ( Definition 55 ) is initialized at genesis to the value returned by calling BabeApi_configuration ( Section C.11.1. ). For efficiency reasons, it is generally updated by the Runtime through the next config data consensus message in the digest ( Definition 11 ) of the first block of an epoch for the next epoch. A block producer aiming to produce a block during E n {\mathcal{{E}}}_{{n}} E n ‚Äã should run \<algo-block-production-lottery > > to identify the slots it is awarded. These are the slots during which the block producer is allowed to build a block. The s k {s}{k} s k is the block producer lottery secret key and n {n} n is the index of the epoch for whose slots the block producer is running the lottery. In order to ensure consistent block production, BABE uses secondary slots in case no authority wins the (primary) block production lottery. Unlike the lottery, secondary slot assignees are known upfront publically ( Definition 57 ). The Runtime provides information on how or if secondary slots are executed ( Section C.11.1. ), explained further in Definition 57 . Definition 55. BABE Constant ‚Äã The BABE constant is the probability that a slot will not be empty and used in the winning threshold calculation ( Definition 56 ). It‚Äôs expressed as a rational, ( x , y ) {\left({x},{y}\right)} ( x , y ) , where x {x} x is the numerator and y {y} y is the denominator. Definition 56. Winning Threshold ‚Äã The Winning threshold denoted by T E n {T}_{{{\mathcal{{E}}}_{{n}}}} T E n ‚Äã ‚Äã is the threshold that is used alongside the result of Block-Production-Lottery to decide if a block producer is the winner of a specific slot. T E n {T}_{{{\mathcal{{E}}}_{{n}}}} T E n ‚Äã ‚Äã is calculated as follows: A w = ‚àë n = 1 ‚à£ Auth C ( B ) ‚à£ ( w A ‚àà Auth C ( B ) n ) {A}_{{w}}={\sum_{{{n}={1}}}^{{{\left|\text{Auth}_{{C}}{\left({B}\right)}\right|}}}}{\left({w}_{{A}}\in\text{Auth}_{{C}}{\left({B}\right)}_{{n}}\right)} A w ‚Äã = n = 1 ‚àë ‚à£ Auth C ‚Äã ( B ) ‚à£ ‚Äã ( w A ‚Äã ‚àà Auth C ‚Äã ( B ) n ‚Äã ) T E n = 1 ‚àí ( 1 ‚àí c ) w a A w {T}_{{{\mathcal{{E}}}_{{n}}}}\:={1}-{\left({1}-{c}\right)}^{{\frac{{w}_{{a}}}{{A}_{{w}}}}} T E n ‚Äã ‚Äã = 1 ‚àí ( 1 ‚àí c ) A w ‚Äã w a ‚Äã ‚Äã where A w {A}_{{w}} A w ‚Äã is the total sum of all authority weights in the authority set ( Definition 33 ) for epoch E n {\mathcal{{E}}}_{{n}} E n ‚Äã , w a {w}_{{a}} w a ‚Äã is the weight of the block author and c ‚àà ( 0 , 1 ) {c}\in{\left({0},{1}\right)} c ‚àà ( 0 , 1 ) is the BABE constant ( Definition 55 ). The numbers should be treated as 64-bit rational numbers. 5.2.1. Primary Block Production Lottery ‚Äã A block producer aiming to produce a block during E n {\mathcal{{E}}}_{{n}} E n ‚Äã should run the Block-Production-Lottery \text{Block-Production-Lottery} Block-Production-Lottery algorithm to identify the slots it is awarded. These are the slots during which the block producer is allowed to build a block. The session secret key, s k {s}{k} s k , is the block producer lottery secret key, and n {n} n is the index of the epoch for whose slots the block producer is running the lottery. Algorithm 6. Block Production Lottery ‚Äã \begin{algorithm} \caption{Block-Production-Lottery} \begin{algorithmic} \require sk \state $r \leftarrow$ \call{Epoch-Randomness}{$n$} \for{$i := 1 ~\textbf{to}~ sc_n$} \state $(\pi, d) \leftarrow$ \call{VRF}{$r, i, sk$} \state $A[i] \leftarrow (d, \pi)$ \endfor \return{A} \end{algorithmic} \end{algorithm} where Epoch-Randomness \text{Epoch-Randomness} Epoch-Randomness is defined in ( Definition 67 ), s c n {s}{c}_{{n}} s c n ‚Äã is defined in Definition 51 , VRF \text{VRF} VRF creates the BABE VRF transcript ( Definition 58 ) and e i {e}_{{i}} e i ‚Äã is the epoch index, retrieved from the Runtime ( Section C.11.1. ). s k {s}_{{k}} s k ‚Äã and p k {p}_{{k}} p k ‚Äã is the secret key, respectively, the public key of the authority. For any slot s {s} s in epoch n {n} n where o < T E n {o}<{T}_{{{\mathcal{{E}}}_{{n}}}} o < T E n ‚Äã ‚Äã ( Definition 56 ), the block producer is required to produce a block. info The secondary slots ( Definition 57 ) are running alongside the primary block production lottery and mainly serve as a fallback to in case no authority was selected in the primary lottery. Definition 57. Secondary Slots ‚Äã Secondary slots work alongside primary slot to ensure consistent block production, as described in Section 5.2. . The secondary assignee of a block is determined by calculating a specific value, i d {i}_{{d}} i d ‚Äã , which indicates the index in the authority set ( Definition 33 ). The corresponding authority in that set has the right to author a secondary block. This calculation is done for every slot in the epoch, s ‚àà s c n {s}\in{s}{c}_{{n}} s ‚àà s c n ‚Äã ( Definition 51 ). p ‚Üê h ( Enc SC ( r , s ) ) {p}\leftarrow{h}{\left(\text{Enc}_{\text{SC}}{\left({r},{s}\right)}\right)} p ‚Üê h ( Enc SC ‚Äã ( r , s ) ) i d ‚Üê p mod A l {i}_{{d}}\leftarrow{p}\text{mod}{A}_{{l}} i d ‚Äã ‚Üê p mod A l ‚Äã where r {r} r is the Epoch randomness ( Definition 67 ). s {s} s is the slot number ( Definition 50 ). Enc SC ( ‚Ä¶ ) \text{Enc}_{\text{SC}}{\left(\ldots\right)} Enc SC ‚Äã ( ‚Ä¶ ) encodes its inner value to the corresponding SCALE value. h ( ‚Ä¶ ) {h}{\left(\ldots\right)} h ( ‚Ä¶ ) creates a 256-bit Blake2 hash from its inner value. A l {A}_{{l}} A l ‚Äã is the lengths of the authority list ( Definition 33 ). If i d {i}_{{d}} i d ‚Äã points to the authority, that authority must claim the secondary slot by creating a BABE VRF transcript ( Definition 58 ). The resulting values o {o} o and p {p} p are then used in the Pre-Digest item ( Definition 65 ). In the case of secondary slots with plain outputs, respectively the Pre-Digest being of value 2 , the transcript respectively the VRF is skipped. Definition 58. BABE Slot VRF transcript ‚Äã The BABE block production lottery requires a specific transcript structure ( Definition 165 ). That structure is used by both primary slots ( Block-Production-Lottery ) and secondary slots ( Definition 57 ). t 1 ‚Üê Transcript ( ‚ÄôBABE‚Äô ) {t}_{{1}}\leftarrow\text{Transcript}{\left(\text{'BABE'}\right)} t 1 ‚Äã ‚Üê Transcript ( ‚ÄôBABE‚Äô ) t 2 ‚Üê append ( t 1 , ‚Äôslot number‚Äô , s ) {t}_{{2}}\leftarrow\text{append}{\left({t}_{{1}},\text{'slot number'},{s}\right)} t 2 ‚Äã ‚Üê append ( t 1 ‚Äã , ‚Äôslot number‚Äô , s ) t 3 ‚Üê append ( t 2 , ‚Äôcurrent epoch‚Äô , e i ) {t}_{{3}}\leftarrow\text{append}{\left({t}_{{2}},\text{'current epoch'},{e}_{{i}}\right)} t 3 ‚Äã ‚Üê append ( t 2 ‚Äã , ‚Äôcurrent epoch‚Äô , e i ‚Äã ) t 4 ‚Üê append ( t 3 , ‚Äôchain randomness‚Äô , r ) {t}_{{4}}\leftarrow\text{append}{\left({t}_{{3}},\text{'chain randomness'},{r}\right)} t 4 ‚Äã ‚Üê append ( t 3 ‚Äã , ‚Äôchain randomness‚Äô , r ) t 5 ‚Üê append ( t 4 , ‚Äôvrf-nm-pk‚Äô , p k ) {t}_{{5}}\leftarrow\text{append}{\left({t}_{{4}},\text{'vrf-nm-pk'},{p}_{{k}}\right)} t 5 ‚Äã ‚Üê append ( t 4 ‚Äã , ‚Äôvrf-nm-pk‚Äô , p k ‚Äã ) t 6 ‚Üê meta-ad ( t 5 , ‚ÄôVRFHash‚Äô , False ) {t}_{{6}}\leftarrow\text{meta-ad}{\left({t}_{{5}},\text{'VRFHash'},\text{False}\right)} t 6 ‚Äã ‚Üê meta-ad ( t 5 ‚Äã , ‚ÄôVRFHash‚Äô , False ) t 7 ‚Üê meta-ad ( t 6 , 64 le , True ) {t}_{{7}}\leftarrow\text{meta-ad}{\left({t}_{{6}},{64}_{\text{le}},\text{True}\right)} t 7 ‚Äã ‚Üê meta-ad ( t 6 ‚Äã , 64 le ‚Äã , True ) h ‚Üê prf ( t 7 , False ) {h}\leftarrow\text{prf}{\left({t}_{{7}},\text{False}\right)} h ‚Üê prf ( t 7 ‚Äã , False ) o = s k ‚ãÖ h {o}={s}_{{k}}\cdot{h} o = s k ‚Äã ‚ãÖ h p ‚Üê dleq_prove ( t 7 , h ) {p}\leftarrow\text{dleq\_prove}{\left({t}_{{7}},{h}\right)} p ‚Üê dleq_prove ( t 7 ‚Äã , h ) The operators are defined in Definition 166 , dleq_prove \text{dleq\_prove} dleq_prove in Definition 162 . The computed outputs, o {o} o and p {p} p , are included in the block Pre-Digest ( Definition 65 ). 5.3. Slot Number Calculation ‚Äã It is imperative for the security of the network that each block producer correctly determines the current slot numbers at a given time by regularly estimating the local clock offset in relation to the network ( Definition 60 ). danger The calculation described in this section is still to be implemented and deployed : For now, each block producer is required to synchronize its local clock using NTP instead. The current slot s {s} s is then calculated by s = t unix T {s}={t}_{\text{unix}}{\mathcal{{T}}} s = t unix ‚Äã T where T {\mathcal{{T}}} T is defined in Definition 50 and t unix {t}_{\text{unix}} t unix ‚Äã is defined in Definition 171 . That also entails that slot numbers are currently not reset at the beginning of each epoch. Polkadot does this synchronization without relying on any external clock source (e.g., through the or the ). To stay in synchronization, each producer is therefore required to periodically estimate its local clock offset in relation to the rest of the network. This estimation depends on the two fixed parameters k {k} k ( Definition 61 ) and s c q {s}_{{{c}{q}}} s c q ‚Äã ( Definition 62 ). These are chosen based on the results of a formal security analysis , currently assuming a 1 s {1}{s} 1 s clock drift per day and targeting a probability lower than 0.5 % {0.5}\% 0.5 % for an adversary to break BABE in 3 years with resistance against a network delay up to 1 3 \frac{{1}}{{3}} 3 1 ‚Äã of the slot time and a Babe constant ( Definition 55 ) of c = 0.38 {c}={0.38} c = 0.38 . All validators are then required to run Median-Algorithm at the beginning of each sync period ( Definition 64 ) to update their synchronization using all block arrival times of the previous period. The algorithm should only be run once all the blocks in this period have been finalized, even if only probabilistically ( Definition 61 ). The target slot to which to synchronize should be the first slot in the new sync period. Definition 59. Slot Offset ‚Äã Let s i {s}_{{i}} s i ‚Äã and s j {s}_{{j}} s j ‚Äã be two slots belonging to epochs E k {\mathcal{{E}}}_{{k}} E k ‚Äã and E l {\mathcal{{E}}}_{{l}} E l ‚Äã . By Slot-Offset ( s i , s j ) {\left({s}_{{i}},{s}_{{j}}\right)} ( s i ‚Äã , s j ‚Äã ) we refer to the function whose value is equal to the number of slots between s i {s}_{{i}} s i ‚Äã and s j {s}_{{j}} s j ‚Äã (counting s j {s}_{{j}} s j ‚Äã ) on the time continuum. As such, we have Slot-Offset ( s i , s i ) = 0 {\left({s}_{{i}},{s}_{{i}}\right)}={0} ( s i ‚Äã , s i ‚Äã ) = 0 . It is imperative for the security of the network that each block producer correctly determines the current slot numbers at a given time by regularly estimating the local clock offset in relation to the network ( Definition 60 ). Definition 60. Relative Time Synchronization ‚Äã The relative time synchronization is a tuple of a slot number and a local clock timestamp ( s sync , t sync ) {\left({s}_{\text{sync}},{t}_{\text{sync}}\right)} ( s sync ‚Äã , t sync ‚Äã ) describing the last point at which the slot numbers have been synchronized with the local clock. Algorithm 7. Slot Time ‚Äã \begin{algorithm} \caption{Slot-Time} \begin{algorithmic} \require $s$ \return{$t_\text{sync} + $\call{Slot-Offset}{$s_{sync}, s$}$ \times \mathcal{T}$} \end{algorithmic} \end{algorithm} where s {s} s is the slot number. Algorithm 8. Median Algorithm ‚Äã \begin{algorithm} \caption{Median-Algorithm} \begin{algorithmic} \require $\mathfrak{E}, s_{sync}$ \state $T_s \leftarrow \{ \}$ \for{$B ~\textbf{in}~ \mathfrak{E}_j$} \state $t_{est}^{B} \leftarrow T_B + $\call{Slot-Offset}{$s_B, s_{sync}$}$ \times \mathcal{T}$ \state $T_s \leftarrow T_s \cup t_{est}^{B}$ \endfor \return \call{Median}{$T_s$} \end{algorithmic} \end{algorithm} where E {\mathfrak{{{E}}}} E is the sync period used for the estimate. s sync {s}_{\text{sync}} s sync ‚Äã is the slot time to estimate. Slot-Offset \text{Slot-Offset} Slot-Offset is defined in Slot-Time . T {\mathcal{{{T}}}} T is the slot duration defined in Definition 50 . Definition 61. Pruned Best Chain ‚Äã The pruned best chain C r k {C}^{{{r}^{{k}}}} C r k is the longest selected chain ( Definition 7 ) with the last k {k} k Blocks pruned. We chose k = 140 {k}={140} k = 140 . The last (probabilistic) finalized block describes the last block in this pruned best chain. Definition 62. Chain Quality ‚Äã The chain quality s c q {s}_{{{c}{q}}} s c q ‚Äã represents the number of slots that are used to estimate the local clock offset. Currently, it is set to s c q = 3000 {s}_{{{c}{q}}}={3000} s c q ‚Äã = 3000 . The prerequisite for such a calculation is that each producer stores the arrival time of each block ( Definition 63 ) measured by a clock that is otherwise not adjusted by any external protocol. Definition 63. Block Arrival Time ‚Äã The block arrival time of block B {B} B for node j {j} j formally represented by T B j {{T}_{{B}}^{{j}}} T B j ‚Äã is the local time of node j {j} j when node j {j} j has received block B {B} B for the first time. If the node j {j} j itself is the producer of B {B} B , T B j {{T}_{{B}}^{{j}}} T B j ‚Äã is set equal to the time that the block is produced. The index j {j} j in T B j {{T}_{{B}}^{{j}}} T B j ‚Äã notation may be dropped, and B‚Äôs arrival time is referred to by T B {T}_{{B}} T B ‚Äã when there is no ambiguity about the underlying node. Definition 64. Sync Period ‚Äã A is an interval at which each validator (re-)evaluates its local clock offsets. The first sync period E 1 {\mathfrak{{E}}}_{{1}} E 1 ‚Äã starts just after the genesis block is released. Consequently, each sync period E i {\mathfrak{{E}}}_{{i}} E i ‚Äã starts after E i ‚àí 1 {\mathfrak{{E}}}_{{{i}-{1}}} E i ‚àí 1 ‚Äã . The length of the sync period ( Definition 62 ) is equal to s q c {s}_{{{q}{c}}} s q c ‚Äã and expressed in the number of slots. Image 5. An exemplary result of Median Algorithm in first sync epoch with s cq = 9 {s}_{\text{cq}}={9} s cq ‚Äã = 9 and k = 1 {k}={1} k = 1 . ‚Äã 5.4. Production Algorithm ‚Äã Throughout each epoch, each block producer should run Invoke-Block-Authoring to produce blocks during the slots it has been awarded during that epoch. The produced block needs to carry the Pre-Digest ( Definition 65 ) as well as the block signature ( Definition 66 ) as Pre-Runtime and Seal digest items. Definition 65. Pre-Digest ‚Äã The Pre-Digest , or BABE header, P {P} P , is a varying datatype of the following format: P = { 1 ‚Üí ( a id , s , o , p ) 2 ‚Üí ( a id , s ) 3 ‚Üí ( a id , s , o , p ) {P}={\left\lbrace\begin{matrix}{1}&\rightarrow&{\left({a}_{\text{id}},{s},{o},{p}\right)}\\{2}&\rightarrow&{\left({a}_{\text{id}},{s}\right)}\\{3}&\rightarrow&{\left({a}_{\text{id}},{s},{o},{p}\right)}\end{matrix}\right.} P = ‚é© ‚é® ‚éß ‚Äã 1 2 3 ‚Äã ‚Üí ‚Üí ‚Üí ‚Äã ( a id ‚Äã , s , o , p ) ( a id ‚Äã , s ) ( a id ‚Äã , s , o , p ) ‚Äã where 1 indicates a primary slot with VRF outputs, 2 a secondary slot with plain outputs and 3 a secondary slot with VRF outputs ( Section 5.2. ). Plain outputs are no longer actively used and only exist for backwards compatibility reasons, respectively to sync old blocks. a id {a}_{\text{id}} a id ‚Äã is the unsigned 32-bit integer indicating the index of the authority in the authority set ( Section 3.3.1. ) who authored the block. s {s} s is the slot number ( Definition 50 ). o {o} o is VRF output ( Block-Production-Lottery respectively Definition 57 ). p {p} p is VRF proof ( Block-Production-Lottery respectively Definition 57 ). The Pre-Digest must be included as a digest item of Pre-Runtime type in the header digest ( Definition 11 ) H d ( B ) {H}_{{d}}{\left({B}\right)} H d ‚Äã ( B ) . Algorithm 9. Invoke-Block-Authoring ‚Äã \begin{algorithm} \caption{Invoke-Block-Authoring} \begin{algorithmic} \require $sk, pk, n, BT$ \state $A \leftarrow$ \call{Block-production-lottery}{$sk, n$} \for{$s \leftarrow 1 ~\textbf{to}~ sc_n$} \state \call{Wait-Until}{\call{Slot-Time}{$s$}} \state $(d, \pi) \leftarrow A[s]$ \if{$\tau > d$} \state $C_{Best} \leftarrow$ \call{Longest-Chain}{$BT$} \state $B_s \leftarrow$ \call{Build-Block}{$C_{Best}$} \state \call{Add-Digest-Item}{$B_s,\text{Pre-Runtime}, E_{id}(\text{BABE}), H_\text{BABE}(B_s)$} \state \call{Add-Digest-Item}{$B_s, \text{Seal}, S_B$} \state \call{Broadcast-Block}{$B_s$} \endif \endfor \end{algorithmic} \end{algorithm} where BT \text{BT} BT is the current block tree, Block-Production-Lottery \text{Block-Production-Lottery} Block-Production-Lottery is defined in Block-Production-Lottery and Add-Digest-Item \text{Add-Digest-Item} Add-Digest-Item appends a digest item to the end of the header digest H d ( B ) {H}_{{d}}{\left({B}\right)} H d ‚Äã ( B ) ( Definition 11 ). Definition 66. Block Signature ‚Äã The Block Signature S B {S}_{{B}} S B ‚Äã is a signature of the block header hash ( Definition 12 ) and defined as Sig SR25519 , sk j s ( H h ( B ) ) \text{Sig}_{{\text{SR25519},{\text{sk}_{{j}}^{{s}}}}}{\left({H}_{{h}}{\left({B}\right)}\right)} Sig SR25519 , sk j s ‚Äã ‚Äã ( H h ‚Äã ( B ) ) m {m} m should be included in H d ( B ) {H}_{{d}}{\left({B}\right)} H d ‚Äã ( B ) as the Seal digest item ( Definition 11 ) of value: ( t , id ( BABE ) , m ) {\left({t},\text{id}{\left(\text{BABE}\right)},{m}\right)} ( t , id ( BABE ) , m ) in which, t = 5 {t}={5} t = 5 is the seal digest identifier and id ( BABE ) \text{id}{\left(\text{BABE}\right)} id ( BABE ) is the BABE consensus engine unique identifier ( Definition 11 ). The Seal digest item is referred to as the BABE Seal . 5.5. Epoch Randomness ‚Äã At the beginning of each epoch, E n {\mathcal{{E}}}_{{n}} E n ‚Äã the host will receive the randomness seed R E n + 1 {\mathcal{{R}}}_{{{\mathcal{{E}}}_{{{n}+{1}}}}} R E n + 1 ‚Äã ‚Äã ( Definition 67 ) necessary to participate in the block production lottery in the next epoch E n + 1 {\mathcal{{E}}}_{{{n}+{1}}} E n + 1 ‚Äã from the Runtime, through the consensus message ( Definition 54 ) in the digest of the first block. Definition 67. Randomness Seed ‚Äã For epoch E {\mathcal{{E}}} E , there is a 32-byte R E {\mathcal{{R}}}_{{{\mathcal{{E}}}}} R E ‚Äã computed based on the previous epochs VRF outputs. For E 0 {\mathcal{{E}}}_{{0}} E 0 ‚Äã and E 1 {\mathcal{{E}}}_{{1}} E 1 ‚Äã , the randomness seed is provided in the genesis state ( Section C.11.1. ). For any further epochs, the randomness is retrieved from the consensus message ( Definition 54 ). 5.6. Verifying Authorship Right ‚Äã When a Polkadot node receives a produced block, it needs to verify if the block producer was entitled to produce the block in the given slot by running Verify-Authorship-Right . Verify-Slot-Winner runs as part of the verification process, when a node is importing a block. Algorithm 10. Verify Authorship Right ‚Äã \begin{algorithm} \caption{Verify-Authorship-Right} \begin{algorithmic} \require $\text{Head}_{s(B)}$ \state $s \leftarrow$ \call{Slot-Number-At-Given-Time}{$T_B$} \state $\mathcal{E}_c \leftarrow$ \call{Current-Epoch}{} \state $(D_1, \ldots, D_{|H_d(B)|}) \leftarrow H_d(B)$ \state $D_s \leftarrow D_{|H_d(B)|}$ \state $H_d(B) \leftarrow \left(D_1, \ldots, D_{|H_d(B)| - 1}\right)$ \comment{remove the seal from the digest} \state $(id, \text{Sig}_B)\leftarrow \text{Dec}_{SC}(D_s)$ \if{$id \neq$ \textsc{Seal-Id}} \state \textbf{error} ``Seal missing'' \endif \state $\text{AuthorID} \leftarrow \text{AuthorityDirectory}^{\mathcal{E}_c}[H_{BABE}(B).\text{SingerIndex}]$ \state \call{Verify-Signature}{$\text{AuthorID}, H_h(B),\text{Sig}_B$} \if{$\exists B' \in BT : H_h(B) \neq H_h (B)$ \and $s_B = s_B'$ \and $\text{SignerIndex}_B = \text{SignerIndex}_{B'}$} \state \textbf{error} ``Block producer is equivocating'' \endif \state \call{Verify-Slot-Winner}{$(d_B, \pi_B), s_B, \text{AuthorID}$} \end{algorithmic} \end{algorithm} where Head s ( B ) \text{Head}_{{s}}{\left({B}\right)} Head s ‚Äã ( B ) is the header of the block that‚Äôs being verified. T B {T}_{{B}} T B ‚Äã is B {B} B ‚Äôs arrival time ( Definition 63 ). H d ( B ) {H}_{{d}}{\left({B}\right)} H d ‚Äã ( B ) is the digest sub-component ( Definition 11 ) of Head ( B ) \text{Head}{\left({B}\right)} Head ( B ) ( Definition 10 ). The Seal D s {D}_{{s}} D s ‚Äã is the last element in the digest array H d ( B ) {H}_{{d}}{\left({B}\right)} H d ‚Äã ( B ) as described in Definition 11 . Seal-Id \text{Seal-Id} Seal-Id is the type index showing that a digest item ( Definition 11 ) of varying type ( Definition 179 ) is of type Seal . AuthorityDirectory E c \text{AuthorityDirectory}^{{{\mathcal{{E}}}_{{c}}}} AuthorityDirectory E c ‚Äã is the set of Authority ID for block producers of epoch E c {\mathcal{{E}}}_{{c}} E c ‚Äã . AuthorId \text{AuthorId} AuthorId is the public session key of the block producer. BT \text{BT} BT is the pruned block tree ( Definition 5 ). Verify-Slot-Winner \text{Verify-Slot-Winner} Verify-Slot-Winner is defined in Verify-Slot-Winner . Algorithm 11. Verify Slot Winner ‚Äã \begin{algorithm} \caption{Verify-Slot-Winner} \begin{algorithmic} \require $B$ \state $\mathcal{E}_c \leftarrow$ \textsc{Current-Epoch} \state $\rho \leftarrow$ \call{Epoch-Randomness}{$c$} \state \call{Verify-VRF}{$\rho, H_{BABE}(B).(d_B, \pi_B), H_{BABE}(B).s, c$} \if{$d_B \geqslant \tau$} \state \textbf{error} ``Block producer is not a winner of the slot'' \endif \end{algorithmic} \end{algorithm} where Epoch-Randomness \text{Epoch-Randomness} Epoch-Randomness is defined in Definition 67 . H BABE ( B ) {H}_{\text{BABE}}{\left({B}\right)} H BABE ‚Äã ( B ) is the BABE header defined in Definition 65 . ( o , p ) {\left({o},{p}\right)} ( o , p ) is the block lottery result for block B {B} B ( Block-Production-Lottery ), respectively the VRF output ( Definition 58 ). Verify-VRF \text{Verify-VRF} Verify-VRF is described in Section A.1.3. . T E n {T}_{{{\mathcal{{E}}}_{{n}}}} T E n ‚Äã ‚Äã is the winning threshold as defined in Definition 56 . 5.7. Block Building Process ‚Äã The block building process is triggered by Invoke-Block-Authoring of the consensus engine which in turn runs Build-Block . Algorithm 12. Build Block ‚Äã \begin{algorithm} \caption{Build-Block} \begin{algorithmic} \state $P_B \leftarrow $\call{Head}{$C_{Best}$} \state $\text{Head}(B) \leftarrow \left(H_p \leftarrow H_h(P_B), H_i \leftarrow H_i(P_B) + 1, H_r \leftarrow \phi, H_e \leftarrow \phi, H_d \leftarrow \phi \right)$ \state \call{Call-Runtime-Entry}{$\texttt{Core\_initialize\_block}, \text{Head}(B)$} \state \textsc{I-D}$ \leftarrow $\call{Call-Runtime-Entry}{$\texttt{BlockBuilder\_inherent\_extrinsics}, $\textsc{Inherent-Data}} \for{$E~\textbf{in} $\textsc{I-D}} \state \call{Call-Runtime-Entry}{$\texttt{BlockBuilder\_apply\_extrinsics}, E$} \endfor \while{\not \call{End-Of-Slot}{$s$}} \state $E \leftarrow$ \call{Next-Ready-Extrinsic}{} \state $R \leftarrow$ \call{Call-Runtime-Entry}{$\texttt{BlockBuilder\_apply\_extrinsics}, E$} \if{\call{Block-Is-Full}{$R$}} \break \endif \if{\call{Should-Drop}{$R$}} \state \call{Drop}{$E$} \endif \state $\text{Head}(B) \leftarrow$ \call{Call-Runtime-Entry}{$\texttt{BlockBuilder\_finalize\_block}, B$} \state $B \leftarrow$ \call{Add-Seal}{$B$} \endwhile \end{algorithmic} \end{algorithm} where C Best {C}_{\text{Best}} C Best ‚Äã is the chain head at which the block should be constructed ("parent"). s {s} s is the slot number. Head ( B ) \text{Head}{\left({B}\right)} Head ( B ) is defined in Definition 10 . Call-Runtime-Entry \text{Call-Runtime-Entry} Call-Runtime-Entry is defined in Definition 32 . Inherent-Data \text{Inherent-Data} Inherent-Data is defined in Definition 15 . End-Of-Slot \text{End-Of-Slot} End-Of-Slot indicates the end of the BABE slot as defined Median-Algorithm respectively Definition 50 . Next-Ready-Extrinsic \text{Next-Ready-Extrinsic} Next-Ready-Extrinsic indicates picking an extrinsic from the extrinsics queue ( Definition 14 ). Block-Is-Full \text{Block-Is-Full} Block-Is-Full indicates that the maximum block size is being used. Should-Drop \text{Should-Drop} Should-Drop determines based on the result R {R} R whether the extrinsic should be dropped or remain in the extrinsics queue and scheduled for the next block. The ApplyExtrinsicResult ( Definition 210 ) describes this behavior in more detail. Drop \text{Drop} Drop indicates removing the extrinsic from the extrinsic queue ( Definition 14 ). Add-Seal \text{Add-Seal} Add-Seal adds the seal to the block (<< > > ) before sending it to peers. The seal is removed again before submitting it to the Runtime. Previous 4. Networking Next 6. Finality 5.1. Introduction 5.1.1. Block Producer 5.1.2. Block Authoring Session Key Pair 5.2. Block Production Lottery 5.2.1. Primary Block Production Lottery 5.3. Slot Number Calculation 5.4. Production Algorithm 5.5. Epoch Randomness 5.6. Verifying Authorship Right 5.7. Block Building Process 
6. Finality | Polkadot Protocol Specification Host 6. Finality On this page 6. Finality 6.1. Introduction ‚Äã The Polkadot Host uses GRANDPA Finality protocol to finalize blocks. Finality is obtained by consecutive rounds of voting by the validator nodes. Validators execute GRANDPA finality process in parallel to Block Production as an independent service. In this section, we describe the different functions that GRANDPA service performs to successfully participate in the block-finalization process. Definition 68. GRANDPA Voter ‚Äã A GRANDPA Voter , v {v} v , represented by a key pair ( K v pr , v id ) {\left({{K}_{{v}}^{{\text{pr}}}},{v}_{{\text{id}}}\right)} ( K v pr ‚Äã , v id ‚Äã ) where k v pr {{k}_{{v}}^{{\text{pr}}}} k v pr ‚Äã represents an ed25519 private key, is a node running a GRANDPA protocol and broadcasting votes to finalize blocks in a Polkadot Host-based chain. The set of all GRANDPA voters for a given block B is indicated by V B {\mathbb{{V}}}_{{B}} V B ‚Äã . In that regard, we have [ To do: change function name, only call at genesis, adjust V_B over the sections ] V = g r a n d p a _ a u t h o r i t i e s ( B ) {\mathbb{{V}}}={\tt{grandpa\_authorities}}{\left({B}\right)} V = grandpa_authorities ( B ) where g r a n d p a _ a u t h o r i t i e s {\tt{grandpa\_authorities}} grandpa_authorities is a function entrypoint of the Runtime described in Section C.10.1. . We refer to V B {\mathbb{{V}}}_{{B}} V B ‚Äã as V {\mathbb{{V}}} V when there is no chance of ambiguity. Analogously we say that a Polkadot node is a non-voter node for block B {B} B , if it does not own any of the key pairs in V B {\mathbb{{V}}}_{{B}} V B ‚Äã . Definition 69. Authority Set Id ‚Äã The authority set Id ( id V \text{id}_{{{\mathbb{{V}}}}} id V ‚Äã ) is an incremental counter which tracks the amount of authority list changes that occurred ( Definition 82 ). Starting with the value of zero at genesis, the Polkadot Host increments this value by one every time a Scheduled Change or a Forced Change occurs. The authority set Id is an unsigned 64-bit integer. Definition 70. GRANDPA State ‚Äã The GRANDPA state , GS \text{GS} GS , is defined as: GS = { V , id V , r } \text{GS}\:={\left\lbrace{\mathbb{{V}}},\text{id}_{{{\mathbb{{V}}}}},{r}\right\rbrace} GS = { V , id V ‚Äã , r } where V {\mathbb{{V}}} V : is the set of voters. id V \text{id}_{{{\mathbb{{V}}}}} id V ‚Äã : is the authority set ID ( Definition 69 ). r {r} r : is the voting round number. Definition 71. GRANDPA Vote ‚Äã A GRANDPA vote or simply a vote for block B {B} B is an ordered pair defined as V ( B ) = ( H h ( B ) , H i ( B ) ) {V}{\left({B}\right)}\:={\left({H}_{{h}}{\left({B}\right)},{H}_{{i}}{\left({B}\right)}\right)} V ( B ) = ( H h ‚Äã ( B ) , H i ‚Äã ( B ) ) where H h ( B ) {H}_{{h}}{\left({B}\right)} H h ‚Äã ( B ) and H i ( B ) {H}_{{i}}{\left({B}\right)} H i ‚Äã ( B ) are the block hash ( Definition 12 ) and the block number ( Definition 10 ). Definition 72. Voting Rounds ‚Äã Voters engage in a maximum of two sub-rounds of voting for each round r {r} r . The first sub-round is called pre-vote and the second sub-round is called pre-commit . By V v r , pv {{V}_{{v}}^{{{r},\text{pv}}}} V v r , pv ‚Äã and V v r , pc {{V}_{{v}}^{{{r},\text{pc}}}} V v r , pc ‚Äã we refer to the vote cast by voter v {v} v in round r {r} r (for block B {B} B ) during the pre-vote and the pre-commit sub-round respectively. Voting is done by means of broadcasting voting messages ( Section 4.8.6. ) to the network. Validators inform their peers about the block finalized in round r {r} r by broadcasting a commit message ( Play-Grandpa-Round ). Definition 73. Vote Signature ‚Äã Sign v i r , stage {\text{Sign}_{{{v}_{{i}}}}^{{{r},\text{stage}}}} Sign v i ‚Äã r , stage ‚Äã refers to the signature of a voter for a specific message in a round and is formally defined as: Sign v i r , stage = Sig ed25519 ( msg , r , id V ) {\text{Sign}_{{{v}_{{i}}}}^{{{r},\text{stage}}}}\:=\text{Sig}_{{\text{ed25519}}}{\left(\text{msg},{r},\text{id}_{{{\mathbb{{V}}}}}\right)} Sign v i ‚Äã r , stage ‚Äã = Sig ed25519 ‚Äã ( msg , r , id V ‚Äã ) where msg \text{msg} msg : is a byte array containing the message to be signed ( Definition 71 ). r {r} r : is an unsigned 64-bit integer is the round number. id V \text{id}_{{{\mathbb{{V}}}}} id V ‚Äã : is an unsigned 64-bit integer indicating the authority set Id ( Definition 69 ). Definition 74. Justification ‚Äã The justification for block B {B} B in round r {r} r , J r , stage ( B ) {J}^{{{r},\text{stage}}}{\left({B}\right)} J r , stage ( B ) , is a vector of pairs of the type: ( V ( B ‚Ä≤ ) , Sign v i r , stage ( B ‚Ä≤ ) , v id ) {\left({V}{\left({B}'\right)},{\text{Sign}_{{{v}_{{i}}}}^{{{r},\text{stage}}}}{\left({B}'\right)},{v}_{{\text{id}}}\right)} ( V ( B ‚Ä≤ ) , Sign v i ‚Äã r , stage ‚Äã ( B ‚Ä≤ ) , v id ‚Äã ) in which either B ‚Ä≤ ‚â• B {B}'\ge{B} B ‚Ä≤ ‚â• B or V v i r , pc ( B ‚Ä≤ ) {{V}_{{{v}_{{i}}}}^{{{r},\text{pc}}}}{\left({B}'\right)} V v i ‚Äã r , pc ‚Äã ( B ‚Ä≤ ) is an equivocatory vote. In all cases, Sign v i r , stage ( B ‚Ä≤ ) {\text{Sign}_{{{v}_{{i}}}}^{{{r},\text{stage}}}}{\left({B}'\right)} Sign v i ‚Äã r , stage ‚Äã ( B ‚Ä≤ ) is the signature ( Definition 73 ) of voter v id ‚àà V B {v}_{{\text{id}}}\in{\mathbb{{V}}}_{{B}} v id ‚Äã ‚àà V B ‚Äã broadcasted during either the pre-vote (stage = pv) or the pre-commit (stage = pc) sub-round of round r. A valid justification must only contain up-to-one valid vote from each voter and must not contain more than two equivocatory votes from each voter. Definition 75. Finalizing Justification ‚Äã We say J r , pc ( B ) {J}^{{{r},\text{pc}}}{\left({B}\right)} J r , pc ( B ) justifies the finalization of B ‚Ä≤ ‚â• B {B}'\ge{B} B ‚Ä≤ ‚â• B for a non-voter node n {n} n if the number of valid signatures in J r , pc ( B ) {J}^{{{r},\text{pc}}}{\left({B}\right)} J r , pc ( B ) for B ‚Ä≤ {B}' B ‚Ä≤ is greater than 2 3 ‚à£ V B ‚à£ \frac{{2}}{{3}}{\left|{\mathbb{{V}}}_{{B}}\right|} 3 2 ‚Äã ‚à£ V B ‚Äã ‚à£ . Note that J r , pc ( B ) {J}^{{{r},\text{pc}}}{\left({B}\right)} J r , pc ( B ) can only be used by a non-voter node to finalize a block. In contrast, a voter node can only be assured of the finality ( Definition 85 ) of block B {B} B by actively participating in the voting process. That is by invoking Play-Grandpa-Round . The GRANDPA protocol dictates how an honest voter should vote in each sub-round, which is described by Play-Grandpa-Round . After defining what constitutes a vote in GRANDPA, we define how GRANDPA counts votes. Definition 76. Equivocation ‚Äã Voter v {v} v equivocates if they broadcast two or more valid votes to blocks during one voting sub-round. In such a situation, we say that v {v} v is an equivocator and any vote V v r , stage ( B ) {{V}_{{v}}^{{{r},\text{stage}}}}{\left({B}\right)} V v r , stage ‚Äã ( B ) cast by v {v} v in that sub-round is an equivocatory vote , and E r , stage {\mathcal{{E}}}^{{{r},\text{stage}}} E r , stage represents the set of all equivocators voters in sub-round stage of round r {r} r . When we want to refer to the number of equivocators whose equivocation has been observed by voter v {v} v we refer to it by: E obs ( v ) r , stage {{\mathcal{{E}}}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{stage}}}} E obs ( v ) r , stage ‚Äã The Polkadot Host must detect equivocations committed by other validators and submit those to the Runtime as described in Section C.10.3. . A vote V v r , stage = V ( B ) {{V}_{{v}}^{{{r},\text{stage}}}}={V}{\left({B}\right)} V v r , stage ‚Äã = V ( B ) is invalid if H ( B ) {H}{\left({B}\right)} H ( B ) does not correspond to a valid block. B {B} B is not an (eventual) descendant of a previously finalized block. M v r , stage {{M}_{{v}}^{{{r},\text{stage}}}} M v r , stage ‚Äã does not bear a valid signature. id V \text{id}_{{{\mathbb{{V}}}}} id V ‚Äã does no match the current V {\mathbb{{V}}} V . V v r , stage {{V}_{{v}}^{{{r},\text{stage}}}} V v r , stage ‚Äã is an equivocatory vote. Definition 77. Set of Observed Direct Votes ‚Äã For validator v {v} v , the set of observed direct votes for Block B {B} B in round r {r} r , formally denoted by VD obs ( v ) r , stage ( B ) {\text{VD}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{stage}}}}{\left({B}\right)} VD obs ( v ) r , stage ‚Äã ( B ) is equal to the union of: set of valid votes V v i r , stage {{V}_{{{v}_{{i}}}}^{{{r},\text{stage}}}} V v i ‚Äã r , stage ‚Äã cast in round r {r} r and received by v {v} v such that V v i r , stage = V ( B ) {{V}_{{{v}_{{i}}}}^{{{r},\text{stage}}}}={V}{\left({B}\right)} V v i ‚Äã r , stage ‚Äã = V ( B ) . Definition 78. Set of Total Observed Votes ‚Äã We refer to the set of total votes observed by voter v {v} v in sub-round stage of round r {r} r by V obs ( v ) r , stage {{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{stage}}}} V obs ( v ) r , stage ‚Äã . The set of all observed votes by v {v} v in the sub-round stage of round r {r} r for block B {B} B , V obs ( v ) r , stage {{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{stage}}}} V obs ( v ) r , stage ‚Äã is equal to all of the observed direct votes cast for block B {B} B and all of the B {B} B ‚Äôs descendants defined formally as: V obs ( v ) r , stage ( B ) = ‚ãÉ v i ‚àà V , B < B ‚Ä≤ VD obs ( v ) r , stage ( B ‚Ä≤ ) {{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{stage}}}}{\left({B}\right)}\:=\bigcup_{{{v}_{{i}}\in{\mathbb{{V}}},{B}<{B}'}}{\text{VD}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{stage}}}}{\left({B}'\right)} V obs ( v ) r , stage ‚Äã ( B ) = v i ‚Äã ‚àà V , B < B ‚Ä≤ ‚ãÉ ‚Äã VD obs ( v ) r , stage ‚Äã ( B ‚Ä≤ ) The total number of observed votes for Block B {B} B in round r {r} r is defined to be the size of that set plus the total number of equivocator voters: V obs ( v ) r , stage ( B ) = ‚à£ V obs ( v ) r , stage ( B ) ‚à£ + ‚à£ E obs ( v ) r , stage ‚à£ {{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{stage}}}}{\left({B}\right)}\:={\left|{{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{stage}}}}{\left({B}\right)}\right|}+{\left|{{\mathcal{{E}}}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{stage}}}}\right|} V obs ( v ) r , stage ‚Äã ( B ) = ‚à£ ‚à£ ‚Äã V obs ( v ) r , stage ‚Äã ( B ) ‚à£ ‚à£ ‚Äã + ‚à£ ‚à£ ‚Äã E obs ( v ) r , stage ‚Äã ‚à£ ‚à£ ‚Äã Note that for genesis state we always have # V obs ( v ) r , pv ( B ) = ‚à£ V ‚à£ \#{{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{pv}}}}{\left({B}\right)}={\left|{\mathbb{{V}}}\right|} # V obs ( v ) r , pv ‚Äã ( B ) = ‚à£ V ‚à£ . Definition 79. Set of Total Potential Votes ‚Äã Let V unobs ( v ) r , stage {{V}_{{\text{unobs}{\left({v}\right)}}}^{{{r},\text{stage}}}} V unobs ( v ) r , stage ‚Äã be the set of voters whose vote in the given stage has not been received. We define the total number of potential votes for Block B {B} B in round r {r} r to be: # V obs ( v ) , pot r , stage ( B ) = ‚à£ V obs ( v ) r , stage ( B ) ‚à£ + ‚à£ V unobs ( v ) r , stage ‚à£ + Min ( 1 3 ‚à£ V ‚à£ , ‚à£ V ‚à£ ‚àí ‚à£ V obs ( v ) r , stage ( B ) ‚à£ ‚àí ‚à£ V unobs ( v ) r , stage ‚à£ ) \#{{V}_{{\text{obs}{\left({v}\right)},\text{pot}}}^{{{r},\text{stage}}}}{\left({B}\right)}\:={\left|{{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{stage}}}}{\left({B}\right)}\right|}+{\left|{{V}_{{\text{unobs}{\left({v}\right)}}}^{{{r},\text{stage}}}}\right|}+\text{Min}{\left(\frac{{1}}{{3}}{\left|{\mathbb{{V}}}\right|},{\left|{\mathbb{{V}}}\right|}-{\left|{{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{stage}}}}{\left({B}\right)}\right|}-{\left|{{V}_{{\text{unobs}{\left({v}\right)}}}^{{{r},\text{stage}}}}\right|}\right)} # V obs ( v ) , pot r , stage ‚Äã ( B ) = ‚à£ ‚à£ ‚Äã V obs ( v ) r , stage ‚Äã ( B ) ‚à£ ‚à£ ‚Äã + ‚à£ ‚à£ ‚Äã V unobs ( v ) r , stage ‚Äã ‚à£ ‚à£ ‚Äã + Min ( 3 1 ‚Äã ‚à£ V ‚à£ , ‚à£ V ‚à£ ‚àí ‚à£ ‚à£ ‚Äã V obs ( v ) r , stage ‚Äã ( B ) ‚à£ ‚à£ ‚Äã ‚àí ‚à£ ‚à£ ‚Äã V unobs ( v ) r , stage ‚Äã ‚à£ ‚à£ ‚Äã ) Definition 80. Current Pre-Voted Block ‚Äã The current pre-voted block B v r , pv {{B}_{{v}}^{{{r},\text{pv}}}} B v r , pv ‚Äã also know as GRANDPA GHOST is the block chosen by GRANDPA-GHOST : B v r , pv = GRANDPA-GHOST ( r ) {{B}_{{v}}^{{{r},\text{pv}}}}\:=\text{GRANDPA-GHOST}{\left({r}\right)} B v r , pv ‚Äã = GRANDPA-GHOST ( r ) Finally, we define when a voter v {v} v sees a round as completable, that is when they are confident that B v r , pv {{B}_{{v}}^{{{r},\text{pv}}}} B v r , pv ‚Äã is an upper bound for what is going to be finalized in this round. Definition 81. Completable Round ‚Äã We say that round r {r} r is completable if ‚à£ V obs ( v ) r , pc ‚à£ + E obs ( v ) r , pc > 2 3 V {\left|{{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{pc}}}}\right|}+{{\mathcal{{E}}}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{pc}}}}>\frac{{2}}{{3}}{\mathbb{{V}}} ‚à£ ‚à£ ‚Äã V obs ( v ) r , pc ‚Äã ‚à£ ‚à£ ‚Äã + E obs ( v ) r , pc ‚Äã > 3 2 ‚Äã V and for all B ‚Ä≤ > B v r , pv {B}'>{{B}_{{v}}^{{{r},\text{pv}}}} B ‚Ä≤ > B v r , pv ‚Äã : ‚à£ V obs ( v ) r , pc ‚à£ ‚àí E obs ( v ) r , pc ‚àí ‚à£ V obs ( v ) r , pc ( B ‚Ä≤ ) ‚à£ > 2 3 ‚à£ V ‚à£ {\left|{{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{pc}}}}\right|}-{{\mathcal{{E}}}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{pc}}}}-{\left|{{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{pc}}}}{\left({B}'\right)}\right|}>\frac{{2}}{{3}}{\left|{\mathbb{{V}}}\right|} ‚à£ ‚à£ ‚Äã V obs ( v ) r , pc ‚Äã ‚à£ ‚à£ ‚Äã ‚àí E obs ( v ) r , pc ‚Äã ‚àí ‚à£ ‚à£ ‚Äã V obs ( v ) r , pc ‚Äã ( B ‚Ä≤ ) ‚à£ ‚à£ ‚Äã > 3 2 ‚Äã ‚à£ V ‚à£ Note that in practice we only need to check the inequality for those B ‚Ä≤ > B v r , pv {B}'>{{B}_{{v}}^{{{r},\text{pv}}}} B ‚Ä≤ > B v r , pv ‚Äã where ‚à£ V obs ( v ) r , pc ( B ‚Ä≤ ) ‚à£ > 0 {\left|{{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},\text{pc}}}}{\left({B}'\right)}\right|}>{0} ‚à£ ‚à£ ‚Äã V obs ( v ) r , pc ‚Äã ( B ‚Ä≤ ) ‚à£ ‚à£ ‚Äã > 0 . Definition 82. GRANDPA Consensus Message ‚Äã CM g \text{CM}_{{g}} CM g ‚Äã , the consensus message for GRANDPA, is of the following format: CM g = { 1 ( Auth C , N delay ) 2 ( m , Auth C , N delay ) 3 A i 4 N delay 5 N delay \text{CM}_{{g}}={\left\lbrace\begin{matrix}{1}&{\left(\text{Auth}_{{C}},{N}_{{\text{delay}}}\right)}\\{2}&{\left({m},\text{Auth}_{{C}},{N}_{{\text{delay}}}\right)}\\{3}&\text{A}_{{i}}\\{4}&{N}_{{\text{delay}}}\\{5}&{N}_{{\text{delay}}}\end{matrix}\right.} CM g ‚Äã = ‚é© ‚é® ‚éß ‚Äã 1 2 3 4 5 ‚Äã ( Auth C ‚Äã , N delay ‚Äã ) ( m , Auth C ‚Äã , N delay ‚Äã ) A i ‚Äã N delay ‚Äã N delay ‚Äã ‚Äã where N delay N_{\text{delay}} N delay ‚Äã is an unsigned 32-bit integer indicating how deep in the chain the announcing block must be before the change is applied. 1 Implies scheduled change : Schedule an authority set change after the given delay of N delay : = ‚à• SubChain ( B , B ‚Ä≤ ) ‚à• {N_{\text{delay}} := \|\text{SubChain}(B,B')\|} N delay ‚Äã := ‚à• SubChain ( B , B ‚Ä≤ ) ‚à• where B ‚Ä≤ {B'} B ‚Ä≤ is the block where the change is applied. The earliest digest of this type in a single block will be respected, unless a force change is present, in which case the force change takes precedence. 2 Implies forced change : Schedule a forced authority set change after the given delay of N delay : = ‚à• SubChain ( B , m + B ‚Ä≤ ) ‚à• {N_{\text{delay}} := \|\text{SubChain}(B,m + B')\|} N delay ‚Äã := ‚à• SubChain ( B , m + B ‚Ä≤ ) ‚à• where B ‚Ä≤ {B'} B ‚Ä≤ is the block where the change is applied. The earliest digest of this type in a block will be respected. Forced changes are explained further in Section 6.5. . 3 Implies on disabled : An index to the individual authority in the current authority list ( Definition 33 ) that should be immediately disabled until the next authority set changes. When an authority gets disabled, the node should stop performing any authority functionality from that authority, including authoring blocks and casting GRANDPA votes for finalization. Similarly, other nodes should ignore all messages from the indicated authority which pertain to their authority role. 4 Implies pause : A signal to pause the current authority set after the given delay of N delay : = ‚à• SubChain ( B , B ‚Ä≤ ) ‚à• {N_{\text{delay}} := \|\text{SubChain}(B,B')\|} N delay ‚Äã := ‚à• SubChain ( B , B ‚Ä≤ ) ‚à• where B ‚Ä≤ {B'} B ‚Ä≤ is a block where the change is applied. Once applied, the authorities should stop voting. 5 Implies resume : A signal to resume the current authority set after the given delay of N delay : = ‚à• SubChain ( B , B ‚Ä≤ ) ‚à• {N_{\text{delay}} := \|\text{SubChain}(B,B')\|} N delay ‚Äã := ‚à• SubChain ( B , B ‚Ä≤ ) ‚à• where B ‚Ä≤ {B'} B ‚Ä≤ is the block where the change is applied. Once applied, the authorities should resume voting. 6.2. Initiating the GRANDPA State ‚Äã In order to participate coherently in the voting process, a validator must initiate its state and sync it with other active validators. In particular, considering that voting is happening in different distinct rounds where each round of voting is assigned a unique sequential round number r v {r}_{{v}} r v ‚Äã , it needs to determine and set its round counter r {r} r equal to the voting round r n {r}_{{n}} r n ‚Äã currently undergoing in the network. The mandated initialization procedure for the GRANDPA protocol for a joining validator is described in detail in Initiate-Grandpa . The process of joining a new voter set is different from the one of rejoining the current voter set after a network disconnect. The details of this distinction are described further in this section. 6.2.1. Voter Set Changes ‚Äã A GRANDPA voter node which is initiating GRANDPA protocol as part of joining a new authority set is required to execute Initiate-Grandpa . The algorithm mandates the initialization procedure for GRANDPA protocol. info The GRANDPA round number reset to 0 for every authority set change. Voter set changes are signaled by Runtime via a consensus engine message ( Section 3.3.2. ). When Authorities process such messages they must not vote on any block with a higher number than the block at which the change is supposed to happen. The new authority set should reinitiate GRANDPA protocol by executing Initiate-Grandpa . Algorithm 13. Initiate Grandpa ‚Äã \begin{algorithm} \caption{Initiate-Grandpa} \begin{algorithmic} \input $r_{last}, B_{last}$ \state \textsc{Last-Finalized-Block} $\leftarrow B_{last}$ \state \textsc{Best-Final-Candidate}$(0) \leftarrow B_{last}$ \state \textsc{GRANDPA-GHOST}$(0) \leftarrow B_{last}$ \state \textsc{Last-Completed-Round}$ \leftarrow 0$ \state $r_n \leftarrow 1$ \state \call{Play-Grandpa-round}{$r_n$} \end{algorithmic} \end{algorithm} where B last {B}_{{\text{last}}} B last ‚Äã is the last block which has been finalized on the chain ( Definition 85 ). r last {r}_{{\text{last}}} r last ‚Äã is equal to the latest round the voter has observed that other voters are voting on. The voter obtains this information through various gossiped messages including those mentioned in Definition 85 . r last {r}_{{\text{last}}} r last ‚Äã is set to 0 if the GRANDPA node is initiating the GRANDPA voting process as a part of a new authority set. This is because the GRANDPA round number resets to 0 for every authority set change. 6.3. Rejoining the Same Voter Set ‚Äã When a voter node rejoins the network after a disconnect from the voter set and with the condition that there has been no change to the voter set at the time of the disconnect, the node must continue performing the GRANDPA protocol at the same state as before getting disconnected from the network, ignoring any possible progress in GRANDPA finalization. Following reconnection, the node eventually gets updated to the current GRANDPA round and synchronizes its state with the rest of the voting set through the process called Catchup ( Section 6.6.1. ). 6.4. Voting Process in Round r {r} r ‚Äã For each round r {r} r , an honest voter v {v} v must participate in the voting process by following Play-Grandpa-Round . Algorithm 14. Play Grandpa Round ‚Äã \begin{algorithm} \caption{Play-Grandpa-Round} \begin{algorithmic} \REQUIRE($r$) \STATE $t_{r, v} \leftarrow$ Current local time \STATE $\textrm{primary} \leftarrow$ \call{Derive-Primary}{$r$} \IF{$v = \textrm{primary}$} \STATE \call{Broadcast}{$M_{v}^{r - 1, \textrm{Fin}}($\call{Best-Final-Candidate}{$r - 1$}$)$} \IF{\call{Best-Final-Candidate}{$r - 1$} $\geqslant$ \textsc{Last-Finalized-Block}} \STATE \call{Broadcast}{$M_{v}^{r - 1, \textrm{Prim}}($\call{Best-Final-Candidate}{$r - 1$}$)$} \ENDIF \ENDIF \STATE \call{Receive-Messages}{\textbf{until} Time $\geqslant t_{r_,v} + 2 \times T$ \or $r$ \textbf{is} completable} \STATE $L \leftarrow$ \call{Best-Final-Candidate}{$r - 1$} \STATE $N \leftarrow$ \call{Best-PreVote-Candidate}{$r$} \STATE \call{Broadcast}{$M_v^{r, \textrm{pv}} (N)$} \STATE \call{Receive-Messages}{\textbf{until} $B^{r,\textrm{pv}}_v \geqslant L$ \and $($ Time $\geqslant t_{r_,v} + 4 \times T$ \or $r$ \textbf{is} completable $)$} \STATE \call{Broadcast}{$M_v^{r, \textrm{pc}}(B_v^{r, \textrm{pv}})$} \REPEAT \STATE \call{Receive-Messages}{} \STATE \call{Attempt-To-Finalize-At-Round}{$r$} \UNTIL{$r$ \textbf{is} completable \and \call{Finalizable}{$r$} \and \textsc{Last-Finalized-Block} $\geqslant$ \call{Best-Final-Candidate}{$r - 1$}} \STATE \call{Play-Grandpa-round}{$r + 1$} \REPEAT \STATE \call{Receive-Messages}{} \STATE \call{Attempt-To-Finalize-At-Round}{$r$} \UNTIL{\textsc{Last-Finalized-Block} $\geqslant$ \call{Best-Final-Candidate}{$r$}} \IF{$r > $ \textsc{Last-Completed-Round}} \STATE \textsc{Last-Completed-Round} $\leftarrow r$ \ENDIF \end{algorithmic} \end{algorithm} where T {T} T is sampled from a log-normal distribution whose mean and standard deviation are equal to the average network delay for a message to be sent and received from one validator to another. Derive-Primary \text{Derive-Primary} Derive-Primary is described in Derive-Primary . The condition of completablitiy is defined in Definition 81 . Best-Final-Candidate \text{Best-Final-Candidate} Best-Final-Candidate function is explained in Best-Final-Candidate . Attempt-To-Finalize-At-Round ( r ) \text{Attempt-To-Finalize-At-Round}{\left({r}\right)} Attempt-To-Finalize-At-Round ( r ) is described in Attempt-To-Finalize-At-Round . Finalizable \text{Finalizable} Finalizable is defined in Finalizable . Algorithm 15. Derive Primary ‚Äã \begin{algorithm} \caption{Derive-Primary} \begin{algorithmic} \input $r$ \return $r \bmod |\mathbb{V}|$ \end{algorithmic} \end{algorithm} where r {r} r is the GRANDPA round whose primary is to be determined. Algorithm 16. Best Final Candidate ‚Äã \begin{algorithm} \caption{Best-Final-Candidate} \begin{algorithmic} \input $r$ \state $B_v^{r, pv} \leftarrow$ \call{GRANDPA-GHOST}{$r$} \if{$r = 0$} \return $B_v^{r, pv}$ \else \state $\mathcal{C} \leftarrow \{ B' | B' \leqslant B_v^{r,pv} | \#V^{r, pc}_{\operatorname{obv}(v), pot}(B') > \frac{2}{3} |\mathbb{V}| \}$ \if{$\mathcal{C} = \phi$} \return $B_v^{r, pv}$ \else \return $E \in \mathcal{C} : H_n (E) = \operatorname{max}\left(H_n (B') | B' \in \mathcal{C}\right)$ \endif \endif \end{algorithmic} \end{algorithm} where # V obv ( v ) , p o t r , p c \#{{V}_{{\text{obv}{\left({v}\right)},{p}{o}{t}}}^{{{r},{p}{c}}}} # V obv ( v ) , p o t r , p c ‚Äã is defined in Definition 79 . Algorithm 17. GRANDPA GHOST ‚Äã \begin{algorithm} \caption{GRANDPA-GHOST} \begin{algorithmic} \input $r$ \if{$r = 0$} \state $G \leftarrow B_{last}$ \else \state $L \leftarrow$ \call{Best-Final-Candidate}{$r - 1$} \state $\mathcal{G} = \{ \forall B > L | \#V_{\operatorname{obs}(v)}^{r, pv}(B) \geqslant \frac{2}{3} |\mathbb{V}| \}$ \if{$\mathcal{G} = \phi$} \state $G \leftarrow L$ \else \state $G \in \mathcal{G} | H_n(G) = \operatorname{max}\left( H_n (B) | \forall B \in \mathcal{G} \right)$ \endif \endif \return $G$ \end{algorithmic} \end{algorithm} where B last {B}_{{\text{last}}} B last ‚Äã is the last block which has been finalized on the chain ( Definition 85 ). # V obs ( v ) r , p v ( B ) \#{{V}_{{\text{obs}{\left({v}\right)}}}^{{{r},{p}{v}}}}{\left({B}\right)} # V obs ( v ) r , p v ‚Äã ( B ) is defined in Definition 78 . Algorithm 18. Best PreVote Candidate ‚Äã \begin{algorithm} \caption{Best-PreVote-Candidate} \begin{algorithmic} \input $r$ \state $B^{r, pv}_v \leftarrow$ \call{GRANDPA-GHOST}{$r$} \if{\call{Received}{$M_{v_{primary}}^{r, prim}(B))$ \and $B^{r, pv}_v \geqslant B > L$}} \state $N \leftarrow B$ \else \state $N \leftarrow B^{r, pv}_v$ \endif \end{algorithmic} \end{algorithm} Algorithm 19. Attempt To Finalize At Round ‚Äã \begin{algorithm} \caption{Attempt-To-Finalize-At-Round} \begin{algorithmic} \REQUIRE($r$) \STATE $L \leftarrow$ \textsc{Last-Finalized-Block} \STATE $E \leftarrow$ \call{Best-Final-Candidate}{$r$} \IF{$E \geqslant L$ \and ${V^{r, \textrm{pc}}_{\textrm{obs}(v)}}(E) > 2 / 3 |\mathbb{V}|$} \STATE{\textsc{Last-Finalized-Block}$\leftarrow E$} \IF{$M_v^{r, \textrm{Fin}} (E) \notin $\textsc{Received-Messages}} \STATE \call{Broadcast}{$M_v^{r, \textrm{Fin}}(E)$} \RETURN \ENDIF \ENDIF \end{algorithmic} \end{algorithm} Algorithm 20. Finalizable ‚Äã \begin{algorithm} \caption{Finalizable} \begin{algorithmic} \REQUIRE($r$) \IF{$r$ \textbf{is not} Completable} \RETURN \textbf{False} \ENDIF \STATE $G \leftarrow$ \call{GRANDPA-GHOST}{$J^{r, pv}(B)$} \IF{$G = \phi$} \RETURN \textbf{False} \ENDIF \STATE $E_r \leftarrow$ \call{Best-Final-Candidate}{$r$} \IF{$E_r \neq \phi$ \and \call{Best-Final-Candidate}{$r - 1$} $\leqslant E_r \leqslant G$} \RETURN \textbf{True} \ELSE \RETURN \textbf{False} \ENDIF \end{algorithmic} \end{algorithm} where the condition for completability is defined in Definition 81 . Note that we might not always succeed in finalizing our best final candidate due to the possibility of equivocation. We might even not finalize anything in a round (although Play-Grandpa-Round prevents us from moving to the round r + 1 {r}+{1} r + 1 before finalizing the best final candidate of round r ‚àí 1 {r}-{1} r ‚àí 1 ) The example in Definition 83 serves to demonstrate a situation where the best final candidate of a round cannot be finalized during its own round: Definition 83. Unfinalized Candidate ‚Äã Let us assume that we have 100 voters and there are two blocks in the chain ( B 1 < B 2 {B}_{{1}}<{B}_{{2}} B 1 ‚Äã < B 2 ‚Äã ). At round 1, we get 67 pre-votes for B 2 {B}_{{2}} B 2 ‚Äã and at least one pre-vote for B 1 {B}_{{1}} B 1 ‚Äã which means that GRANDPA-GHOST ( 1 ) = B 2 \text{GRANDPA-GHOST}{\left({1}\right)}={B}_{{2}} GRANDPA-GHOST ( 1 ) = B 2 ‚Äã . Subsequently, potentially honest voters who could claim not seeing all the pre-votes for B 2 {B}_{{2}} B 2 ‚Äã but receiving the pre-votes for B 1 {B}_{{1}} B 1 ‚Äã would pre-commit to B 1 {B}_{{1}} B 1 ‚Äã . In this way, we receive 66 pre-commits for B 1 {B}_{{1}} B 1 ‚Äã and 1 pre-commit for B 2 {B}_{{2}} B 2 ‚Äã . Henceforth, we finalize B 1 {B}_{{1}} B 1 ‚Äã since we have a threshold commit (67 votes) for B 1 {B}_{{1}} B 1 ‚Äã . At this point, though, we have Best-Final-Candidate ( r ) = B 2 {\mathtt{\text{Best-Final-Candidate}}}{\left({r}\right)}={B}_{{2}} Best-Final-Candidate ( r ) = B 2 ‚Äã as # V obs ( v ) , pot r , stage ( B 2 ) = 67 \#{{V}_{{\text{obs}{\left({v}\right)},\text{pot}}}^{{{r},\text{stage}}}}{\left({B}_{{2}}\right)}={67} # V obs ( v ) , pot r , stage ‚Äã ( B 2 ‚Äã ) = 67 and 2 > 1 {2}>{1} 2 > 1 . However, at this point, the round is already completable as we know that we have GRANDPA-GHOST ( 1 ) = B 2 {\mathtt{\text{GRANDPA-GHOST}}}{\left({1}\right)}={B}_{{2}} GRANDPA-GHOST ( 1 ) = B 2 ‚Äã as an upper limit on what we can finalize and nothing greater than B 2 {B}_{{2}} B 2 ‚Äã can be finalized at r = 1 {r}={1} r = 1 . Therefore, the condition of Play-Grandpa-Round is satisfied and we must proceed to round 2. Nonetheless, we must continue to attempt to finalize round 1 in the background as the condition of Attempt-To-Finalize-At-Round has not been fulfilled. This prevents us from proceeding to round 3 until either: We finalize B 2 {B}_{{2}} B 2 ‚Äã in round 2, or We receive an extra pre-commit vote for B 1 {B}_{{1}} B 1 ‚Äã in round 1. This will make it impossible to finalize B 2 {B}_{{2}} B 2 ‚Äã in round 1, no matter to whom the remaining pre-commits are going to be cast for (even with considering the possibility of 1/3 of voter equivocating) and therefore we have Best-Final-Candidate ( r ) = B 1 {\mathtt{\text{Best-Final-Candidate}}}{\left({r}\right)}={B}_{{1}} Best-Final-Candidate ( r ) = B 1 ‚Äã . Both scenarios unblock Play-Grandpa-Round , Last-Finalized-Block ‚â• Best-Final-Candidate ( r ‚àí 1 ) {\mathtt{\text{Last-Finalized-Block}}}\ge{\mathtt{\text{Best-Final-Candidate}}}{\left({r}-{1}\right)} Last-Finalized-Block ‚â• Best-Final-Candidate ( r ‚àí 1 ) albeit in different ways: the former with increasing the Last-Finalized-Block {\mathtt{\text{Last-Finalized-Block}}} Last-Finalized-Block and the latter with decreasing Best-Final-Candidate ( r ‚àí 1 ) {\mathtt{\text{Best-Final-Candidate}}}{\left({r}-{1}\right)} Best-Final-Candidate ( r ‚àí 1 ) . 6.5. Forced Authority Set Changes ‚Äã In a case of emergency where the Polkadot network is unable to finalize blocks, such as in an event of mass validator outage, the Polkadot governance mechanism must enact a forced change, which the Host must handle in a specific manner. Given that in such a case finality cannot be relied on, the Host must detect the forced change ( Definition 82 ) in a (valid) block and apply it to all forks. The m ‚àà C M g {m}\in{C}{M}_{{g}} m ‚àà C M g ‚Äã , which is specified by the governance mechanism, defines the starting block at which N delay {N}_{{\text{delay}}} N delay ‚Äã is applied. This provides some degree of probabilistic consensus to the network with the assumption that the forced change was received by most participants and that finality can be continued. Image 6. Applying a scheduled change ‚Äã Image 7. Applying a forced change ‚Äã 6.6. Block Finalization ‚Äã Definition 84. Justified Block Header ‚Äã The Justified Block Header is provided by the consensus engine and presented to the Polkadot Host, for the block to be appended to the blockchain. It contains the following parts: block_header the complete block header ( Definition 10 ) and denoted by Head ( B ) \text{Head}{\left({B}\right)} Head ( B ) . justification : as defined by the consensus specification indicated by Just ( B ) \text{Just}{\left({B}\right)} Just ( B ) as defined in Definition 74 . authority Ids : This is the list of the Ids of authorities, which have voted for the block to be stored and is formally referred to as A ( B ) {A}{\left({B}\right)} A ( B ) . An authority Id is 256-bit. Definition 85. Finalized ‚Äã A Polkadot relay chain node n {n} n should consider block B {B} B as finalized if any of the following criteria hold for B ‚Ä≤ ‚â• B {B}'\ge{B} B ‚Ä≤ ‚â• B : V obs ( n ) r , pc ( B ‚Ä≤ ) > 2 3 ‚à£ V B ‚Ä≤ ‚à£ {{V}_{{\text{obs}{\left({n}\right)}}}^{{{r},\text{pc}}}}{\left({B}'\right)}>\frac{{2}}{{3}}{\left|{\mathbb{{V}}}_{{{B}'}}\right|} V obs ( n ) r , pc ‚Äã ( B ‚Ä≤ ) > 3 2 ‚Äã ‚à£ V B ‚Ä≤ ‚Äã ‚à£ . It receives a M v r , Fin ( B ‚Ä≤ ) {{M}_{{v}}^{{{r},\text{Fin}}}}{\left({B}'\right)} M v r , Fin ‚Äã ( B ‚Ä≤ ) message in which J r ( B ) {J}^{{r}}{\left({B}\right)} J r ( B ) justifies the finalization ( Definition 74 ). It receives a block data message for B ‚Ä≤ {B}' B ‚Ä≤ with Just ( B ‚Ä≤ ) \text{Just}{\left({B}'\right)} Just ( B ‚Ä≤ ) ( Definition 84 ) which justifies the finalization. for: Any round r {r} r if the node n {n} n is not a GRANDPA voter. Only for round r {r} r for which the node n {n} n has invoked Play-Grandpa-Round and round r + 1 {r}+{1} r + 1 if n {n} n is a GRANDPA voter and has already caught up to its peers according to the process described in Section Section 6.6.1. . Note that all Polkadot relay chain nodes are supposed to process GRANDPA commit messages regardless of their GRANDPA voter status. 6.6.1. Catching up ‚Äã When a Polkadot node (re)joins the network, it requests the history of state transitions in the form of blocks, which it is missing. Nonetheless, the process is different for a GRANDPA voter node. When a voter node joins the network, it needs to gather the justification ( Definition 74 ) of the rounds it has missed. Through this process, they can safely join the voting process of the current round, on which the voting is taking place. 6.6.1.1. Sending the catch-up requests ‚Äã When a Polkadot voter node has the same authority list as a peer voter node who is reporting a higher number for the finalized round field, it should send a catch-up request message ( Definition 48 ) to the reporting peer. This will allow the node to to catch up to the more advanced finalized round, provided that the following criteria hold: The peer node is a GRANDPA voter, and: The last known finalized round for the Polkadot node is at least 2 rounds behind the finalized round for the peer. 6.6.1.2. Processing the catch-up requests ‚Äã Only GRANDPA voter nodes are required to respond to the catch-up requests. Additionally, it is only GRANDPA voters who are supposed to send catch-up requests. As such GRANDPA voters could safely ignore the catch-up requests from non-voter nodes. When a GRANDPA voter node receives a catch-up request message, it needs to execute Process-Catchup-Request . Note: a voter node should not respond to catch-up requests for rounds that are actively being voted on, those are the rounds for which Play-Grandpa-Round is not concluded. Algorithm 21. Process Catchup Request ‚Äã \begin{algorithm} \caption{Process-Catchup-Request} \begin{algorithmic} \input $M_{i, v}^\text{Cat-q}(\text{id}_\mathbb{V}, r)$ \if{$M_{i, v}^\text{Cat-q}(\text{id}_\mathbb{V}, r).\text{id}_\mathbb{V} \neq \text{id}_\mathbb{V}$} \state \textbf{error} ``Catching up on different set'' \endif \if{$i \notin \mathbb{P}$} \state \textbf{error} ``Requesting catching up from a non-peer'' \endif \if{$r >$ \textsc{Last-Completed-Round}} \state \textbf{error} ``Catching up on a round in the future'' \endif \state \call{Send}{$i, M_{v, i}^\text{Cat-s}(\text{id}_\mathbb{V}, r)$} \end{algorithmic} \end{algorithm} where M i , v Cat ‚àí q ( id V , r ) {{M}_{{{i},{v}}}^{{\text{Cat}-{q}}}}{\left(\text{id}_{{{\mathbb{{V}}}}},{r}\right)} M i , v Cat ‚àí q ‚Äã ( id V ‚Äã , r ) is the catch-up message received from peer i {i} i ( Definition 48 ). id V \text{id}_{{{\mathbb{{V}}}}} id V ‚Äã ( Definition 69 ) is the voter set id with which the serving node is operating r {r} r is the round number for which the catch-up is requested for. P {\mathbb{{P}}} P is the set of immediate peers of node v {v} v . Last-Completed-Round {\mathtt{\text{Last-Completed-Round}}} Last-Completed-Round is initiated in Initiate-Grandpa and gets updated by Play-Grandpa-Round . M v , i Cat ‚àí s ( id V , r ) {{M}_{{{v},{i}}}^{{\text{Cat}-{s}}}}{\left(\text{id}_{{{\mathbb{{V}}}}},{r}\right)} M v , i Cat ‚àí s ‚Äã ( id V ‚Äã , r ) is the catch-up response ( Definition 49 ). 6.6.1.3. Processing catch-up responses ‚Äã A Catch-up response message contains critical information for the requester node to update their view on the active rounds which are being voted on by GRANDPA voters. As such, the requester node should verify the content of the catch-up response message and subsequently updates its view of the state of the finality of the Relay chain according to Process-Catchup-Response . Algorithm 22. Process Catchup Response ‚Äã \begin{algorithm} \caption{Process-Catchup-Response} \begin{algorithmic} \input $M_{v,i}^\text{Cat-s}(\text{id}_{\mathbb{V}}, r)$ \state $M_{v,i}^\text{Cat-s}(\text{id}_{\mathbb{V}}, r).\text{id}_{\mathbb{V}}, r, J^{r, pv}(B), J^{r, pc}(B), H_h(B'), H_i(B') \leftarrow \text{Dec}_{SC}(M_{v, i}^{Cat-s}(\text{id}_{\mathbb{V}}, r)$ \if{$M_{v, i}^\text{Cat-s}(\text{id}_{\mathbb{V}}, r).\text{id}_{\mathbb{V}} \neq \text{id}_{\mathbb{V}}$} \state \textbf{error} ``Catching up on different set'' \endif \if{$r \leqslant$ \textsc{Leading-Round}} \state \textbf{error} ``Catching up in to the past'' \endif \if{$J^{r, pv}(B)$ \textbf{is not} valid} \state \textbf{error} ``Invalid pre-vote justification'' \endif \if{$J^{r, pc}(B)$ \textbf{is not} valid} \state \textbf{error} ``Invalid pre-commit justification'' \endif \state $G \leftarrow$ \call{GRANDPA-GHOST}{$J^{r, pv}(B)$} \if{$G = \phi$} \state \textbf{error} ``GHOST-less Catch-up'' \endif \if{$r$ \textbf{is not} completable} \state \textbf{error} ``Catch-up round is not completable'' \endif \if{$J^{r, pc}(B)$ justifies $B'$ finalization} \state \textbf{error} ``Unjustified Catch-up target finalization'' \endif \state \textsc{Last-Completed-Round} $\leftarrow r$ \if{$i \in \mathbb{V}$} \state \call{Play-Grandpa-round}{$r + 1$} \endif \end{algorithmic} \end{algorithm} where M v , i Cat ‚àí s ( id V , r ) {{M}_{{{v},{i}}}^{{\text{Cat}-{s}}}}{\left(\text{id}_{{{\mathbb{{V}}}}},{r}\right)} M v , i Cat ‚àí s ‚Äã ( id V ‚Äã , r ) is the catch-up response received from node v {v} v ( Definition 49 ). Previous 5. Block Production Next 7. Light Clients 6.1. Introduction 6.2. Initiating the GRANDPA State 6.2.1. Voter Set Changes 6.3. Rejoining the Same Voter Set 6.4. Voting Process in Round {r} 6.5. Forced Authority Set Changes 6.6. Block Finalization 6.6.1. Catching up 
7. Light Clients | Polkadot Protocol Specification Host 7. Light Clients On this page 7. Light Clients 7.1. Requirements for Light Clients ‚Äã We list the requirements of a Light Client categorized along the three dimensions of Functionality, Efficiency, and Security. Functional Requirements: Update state ( Section 2.4. ) to reflect the latest view of the blockchain via synchronization with full nodes. (Optional) Verify validity of runtime transitions ( Section 2.6. ). Make queries for data at the latest block height or across a range of blocks. Append extrinsics ( Section 2.3. ) to the blockchain via full nodes. Efficiency Requirements: Efficient bootstrapping and syncing: initializations and update functions of the state have tractable computation and communication complexity and grows at most linearly with the chain size. Generally, the complexity is proportional to the GRANDPA validator set change. Querying operations happen by requesting the key-value pair from a full node. Further, verifying the validity of responses by the full node is logarithmic in the size of the state. Security Requirements: Secure bootstrapping and Synchronizing: The probability that an adversarial full node convinces a light client of a forged blockchain state is negligible. Secure querying: The probability that an adversary convinces a light client to accept a forged account state is negligible. Assure that the submitted extrinsics are appended in a successor block or inform the user in case of failure. Polkadot Specific Requirements: The client MUST be able to connect to a relay chain using chain state. The client MUST be able to retrieve the checkpoint state from a trusted source to speed up initialization. The client MUST be able to subscribe/unsubscribe to/from any polkadot-spec-conformant relay chain (Polkadot, Westend, Kusama) The client MUST be able to subscribe/unsubscribe to/from parachains that do not use custom protocols or cryptography methods other than those that Polkadot, Westend and Kusama use. The client MUST support the following RPC methods : rpc_methods , chainHead_unstable_follow , chainHead_unstable_unfollow , chainHead_unstable_unpin , chainHead_unstable_storage , chainHead_unstable_call chainHead_unstable_stopCall . transaction_unstable_submitAndWatch , and transaction_unstable_unwatch The client MUST support the @substrate/connect connection extension protocol : ToApplicationError , ToApplicationChainReady , ToApplicationRpc , ToExtensionAddChain , ToExtensionAddWellKnownChain , ToExtensionRpc , ToExtensionRemoveChain . 7.2. Warp Sync for Light Clients ‚Äã Warp sync ( Section 4.8.4. ) only downloads the block headers where authority set changes occurred, so-called fragments ( Definition 41 ), and by verifying the GRANDPA justifications ( Definition 74 ). This protocol allows nodes to arrive at the desired state much faster than fast sync. Warp sync is primarily designed for Light Clients. Although, warp sync could be used by full nodes, the sync process may lack information to cater to complete functionality set of full nodes. For light clients, it is too expensive to download the state (approx. 550MB) to respond to queries. Rather, the queries are submitted to the Full node, and only the response of the full node is validated using the hash of the state root. Requests for warp sync are performed using the /dot/sync/warp Request-Response substream, the corresponding network messages are detailed in Section 4.7. . Light clients base their trust in provided snapshots and the ability to slash grandpa votes for equivocation for the period they are syncing via warp sync. Full nodes and above, in contrast, verify each block individually. In theory, the warp sync process takes the Genesis Block as input and outputs the hash of the state trie root at the latest finalized block. This root hash acts as proof to further validate the responses to queries by the full node. The warp sync works by starting from a trusted specified block (e.g., from a snapshot) and verifying the block headers only at the authority set changes. Eventually, the light client verifies the finality of the block returned by a full node to ensure that the block is indeed the latest finalized block. This entails two things: Check the authenticity of GRANDPA Justifications messages from Genesis to the last finalized block. Check the timestamp of the last finalized block to ensure that no other blocks might have been finalized at a later timestamp. caution Long-Range Attack Vulnerabilities : Warp syncing is particularly vulnerable to what is called long-range attacks. The authorities allowed to finalize blocks can generate multiple proofs of finality for multiple different blocks of the same height. Hence, they can finalize more than one chain at a time. It is possible for two-thirds of the validators that were active at a certain past block N to collude and decide to finalize a different block N', even when N has been finalized for the first time several weeks or months in the past. When a client then warp syncs, it can be tricked to consider this alternative block N' as the finalized one. However, in practice, to mitigate Long-Range Attacks, the starting point of the warp syncing is not too far in the past. How far exactly depends on the logic of the runtime of the chain. For example, in Polkadot, the starting block for the sync should be at max 28 days old to be within the purview of the slashing period for misbehaving nodes. Hence, even though, in theory, warp sync can start from Genesis Block, it is not advised to implement the same in practice. We outline the warp sync process, abstracting out details of verifying the finality and how the full node to sync with is selected. Algorithm 23. Warp Sync Light Clients ‚Äã \begin{algorithm} \caption{Warp-Sync-Light-Clients} \begin{algorithmic} \INPUT BlockHeader startblock, the initial block to start the sync. May not be the Genesis Block. \OUTPUT CommitmentRootHash $root$, State Tries Root hash of the latest finalized Block. \STATE \textsc{fullnode} $\leftarrow$ SelectFullNode \STATE \textsc{latestBlockHeader, grandpaJustifications} $\leftarrow$ SyncWithNode(\textsc{fullnode}) \STATE \textsc{isVerified} $\leftarrow$ verifyAuthoritySetChange(\textsc{grandpaJustifications}) $\land$ verifyFinality(\textsc{latestBlockHeader}) \IF{\textsc{isVerified}} \STATE $return$ $SOME$ getCommitmentRootHash(\textsc{latestBlockHeader}) \ENDIF \STATE {$throw$ $ERROR$} \end{algorithmic} \end{algorithm} Abstraction of Warp Sync and verification of the latest block‚Äôs finality. S e l e c t F u l l N o d e {SelectFullNode} S e l ec tF u llN o d e : Determines the full node that the light client syncs with. S y n c S i t h N o d e {SyncSithNode} S y n c S i t h N o d e : Returns the header of the latest finalized block and a list of Grandpa Justifications by the full node. v e r i f y A u t h o r i t y S e t C h a n g e {verifyAuthoritySetChange} v er i f y A u t h or i t y S e tC han g e : Verification algorithm which checks the authenticity of the header only at the end of an era where the authority set changes iteratively until reaching the latest era. v e r i f y F i n a l t y {verifyFinalty} v er i f y F ina lt y : Verifies the finality of the latest block using the Grandpa Justifications messages. The warp syncing process is closely coupled with the state querying procedure used by the light client. We outline the process of querying the state by a light client and validating the response. Algorithm 24. Querying State Light Clients ‚Äã \begin{algorithm} \caption{Querying-State-Light-Clients} \begin{algorithmic} \INPUT Query q, BlockHeight h, CommitmentRootHash $root$ \OUTPUT Maybe Result $res$ \STATE ($res$, $\pi$) $\leftarrow QueryFullNode (q, h)$ \IF{$validityCheck_{root}(res, \pi)$} \STATE $return$ $SOME$ $res$ \ENDIF \STATE {$throw$ $ERROR$} \end{algorithmic} \end{algorithm} Querying State Algorithm. Q u e r y F u l l N o d e {QueryFullNode} Q u ery F u llN o d e : Returns the response to the query requested from the Full Node for the query q {q} q at block height h {h} h . v a l i d i t y C h e c k r o o t {validityCheck}_{root} v a l i d i t y C h ec k roo t ‚Äã : Predicate that checks the validity of response r e s {res} res and associated merkle proof œÄ \pi œÄ by matching it against the Commit Root Hash r o o t {root} roo t obtained as a result of warp sync. 7.3. Runtime Environment for Light Clients ‚Äã Technically, though a runtime execution environment is not necessary to build a light client, most clients require interacting with the Runtime and the state of the blockchain for integrity checks at the minimum. One can imagine an application scenario like an on-chain light client which only listens to the latest state without ever adding extrinsics. Current implementations of Light Nodes (for e.g., Smoldot) use the wasmtime as its runtime environment to drastically simplify the code. The performance of wasmtime is satisfying enough not to require a native runtime. The details of the runtime API that the environment needs to support can be found in ( Appendix C ). 7.4. Light Client Messages ‚Äã Light clients are applications that fetch the required data that they need from a Polkadot node with an associated proof to validate the data. This makes it possible to interact with the Polkadot network without requiring to run a full node or having to trust the remote peers. The light client messages make this functionality possible. All light client messages are protobuf encoded and are sent over the /dot/light/2 substream. 7.4.1. Request ‚Äã A message with all possible request messages. All messages are sent as part of this message. Type Id Description oneof ( request ) The request type Where the request can be one of the following fields: Type Id Description RemoteCallRequest 1 A remote call request ( Definition 86 ) RemoteReadRequest 2 A remote read request ( Definition 88 ) RemoteReadChildRequest 4 A remote read child request ( Definition 90 ) 7.4.2. Response ‚Äã A message with all possible response messages. All messages are sent as part of this message. Type Id Description oneof ( response ) The response type Where the response can be one of the following fields: Type Id Description RemoteCallResponse 1 A remote call response ( Definition 87 ) RemoteReadResponse 2 A remote read response ( Definition 89 ) 7.4.3. Remote Call Messages ‚Äã Execute a call to a contract at the given block. Definition 86. Remote Call Request ‚Äã Remote call request. Type Id Description bytes 2 Block at which to perform call string 3 Method name bytes 4 Call data Definition 87. Remote Call Response ‚Äã Remote call response. Type Id Description bytes 2 An Option type ( Definition 180 ) containing the call proof or None if proof generation failed. 7.4.4. Remote Read Messages ‚Äã Read a storage value at the given block. Definition 88. Remote Read Request ‚Äã Remote read request. Type Id Description bytes 2 Block at which to perform call repeated bytes 3 Storage keys Definition 89. Remote Read Response ‚Äã Remote read response. Type Id Description bytes 2 An Option type ( Definition 180 ) containing the read proof or None if proof generation failed. 7.4.5. Remote Read Child Messages ‚Äã Read a child storage value at the given block. Definition 90. Remote Read Child Request ‚Äã Remote read child request. Type Id Description bytes 2 Block at which to perform call bytes 3 Child storage key, this is relative to the child type storage location bytes 6 Storage keys The response is the same as for the Remote Read Request message, respectively Definition 89 . 7.5. Storage for Light Clients ‚Äã The light client requires a persistent storage for saving the state of the blockchain. In addition, it requires efficient Serialization/De-serialization methods to transform SCALE ( Section A.2.2. ) encoded network traffic for storing and reading from the persistent storage. Previous 6. Finality Next 8. Availability & Validity 7.1. Requirements for Light Clients 7.2. Warp Sync for Light Clients 7.3. Runtime Environment for Light Clients 7.4. Light Client Messages 7.4.1. Request 7.4.2. Response 7.4.3. Remote Call Messages 7.4.4. Remote Read Messages 7.4.5. Remote Read Child Messages 7.5. Storage for Light Clients 
8. Availability & Validity | Polkadot Protocol Specification Host 8. Availability & Validity On this page 8. Availability & Validity Polkadot serves as a replicated shared-state machine designed to resolve scalability issues and interoperability among blockchains. The validators of Polkadot execute transactions and participate in the consensus of Polkadots primary chain, the so-called relay chain. Parachains are independent networks that maintain their own state and are connected to the relay chain. Those parachains can take advantage of the relay chain consensus mechanism, including sending and receiving messages to and from other parachains. Parachain nodes that send parachain blocks, known as candidates, to the validators in order to be included in relay chain are referred to as collators. The Polkadot relay chain validators are responsible for guaranteeing the validity of both relay chain and parachain blocks. Additionally, the validators are required to keep enough parachain blocks that should be included in the relay chain available in their local storage in order to make those retrievable by peers, who lack the information to reliably confirm the issued validity statements about parachain blocks. The Availability & Validity (AnV) protocol consists of multiple steps for successfully upholding those responsibilities. Parachain blocks themselves are produced by collators ( Section 8.1. ), whereas the relay chain validators only verify their validity (and later, their availability). It is possible that the collators of a parachain produce multiple parachain block candidates for a child of a specific block. Subsequently, they send the block candidates to the relay chain validators who are assigned to the specific parachain. The assignment is determined by the Runtime ( Section 8.2. ). Those validators are then required to check the validity of submitted candidates ( Section 8.3. ), then issue and collect statements ( Section 8.2.1. ) about the validity of candidates to other validators. This process is known as candidate backing. Once a candidate meets specified criteria for inclusion, the selected relay chain block author then chooses any of the backed candidates for each parachain and includes those into the relay chain block ( Section 8.2.2. ). Every relay chain validator must fetch the proposed candidates and issue votes on whether they have the candidate saved in their local storage, so-called availability votes ( Section 8.4.1. ), then also collect the votes sent by other validators and include them in the relay chain state ( Section 8.2.2. ). This process ensures that only relay chain blocks get finalized where each candidate is available on enough nodes of validators. Parachain candidates contained in non-finalized relay chain blocks must then be retrieved by a secondary set of relay chain validators, unrelated from the candidate backing process, who are randomly assigned to determine the validity of specific parachains based on a VRF lottery and are then required to vote on the validity of those candidates. This process is known as approval voting ( Section 8.5. ). If a validator does not have the candidate data, it must recover the candidate data ( Section 8.4.2. ). 8.1. Collations ‚Äã Collations are proposed candidates Definition 121 to the Polkadot relay chain validators. The Polkodat network protocol is agnostic on what candidate production mechanism each parachain uses and does not specify or mandate any of such production methods (e.g. BABE-GRANDPA, Aura, etc). Furthermore, the relay chain validator host implementation itself does not directly interpret or process the internal transactions of the candidate but rather rely on the parachain Runtime to validate the candidate ( Section 8.3. ). Collators, which are parachain nodes which produce candidate proposals and send them to the relay chain validator, must prepare pieces of data ( Definition 91 ) in order to correctly comply with the requirements of the parachain protocol. Definition 91. Collation ‚Äã A collation is a data structure that contains the proposed parachain candidate, including an optional validation parachain Runtime update and upward messages. The collation data structure, C, is a data structure of the following format: C = ( M , H , R , h , P , p , w ) {C}={\left({M},{H},{R},{h},{P},{p},{w}\right)} C = ( M , H , R , h , P , p , w ) M = ( u n , ‚Ä¶ u m ) {M}={\left({u}_{{n}},‚Ä¶{u}_{{m}}\right)} M = ( u n ‚Äã , ‚Ä¶ u m ‚Äã ) H = ( z n , ‚Ä¶ z m ) {H}={\left({z}_{{n}},‚Ä¶{z}_{{m}}\right)} H = ( z n ‚Äã , ‚Ä¶ z m ‚Äã ) where M {M} M is an array of upward messages ( Definition 127 ), u {u} u , interpreted by the relay chain itself. H {H} H is an array of outbound horizontal messages ( Definition 129 ), z {z} z , interpreted by other parachains. R {R} R is an Option type ( Definition 180 ) which can contain a parachain Runtime update. The new Runtime code is an array of bytes. h {h} h is the head data ( Definition 123 ) produced as a result of execution of the parachain specific logic. P {P} P is the PoV block ( Definition 122 ). p {p} p is an unsigned 32-bit integer indicating the number of processed downward messages ( Definition 128 ). w {w} w is an unsigned 32-bit integer indicating the mark up to which all inbound HRMP messages have been processed by the parachain. 8.2. Candidate Backing ‚Äã The Polkadot validator receives an arbitrary number of parachain candidates with associated proofs from untrusted collators. The assigned validators of each parachain ( Definition 126 ) must verify and select a specific quantity of the proposed candidates and issue those as backable candidates to their peers. A candidate is considered backable when at least 2/3 of all assigned validators have issued a Valid statement about that candidate, as described in Section 8.2.1. . Validators can retrieve information about assignments via the Runtime APIs Section C.9.2. respectively Section C.9.3. . 8.2.1. Statements ‚Äã The assigned validator checks the validity of the proposed parachains blocks ( Section 8.3. ) and issues Valid statements ( Definition 92 ) to its peers if the verification succeeded. Broadcasting failed verification as Valid statements is a slashable offense. The validator must only issue one Seconded statement based on an arbitrary metric, which implies an explicit vote for a candidate to be included in the relay chain. This protocol attempts to produce as many backable candidates as possible but does not attempt to determine a final candidate for inclusion. Once a parachain candidate has been seconded by at least one other validator, and enough Valid statements have been issued about that candidate to meet the 2/3 quorum, the candidate is ready to be included in the relay chain ( Section 8.2.2. ). The validator issues validity statements votes in form of a validator protocol message ( Definition 104 ). Definition 92. Statement ‚Äã A statement, S {S} S , is a data structure of the following format: S = ( d , A i , A s ) {S}={\left({d},{A}_{{i}},{A}_{{s}}\right)} S = ( d , A i ‚Äã , A s ‚Äã ) d = { 1 ‚Üí C r 2 ‚Üí C h {d}={\left\lbrace\begin{matrix}{1}&\rightarrow&{C}_{{r}}\\{2}&\rightarrow&{C}_{{h}}\end{matrix}\right.} d = { 1 2 ‚Äã ‚Üí ‚Üí ‚Äã C r ‚Äã C h ‚Äã ‚Äã where d {d} d is a varying datatype where 1 indicates that the validator ‚Äúseconds‚Äù a candidate, meaning that the candidate should be included in the relay chain, followed by the committed candidate receipt ( Definition 95 ), C r {C}_{{r}} C r ‚Äã . 2 indicates that the validator has deemed the candidate valid, followed by the candidate hash. C h {C}_{{h}} C h ‚Äã is the candidate hash. A i {A}_{{i}} A i ‚Äã is the validator index in the authority set that signed this statement. A s {A}_{{s}} A s ‚Äã is the signature of the validator. 8.2.2. Inclusion ‚Äã The Polkadot validator includes the backed candidates as parachain inherent data ( Definition 93 ) into a block as described Section 2.3.3. . The relay chain block author decides on whatever metric which candidate should be selected for inclusion, as long as that candidate is valid and meets the validity quorum of 2/3+ as described in Section 8.2.1. . The candidate approval process ( Section 8.5. ) ensures that only relay chain blocks are finalized where each candidate for each availability core meets the requirement of 2/3+ availability votes. Definition 93. Parachain Inherent Data ‚Äã The parachain inherent data contains backed candidates and is included when authoring a relay chain block. The data structure, I {I} I , is of the following format: I = ( A , T , D , P h ) {I}={\left({A},{T},{D},{P}_{{h}}\right)} I = ( A , T , D , P h ‚Äã ) T = ( C 0 , ‚Ä¶ C n ) {T}={\left({C}_{{0}},‚Ä¶{C}_{{n}}\right)} T = ( C 0 ‚Äã , ‚Ä¶ C n ‚Äã ) D = ( d n , ‚Ä¶ d m ) {D}={\left({d}_{{n}},‚Ä¶{d}_{{m}}\right)} D = ( d n ‚Äã , ‚Ä¶ d m ‚Äã ) C = ( R , V , i ) {C}={\left({R},{V},{i}\right)} C = ( R , V , i ) V = ( a n , ‚Ä¶ a m ) {V}={\left({a}_{{n}},‚Ä¶{a}_{{m}}\right)} V = ( a n ‚Äã , ‚Ä¶ a m ‚Äã ) a = { 1 ‚Üí s 2 ‚Üí s {a}={\left\lbrace\begin{matrix}{1}&\rightarrow&{s}\\{2}&\rightarrow&{s}\end{matrix}\right.} a = { 1 2 ‚Äã ‚Üí ‚Üí ‚Äã s s ‚Äã A = ( L n , ‚Ä¶ L m ) {A}={\left({L}_{{n}},‚Ä¶{L}_{{m}}\right)} A = ( L n ‚Äã , ‚Ä¶ L m ‚Äã ) L = ( b , v i , s ) {L}={\left({b},{v}_{{i}},{s}\right)} L = ( b , v i ‚Äã , s ) where A {A} A is an array of signed bitfields by validators claiming the candidate is available (or not). The array must be sorted by validator index corresponding to the authority set ( Definition 33 ). T {T} T is an array of backed candidates for including in the current block. D {D} D is an array of disputes. P h {P}_{{h}} P h ‚Äã is the parachain parent head data ( Definition 123 ). d {d} d is a dispute statement ( Section 8.7.2.1. ). R {R} R is a committed candidate receipt ( Definition 95 ). V {V} V is an array of validity votes themselves, expressed as signatures. i {i} i is a bitfield of indices of the validators within the validator group ( Definition 126 ). a {a} a is either an implicit or explicit attestation of the validity of a parachain candidate, where 1 implies an implicit vote (in correspondence of a Seconded statement) and 2 implies an explicit attestation (in correspondence of a Valid statement). Both variants are followed by the signature of the validator. s {s} s is the signature of the validator. b {b} b the availability bitfield ( Section 8.4.1. ). v i {v}_{{i}} v i ‚Äã is the validator index of the authority set ( Definition 33 ). Definition 94. Candidate Receipt ‚Äã A candidate receipt, R {R} R , contains information about the candidate and a proof of the results of its execution. It‚Äôs a data structure of the following format: R = ( D , C h ) {R}={\left({D},{C}_{{h}}\right)} R = ( D , C h ‚Äã ) where D {D} D is the candidate descriptor ( Definition 96 ) and C h {C}_{{h}} C h ‚Äã is the hash of candidate commitments ( Definition 97 ). Definition 95. Committed Candidate Receipt ‚Äã The committed candidate receipt, R {R} R , contains information about the candidate and the result of its execution that is included in the relay chain. This type is similar to the candidate receipt ( Definition 94 ), but actually contains the execution results rather than just a hash of it. It‚Äôs a data structure of the following format: R = ( D , C ) {R}={\left({D},{C}\right)} R = ( D , C ) where D {D} D is the candidate descriptor ( Definition 96 ) and C {C} C is the candidate commitments ( Definition 97 ). Definition 96. Candidate Descriptor ‚Äã The candidate descriptor, D {D} D , is a unique descriptor of a candidate receipt. It‚Äôs a data structure of the following format: D = ( p , H , C i , V , B , r , s , p h , R h ) {D}={\left({p},{H},{C}_{{i}},{V},{B},{r},{s},{p}_{{h}},{R}_{{h}}\right)} D = ( p , H , C i ‚Äã , V , B , r , s , p h ‚Äã , R h ‚Äã ) where p {p} p is the parachain Id ( Definition 124 ). H {H} H is the hash of the relay chain block the candidate is executed in the context of. C i {C}_{{i}} C i ‚Äã is the collators public key. V {V} V is the hash of the persisted validation data ( Definition 220 ). B {B} B is the hash of the PoV block. r {r} r is the root of the block‚Äôs erasure encoding Merkle tree. s {s} s the collator signature of the concatenated components p {p} p , H {H} H , R h {R}_{{h}} R h ‚Äã and B {B} B . p h {p}_{{h}} p h ‚Äã is the hash of the parachain head data ( Definition 123 ) of this candidate. R h {R}_{{h}} R h ‚Äã is the hash of the parachain Runtime. Definition 97. Candidate Commitments ‚Äã The candidate commitments, C {C} C , is the result of the execution and validation of a parachain (or parathread) candidate whose produced values must be committed to the relay chain. Those values are retrieved from the validation result ( Definition 99 ). A candidate commitment is a datastructure of the following format: C = ( M u , M h , R , h , p , w ) {C}={\left({M}_{{u}},{M}_{{h}},{R},{h},{p},{w}\right)} C = ( M u ‚Äã , M h ‚Äã , R , h , p , w ) where M u {M}_{{u}} M u ‚Äã is an array of upward messages sent by the parachain. Each individual message, m, is an array of bytes. M h {M}_{{h}} M h ‚Äã is an array of individual outbound horizontal messages ( Definition 129 ) sent by the parachain. R {R} R is an Option value ( Definition 180 ) that can contain a new parachain Runtime in case of an update. h {h} h is the parachain head data ( Definition 123 ). p {p} p is an unsigned 32-bit integer indicating the number of downward messages that were processed by the parachain. It is expected that the parachain processes the messages from first to last. w {w} w is an unsigned 32-bit integer indicating the watermark, which specifies the relay chain block number up to which all inbound horizontal messages have been processed. 8.3. Candidate Validation ‚Äã Received candidates submitted by collators and must have their validity verified by the assigned Polkadot validators. For each candidate to be valid, the validator must successfully verify the following conditions in the following order: The candidate does not exceed any parameters in the persisted validation data ( Definition 220 ). The signature of the collator is valid. Validate the candidate by executing the parachain Runtime ( Section 8.3.1. ). If all steps are valid, the Polkadot validator must create the necessary candidate commitments ( Definition 97 ) and submit the appropriate statement for each candidate ( Section 8.2.1. ). 8.3.1. Parachain Runtime ‚Äã Parachain Runtimes are stored in the relay chain state, and can either be fetched by the parachain Id or the Runtime hash via the relay chain Runtime API as described in Section C.9.8. and Section C.9.9. respectively. The retrieved parachain Runtime might need to be decompressed based on the magic identifier as described in Section 8.3.2. . In order to validate a parachain block, the Polkadot validator must prepare the validation parameters ( Definition 98 ), then use its local Wasm execution environment ( Section 2.6.3. ) to execute the validate_block parachain Runtime API by passing on the validation parameters as an argument. The parachain Runtime function returns the validation result ( Definition 99 ). Definition 98. Validation Parameters ‚Äã The validation parameters structure, P {P} P , is required to validate a candidate against a parachain Runtime. It‚Äôs a data structure of the following format: P = ( h , b , B i , S r ) {P}={\left({h},{b},{B}_{{i}},{S}_{{r}}\right)} P = ( h , b , B i ‚Äã , S r ‚Äã ) where h {h} h is the parachain head data ( Definition 123 ). b {b} b is the block body ( Definition 122 ). B i {B}_{{i}} B i ‚Äã is the latest relay chain block number. S r {S}_{{r}} S r ‚Äã is the relay chain block storage root ( Section 2.4.4. ). Definition 99. Validation Result ‚Äã The validation result is returned by the validate_block parachain Runtime API after attempting to validate a parachain block. Those results are then used in candidate commitments ( Definition 97 ), which then will be inserted into the relay chain via the parachain inherent data ( Definition 93 ). The validation result, V {V} V , is a data structure of the following format: V = ( h , R , M u , M h , p , w ) {V}={\left({h},{R},{M}_{{u}},{M}_{{h}},{p}_{,}{w}\right)} V = ( h , R , M u ‚Äã , M h ‚Äã , p , ‚Äã w ) M u = ( m 0 , ‚Ä¶ m n ) {M}_{{u}}={\left({m}_{{0}},‚Ä¶{m}_{{n}}\right)} M u ‚Äã = ( m 0 ‚Äã , ‚Ä¶ m n ‚Äã ) M h = ( t 0 , ‚Ä¶ t n ) {M}_{{h}}={\left({t}_{{0}},‚Ä¶{t}_{{n}}\right)} M h ‚Äã = ( t 0 ‚Äã , ‚Ä¶ t n ‚Äã ) where h {h} h is the parachain head data ( Definition 123 ). R {R} R is an Option value ( Definition 180 ) that can contain a new parachain Runtime in case of an update. M u {M}_{{u}} M u ‚Äã is an array of upward messages sent by the parachain. Each individual message, m, is an array of bytes. M h {M}_{{h}} M h ‚Äã is an array of individual outbound horizontal messages ( Definition 129 ) sent by the parachain. p {p} p is an unsigned 32-bit integer indicating the number of downward messages that were processed by the parachain. It is expected that the parachain processes the messages from first to last. w {w} w is an unsigned 32-bit integer indicating the watermark, which specifies the relay chain block number up to which all inbound horizontal messages have been processed. 8.3.2. Runtime Compression ‚Äã Runtime compression is not documented yet. 8.4. Availability ‚Äã 8.4.1. Availability Votes ‚Äã The Polkadot validator must issue a bitfield ( Definition 131 ) which indicates votes for the availability of candidates. Issued bitfields can be used by the validator and other peers to determine which backed candidates meet the 2/3+ availability quorum. Candidates are inserted into the relay chain in the form of parachain inherent data ( Section 8.2.2. ) by a block author. A validator can retrieve that data by calling the appropriate Runtime API entry ( Section C.9.3. ), then create a bitfield indicating for which candidate the validator has availability data stored and broadcast it to the network ( Definition 108 ). When sending the bitfield distribution message, the validator must ensure B h {B}_{{h}} B h ‚Äã is set appropriately, therefore clarifying to which state the bitfield is referring to, given that candidates can vary based on the chain fork. Missing availability data of candidates must be recovered by the validator as described in Section 8.4.2. . If previously issued bitfields are no longer accurate, i.e., the availability data has been recovered or the candidate of an availability core has changed, the validator must create a new bitfield and broadcast it to the network. Candidates must be kept available by validators for a specific amount of time. If a candidate does not receive any backing, validators should keep it available for about one hour, in case the state of backing does change. Backed and even approved candidates ( Section 8.5. ) must be kept by validators for about 25 hours since disputes ( Section 8.6. ) can occur and the candidate needs to be checked again. The validator issues availability votes in form of a validator protocol message ( Definition 105 ). 8.4.2. Candidate Recovery ‚Äã The availability distribution of the Polkadot validator must be able to recover parachain candidates that the validator is assigned to, in order to determine whether the candidate should be backed ( Section 8.2. ) respectively whether the candidate should be approved ( Section 8.5. ). Additionally, peers can send availability requests as defined in Definition 112 and Definition 114 to the validator, which the validator should be able to respond to. Candidates are recovered by sending requests for specific indices of erasure encoded chunks ( Section A.4.1. ). A validator should request chunks by picking peers randomly and must recover at least f + 1 {f}+{1} f + 1 chunks, where n = 3 f + k {n}={3}{f}+{k} n = 3 f + k and k ‚àà { 1 , 2 , 3 } {k}\in{\left\lbrace{1},{2},{3}\right\rbrace} k ‚àà { 1 , 2 , 3 } . n {n} n is the number of validators as specified in the session info, which can be fetched by the Runtime API as described in Section C.9.13. . 8.5. Approval Voting ‚Äã The approval voting process ensures that only valid parachain blocks are finalized on the relay chain. After backable parachain candidates were submitted to the relay chain ( Section 8.2.2. ), which can be retrieved via the Runtime API ( Section C.9.3. ), validators need to determine their assignments for each parachain and issue approvals for valid candidates, respectively disputes for invalid candidates. Since it cannot be expected that each validator verifies every single parachain candidate, this mechanism ensures that enough honest validators are selected to verify parachain candidates in order to prevent the finalization of invalid blocks. If an honest validator detects an invalid block that was approved by one or more validators, the honest validator must issue a dispute which will cause escalations, resulting in consequences for all malicious parties, i.e., slashing. This mechanism is described more in Section 8.5.1. . 8.5.1. Assignment Criteria ‚Äã Validators determine their assignment based on a VRF mechanism, similar to the BABE consensus mechanism. First, validators generate an availability core VRF assignment ( Definition 101 ), which indicates which availability core a validator is assigned to. Then a delayed availability core VRF assignment is generated, which indicates at what point a validator should start the approval process. The delays are based on ‚Äútranches‚Äù ( Section 8.5.2. ). An assigned validator never broadcasts their assignment until relevant. Once the assigned validator is ready to check a candidate, the validator broadcasts their assignment by issuing an approval distribution message ( Definition 109 ), where M {M} M is of variant 0 . Other assigned validators that receive that network message must keep track of if, expecting an approval vote following shortly after. Assigned validators can retrieve the candidate by using the availability recovery ( Section 8.4.2. ) and then validate the candidate ( Section 8.3. ). The validator issues approval votes in form of a validator protocol message ( Definition 104 ) respectively disputes ( Section 8.6. ). 8.5.2. Tranches ‚Äã Validators use a subjective, tick-based system to determine when the approval process should start. A validator starts the tick-based system when a new availability core candidate have been proposed, which can be retrieved via the Runtime API ( Section C.9.3. ), and increments the tick every 500 milliseconds . Each tick/increment is referred to as a ‚Äútranche‚Äù, represented as an integer, starting at 0 . As described in Section 8.5.1. , the validator first executes the VRF mechanism to determine which parachains (availability cores) the validator is assigned to, then an additional VRF mechanism for each assigned parachain to determine the delayed assignment . The delayed assignment indicates the tranche at which the validator should start the approval process. A tranche of value 0 implies that the assignment should be started immediately, while later assignees of later tranches wait until it‚Äôs their term to issue assignments, determined by their subjective, tick-based system. Validators are required to track broadcasted assignments by other validators assigned to the same parachain, including verifying the VRF output. Once a valid assignment from a peer was received, the validator must wait for the following approval vote within a certain period as described in Section C.9.13. by orienting itself on its local, tick-based system. If the waiting time after a broadcasted assignment exceeds the specified period, the validator interprets this behavior as a ‚Äúno-show‚Äù, indicating that more validators should commit on their tranche until enough approval votes have been collected. If enough approval votes have been collected as described in Section C.9.13. , then assignees of later tranches do not have to start the approval process. Therefore, this tranche system serves as a mechanism to ensure that enough candidate approvals from a random set of validators are created without requiring all assigned validators to check the candidate. Definition 100. Relay VRF Story ‚Äã The relay VRF story is an array of random bytes derived from the VRF submitted within the block by the block author. The relay VRF story, T, is used as input to determine approval voting criteria and generated in the following way: T = Transcript ( b r , b s , e i , A ) {T}=\text{Transcript}{\left({b}_{{r}},{b}_{{s}},{e}_{{i}},{A}\right)} T = Transcript ( b r ‚Äã , b s ‚Äã , e i ‚Äã , A ) where Transcript \text{Transcript} Transcript constructs a VRF transcript ( Definition 165 ). b r {b}_{{r}} b r ‚Äã is the BABE randomness of the current epoch ( Definition 67 ). b s {b}_{{s}} b s ‚Äã is the current BABE slot ( Definition 50 ). e i {e}_{{i}} e i ‚Äã is the current BABE epoch index ( Definition 50 ). A {A} A is the public key of the authority. Definition 101. Availability Core VRF Assignment ‚Äã An availability core VRF assignment is computed by a relay chain validator to determine which availability core ( Definition 125 ) a validator is assigned to and should vote for approvals. Computing this assignment relies on the VRF mechanism, transcripts, and STROBE operations described further in Section A.1.3. . The Runtime dictates how many assignments should be conducted by a validator, as specified in the session index, which can be retrieved via the Runtime API ( Section C.9.13. ). The amount of assignments is referred to as ‚Äúsamples.‚Äù For each iteration of the number of samples, the validator calculates an individual assignment, T {T} T , where the little-endian encoded sample number, s {s} s , is incremented by one. At the beginning of the iteration, S {S} S starts at value 0 . The validator executes the following steps to retrieve a (possibly valid) core index: t 1 ‚Üê Transcript ( ‚ÄôA&V MOD‚Äô ) {t}_{{1}}\leftarrow\text{Transcript}{\left(\text{'A\&V MOD'}\right)} t 1 ‚Äã ‚Üê Transcript ( ‚ÄôA&V MOD‚Äô ) t 2 ‚Üê append ( t 1 , ‚ÄôRC-VRF‚Äô , R s ) {t}_{{2}}\leftarrow\text{append}{\left({t}_{{1}},\text{'RC-VRF'},{R}_{{s}}\right)} t 2 ‚Äã ‚Üê append ( t 1 ‚Äã , ‚ÄôRC-VRF‚Äô , R s ‚Äã ) t 3 ‚Üê append ( t 2 , ‚Äôsample‚Äô , s ) {t}_{{3}}\leftarrow\text{append}{\left({t}_{{2}},\text{'sample'},{s}\right)} t 3 ‚Äã ‚Üê append ( t 2 ‚Äã , ‚Äôsample‚Äô , s ) t 4 ‚Üê append ( t 3 , ‚Äôvrf-nm-pk‚Äô , p k ) {t}_{{4}}\leftarrow\text{append}{\left({t}_{{3}},\text{'vrf-nm-pk'},{p}_{{k}}\right)} t 4 ‚Äã ‚Üê append ( t 3 ‚Äã , ‚Äôvrf-nm-pk‚Äô , p k ‚Äã ) t 5 ‚Üê meta-ad ( t 4 , ‚ÄôVRFHash‚Äô , False ) {t}_{{5}}\leftarrow\text{meta-ad}{\left({t}_{{4}},\text{'VRFHash'},\text{False}\right)} t 5 ‚Äã ‚Üê meta-ad ( t 4 ‚Äã , ‚ÄôVRFHash‚Äô , False ) t 6 ‚Üê meta-ad ( t 5 , 64 le , True ) {t}_{{6}}\leftarrow\text{meta-ad}{\left({t}_{{5}},{64}_{{\text{le}}},\text{True}\right)} t 6 ‚Äã ‚Üê meta-ad ( t 5 ‚Äã , 64 le ‚Äã , True ) i ‚Üê prf ( t 6 , False ) {i}\leftarrow\text{prf}{\left({t}_{{6}},\text{False}\right)} i ‚Üê prf ( t 6 ‚Äã , False ) o = s k ‚ãÖ i {o}={s}_{{k}}\cdot{i} o = s k ‚Äã ‚ãÖ i where s k {s}_{{k}} s k ‚Äã is the secret key, p k {p}_{{k}} p k ‚Äã is the public key and 64 le {64}_{{\text{le}}} 64 le ‚Äã is the integer 64 encoded as little endian. R s {R}_{{s}} R s ‚Äã is the relay VRF story as defined in Definition 100 . Following: t 1 ‚Üê Transcript ( ‚ÄôVRFResult‚Äô ) {t}_{{1}}\leftarrow\text{Transcript}{\left(\text{'VRFResult'}\right)} t 1 ‚Äã ‚Üê Transcript ( ‚ÄôVRFResult‚Äô ) t 2 ‚Üê append ( t 1 , ‚Äù , ‚ÄôA&V CORE‚Äô ) {t}_{{2}}\leftarrow\text{append}{\left({t}_{{1}},\text{''},\text{'A\&V CORE'}\right)} t 2 ‚Äã ‚Üê append ( t 1 ‚Äã , ‚Äù , ‚ÄôA&V CORE‚Äô ) t 3 ‚Üê append ( t 2 , ‚Äôvrf-in‚Äô , i ) {t}_{{3}}\leftarrow\text{append}{\left({t}_{{2}},\text{'vrf-in'},{i}\right)} t 3 ‚Äã ‚Üê append ( t 2 ‚Äã , ‚Äôvrf-in‚Äô , i ) t 4 ‚Üê append ( t 3 , ‚Äôvrf-out‚Äô , o ) {t}_{{4}}\leftarrow\text{append}{\left({t}_{{3}},\text{'vrf-out'},{o}\right)} t 4 ‚Äã ‚Üê append ( t 3 ‚Äã , ‚Äôvrf-out‚Äô , o ) t 5 ‚Üê meta-ad ( t 4 , ‚Äù , False ) {t}_{{5}}\leftarrow\text{meta-ad}{\left({t}_{{4}},\text{''},\text{False}\right)} t 5 ‚Äã ‚Üê meta-ad ( t 4 ‚Äã , ‚Äù , False ) t 6 ‚Üê meta-ad ( t 5 , 4 le , True ) {t}_{{6}}\leftarrow\text{meta-ad}{\left({t}_{{5}},{4}_{\text{le}},\text{True}\right)} t 6 ‚Äã ‚Üê meta-ad ( t 5 ‚Äã , 4 le ‚Äã , True ) r ‚Üê prf ( t 6 , False ) {r}\leftarrow\text{prf}{\left({t}_{{6}},\text{False}\right)} r ‚Üê prf ( t 6 ‚Äã , False ) c i = r mod a c {c}_{{i}}={r}\text{mod}{a}_{{c}} c i ‚Äã = r mod a c ‚Äã where 4 le {4}_{{\text{le}}} 4 le ‚Äã is the integer 4 encoded as little endian, r {r} r is the 4-byte challenge interpreted as a little endian encoded interger and a c {a}_{{c}} a c ‚Äã is the number of availability cores used during the active session, as defined in the session info retrieved by the Runtime API ( Section C.9.13. ). The resulting integer, c i {c}_{{i}} c i ‚Äã , indicates the parachain Id ( Definition 124 ). If the parachain Id doesn‚Äôt exist, as can be retrieved by the Runtime API ( Section C.9.3. ), the validator discards that value and continues with the next iteration. If the Id does exist, the validator continues with the following steps: t 1 ‚Üê Transcript ( ‚ÄôA&V ASSIGNED‚Äô ) {t}_{{1}}\leftarrow\text{Transcript}{\left(\text{'A\&V ASSIGNED'}\right)} t 1 ‚Äã ‚Üê Transcript ( ‚ÄôA&V ASSIGNED‚Äô ) t 2 ‚Üê append ( t 1 , ‚Äôcore‚Äô , c i ) {t}_{{2}}\leftarrow\text{append}{\left({t}_{{1}},\text{'core'},{c}_{{i}}\right)} t 2 ‚Äã ‚Üê append ( t 1 ‚Äã , ‚Äôcore‚Äô , c i ‚Äã ) p ‚Üê dleq_prove ( t 2 , i ) {p}\leftarrow\text{dleq\_prove}{\left({t}_{{2}},{i}\right)} p ‚Üê dleq_prove ( t 2 ‚Äã , i ) where dleq_prove \text{dleq\_prove} dleq_prove is described in Definition 162 . The resulting values of o {o} o , p {p} p and s {s} s are used to construct an assignment certificate ( Definition 103 ) of kind 0 . Definition 102. Delayed Availability Core VRF Assignment ‚Äã The delayed availability core VRF assignments determined at what point a validator should start the approval process as described in Section 8.5.2. . Computing this assignment relies on the VRF mechanism, transcripts, and STROBE operations described further in Section A.1.3. . The validator executes the following steps: t 1 ‚Üê Transcript ( ‚ÄôA&V DELAY‚Äô ) {t}_{{1}}\leftarrow\text{Transcript}{\left(\text{'A\&V DELAY'}\right)} t 1 ‚Äã ‚Üê Transcript ( ‚ÄôA&V DELAY‚Äô ) t 2 ‚Üê append ( t 1 , ‚ÄôRC-VRF‚Äô , R s ) {t}_{{2}}\leftarrow\text{append}{\left({t}_{{1}},\text{'RC-VRF'},{R}_{{s}}\right)} t 2 ‚Äã ‚Üê append ( t 1 ‚Äã , ‚ÄôRC-VRF‚Äô , R s ‚Äã ) t 3 ‚Üê append ( t 2 , ‚Äôcore‚Äô , c i ) {t}_{{3}}\leftarrow\text{append}{\left({t}_{{2}},\text{'core'},{c}_{{i}}\right)} t 3 ‚Äã ‚Üê append ( t 2 ‚Äã , ‚Äôcore‚Äô , c i ‚Äã ) t 4 ‚Üê append ( t 3 , ‚Äôvrf-nm-pk‚Äô , p k ) {t}_{{4}}\leftarrow\text{append}{\left({t}_{{3}},\text{'vrf-nm-pk'},{p}_{{k}}\right)} t 4 ‚Äã ‚Üê append ( t 3 ‚Äã , ‚Äôvrf-nm-pk‚Äô , p k ‚Äã ) t 5 ‚Üê meta-ad ( t 4 , ‚ÄôVRFHash‚Äô , False ) {t}_{{5}}\leftarrow\text{meta-ad}{\left({t}_{{4}},\text{'VRFHash'},\text{False}\right)} t 5 ‚Äã ‚Üê meta-ad ( t 4 ‚Äã , ‚ÄôVRFHash‚Äô , False ) t 6 ‚Üê meta-ad ( t 5 , 64 le , True ) {t}_{{6}}\leftarrow\text{meta-ad}{\left({t}_{{5}},{64}_{{\text{le}}},\text{True}\right)} t 6 ‚Äã ‚Üê meta-ad ( t 5 ‚Äã , 64 le ‚Äã , True ) i ‚Üê prf ( t 6 , False ) {i}\leftarrow\text{prf}{\left({t}_{{6}},\text{False}\right)} i ‚Üê prf ( t 6 ‚Äã , False ) o = s k ‚ãÖ i {o}={s}_{{k}}\cdot{i} o = s k ‚Äã ‚ãÖ i p ‚Üê dleq_prove ( t 6 , i ) {p}\leftarrow\text{dleq\_prove}{\left({t}_{{6}},{i}\right)} p ‚Üê dleq_prove ( t 6 ‚Äã , i ) The resulting value p {p} p is the VRF proof ( Definition 161 ). dleq_prove \text{dleq\_prove} dleq_prove is described in Definition 162 . The tranche, d {d} d , is determined as: t 1 ‚Üê Transcript ( ‚ÄôVRFResult‚Äô ) {t}_{{1}}\leftarrow\text{Transcript}{\left(\text{'VRFResult'}\right)} t 1 ‚Äã ‚Üê Transcript ( ‚ÄôVRFResult‚Äô ) t 2 ‚Üê append ( t 1 , ‚Äù , ‚ÄôA&V TRANCHE‚Äô ) {t}_{{2}}\leftarrow\text{append}{\left({t}_{{1}},\text{''},\text{'A\&V TRANCHE'}\right)} t 2 ‚Äã ‚Üê append ( t 1 ‚Äã , ‚Äù , ‚ÄôA&V TRANCHE‚Äô ) t 3 ‚Üê append ( t 2 , ‚Äôvrf-in‚Äô , i ) {t}_{{3}}\leftarrow\text{append}{\left({t}_{{2}},\text{'vrf-in'},{i}\right)} t 3 ‚Äã ‚Üê append ( t 2 ‚Äã , ‚Äôvrf-in‚Äô , i ) t 4 ‚Üê append ( t 3 , ‚Äôvrf-out‚Äô , o ) {t}_{{4}}\leftarrow\text{append}{\left({t}_{{3}},\text{'vrf-out'},{o}\right)} t 4 ‚Äã ‚Üê append ( t 3 ‚Äã , ‚Äôvrf-out‚Äô , o ) t 5 ‚Üê meta-ad ( t 4 , ‚Äù , False ) {t}_{{5}}\leftarrow\text{meta-ad}{\left({t}_{{4}},\text{''},\text{False}\right)} t 5 ‚Äã ‚Üê meta-ad ( t 4 ‚Äã , ‚Äù , False ) t 6 ‚Üê meta-ad ( t 5 , 4 le , True ) {t}_{{6}}\leftarrow\text{meta-ad}{\left({t}_{{5}},{4}_{{\text{le}}},\text{True}\right)} t 6 ‚Äã ‚Üê meta-ad ( t 5 ‚Äã , 4 le ‚Äã , True ) c ‚Üê prf ( t 6 , False ) {c}\leftarrow\text{prf}{\left({t}_{{6}},\text{False}\right)} c ‚Üê prf ( t 6 ‚Äã , False ) d = d mod ( d c + d z ) ‚àí d z {d}={d}\text{mod}{\left({d}_{{c}}+{d}_{{z}}\right)}-{d}_{{z}} d = d mod ( d c ‚Äã + d z ‚Äã ) ‚àí d z ‚Äã where d c {d}_{{c}} d c ‚Äã is the number of delayed tranches by total as specified by the session info, retrieved via the Runtime API ( Section C.9.13. ). d z {d}_{{z}} d z ‚Äã is the zeroth delay tranche width as specified by the session info, retrieved via the Runtime API ( Section C.9.13. ). The resulting tranche, n {n} n , cannot be less than 0 {0} 0 . If the tranche is less than 0 {0} 0 , then d = 0 {d}={0} d = 0 . The resulting values o {o} o , p {p} p and c i {c}_{{i}} c i ‚Äã are used to construct an assignment certificate (\< Definition 103 ) of kind 1 . Definition 103. Assignment Certificate ‚Äã The Assignment Certificate proves to the network that a Polkadot validator is assigned to an availability core and is, therefore, qualified for the approval of candidates, as clarified in Definition 101 . This certificate contains the computed VRF output and is a data structure of the following format: ( k , o , p ) {\left({k},{o},{p}\right)} ( k , o , p ) k = { 0 ‚Üí s 1 ‚Üí c i {k}={\left\lbrace\begin{matrix}{0}&\rightarrow&{s}\\{1}&\rightarrow&{c}_{{i}}\end{matrix}\right.} k = { 0 1 ‚Äã ‚Üí ‚Üí ‚Äã s c i ‚Äã ‚Äã where k {k} k indicates the kind of the certificate, respectively the value 0 proves the availability core assignment ( Definition 101 ), followed by the sample number s {s} s , and the value 1 proves the delayed availability core assignment ( Definition 102 ), followed by the core index c i {c}_{{i}} c i ‚Äã ( Section C.9.3. ). o {o} o is the VRF output and p {p} p is the VRF proof. 8.6. Disputes ‚Äã info Disputes are not documented yet. 8.7. Network Messages ‚Äã The availability and validity process requires certain network messages to be exchanged between validators and collators. 8.7.1. Notification Messges ‚Äã The notification messages are exchanged between validators, including messages sent by collators to validators. The protocol messages are exchanged based on a streaming notification substream ( Section 4.5. ). The messages are SCALE encoded ( Section A.2.2. ). Definition 104. Validator Protocol Message ‚Äã The validator protocol message is a varying datatype used by validators to broadcast relevant information about certain steps in the A&V process. Specifically, this includes the backing process ( Section 8.2. ) and the approval process ( Section 8.5. ). The validator protocol message, M {M} M , is a varying datatype of the following format: M = { 1 ‚Üí M f 3 ‚Üí M s 4 ‚Üí M a {M}={\left\lbrace\begin{matrix}{1}&\rightarrow&{M}_{{f}}\\{3}&\rightarrow&{M}_{{s}}\\{4}&\rightarrow&{M}_{{a}}\end{matrix}\right.} M = ‚é© ‚é® ‚éß ‚Äã 1 3 4 ‚Äã ‚Üí ‚Üí ‚Üí ‚Äã M f ‚Äã M s ‚Äã M a ‚Äã ‚Äã where M f {M}_{{f}} M f ‚Äã is a bitfield distribution message ( Definition 108 ). M s {M}_{{s}} M s ‚Äã is a statement distribution message ( Definition 107 ). M a {M}_{{a}} M a ‚Äã is a approval distribution message ( Definition 109 ). Definition 105. Collation Protocol Message ‚Äã The collation protocol message, M, is a varying datatype of the following format: M = { 0 ‚Üí M c {M}={\left\lbrace\begin{matrix}{0}&\rightarrow&{M}_{{c}}\end{matrix}\right.} M = { 0 ‚Äã ‚Üí ‚Äã M c ‚Äã ‚Äã where M c {M}_{{c}} M c ‚Äã is the collator message ( Definition 106 ). Definition 106. Collator Message ‚Äã The collator message is sent as part of the collator protocol message ( Definition 105 ). The collator message, M {M} M , is a varying datatype of the following format: M = { 0 ‚Üí ( C i , P i , C s ) 1 ‚Üí H 4 ‚Üí ( B h , S ) {M}={\left\lbrace\begin{matrix}{0}&\rightarrow&{\left({C}_{{i}},{P}_{{i}},{C}_{{s}}\right)}\\{1}&\rightarrow&{H}\\{4}&\rightarrow&{\left({B}_{{h}},{S}\right)}\end{matrix}\right.} M = ‚é© ‚é® ‚éß ‚Äã 0 1 4 ‚Äã ‚Üí ‚Üí ‚Üí ‚Äã ( C i ‚Äã , P i ‚Äã , C s ‚Äã ) H ( B h ‚Äã , S ) ‚Äã where M {M} M is a varying datatype where 0 indicates the intent to advertise a collation and 1 indicates the advertisement of a collation to a validator. 4 indicates that a collation sent to a validator was seconded. C i {C}_{{i}} C i ‚Äã is the public key of the collator. P i {P}_{{i}} P i ‚Äã is the parachain Id ( Definition 124 ). C s {C}_{{s}} C s ‚Äã is the signature of the collator using the PeerId of the collators node. H {H} H is the hash of the parachain block ( Definition 122 ). S {S} S is a full statement ( Definition 92 ). Definition 107. Statement Distribution Message ‚Äã The statement distribution message is sent as part of the validator protocol message ( Definition 105 ) indicates the validity vote of a validator for a given candidate, described further in Section 8.2.1. . The statement distribution message, M {M} M , is of varying type of the following format: M = { 0 ‚Üí ( B h , S ) 1 ‚Üí S m {M}={\left\lbrace\begin{matrix}{0}&\rightarrow&{\left({B}_{{h}},{S}\right)}\\{1}&\rightarrow&{S}_{{m}}\end{matrix}\right.} M = { 0 1 ‚Äã ‚Üí ‚Üí ‚Äã ( B h ‚Äã , S ) S m ‚Äã ‚Äã S m = ( B h , C h , A i , A s ) {S}_{{m}}={\left({B}_{{h}},{C}_{{h}},{A}_{{i}},{A}_{{s}}\right)} S m ‚Äã = ( B h ‚Äã , C h ‚Äã , A i ‚Äã , A s ‚Äã ) where M {M} M is a varying datatype where 0 indicates a signed statement and 1 contains metadata about a seconded statement with a larger payload, such as a runtime upgrade. The candidate itself can be fetched via the request/response message ( Definition 118 ). B h {B}_{{h}} B h ‚Äã is the hash of the relay chain parent, indicating the state this message is for. S {S} S is a full statement ( Definition 92 ). A i {A}_{{i}} A i ‚Äã is the validator index in the authority set ( Definition 33 ) that signed this message. A s {A}_{{s}} A s ‚Äã is the signature of the validator. Definition 108. Bitfield Distribution Message ‚Äã The bitfield distribution message is sent as part of the validator protocol message ( Definition 104 ) and indicates the availability vote of a validator for a given candidate, described further in Section 8.4.1. . This message is sent in the form of a validator protocol message ( Definition 104 ). The bitfield distribution message, M {M} M , is a datastructure of the following format: M = { 0 ‚Üí ( B h , P ) {M}={\left\lbrace\begin{matrix}{0}&\rightarrow&{\left({B}_{{h}},{P}\right)}\end{matrix}\right.} M = { 0 ‚Äã ‚Üí ‚Äã ( B h ‚Äã , P ) ‚Äã P = ( d , A i , A s ) {P}={\left({d},{A}_{{i}},{A}_{{s}}\right)} P = ( d , A i ‚Äã , A s ‚Äã ) where B h {B}_{{h}} B h ‚Äã is the hash of the relay chain parent, indicating the state this message is for. d {d} d is the bitfield array ( Definition 131 ). A i {A}_{{i}} A i ‚Äã is the validator index in the authority set ( Definition 33 ) that signed this message. A s {A}_{{s}} A s ‚Äã is the signature of the validator. Definition 109. Approval Distribution Message ‚Äã The approval distribution message is sent as part of the validator protocol message ( Definition 104 ) and indicates the approval vote of a validator for a given candidate, described further in Section 8.5.1. . The approval distribution message, M {M} M , is a varying datatype of the following format: M = { 0 ‚Üí ( ( C , I ) 0 ‚Ä¶ ( C , I ) n ) 1 ‚Üí ( V 0 , ‚Ä¶ V n ) {M}={\left\lbrace\begin{matrix}{0}&\rightarrow&{\left({\left({C}_{,}{I}_{{}}\right)}_{{0}}‚Ä¶{\left({C},{I}\right)}_{{n}}\right)}\\{1}&\rightarrow&{\left({V}_{{0}},‚Ä¶{V}_{{n}}\right)}\end{matrix}\right.} M = { 0 1 ‚Äã ‚Üí ‚Üí ‚Äã ( ( C , ‚Äã I ‚Äã ) 0 ‚Äã ‚Ä¶ ( C , I ) n ‚Äã ) ( V 0 ‚Äã , ‚Ä¶ V n ‚Äã ) ‚Äã C = ( B h , A i , c a ) {C}={\left({B}_{{h}},{A}_{{i}},{c}_{{a}}\right)} C = ( B h ‚Äã , A i ‚Äã , c a ‚Äã ) c a = ( c k , P o , P p ) {c}_{{a}}={\left({c}_{{k}},{P}_{{o}},{P}_{{p}}\right)} c a ‚Äã = ( c k ‚Äã , P o ‚Äã , P p ‚Äã ) c k = { 0 ‚Üí s 1 ‚Üí i {c}_{{k}}={\left\lbrace\begin{matrix}{0}‚Üí{s}\\{1}‚Üí{i}\end{matrix}\right.} c k ‚Äã = { 0 ‚Üí s 1 ‚Üí i ‚Äã V = ( B h , I , A i , A s ) {V}={\left({B}_{{h}},{I},{A}_{{i}},{A}_{{s}}\right)} V = ( B h ‚Äã , I , A i ‚Äã , A s ‚Äã ) where M {M} M is a varying datatype where 0 indicates assignments for candidates in recent, unfinalized blocks and 1 indicates approvals for candidates in some recent, unfinalized block. C {C} C is an assignment criterion that refers to the candidate under which the assignment is relevant by the block hash. I {I} I is an unsigned 32-bit integer indicating the index of the candidate, corresponding to the order of the availability cores ( Section C.9.3. ). B h {B}_{{h}} B h ‚Äã is the relay chain block hash where the candidate appears. A i {A}_{{i}} A i ‚Äã is the authority set Id ( Definition 69 ) of the validator that created this message. A s {A}_{{s}} A s ‚Äã is the signature of the validator issuing this message. c a {c}_{{a}} c a ‚Äã is the certification of the assignment. c k {c}_{{k}} c k ‚Äã is a varying datatype where 0 indicates an assignment based on the VRF that authorized the relay chain block where the candidate was included, followed by a sample number, s {s} s . 1 indicates an assignment story based on the VRF that authorized the relay chain block where the candidate was included combined with the index of a particular core. This is described further in Section 8.5. . P o {P}_{{o}} P o ‚Äã is a VRF output and P p {P}_{{p}} P p ‚Äã its corresponding proof. 8.7.2. Request & Response ‚Äã The request & response network messages are sent and received between peers in the Polkadot network, including collators and non-validator nodes. Those messages are conducted on the request-response substreams ( Section 4.5. ). The network messages are SCALE encoded as described in Section ?. Definition 110. PoV Fetching Request ‚Äã The PoV fetching request is sent by clients who want to retrieve a PoV block from a node. The request is a data structure of the following format: C h {C}_{{h}} C h ‚Äã where C h {C}_{{h}} C h ‚Äã is the 256-bit hash of the PoV block. The response message is defined in Definition 111 . Definition 111. PoV Fetching Response ‚Äã The PoV fetching response is sent by nodes to the clients who issued a PoV fetching request ( Definition 110 ). The response, R {R} R , is a varying datatype of the following format: R = { 0 ‚Üí B 1 ‚Üí œï {R}={\left\lbrace\begin{matrix}{0}&\rightarrow&{B}\\{1}&\rightarrow&\phi\end{matrix}\right.} R = { 0 1 ‚Äã ‚Üí ‚Üí ‚Äã B œï ‚Äã where 0 is followed by the PoV block and 1 indicates that the PoV block was not found. Definition 112. Chunk Fetching Request ‚Äã The chunk fetching request is sent by clients who want to retrieve chunks of a parachain candidate. The request is a data structure of the following format: ( C h , i ) {\left({C}_{{h}},{i}\right)} ( C h ‚Äã , i ) where C h {C}_{{h}} C h ‚Äã is the 256-bit hash of the parachain candidate and i {i} i is a 32-bit unsigned integer indicating the index of the chunk to fetch. The response message is defined in Definition 113 . Definition 113. Chunk Fetching Response ‚Äã The chunk fetching response is sent by nodes to the clients who issued a chunk fetching request ( Definition 112 ). The response, R {R} R , is a varying datatype of the following format: R = { 0 ‚Üí C r 1 ‚Üí œï {R}={\left\lbrace\begin{matrix}{0}&\rightarrow&{C}_{{r}}\\{1}&\rightarrow&\phi\end{matrix}\right.} R = { 0 1 ‚Äã ‚Üí ‚Üí ‚Äã C r ‚Äã œï ‚Äã C r = ( c , c p ) {C}_{{r}}={\left({c},{c}_{{p}}\right)} C r ‚Äã = ( c , c p ‚Äã ) where 0 is followed by the chunk response, C r {C}_{{r}} C r ‚Äã and 1 indicates that the requested chunk was not found. C r {C}_{{r}} C r ‚Äã contains the erasure-encoded chunk of data belonging to the candidate block, c {c} c , and c p {c}_{{p}} c p ‚Äã is that chunks proof in the Merkle tree. Both c {c} c and c p {c}_{{p}} c p ‚Äã are byte arrays of type ( b n ‚Ä¶ b m ) {\left({b}_{{n}}‚Ä¶{b}_{{m}}\right)} ( b n ‚Äã ‚Ä¶ b m ‚Äã ) . Definition 114. Available Data Request ‚Äã The available data request is sent by clients who want to retrieve the PoV block of a parachain candidate. The request is a data structure of the following format: C h {C}_{{h}} C h ‚Äã where C h {C}_{{h}} C h ‚Äã is the 256-bit candidate hash to get the available data for. The response message is defined in Definition 115 . Definition 115. Available Data Response ‚Äã The available data response is sent by nodes to the clients who issued an available data request ( Definition 114 ). The response, R {R} R , is a varying datatype of the following format: R = { 0 ‚Üí A 1 ‚Üí œï {R}={\left\lbrace\begin{matrix}{0}&\rightarrow&{A}\\{1}&\rightarrow&\phi\end{matrix}\right.} R = { 0 1 ‚Äã ‚Üí ‚Üí ‚Äã A œï ‚Äã A = ( P o v , D p v ) {A}={\left({P}_{{{ov}}},{D}_{{{pv}}}\right)} A = ( P o v ‚Äã , D p v ‚Äã ) where 0 is followed by the available data, A {A} A , and 1 indicates the the requested candidate hash was not found. P o v {P}_{{{o}{v}}} P o v ‚Äã is the PoV block ( Definition 122 ) and D p v {D}_{{{p}{v}}} D p v ‚Äã is the persisted validation data ( Definition 220 ). Definition 116. Collation Fetching Request ‚Äã The collation fetching request is sent by clients who want to retrieve the advertised collation at the specified relay chain block. The request is a data structure of the following format: ( B h , P i d ) {\left({B}_{{h}},{P}_{{{id}}}\right)} ( B h ‚Äã , P i d ‚Äã ) where B h {B}_{{h}} B h ‚Äã is the hash of the relay chain block and P i d {P}_{{{i}{d}}} P i d ‚Äã is the parachain Id ( Definition 124 ). The response message is defined in Definition 117 . Definition 117. Collation Fetching Response ‚Äã The collation fetching response is sent by nodes to the clients who issued a collation fetching request ( Definition 116 ). The response, R {R} R , is a varying datatype of the following format: R = { 0 ‚Üí ( C r , B ) {R}={\left\lbrace\begin{matrix}{0}&\rightarrow&{\left({C}_{{r}},{B}\right)}\end{matrix}\right.} R = { 0 ‚Äã ‚Üí ‚Äã ( C r ‚Äã , B ) ‚Äã where 0 {0} 0 is followed by the candidate receipt ( Definition 94 ), C r {C}_{{r}} C r ‚Äã , as and the PoV block ( Definition 122 ), B {B} B . This type does not notify the client about a statement that was not found. Definition 118. Statement Fetching Request ‚Äã The statement fetching request is sent by clients who want to retrieve statements about a given candidate. The request is a data structure of the following format: ( B h , C h ) {\left({B}_{{h}},{C}_{{h}}\right)} ( B h ‚Äã , C h ‚Äã ) where B h {B}_{{h}} B h ‚Äã is the hash of the relay chain parent and C h {C}_{{h}} C h ‚Äã is the candidate hash that was used to create a committed candidate receipt ( Definition 95 ). The response message is defined in Definition 119 . Definition 119. Statement Fetching Response ‚Äã The statement fetching response is sent by nodes to the clients who issued a collation fetching request ( Definition 118 ). The response, R {R} R , is a varying datatype of the following format: R = { 0 ‚Üí C r {R}={\left\lbrace\begin{matrix}{0}&\rightarrow&{C}_{{r}}\end{matrix}\right.} R = { 0 ‚Äã ‚Üí ‚Äã C r ‚Äã ‚Äã where C r {C}_{{r}} C r ‚Äã is the committed candidate receipt ( Definition 95 ). No response is returned if no statement is found. 8.7.2.1. Dispute Request ‚Äã The dispute request is sent by clients who want to issue a dispute about a candidate. The request, D r {D}_{{r}} D r ‚Äã , is a data structure of the following format: D r = ( C r , S i , I v , V v ) {D}_{{r}}={\left({C}_{{r}},{S}_{{i}},{I}_{{v}},{V}_{{v}}\right)} D r ‚Äã = ( C r ‚Äã , S i ‚Äã , I v ‚Äã , V v ‚Äã ) I v = ( A i , A s , k i ) {I}_{{v}}={\left({A}_{{i}},{A}_{{s}},{k}_{{i}}\right)} I v ‚Äã = ( A i ‚Äã , A s ‚Äã , k i ‚Äã ) V v = ( A i , A s , k v ) {V}_{{v}}={\left({A}_{{i}},{A}_{{s}},{k}_{{v}}\right)} V v ‚Äã = ( A i ‚Äã , A s ‚Äã , k v ‚Äã ) k i = { 0 ‚Üí œï {k}_{{i}}={\left\lbrace\begin{matrix}{0}&\rightarrow&\phi\end{matrix}\right.} k i ‚Äã = { 0 ‚Äã ‚Üí ‚Äã œï ‚Äã k v = { 0 ‚Üí œï 1 ‚Üí C h 2 ‚Üí C h 3 ‚Üí œï {k}_{{v}}={\left\lbrace\begin{matrix}{0}&\rightarrow&\phi\\{1}&\rightarrow&{C}_{{h}}\\{2}&\rightarrow&{C}_{{h}}\\{3}&\rightarrow&\phi\end{matrix}\right.} k v ‚Äã = ‚é© ‚é® ‚éß ‚Äã 0 1 2 3 ‚Äã ‚Üí ‚Üí ‚Üí ‚Üí ‚Äã œï C h ‚Äã C h ‚Äã œï ‚Äã where C r {C}_{{r}} C r ‚Äã is the candidate that is being disputed. The structure is a candidate receipt ( Definition 94 ). S i {S}_{{i}} S i ‚Äã is an unsigned 32-bit integer indicating the session index the candidate appears in. I v {I}_{{v}} I v ‚Äã is the invalid vote that makes up the request. V v {V}_{{v}} V v ‚Äã is the valid vote that makes this dispute request valid. A i {A}_{{i}} A i ‚Äã is an unsigned 32-bit integer indicating the validator index in the authority set ( Definition 33 ). A s {A}_{{s}} A s ‚Äã is the signature of the validator. k i {k}_{{i}} k i ‚Äã is a varying datatype and implies the dispute statement. 0 indicates an explicit statement. k v {k}_{{v}} k v ‚Äã is a varying datatype and implies the dispute statement. 0 {0} 0 indicates an explicit statement. 1 {1} 1 indicates a seconded statement on a candidate, C h {C}_{{h}} C h ‚Äã , from the backing phase. C h {C}_{{h}} C h ‚Äã is the hash of the candidate. 2 {2} 2 indicates a valid statement on a candidate, C h {C}_{{h}} C h ‚Äã , from the backing phase. C h {C}_{{h}} C h ‚Äã is the hash of the candidate. 3 {3} 3 indicates an approval vote from the approval checking phase. The response message is defined in Section 8.7.2.2. . 8.7.2.2. Dispute Response ‚Äã The dispute response is sent by nodes to the clients who issued a dispute request ( Section 8.7.2.1. ). The response, R {R} R , is a varying type of the following format: R = { 0 ‚Üí œï {R}={\left\lbrace\begin{matrix}{0}&\rightarrow&\phi\end{matrix}\right.} R = { 0 ‚Äã ‚Üí ‚Äã œï ‚Äã where 0 {0} 0 indicates that the dispute was successfully processed. 8.8. Definitions ‚Äã Definition 120. Collator ‚Äã A collator is a parachain node that sends parachain blocks, known as candidates ( Definition 121 ), to the relay chain validators. The relay chain validators are not concerned with how the collator works or how it creates candidates. Definition 121. Candidate ‚Äã A candidate is a submitted parachain block ( Definition 122 ) to the relay chain validators. A parachain block stops being referred to as a candidate as soon it has been finalized. Definition 122. Parachain Block ‚Äã A parachain block or a Proof-of-Validity block (PoV block) contains the necessary data for the parachain-specific state transition logic. Relay chain validators are not concerned with the inner structure of the block and treat it as a byte array. Definition 123. Head Data ‚Äã The head data contains information about a parachain block ( Definition 122 ). The head data is returned by executing the parachain Runtime, and relay chain validators are not concerned with its inner structure and treat it as a byte array. Definition 124. Parachain Id ‚Äã The Parachain Id is a unique, unsigned 32-bit integer which serves as an identifier of a parachain, assigned by the Runtime. Definition 125. Availability Core ‚Äã Availability cores are slots used to process parachains. The Runtime assigns each parachain to an availability core, and validators can fetch information about the cores, such as parachain block candidates, by calling the appropriate Runtime API ( Section C.9.3. ). Validators are not concerned with the internal workings from the Runtimes perspective. Definition 126. Validator Groups ‚Äã Validator groups indicate which validators are responsible for creating backable candidates for parachains ( Section 8.2. ), and are assigned by the Runtime ( Section C.9.2. ). Validators are not concerned with the internal workings from the Runtimes perspective. Collators can use this information for submitting blocks. Definition 127. Upward Message ‚Äã An upward message is an opaque byte array sent from a parachain to a relay chain. Definition 128. Downward Message ‚Äã A downward message is an opaque byte array received by the parachain from the relay chain. Definition 129. Outbound HRMP Message ‚Äã An outbound HRMP message (Horizontal Relay-routed Message Passing) is sent from the perspective of a sender of a parachain to another parachain by passing it through the relay chain. It‚Äôs a data structure of the following format: ( I , M ) {\left({I},{M}\right)} ( I , M ) where I {I} I is the recipient Id ( Definition 124 ) and M {M} M is an upward message ( Definition 127 ). Definition 130. Inbound HRMP Message ‚Äã An inbound HRMP message (Horizontal Relay-routed Message Passing) is seen from the perspective of a recipient parachain sent from another parachain by passing it through the relay chain. It‚Äôs a data structure of the following format: ( N , M ) {\left({N},{M}\right)} ( N , M ) where N {N} N is the unsigned 32-bit integer indicating the relay chain block number at which the message was passed down to the recipient parachain and M {M} M is a downward message ( Definition 128 ). Definition 131. Bitfield Array ‚Äã A bitfield array contains single-bit values, which indicates whether a candidate is available. The number of items is equal to the number of availability cores ( Definition 125 ), and each bit represents a vote on the corresponding core in the given order. Respectively, if the single bit equals 1, then the Polkadot validator claims that the availability core is occupied, there exists a committed candidate receipt ( Definition 95 ) and that the validator has a stored chunk of the parachain block ( Definition 122 ). Previous 7. Light Clients Next Polkadot Runtime 8.1. Collations 8.2. Candidate Backing 8.2.1. Statements 8.2.2. Inclusion 8.3. Candidate Validation 8.3.1. Parachain Runtime 8.3.2. Runtime Compression 8.4. Availability 8.4.1. Availability Votes 8.4.2. Candidate Recovery 8.5. Approval Voting 8.5.1. Assignment Criteria 8.5.2. Tranches 8.6. Disputes 8.7. Network Messages 8.7.1. Notification Messges 8.7.2. Request & Response 8.8. Definitions 
Polkadot Runtime | Polkadot Protocol Specification Runtime Polkadot Runtime Description of various useful Runtime internals üìÑÔ∏è 9. Extrinsics 9.1. Introduction üìÑÔ∏è 10. Weights 10.1. Motivation üìÑÔ∏è 11. Consensus 11.1. BABE digest messages üìÑÔ∏è 12. Metadata The runtime metadata structure contains all the information necessary on how to interact with the Polkadot runtime. Considering that Polkadot runtimes are upgradable and, therefore, any interfaces are subject to change, the metadata allows developers to structure any extrinsics or storage entries accordingly. Previous 8. Availability & Validity Next 9. Extrinsics 
9. Extrinsics | Polkadot Protocol Specification Runtime 9. Extrinsics On this page 9. Extrinsics 9.1. Introduction ‚Äã An extrinsic is a SCALE encoded array consisting of a version number, signature, and varying data types indicating the resulting Runtime function to be called, including the parameters required for that function to be executed. 9.2. Preliminaries ‚Äã Definition 132. Extrinsic ‚Äã An extrinsic , t x {t}{x} t x , is a tuple consisting of the extrinsic version, T v {T}_{{v}} T v ‚Äã ( Definition 133 ), and the body of the extrinsic, T b {T}_{{b}} T b ‚Äã . t x = ( T v , T b ) {t}{x}\:={\left({T}_{{v}},{T}_{{b}}\right)} t x = ( T v ‚Äã , T b ‚Äã ) The value of T b {T}_{{b}} T b ‚Äã varies for each version. The current version 4 is described in Section 9.3.1. . Definition 133. Extrinsic Version ‚Äã T v {T}_{{v}} T v ‚Äã is a 8-bit bitfield and defines the extrinsic version. The required format of an extrinsic body, T b {T}_{{b}} T b ‚Äã , is dictated by the Runtime. Older or unsupported versions are rejected. The most significant bit of T v {T}_{{v}} T v ‚Äã indicates whether the transaction is signed ( 1 {1} 1 ) or unsigned ( 0 {0} 0 ). The remaining 7-bits represent the version number. As an example, for extrinsic format version 4, a signed extrinsic represents T v {T}_{{v}} T v ‚Äã as 132 while an unsigned extrinsic represents it as 4 . 9.3. Extrinsics Body ‚Äã 9.3.1. Version 4 ‚Äã Version 4 of the Polkadot extrinsic format is defined as follows: T b = ( A i , S i g , E , M i , F i ( m ) ) {T}_{{b}}\:={\left({A}_{{i}},{Sig},{E},{M}_{{i}},{F}_{{i}}{\left({m}\right)}\right)} T b ‚Äã = ( A i ‚Äã , S i g , E , M i ‚Äã , F i ‚Äã ( m ) ) where A i {A}_{{i}} A i ‚Äã : the 32-byte address of the sender ( Definition 134 ). S i g {Sig} S i g : the signature of the sender ( Definition 135 ). E {E} E : the extra data for the extrinsic ( Definition 136 ). M i {M}_{{i}} M i ‚Äã : the indicator of the Polkadot module ( Definition 137 ). F i ( m ) {F}_{{i}}{\left({m}\right)} F i ‚Äã ( m ) : the indicator of the function of the Polkadot module ( Definition 138 ). Definition 134. Extrinsic Address ‚Äã Account Id, A i {A}_{{i}} A i ‚Äã , is the 32-byte address of the sender of the extrinsic as described in the external SS58 address format . Definition 135. Extrinsic Signature ‚Äã The signature, S i g {Sig} S i g , is a varying data type indicating the used signature type, followed by the signature created by the extrinsic author. The following types are supported: S i g : = { 0 , Ed25519, followed by: ( b 0 , ‚Ä¶ , b 63 ) 1 , Sr25519, followed by: ( b 0 , ‚Ä¶ , b 63 ) 2 , Ecdsa, followed by: ( b 0 , ‚Ä¶ , b 64 ) Sig := \begin{cases} 0, & \text{Ed25519, followed by: } (b_0, \ldots,b_{63}) \\ 1, & \text{Sr25519, followed by: } (b_0, \ldots,b_{63}) \\ 2, & \text{Ecdsa, followed by: } (b_0, \ldots,b_{64}) \end{cases} S i g := ‚é© ‚é® ‚éß ‚Äã 0 , 1 , 2 , ‚Äã Ed25519, followed by: ( b 0 ‚Äã , ‚Ä¶ , b 63 ‚Äã ) Sr25519, followed by: ( b 0 ‚Äã , ‚Ä¶ , b 63 ‚Äã ) Ecdsa, followed by: ( b 0 ‚Äã , ‚Ä¶ , b 64 ‚Äã ) ‚Äã Signature types vary in size, but each individual type is always fixed-size and therefore does not contain a length prefix. Ed25519 and Sr25519 signatures are 512-bit while Ecdsa is 520-bit, where the last 8 bits are the recovery ID. The signature is created by signing payload P {P} P . P : = { R a w , if ‚à• R a w ‚à• ‚â§ 256 Blake2 ( R a w ) , if ‚à• R a w ‚à• > 256 R a w : = ( M i , F i ( m ) , E , R v , F v , H h ( G ) , H h ( B ) ) \begin{aligned} P &:= \begin{cases} Raw, & \text{if } \|Raw\| \leq 256 \\ \text{Blake2}(Raw), & \text{if } \|Raw\| > 256 \\ \end{cases} \\ Raw &:= (M_i, F_i(m), E, R_v, F_v, H_h(G), H_h(B)) \end{aligned} P R a w ‚Äã := { R a w , Blake2 ( R a w ) , ‚Äã if ‚à• R a w ‚à• ‚â§ 256 if ‚à• R a w ‚à• > 256 ‚Äã := ( M i ‚Äã , F i ‚Äã ( m ) , E , R v ‚Äã , F v ‚Äã , H h ‚Äã ( G ) , H h ‚Äã ( B )) ‚Äã where M i {M}_{{i}} M i ‚Äã : the module indicator ( Definition 137 ). F i ( m ) {F}_{{i}}{\left({m}\right)} F i ‚Äã ( m ) : the function indicator of the module ( Definition 138 ). E {E} E : the extra data ( Definition 136 ). R v {R}_{{v}} R v ‚Äã : a UINT32 containing the specification version ( spec_version ) of the Runtime ( Section C.4.1. ), which can be updated and is therefore subject to change. F v {F}_{{v}} F v ‚Äã : a UINT32 containing the transaction version ( transaction_version ) of the Runtime ( Section C.4.1. ), which can be updated and is therefore subject to change. H h ( G ) {H}_{{h}}{\left({G}\right)} H h ‚Äã ( G ) : a 32-byte array containing the genesis hash. H h ( B ) {H}_{{h}}{\left({B}\right)} H h ‚Äã ( B ) : a 32-byte array containing the hash of the block which starts the mortality period, as described in Definition 139 . Definition 136. Extra Data ‚Äã Extra data, E {E} E , is a tuple containing additional metadata about the extrinsic and the system it is meant to be executed in. E = ( T m o r , N , P t ) {E}\:={\left({T}_{mor},{N},{P}_{{t}}\right)} E = ( T m or ‚Äã , N , P t ‚Äã ) where T m o r {T}_{mor} T m or ‚Äã : contains the SCALE encoded mortality of the extrinsic ( Definition 139 ). N {N} N : a compact integer containing the nonce of the sender. The nonce must be incremented by one for each extrinsic created, otherwise, the Polkadot network will reject the extrinsic. P t {P}_{{t}} P t ‚Äã : a compact integer containing the transactor pay including tip. Definition 137. Module Indicator ‚Äã M i {M}_{{i}} M i ‚Äã is an indicator for the Runtime to which Polkadot module , m {m} m , the extrinsic should be forwarded to. M i {M}_{{i}} M i ‚Äã is a varying data type pointing to every module exposed to the network. M i : = { 0 , System 1 , Utility ‚Ä¶ 7 , Balances ‚Ä¶ M_i := \begin{cases} 0, & \text{System} \\ 1, & \text{Utility} \\ \ldots & \\ 7, & \text{Balances} \\ \ldots & \end{cases} M i ‚Äã := ‚é© ‚é® ‚éß ‚Äã 0 , 1 , ‚Ä¶ 7 , ‚Ä¶ ‚Äã System Utility Balances ‚Äã Definition 138. Function Indicator ‚Äã F i ( m ) {F}_{{i}}{\left({m}\right)} F i ‚Äã ( m ) is a tuple which contains an indicator, m i {m}_{{i}} m i ‚Äã , for the Runtime to which function within the Polkadot module , m {m} m , the extrinsic should be forwarded to. This indicator is followed by the concatenated and SCALE encoded parameters of the corresponding function, p a r a m s {p}{a}{r}{a}{m}{s} p a r a m s . F i ( m ) = ( m i , p a r a m s ) {F}_{{i}}{\left({m}\right)}\:={\left({m}_{{i}},{p}{a}{r}{a}{m}{s}\right)} F i ‚Äã ( m ) = ( m i ‚Äã , p a r a m s ) The value of m i {m}_{{i}} m i ‚Äã varies for each Polkadot module since every module offers different functions. As an example, the Balances module has the following functions: B a l a n c e s i : = { 0 , transfer 1 , set_balance 2 , force_transfer 3 , transfer_keep_alive ‚Ä¶ Balances_i := \begin{cases} 0, & \text{transfer} \\ 1, & \text{set\_balance} \\ 2, & \text{force\_transfer} \\ 3, & \text{transfer\_keep\_alive} \\ \ldots & \end{cases} B a l an ce s i ‚Äã := ‚é© ‚é® ‚éß ‚Äã 0 , 1 , 2 , 3 , ‚Ä¶ ‚Äã transfer set_balance force_transfer transfer_keep_alive ‚Äã 9.3.2. Mortality ‚Äã Definition 139. Extrinsic Mortality ‚Äã Extrinsic mortality is a mechanism which ensures that an extrinsic is only valid within a certain period of the ongoing Polkadot lifetime. Extrinsics can also be immortal, as clarified in Section 9.3.2.2. . The mortality mechanism works with two related values: M p e r {M}_{{{per}}} M p er ‚Äã : the period of validity in terms of block numbers from the block hash specified as H h ( B ) {H}_{{h}}{\left({B}\right)} H h ‚Äã ( B ) in the payload ( Definition 135 ). The requirement is M p e r ‚â• 4 {M}_{{{per}}}\geq{4} M p er ‚Äã ‚â• 4 and M p e r {M}_{{{per}}} M p er ‚Äã must be the power of two, such as 32 , 64 , 128 , etc. M p h a {M}_{{{pha}}} M p ha ‚Äã : the phase in the period that this extrinsic‚Äôs lifetime begins. This value is calculated with a formula, and validators can use this value in order to determine which block hash is included in the payload. The requirement is M p h a < M p e r {M}_{{{pha}}}<{M}_{{{per}}} M p ha ‚Äã < M p er ‚Äã . In order to tie a transaction‚Äôs lifetime to a certain block ( H i ( B ) {H}_{{i}}{\left({B}\right)} H i ‚Äã ( B ) ) after it was issued, without wasting precious space for block hashes, block numbers are divided into regular periods and the lifetime is instead expressed as a "phase" ( M p h a {M}_{{{pha}}} M p ha ‚Äã ) from these regular boundaries: M p h a = H i ( B ) m o d M p e r {M}_{pha}={H}_{{i}}{\left({B}\right)} \; mod \; {M}_{{{per}}} M p ha ‚Äã = H i ‚Äã ( B ) m o d M p er ‚Äã M p e r {M}_{{{per}}} M p er ‚Äã and M p h a {M}_{{{pha}}} M p ha ‚Äã are then included in the extrinsic, as clarified in Definition 136 , in the SCALE encoded form of T m o r {T}_{mor} T m or ‚Äã ( Section 9.3.2.2. ). Polkadot validators can use M p h a {M}_{{{pha}}} M p ha ‚Äã to figure out the block hash included in the payload, which will therefore result in a valid signature if the extrinsic is within the specified period or an invalid signature if the extrinsic "died". 9.3.2.1. Example ‚Äã The extrinsic author choses M p e r = 256 {M}_{{{per}}}={256} M p er ‚Äã = 256 at block 10'000 , resulting with M p h a = 16 {M}_{{{pha}}}={16} M p ha ‚Äã = 16 . The extrinsic is then valid for blocks ranging from 10'000 to 10'256 . 9.3.2.2. Encoding ‚Äã T m o r {T}_{mor} T m or ‚Äã refers to the SCALE encoded form of type M p e r {M}_{{{per}}} M p er ‚Äã and M p h a {M}_{{{pha}}} M p ha ‚Äã . T m o r {T}_{mor} T m or ‚Äã is the size of two bytes if the extrinsic is considered mortal, or simply one bytes with a value equal to zero if the extrinsic is considered immortal. T m o r = E n c S C ( M p e r , M p h a ) {T}_{mor}\:={E}{n}{c}_{{{S}{C}}}{\left({M}_{{{per}}},{M}_{{{pha}}}\right)} T m or ‚Äã = E n c S C ‚Äã ( M p er ‚Äã , M p ha ‚Äã ) The SCALE encoded representation of mortality T m o r {T}_{mor} T m or ‚Äã deviates from most other types, as it‚Äôs specialized to be the smallest possible value, as described in Encode Mortality and Decode Mortality . If the extrinsic is immortal, specify a single byte with a value equal to zero. Algorithm 25. Encode Mortality ‚Äã \begin{algorithm} \caption{Encode Mortality} \begin{algorithmic} \Require{$M_{per}, M_{pha}$} \Return $0 \enspace \textbf{if} \enspace \textit{extrinsic is immortal}$ \State \textbf{init} $factor = $\call{Limit}{$M_{per} \gg 12, 1, \phi$} \State \textbf{init} $left = $\call{Limit}{\call{TZ}{$M_{per}$}$ - 1, 1, 15$} \State \textbf{init} $right = \frac{M_{pha}}{factor} \ll 4$ \Return $left|right$ \end{algorithmic} \end{algorithm} Algorithm 26. Decode Mortality ‚Äã \begin{algorithm} \caption{Decode Mortality} \begin{algorithmic} \Require{$T_{mor}$} \Return $\textit{Immortal} \enspace \textbf{if} \enspace T^{b0}_{mor} = 0$ \State \textbf{init} $enc = T^{b0}_{mor} + (T^{b1}_{mor} \ll 8)$ \State \textbf{init} $M_{per} = 2 \ll (enc\ mod\ (1 \ll 4))$ \State \textbf{init} $factor =$ \call{Limit}{$M_{per} \gg 12, 1, \phi$} \State \textbf{init} $M_{pha} = (enc \gg 4) * factor$ \Return $(M_{per}, M_{pha})$ \end{algorithmic} \end{algorithm} where T { m o r } b 0 {T}^{{{b}{0}}}_{\left\lbrace{mor}\right\rbrace} T { m or } b 0 ‚Äã : the first byte of T m o r {T}_{mor} T m or ‚Äã . T { m o r } b 1 {T}^{{{b}{1}}}_{\left\lbrace{mor}\right\rbrace} T { m or } b 1 ‚Äã : the second byte of T m o r {T}_{mor} T m or ‚Äã . Limit( n u m {num} n u m , m i n {min} min , m a x {max} ma x ): Ensures that n u m {num} n u m is between m i n {min} min and m a x {max} ma x . If m i n {min} min or m a x {max} ma x is defined as œï \phi œï , then there is no requirement for the specified minimum/maximum. TZ( n u m {num} n u m ): returns the number of trailing zeros in the binary representation of n u m {num} n u m . For example, the binary representation of 40 is 0010 1000 , which has three trailing zeros. ‚â´ \gg ‚â´ : performs a binary right shift operation. ‚â™ \ll ‚â™ : performs a binary left shift operation. ‚à£ {\mid} ‚à£ : performs a bitwise OR operation. Previous Polkadot Runtime Next 10. Weights 9.1. Introduction 9.2. Preliminaries 9.3. Extrinsics Body 9.3.1. Version 4 9.3.2. Mortality 
10. Weights | Polkadot Protocol Specification Runtime 10. Weights On this page 10. Weights 10.1. Motivation ‚Äã The Polkadot network, like any other permissionless system, needs to implement a mechanism to measure and limit the usage in order to establish an economic incentive structure, prevent network overload, and mitigate DoS vulnerabilities. In particular, Polkadot enforces a limited time window for block producers to create a block, including limitations on block size, which can make the selection and execution of certain extrinsics too expensive and decelerate the network. In contrast to some other systems, such as Ethereum, which implement fine measurement for each executed low-level operation by smart contracts, known as gas metering, Polkadot takes a more relaxed approach by implementing a measuring system where the cost of the transactions (referred to as ‚Äôextrinsics‚Äô) are determined before execution and are known as the weight system. The Polkadot weight system introduces a mechanism for block producers to measure the cost of running the extrinsics and determine how "heavy" it is in terms of execution time. Within this mechanism, block producers can select a set of extrinsics and saturate the block to its fullest potential without exceeding any limitations (as described in Section 10.2.1. ). Moreover, the weight system can be used to calculate a fee for executing each extrinsics according to its weight (as described in Section 10.6.1. ). Additionally, Polkadot introduces a specified block ratio (as defined in Section 10.2.1. ), ensuring that only a certain portion of the total block size gets used for regular extrinsics. The remaining space is reserved for critical, operational extrinsics required for the functionality of Polkadot itself. To begin, we introduce in Section 10.2. the assumption upon which the Polkadot transaction weight system is designed. In Section 10.2.1. , we discuss the limitation Polkadot needs to enforce on the block size. In Section 10.3. , we describe in detail the procedure upon which the weight of any transaction should be calculated. In Section 10.5. , we present how we apply this procedure to compute the weight of particular runtime functions. 10.2. Assumptions ‚Äã In this section, we define the concept of weight, and we discuss the considerations that need to be accounted for when assigning weight to transactions. These considerations are essential in order for the weight system to deliver its fundamental mission, i.e. the fair distribution of network resources and preventing a network overload. In this regard, weights serve as an indicator on whether a block is considered full and how much space is left for remaining, pending extrinsics. Extrinsics that require too many resources are discarded. More formally, the weight system should: prevent the block from being filled with too many extrinsics avoid extrinsics where its execution takes too long, by assigning a transaction fee to each extrinsic proportional to their resource consumption. These concepts are formalized in Definition 140 and Definition 143 : Definition 140. Block Length ‚Äã For a block B {B} B with H e a d ( B ) {H}{e}{a}{d}{\left({B}\right)} H e a d ( B ) and B o d y ( B ) {B}{o}{\left.{d}{y}\right.}{\left({B}\right)} B o d y ( B ) the block length of B {B} B , L e n ( B ) {L}{e}{n}{\left({B}\right)} L e n ( B ) , is defined as the amount of raw bytes of B {B} B . Definition 141. Target Time per Block ‚Äã ·πÆargeted time per block denoted by T ( B ) {T}{\left({B}\right)} T ( B ) implies the amount of seconds that a new block should be produced by a validator. The transaction weights must consider T ( B ) {T}{\left({B}\right)} T ( B ) in order to set restrictions on time-intensive transactions in order to saturate the block to its fullest potential until T ( B ) {T}{\left({B}\right)} T ( B ) is reached. Definition 142. Block Target Time ‚Äã Available block ration reserved for normal, noted by R ( B ) {R}{\left({B}\right)} R ( B ) , is defined as the maximum weight of none-operational transactions in the Body of B {B} B divided by L e n ( B ) {L}{e}{n}{\left({B}\right)} L e n ( B ) . Definition 143. Block Limits ‚Äã PÃ±olkadot block limits, as defined here, should be respected by each block producer for the produced block B {B} B to be deemed valid: L e n ( B ) ‚â§ 5 √ó 1 ‚Ä≤ 024 √ó 1 ‚Ä≤ 024 = 5 ‚Ä≤ 242 ‚Ä≤ 880 {L}{e}{n}{\left({B}\right)}\le{5}\times{1}'{024}\times{1}'{024}={5}'{242}'{880} L e n ( B ) ‚â§ 5 √ó 1 ‚Ä≤ 024 √ó 1 ‚Ä≤ 024 = 5 ‚Ä≤ 242 ‚Ä≤ 880 Bytes T ( B ) = 6 {T}{\left({B}\right)}={6} T ( B ) = 6 seconds R ( B ) ‚â§ 0.75 {R}{\left({B}\right)}\le{0.75} R ( B ) ‚â§ 0.75 Definition 144. Weight Function ‚Äã The PÃ±olkadot transaction weight function denoted by W {\mathcal{{{W}}}} W as follows: W : E ‚Üí N W : E ‚Ü¶ w \begin{aligned} \mathcal{W} &: \mathcal{E} \rightarrow \mathbb{N} \\ \mathcal{W} &: E \mapsto w \end{aligned} W W ‚Äã : E ‚Üí N : E ‚Ü¶ w ‚Äã where w {w} w is a non-negative integer representing the weight of the extrinsic E {E} E . We define the weight of all inherent extrinsics as defined in the Section 2.3.3. to be equal to 0. We extend the definition of W {\mathcal{{{W}}}} W function to compute the weight of the block as sum of weight of all extrinsics it includes: W : B ‚Üí N W : B ‚Ü¶ ‚àë E ‚àà B ( W ( E ) ) \begin{aligned} \mathcal{W} &: \mathcal{B} \rightarrow \mathbb{N} \\ \mathcal{W} &: B \mapsto \sum_{E\in B}(W(E)) \end{aligned} W W ‚Äã : B ‚Üí N : B ‚Ü¶ E ‚àà B ‚àë ‚Äã ( W ( E )) ‚Äã In the remainder of this section, we discuss the requirements to which the weight function needs to comply to. Computations of function W ( E ) {\mathcal{{{W}}}}{\left({E}\right)} W ( E ) must be determined before execution of that E {E} E . Due to the limited time window, computations of W {\mathcal{{{W}}}} W must be done quickly and consume few resources themselves. W {\mathcal{{{W}}}} W must be self contained and must not require I/O on the chain state. W ( E ) {\mathcal{{{W}}}}{\left({E}\right)} W ( E ) must depend solely on the Runtime function representing E {E} E and its parameters. Heuristically, "heaviness" corresponds to the execution time of an extrinsic. In that way, the W {\mathcal{{{W}}}} W value for various extrinsics should be proportional to their execution time. For example, if Extrinsic A takes three times longer to execute than Extrinsic B, then Extrinsic A should roughly weighs 3 times of Extrinsic B. Or: W ( A ) ‚âà 3 √ó W ( B ) {\mathcal{{{W}}}}{\left({A}\right)}\approx{3}\times{\mathcal{{{W}}}}{\left({B}\right)} W ( A ) ‚âà 3 √ó W ( B ) Nonetheless, W ( E ) {\mathcal{{{W}}}}{\left({E}\right)} W ( E ) can be manipulated depending on the priority of E {E} E the chain is supposed to endorse. 10.2.1. Limitations ‚Äã In this section, we discuss how applying the limitation defined in Definition 143 can be translated to limitation W {\mathcal{{{W}}}} W . In order to be able to translate those into concrete numbers, we need to identify an arbitrary maximum weight to which we scale all other computations. For that, we first define the block weight and then assume a maximum on its block length in Definition 145 : Definition 145. Block Weight ‚Äã We define the block weight of block B {B} B , formally denoted as W ( B ) {\mathcal{{{W}}}}{\left({B}\right)} W ( B ) , to be: W ( B ) = ‚àë { n = 0 } ‚à£ E ‚à£ ( W ( E n ) ) {\mathcal{{{W}}}}{\left({B}\right)}=\sum^{{{\left|{\mathcal{{{E}}}}\right|}}}_{\left\lbrace{n}={0}\right\rbrace}{\left({W}{\left({E}_{{n}}\right)}\right)} W ( B ) = { n = 0 } ‚àë ‚à£ E ‚à£ ‚Äã ( W ( E n ‚Äã ) ) We require that: W ( B ) < 2 ‚Ä≤ 000 ‚Ä≤ 000 ‚Ä≤ 000 ‚Ä≤ 000 {\mathcal{{{W}}}}{\left({B}\right)}<{2}'{000}'{000}'{000}'{000} W ( B ) < 2 ‚Ä≤ 000 ‚Ä≤ 000 ‚Ä≤ 000 ‚Ä≤ 000 The weights must fulfill the requirements as noted by the fundamentals and limitations and can be assigned as the author sees fit. As a simple example, consider a maximum block weight of 1‚Äô000‚Äô000‚Äô000, an available ratio of 75%, and a targeted transaction throughput of 500 transactions. We could assign the (average) weight for each transaction at about 1‚Äô500‚Äô000. Block producers have an economic incentive to include as many extrinsics as possible (without exceeding limitations) into a block before reaching the targeted block time. Weights give indicators to block producers on which extrinsics to include in order to reach the blocks fullest potential. 10.3. Calculation of the weight function ‚Äã In order to calculate weight of block B {B} B , W ( B ) {\mathcal{{{W}}}}{\left({B}\right)} W ( B ) , one needs to evaluate the weight of each transaction included in the block. Each transaction causes the execution of certain Runtime functions. As such, to calculate the weight of a transaction, those functions must be analyzed in order to determine parts of the code which can significantly contribute to the execution time and consume resources such as loops, I/O operations, and data manipulation. Subsequently, the performance and execution time of each part will be evaluated based on variety of input parameters. Based on those observations, weights are assigned Runtime functions or parameters which contribute to long execution times. These sub component of the code are discussed in Section 10.4.1. . The general algorithm to calculate W ( E ) {\mathcal{{{W}}}}{\left({E}\right)} W ( E ) is described in the Section 10.4. . 10.4. Benchmarking ‚Äã Calculating the extrinsic weight solely based on the theoretical complexity of the underlying implementation proves to be too complicated and unreliable at the same time. Certain decisions in the source code architecture, internal communication within the Runtime or other design choices could add enough overhead to make the asymptotic complexity practically meaningless. On the other hand, benchmarking an extrinsics in a black-box fashion could (using random parameters) most certainly results in missing corner cases and worst case scenarios. Instead, we benchmark all available Runtime functions which are invoked in the course of execution of extrinsics with a large collection of carefully selected input parameters and use the result of the benchmarking process to evaluate W ( E ) {\mathcal{{{W}}}}{\left({E}\right)} W ( E ) . In order to select useful parameters, the Runtime functions have to be analyzed to fully understand which behaviors or conditions can result in expensive execution times, which is described closer in Section 10.4.1. . Not every possible benchmarking outcome can be invoked by varying input parameters of the Runtime function. In some circumstances, preliminary work is required before a specific benchmark can be reliably measured, such as creating certain preexisting entries in the storage or other changes to the environment. The Practical Examples ( Section 10.5. ) covers the analysis process and the implementation of preliminary work in more detail. 10.4.1. Primitive Types ‚Äã The Runtime reuses components, known as "primitives", to interact with the state storage. The execution cost of those primitives can be measured and a weight should be applied for each occurrence within the Runtime code. For storage, Polkadot uses three different types of storage types across its modules, depending on the context: Value : Operations on a single value. The final key-value pair is stored under the key: hash(module_prefix) + hash(storage_prefix) Map : Operations on multiple values, datasets, where each entry has its corresponding, unique key. The final key-value pair is stored under the key: hash(module_prefix) + hash(storage_prefix) + hash(encode(key)) Double map : Just like Map , but uses two keys instead of one. This type is also known as "child storage", where the first key is the "parent key" and the second key is the "child key". This is useful in order to scope storage entries (child keys) under a certain context (parent key), which is arbitrary. Therefore, one can have separated storage entries based on the context. The final key-value pair is stored under the key: hash(module_prefix) + hash(storage_prefix) + hash(encode(key1)) + hash(encode(key2)) It depends on the functionality of the Runtime module (or its sub-processes, rather) which storage type to use. In some cases, only a single value is required. In others, multiple values need to be fetched or inserted from/into the database. Those lower-level types get abstracted over in each individual Runtime module using the decl_storage! macro. Therefore, each module specifies its own types that are used as input and output values. The abstractions do give indicators on what operations must be closely observed and where potential performance penalties and attack vectors are possible. 10.4.1.1. Considerations ‚Äã The storage layout is mostly the same for every primitive type, primarily differentiated by using special prefixes for the storage key. Big differences arise on how the primitive types are used in the Runtime function, on whether single values or entire datasets are being worked on. Single value operations are generally quite cheap and its execution time does not vary depending on the data that‚Äôs being processed. However, excessive overhead can appear when I/O operations are executed repeatedly, such as in loops. Especially, when the amount of loop iterations can be influenced by the caller of the function or by certain conditions in the state storage. Maps, in contrast, have additional overhead when inserting or retrieving datasets, which vary in sizes. Additionally, the Runtime function has to process each item inside that list. Indicators for performance penalties: Fixed iterations and datasets - Fixed iterations and datasets can increase the overall cost of the Runtime functions, but the execution time does not vary depending on the input parameters or storage entries. A base Weight is appropriate in this case. Adjustable iterations and datasets - If the amount of iterations or datasets depends on the input parameters of the caller or specific entries in storage, then a certain weight should be applied for each (additional) iteration or item. The Runtime defines the maximum value for such cases. If it doesn‚Äôt, it unconditionally has to and the Runtime module must be adjusted. When selecting parameters for benchmarking, the benchmarks should range from the minimum value to the maximum value, as described in Definition 146 . Input parameters - Input parameters that users pass on to the Runtime function can result in expensive operations. Depending on the data type, it can be appropriate to add additional weights based on certain properties, such as data size, assuming the data type allows varying sizes. The Runtime must define limits on those properties. If it doesn‚Äôt, it unconditionally has to, and the Runtime module must be adjusted. When selecting parameters for benchmarking, the benchmarks should range from the minimum values to the maximum value, as described in paragraph Definition 146 . Definition 146. Maximum Value ‚Äã What the maximum value should be really depends on the functionality that the Runtime function is trying to provide. If the choice for that value is not obvious, then it‚Äôs advised to run benchmarks on a big range of values and pick a conservative value below the targeted time per block limit as described in section Section 10.2.1. . 10.4.2. Parameters ‚Äã The input parameters highly vary depending on the Runtime function and must therefore be carefully selected. The benchmarks should use input parameters which will most likely be used in regular cases, as intended by the authors, but must also consider worst-case scenarios and inputs that might decelerate or heavily impact the performance of the function. The input parameters should be randomized in order to cause various effects in behaviors on certain values, such as memory relocations and other outcomes that can impact performance. It‚Äôs not possible to benchmark every single value. However, one should select a range of inputs to benchmark, spanning from the minimum value to the maximum value, which will most likely exceed the expected usage of that function. This is described in more detail in Section 10.4.1.1. . The benchmarks should run individual executions/iterations within that range, where the chosen parameters should give insight on the execution time. Selecting imprecise parameters or too extreme ranges might indicate an inaccurate result of the function as it will be used in production. Therefore, when a range of input parameters gets benchmarked, the result of each individual parameter should be recorded and optionally visualized, then the necessary adjustment can be made. Generally, the worst-case scenario should be assigned as the weight value for the corresponding runtime function. Additionally, given the distinction between theoretical and practical usage, the author reserves the right to make adjustments to the input parameters and assign weights according to the observed behavior of the actual, real-world network. 10.4.2.1. Weight Refunds ‚Äã When assigning the final weight, the worst-case scenario of each runtime function should be used. The runtime can then additional "refund" the amount of weights which were overestimated once the runtime function is actually executed. The Polkadot runtime only returns weights if the difference between the assigned weight and the actual weight calculated during execution is greater than 20%. 10.4.3. Storage I/O cost ‚Äã It is advised to benchmark the raw I/O operations of the database and assign "base weights" for each I/O operation type, such as insertion, deletion, querying, etc. When a runtime function is executed, the runtime can then add those base weights of each used operation in order to calculate the final weight. 10.4.4. Environment ‚Äã The benchmarks should be executed on clean systems without interference of other processes or software. Additionally, the benchmarks should be executed on multiple machines with different system resources, such as CPU performance, CPU cores, RAM, and storage speed. 10.5. Practical examples ‚Äã This section walks through Runtime functions available in the Polkadot Runtime to demonstrate the analysis process as described in Section 10.4.1. . In order for certain benchmarks to produce conditions where resource heavy computation or excessive I/O can be observed, the benchmarks might require some preliminary work on the environment, since those conditions cannot be created with simply selected parameters. The analysis process shows indicators on how the preliminary work should be implemented. 10.5.1. Practical Example # 1: request_judgement ‚Äã In Polkadot, accounts can save information about themselves on-chain, known as the "Identity Info". This includes information such as display name, legal name, email address and so on. Polkadot offers a set of trusted registrars, entities elected by a Polkadot public referendum, which can verify the specified contact addresses of the identities, such as Email, and vouch on whether the identity actually owns those accounts. This can be achieved, for example, by sending a challenge to the specified address and requesting a signature as a response. The verification is done off-chain, while the final judgement is saved on-chain, directly in the corresponding Identity Info. It‚Äôs also noteworthy that Identity Info can contain additional fields, set manually by the corresponding account holder. Information such as legal name must be verified by ID card or passport submission. The function request_judgement from the identity pallet allows users to request judgment from a specific registrar. (func $request_judgement (param $req_index int) (param $max_fee int)) req_index : the index which is assigned to the registrar. max_fee : the maximum fee the requester is willing to pay. The judgment fee varies for each registrar. Studying this function reveals multiple design choices that can impact performance, as it will be revealed by this analysis. 10.5.1.1. Analysis ‚Äã First, it fetches a list of current registrars from storage and then searches that list for the specified registrar index. let registrars = < Registrars < T >> :: get ( ) ; let registrar = registrars . get ( reg_index as usize ) . and_then ( Option :: as_ref ) . ok_or ( Error :: < T > :: EmptyIndex ) ? ; Then, it searches for the Identity Info from storage, based on the sender of the transaction. let mut id = < IdentityOf < T >> :: get ( & sender ) . ok_or ( Error :: < T > :: NoIdentity ) ? ; The Identity Info contains all fields that have a data in them, set by the corresponding owner of the identity, in an ordered form. It then proceeds to search for the specific field type that will be inserted or updated, such as email address. If the entry can be found, the corresponding value is to the value passed on as the function parameters (assuming the registrar is not "stickied", which implies it cannot be changed). If the entry cannot be found, the value is inserted into the index where a matching element can be inserted while maintaining sorted order. This results in memory reallocation, which increases resource consumption. match id . judgements . binary_search_by_key ( & reg_index , | x | x .0 ) { Ok ( i ) => if id . judgements [ i ] . 1 . is_sticky ( ) { Err ( Error :: < T > :: StickyJudgement ) ? } else { id . judgements [ i ] = item } , Err ( i ) => id . judgements . insert ( i , item ) , } In the end, the function deposits the specified max_fee balance, which can later be redeemed by the registrar. Then, an event is created to insert the Identity Info into storage. The creation of events is lightweight, but its execution is what will actually commit the state changes. T :: Currency :: reserve ( & sender , registrar . fee ) ? ; < IdentityOf < T >> :: insert ( & sender , id ) ; Self :: deposit_event ( RawEvent :: JudgementRequested ( sender , reg_index ) ) ; 10.5.1.2. Considerations ‚Äã The following points must be considered: Varying count of registrars. Varying count of preexisting accounts in storage. The specified registrar is searched for in the Identity Info. An identity can be judged by as many registrars as the identity owner issues requests, therefore increasing its footprint in the state storage. Additionally, if a new value gets inserted into the byte array, memory gets reallocated. Depending on the size of the Identity Info, the execution time can vary. The Identity-Info can contain only a few fields or many. It is legitimate to introduce additional weights for changes the owner/sender has influence over, such as the additional fields in the Identity-Info. 10.5.1.3. Benchmarking Framework ‚Äã The Polkadot Runtime specifies the MaxRegistrars constant, which will prevent the list of registrars of reaching an undesired length. This value should have some influence on the benchmarking process. The benchmarking implementation of for the function r e q u e s t {request} re q u es t j u d g e m e n t {judgement} j u d g e m e n t can be defined as follows: Algorithm 27. request_judgement Runtime Function Benchmark ‚Äã \begin{algorithm} \caption{"request\_judgement"` Runtime function benchmark} \begin{algorithmic} \Ensure $\mathcal{W}$ \State \textbf{init} $collection = \{\}$ \For{$amount \leftarrow 1,MaxRegistrars$} \State \call{Generate-Registrars}{$amount$} \State $caller \leftarrow$ \call{Create-Account}{$caller, 1$} \State \call{Set-Balance}{$caller, 100$} \State $time \leftarrow$ \call{Timer}{\call{Request-Judgement}{\call{Random}{$amount$}$, 100$}} \State \call{Add-To}{$collection, time$} \EndFor \State $\mathcal{W} \leftarrow$ \call{Compute-Weight}{$collection$} \Return $\mathcal{W}$ \end{algorithmic} \end{algorithm} where Generate-Registrars( a m o u n t {amount} am o u n t ) Creates a number of registrars and inserts those records into storage. Create-Account( n a m e {name} nam e , i n d e x {index} in d e x ) Creates a Blake2 hash of the concatenated input of name and index represent- ing the address of an account. This function only creates an address and does not conduct any I/O. Set-Balance( a m o u n t {amount} am o u n t , b a l a n c e {balance} ba l an ce ) Sets an initial balance for the specified account in the storage state. Timer( f u n c t i o n {function} f u n c t i o n ) Measures the time from the start of the specified function to its completion. Request-Judgement( r e g i s t r a r {registrar} re g i s t r a r i n d e x {index} in d e x , m a x {max} ma x f e e {fee} f ee ) Calls the corresponding request_judgement Runtime function and passes on the required parameters. Random( n u m {num} n u m ) Picks a random number between 0 and num. This should be used when the benchmark should account for unpredictable values. Add-To( c o l l e c t i o n {collection} co ll ec t i o n , t i m e {time} t im e ) Adds a returned time measurement (time) to collection. Compute-Weight( c o l l e c t i o n {collection} co ll ec t i o n ) Computes the resulting weight based on the time measurements in the collection. The worst-case scenario should be chosen (the highest value). 10.5.2. Practical Example # 2: payout_stakers ‚Äã 10.5.2.1. Analysis ‚Äã The function payout_stakers from the staking Pallet can be called by a single account in order to payout the reward for all nominators who back a particular validator. The reward also covers the validator‚Äôs share. This function is interesting because it iterates over a range of nominators, which varies, and does I/O operations for each of them. First, this function makes a few basic checks to verify if the specified era is not higher then the current era (as it is not in the future) and is within the allowed range also known as "history depth", as specified by the Runtime. After that, it fetches the era payout from storage and additionally verifies whether the specified account is indeed a validator and receives the corresponding "Ledger". The Ledger keeps information about the stash key, controller key, and other information such as actively bonded balance and a list of tracked rewards. The function only retains the entries of the history depth and conducts a binary search for the specified era. let era_payout = < ErasValidatorReward < T >> :: get ( & era ) . ok_or_else ( | | Error :: < T > :: InvalidEraToReward ) ? ; let controller = Self :: bonded ( & validator_stash ) . ok_or ( Error :: < T > :: NotStash ) ? ; let mut ledger = < Ledger < T >> :: get ( & controller ) . ok_or_else ( | | Error :: < T > :: NotController ) ? ; ledger . claimed_rewards . retain ( | & x | x >= current_era . saturating_sub ( history_depth ) ) ; match ledger . claimed_rewards . binary_search ( & era ) { Ok ( _ ) => Err ( Error :: < T > :: AlreadyClaimed ) ? , Err ( pos ) => ledger . claimed_rewards . insert ( pos , era ) , } The retained claimed rewards are inserted back into storage. < Ledger < T >> :: insert ( & controller , & ledger ) ; As an optimization, Runtime only fetches a list of the 64 highest-staked nominators, although this might be changed in the future. Accordingly, any lower-staked nominator gets no reward. let exposure = < ErasStakersClipped < T >> :: get ( & era , & ledger . stash ) ; Next, the function gets the era reward points from storage. let era_reward_points = < ErasRewardPoints < T >> :: get ( & era ) ; After that, the payout is split among the validator and its nominators. The validators receive the payment first, creating an insertion into storage and sending a deposit event to the scheduler. if let Some ( imbalance ) = Self :: make_payout ( & ledger . stash , validator_staking_payout + validator_commission_payout ) { Self :: deposit_event ( RawEvent :: Reward ( ledger . stash , imbalance . peek ( ) ) ) ; } Then, the nominators receive their payout rewards. The functions loop over the nominator list, conducting an insertion into storage and a creation of a deposit event for each of the nominators. for nominator in exposure . others . iter ( ) { let nominator_exposure_part = Perbill :: from_rational_approximation ( nominator . value , exposure . total , ) ; let nominator_reward : BalanceOf < T > = nominator_exposure_part * validator_leftover_payout ; // We can now make nominator payout: if let Some ( imbalance ) = Self :: make_payout ( & nominator . who , nominator_reward ) { Self :: deposit_event ( RawEvent :: Reward ( nominator . who . clone ( ) , imbalance . peek ( ) ) ) ; } } 10.5.2.2. Considerations ‚Äã The following points must be considered: The Ledger contains a varying list of claimed rewards. Fetching, retaining, and searching through it can affect execution time. The retained list is inserted back into storage. Looping through a list of nominators and creating I/O operations for each increases execution time. The Runtime fetches up to 64 nominators. 10.5.2.3. Benchmarking Framework ‚Äã Definition 147. History Depth ‚Äã HÃ±istory Depth indicated as MaxNominatorRewardedPerValidator is a fixed constant specified by the Polkadot Runtime which dictates the number of Eras the Runtime will reward nominators and validators for. Definition 148. Maximum Nominator Reward ‚Äã MÃ±aximum Nominator Rewarded Per Validator indicated as MaxNominatorRewardedPerValidator , specifies the maximum amount of the highest-staked nominators which will get a reward. Those values should have some influence in the benchmarking process. The benchmarking implementation for the function p a y o u t {payout} p a yo u t s t a k e r s {stakers} s t ak ers can be defined as follows: Algorithm 28. payout_stakers Runtime Function Benchmark ‚Äã \begin{algorithm} \caption{"payout\_stakers"` Runtime function benchmark} \begin{algorithmic} \Ensure $\mathcal{W}$ \State \textbf{init} $collection = \{\}$ \For{$amount \leftarrow 1,MaxNominatorRewardedPerValidator$} \For{$era\_depth \leftarrow 1,HistoryDepth$} \State $validator \leftarrow$ \call{Generate-Validator}{} \State \call{Validate}{$validator$} \State $nominators \leftarrow$ \call{Generate-Nominators}{$amount$} \For{$nominator \in nominators$} \State \call{Nominate}{$validator, nominator$} \EndFor \State $era\_index \leftarrow$ \call{Create-Rewards}{$validator, nominators, era\_depth$} \State $time \leftarrow$ \call{Timer}{\call{Payout-Stakers}{$validator$}$, era\_index$} \State \call{Add-To}{$collection, time$} \EndFor \EndFor \State $\mathcal{W} \leftarrow$ \call{Compute-Weight}{$collection$} \Return $\mathcal{W}$ \end{algorithmic} \end{algorithm} where Generate-Validator() Creates a validator with some unbonded balances. Validate( v a l i d a t o r {validator} v a l i d a t or ) Bonds balances of validator and bonds balances. Generate-Nominators( a m o u n t {amount} am o u n t ) Creates the amount of nominators with some unbonded balances. Nominate( v a l i d a t o r {validator} v a l i d a t or , n o m i n a t o r {nominator} n o mina t or ) Starts nomination of nominator for validator by bonding balances. Create-Rewards( v a l i d a t o r {validator} v a l i d a t or , n o m i n a t o r s {nominators} n o mina t ors , e r a {era} er a d e p t h {depth} d e pt h ) Starts an Era and creates pending rewards for validator and nominators. Timer( f u n c t i o n {function} f u n c t i o n ) Measures the time from the start of the specified function to its completion. Add-To( c o l l e c t i o n {collection} co ll ec t i o n , t i m e {time} t im e ) Adds a returned time measurement (time) to collection. Compute-Weight( c o l l e c t i o n {collection} co ll ec t i o n ) Computes the resulting weight based on the time measurements in the collection. The worst-case scenario should be chosen (the highest value). 10.5.3. Practical Example # 3: transfer ‚Äã The t r a n s f e r {transfer} t r an s f er function of the balances module is designed to move the specified balance by the sender to the receiver. 10.5.3.1. Analysis ‚Äã The source code of this function is quite short: let transactor = ensure_signed ( origin ) ? ; let dest = T :: Lookup :: lookup ( dest ) ? ; < Self as Currency < _ >> :: transfer ( & transactor , & dest , value , ExistenceRequirement :: AllowDeath ) ? ; However, one needs to pay close attention to the property AllowDeath and to how the function treats existings and non-existing accounts differently. Two types of behaviors are to consider: If the transfer completely depletes the sender account balance to zero (or below the minimum "keep-alive" requirement), it removes the address and all associated data from storage. If the recipient account has no balance, the transfer also needs to create the recipient account. 10.5.3.2. Considerations ‚Äã Specific parameters can could have a significant impact for this specific function. In order to trigger the two behaviors mentioned above, the following parameters are selected: Type From To Description Account index index in‚Ä¶‚Äã 1 1000 Used as a seed for account creation Balance balance in‚Ä¶‚Äã 2 1000 Sender balance and transfer amount Executing a benchmark for each balance increment within the balance range for each index increment within the index range will generate too many variants ( 1000 √ó 999 {1000}\times{999} 1000 √ó 999 ) and highly increase execution time. Therefore, this benchmark is configured to first set the balance at value 1‚Äô000 and then to iterate from 1 to 1‚Äô000 for the index value. Once the index value reaches 1‚Äô000, the balance value will reset to 2 and iterate to 1‚Äô000 (see "transfer" Runtime function benchmark for more detail): index : 1, balance : 1000 index : 2, balance : 1000 index : 3, balance : 1000 ‚Ä¶‚Äã index : 1000, balance : 1000 index : 1000, balance : 2 index : 1000, balance : 3 index : 1000, balance : 4 ‚Ä¶‚Äã The parameters themselves do not influence or trigger the two worst conditions and must be handled by the implemented benchmarking tool. The t r a n s f e r {transfer} t r an s f er benchmark is implemented as defined in "transfer" Runtime function benchmark . 10.5.3.3. Benchmarking Framework ‚Äã The benchmarking implementation for the Polkadot Runtime function t r a n s f e r {transfer} t r an s f er is defined as follows (starting with the Main function): Algorithm 29. transfer Runtime Function Benchmark ‚Äã \begin{algorithm} \caption{"transfer" Runtime function benchmark} \begin{algorithmic} \Ensure{$collection$: a collection of time measurements of all benchmark iterations} \Function{Main}{} \State \textbf{init} $collection = \{ \}$ \State \textbf{init} $balance = 1'000$ \For{$index \gets 1,1'000$} \State $time \leftarrow$ \call{Run-Benchmark}{$index, balance$} \State \call{Add-To}{$collection, time$} \EndFor \State \textbf{init} $index = 1'000$ \For{$balance \gets 2,1'000$} \State $time \leftarrow$ \call{Run-Benchmark}{$index, balance$} \State \call{Add-To}{$collection, time$} \EndFor \State $\mathcal{W} \leftarrow$ \call{Compute-Weight}{$collection$} \Return $\mathcal{W}$ \EndFunction \Function{Run-Benchmark}{$index$, $balance$} \State $sender \leftarrow$ \call{Create-Account}{$caller, index$} \State $recipient \leftarrow$ \call{Create-Accouny}{$recipient, index$} \State \call{Set-Balance}{$sender, balance$} \State $time \leftarrow$ \call{Timer}{\call{Transfer}{$sender, recipient, balance$}} \Return $time$ \EndFunction \end{algorithmic} \end{algorithm} where Create-Account( n a m e {name} nam e , i n d e x {index} in d e x ) Creates a Blake2 hash of the concatenated input of name and index representing the address of a account. This function only creates an address and does not conduct any I/O. Set-Balance( a c c o u n t {account} a cco u n t , b a l a n c e {balance} ba l an ce ) Sets a initial balance for the specified account in the storage state. Transfer( s e n d e r {sender} se n d er , r e c i p i e n t {recipient} rec i p i e n t , b a l a n c e {balance} ba l an ce ) Transfers the specified balance from sender to recipient by calling the corresponding Runtime function. This represents the target Runtime function to be benchmarked. Add-To( c o l l e c t i o n {collection} co ll ec t i o n , t i m e {time} t im e ) Adds a returned time measurement (time) to collection. Timer( f u n c t i o n {function} f u n c t i o n ) Adds a returned time measurement (time) to collection. Compute-Weight( c o l l e c t i o n {collection} co ll ec t i o n ) Computes the resulting weight based on the time measurements in the collection. The worst case scenario should be chosen (the highest value). 10.5.4. Practical Example # 4: withdraw_unbounded ‚Äã The withdraw_unbonded function of the staking module is designed to move any unlocked funds from the staking management system to be ready for transfer. It contains some operations which have some I/O overhead. 10.5.4.1. Analysis ‚Äã Similarly to the payout_stakers function ( Section 10.5.2. ), this function fetches the Ledger which contains information about the stash, such as bonded balance and unlocking balance (balance that will eventually be freed and can be withdrawn). if let Some ( current_era ) = Self :: current_era ( ) { ledger = ledger . consolidate_unlocked ( current_era ) } The function consolidate_unlocked does some cleaning up on the ledger, where it removes outdated entries from the unlocking balance (which implies that balance is now free and is no longer awaiting unlock). let mut total = self . total ; let unlocking = self . unlocking . into_iter ( ) . filter ( | chunk | if chunk . era > current_era { true } else { total = total . saturating_sub ( chunk . value ) ; false } ) . collect ( ) ; This function does a check on wether the updated ledger has any balance left in regards to staking, both in terms of locked, staking balance and unlocking balance. If not amount is left, the all information related to the stash will be deleted. This results in multiple I/O calls. if ledger . unlocking . is_empty ( ) && ledger . active . is_zero ( ) { // This account must have called `unbond()` with some value that caused the active // portion to fall below existential deposit + will have no more unlocking chunks // left. We can now safely remove all staking-related information. Self :: kill_stash ( & stash , num_slashing_spans ) ? ; // remove the lock. T :: Currency :: remove_lock ( STAKING_ID , & stash ) ; // This is worst case scenario, so we use the full weight and return None None } The resulting call to Self::kill_stash() triggers: slashing :: clear_stash_metadata :: < T > ( stash , num_slashing_spans ) ? ; < Bonded < T >> :: remove ( stash ) ; < Ledger < T >> :: remove ( & controller ) ; < Payee < T >> :: remove ( stash ) ; < Validators < T >> :: remove ( stash ) ; < Nominators < T >> :: remove ( stash ) ; Alternatively, if there‚Äôs some balance left, the adjusted ledger simply gets updated back into storage. // This was the consequence of a partial unbond. just update the ledger and move on. Self :: update_ledger ( & controller , & ledger ) ; Finally, it withdraws the unlocked balance, making it ready for transfer: let value = old_total - ledger . total ; Self :: deposit_event ( RawEvent :: Withdrawn ( stash , value ) ) ; 10.5.4.2. Parameters ‚Äã The following parameters are selected: Type From To Description Account index index in‚Ä¶‚Äã 0 1000 Used as a seed for account creation This benchmark does not require complex parameters. The values are used solely for account generation. 10.5.4.3. Considerations ‚Äã Two important points in the withdraw_unbonded function must be considered. The benchmarks should trigger both conditions The updated ledger is inserted back into storage. If the stash gets killed, then multiple, repetitive deletion calls are performed in the storage. 10.5.4.4. Benchmarking Framework ‚Äã The benchmarking implementation for the Polkadot Runtime function withdraw_unbonded is defined as follows: Algorithm 30. withdraw_unbonded Runtime Function Benchmark ‚Äã \begin{algorithm} \caption{"withdraw\_unbonded" Runtime function benchmark} \begin{algorithmic} \Ensure $\mathcal{W}$ \Function{Main}{} \State \textbf{init} $collection = \{\}$ \For{$balance \gets 1,100$} \State $stash \leftarrow$ \call{Create-Account}{$stash, 1$} \State $controller \leftarrow$ \call{Create-Account}{$controller, 1$} \State \call{Set-Balance}{$stash, 100$} \State \call{Set-Balance}{$controller, 1$} \State \call{Bond}{$stash, controller, balance$} \State \call{Pass-Era}{} \State \call{UnBond}{$controller, balance$} \State \call{Pass-Era}{} \State $time \leftarrow$ \call{Timer}{\call{Withdraw-Unbonded}{$controller$}} \State \call{Add-To}{$collection, time$} \EndFor \State $\mathcal{W} \leftarrow$ \call{Compute-Weight}{$collection$} \Return $\mathcal{W}$ \EndFunction \end{algorithmic} \end{algorithm} where Create-Account( n a m e {name} nam e , i n d e x index in d e x ) Creates a Blake2 hash of the concatenated input of name and index representing the address of a account. This function only creates an address and does not conduct any I/O. Set-Balance( a m o u n t {amount} am o u n t , b a l a n c e {balance} ba l an ce ) Sets a initial balance for the specified account in the storage state. Bond( s t a s h {stash} s t a s h , c o n t r o l l e r {controller} co n t ro ll er , a m o u n t {amount} am o u n t ) Bonds the specified amount for the stash and controller pair. UnBond( a c c o u n t {account} a cco u n t , a m o u n t {amount} am o u n t ) Unbonds the specified amount for the given account. Pass-Era() Pass one era. Forces the function withdraw_unbonded to update the ledger and eventually delete information. Withdraw-Unbonded( c o n t r o l l e r {controller} co n t ro ll er ) Withdraws the the full unbonded amount of the specified controller account. This represents the target Runtime function to be benchmarked. Add-To( c o l l e c t i o n {collection} co ll ec t i o n , t i m e {time} t im e ) Adds a returned time measurement (time) to collection. Timer( f u n c t i o n {function} f u n c t i o n ) Measures the time from the start of the specified f unction to its completion. Compute-Weight( c o l l e c t i o n {collection} co ll ec t i o n ) Computes the resulting weight based on the time measurements in the collection. The worst case scenario should be chosen (the highest value). 10.6. Fees ‚Äã Block producers charge a fee in order to be economically sustainable. That fee must always be covered by the sender of the transaction. Polkadot has a flexible mechanism to determine the minimum cost to include transactions in a block. 10.6.1. Fee Calculation ‚Äã Polkadot fees consists of three parts: Base fee: a fixed fee that is applied to every transaction and set by the Runtime. Length fee: a fee that gets multiplied by the length of the transaction, in bytes. Weight fee: a fee for each, varying Runtime function. Runtime implementers need to implement a conversion mechanism which determines the corresponding currency amount for the calculated weight. The final fee can be summarized as: f e e = b a s e f e e + length of transaction in bytes √ó length fee + weight to fee \begin{aligned} fee &= base\ fee \\ &{} + \text{length of transaction in bytes} \times \text{length fee} \\ &{} + \text{weight to fee} \end{aligned} f ee ‚Äã = ba se f ee + length of transaction in bytes √ó length fee + weight to fee ‚Äã 10.6.2. Definitions in Polkadot ‚Äã The Polkadot Runtime defines the following values: Base fee: 100 uDOTs Length fee: 0.1 uDOTs Weight to fee conversion: w e i g h t {weight} w e i g h t fee = weight \times (100 u D O T s √∑ ( 10 √ó 10 ‚Ä≤ 000 ) ) {u}{D}{O}{T}{s}\div{\left({10}\times{10}'{000}\right)}{)} u D O T s √∑ ( 10 √ó 10 ‚Ä≤ 000 ) ) A weight of 10‚Äô000 (the smallest non-zero weight) is mapped to 1 10 {\frac{{{1}}}{{{10}}}} 10 1 ‚Äã of 100 uDOT. This fee will never exceed the max size of an unsigned 128 bit integer. 10.6.3. Fee Multiplier ‚Äã Polkadot can add a additional fee to transactions if the network becomes too busy and starts to decelerate the system. This fee can create an incentive to avoid the production of low priority or insignificant transactions. In contrast, those additional fees will decrease if the network calms down and it can execute transactions without much difficulties. That additional fee is known as the Fee Multiplier and its value is defined by the Polkadot Runtime. The multiplier works by comparing the saturation of blocks; if the previous block is less saturated than the current block (implying an uptrend), the fee is slightly increased. Similarly, if the previous block is more saturated than the current block (implying a downtrend), the fee is slightly decreased. The final fee is calculated as: f i n a l f e e = f e e √ó F e e M u l t i p l i e r final fee = fee \times Fee Multiplier f ina l f ee = f ee √ó F ee M u lt i pl i er 10.6.3.1. Update Multiplier ‚Äã The Update Multiplier defines how the multiplier can change. The Polkadot Runtime internally updates the multiplier after each block according the following formula: d i f f = ( t a r g e t w e i g h t ‚àí p r e v i o u s b l o c k w e i g h t ) v = 0.00004 n e x t w e i g h t = w e i g h t √ó ( 1 + ( v √ó d i f f ) + ( v √ó d i f f ) 2 / 2 ) \begin{aligned} diff &=& (target\ weight - previous\ block\ weight) \\ v &=& 0.00004 \\ next\ weight &=& weight \times (1 + (v \times diff) + (v \times diff)^2 / 2) \end{aligned} d i ff v n e x t w e i g h t ‚Äã = = = ‚Äã ( t a r g e t w e i g h t ‚àí p re v i o u s b l oc k w e i g h t ) 0.00004 w e i g h t √ó ( 1 + ( v √ó d i ff ) + ( v √ó d i ff ) 2 /2 ) ‚Äã Polkadot defines the target_weight as 0.25 (25%). More information about this algorithm is described in the Web3 Foundation research paper . Previous 9. Extrinsics Next 11. Consensus 10.1. Motivation 10.2. Assumptions 10.2.1. Limitations 10.3. Calculation of the weight function 10.4. Benchmarking 10.4.1. Primitive Types 10.4.2. Parameters 10.4.3. Storage I/O cost 10.4.4. Environment 10.5. Practical examples 10.5.1. Practical Example #1: request_judgement 10.5.2. Practical Example #2: payout_stakers 10.5.3. Practical Example #3: transfer 10.5.4. Practical Example #4: withdraw_unbounded 10.6. Fees 10.6.1. Fee Calculation 10.6.2. Definitions in Polkadot 10.6.3. Fee Multiplier 
11. Consensus | Polkadot Protocol Specification Runtime 11. Consensus On this page 11. Consensus 11.1. BABE digest messages ‚Äã The Runtime is required to provide the BABE authority list and randomness to the host via a consensus message in the header of the first block of each epoch. The digest published in Epoch E n {\mathcal{{{E}}}}_{{n}} E n ‚Äã is enacted in E n + 1 {\mathcal{{{E}}}}_{{{n}+{1}}} E n + 1 ‚Äã . The randomness in this digest is computed based on all the VRF outputs up to including Epoch E n ‚àí 2 {\mathcal{{{E}}}}_{{{n}-{2}}} E n ‚àí 2 ‚Äã while the authority set is based on all transaction included up to Epoch E n ‚àí 1 {\mathcal{{{E}}}}_{{{n}-{1}}} E n ‚àí 1 ‚Äã . The computation of the randomness seed is described in Epoch-Randomness , which uses the concept of epoch subchain as described in host specification and the value d B {d}_{{B}} d B ‚Äã , which is the VRF output computed for slot s B {s}_{{B}} s B ‚Äã . Algorithm 31. Epoch Randomness ‚Äã \begin{algorithm} \caption{Epoch-Randomness} \begin{algorithmic} \Require $n > 2$ \State \textbf{init} $\rho \leftarrow \phi$ \For{$B$ in \call{SubChain}{$\mathcal{E}_{n-2}$}} \State $\rho \leftarrow \rho || d_B$ \EndFor \Return \call{Blake2b}{\call{Epoch-Randomness}{$n-1$}$||n||\rho$} \end{algorithmic} \end{algorithm} where n {n} n is the epoch index. Previous 10. Weights Next 12. Metadata 11.1. BABE digest messages 
12. Metadata | Polkadot Protocol Specification Runtime 12. Metadata On this page 12. Metadata The runtime metadata structure contains all the information necessary on how to interact with the Polkadot runtime. Considering that Polkadot runtimes are upgradable and, therefore, any interfaces are subject to change, the metadata allows developers to structure any extrinsics or storage entries accordingly. The metadata of a runtime is provided by a call to Metadata_metadata ( Section C.5.1. ) and is returned as a scale encoded ( Section A.2.2. ) binary blob. How to interpret and decode this data is described in this chapter. 12.1. Structure ‚Äã The Runtime Metadata is a data structure of the following format: ( M , v m , R , P , t e , v e , E , t r ) {\left({M},{v}_{{m}},{R},{P},{t}_{{e}},{v}_{{e}},{E},{t}_{{r}}\right)} ( M , v m ‚Äã , R , P , t e ‚Äã , v e ‚Äã , E , t r ‚Äã ) R = ( r 0 , ‚Ä¶ , r n ) {R}={\left({r}_{{0}},\ldots,{r}_{{n}}\right)} R = ( r 0 ‚Äã , ‚Ä¶ , r n ‚Äã ) P = ( p 0 , ‚Ä¶ , p n ) {P}={\left({p}_{{0}},\ldots,{p}_{{n}}\right)} P = ( p 0 ‚Äã , ‚Ä¶ , p n ‚Äã ) E = ( e 0 , ‚Ä¶ , e n ) {E}={\left({e}_{{0}},\ldots,{e}_{{n}}\right)} E = ( e 0 ‚Äã , ‚Ä¶ , e n ‚Äã ) where M {M} M are the first four constant bytes, spelling "meta" in ASCII. v m {v}_{{m}} v m ‚Äã is an unsigned 8-bit integer indicating the format version of the metadata structure (currently the value of 14 ). R {R} R is a sequence ( Definition 182 ) of type definitions r i {r}_{{i}} r i ‚Äã ( Definition 149 ). P {P} P is a sequence ( Definition 182 ) of pallet metadata p i {p}_{{i}} p i ‚Äã ( Section 12.2. ). t e {t}_{{e}} t e ‚Äã is the type Id ( Definition 150 ) of the extrinsics. v e {v}_{{e}} v e ‚Äã is an unsigned 8-bit integer indicating the format version of the extrinsics (implying a possible breaking change). E {E} E is a sequence ( Definition 182 ) of extrinsics metadata e i {e}_{{i}} e i ‚Äã ( Definition 160 ). t r {t}_{{r}} t r ‚Äã is the type Id ( Definition 150 ) of the runtime. Image 8. Metadata ‚Äã cluster__metadata Metadata metadata__seq pos size type id 0 4 magic 4 1 u1 metadata_version 5 ... Scale::CompactInt num_types ... ... MetadataType types repeat num_types.value times ... ... Scale::CompactInt num_pallets ... ... MetadataPallet pallets repeat num_pallets.value times ... ... Scale::CompactInt extrinsic_type ... 1 u1 extrinsic_version ... ... Scale::CompactInt num_extrinsics ... ... MetadataExtrinsic extrinsics repeat num_extrinsics.value times ... ... Scale::CompactInt runtime_type metadata_type__seq MetadataType metadata__seq:types_type->metadata_type__seq metadata_pallet__seq MetadataPallet metadata__seq:pallets_type->metadata_pallet__seq metadata_extrinsic__seq MetadataExtrinsic metadata__seq:extrinsics_type->metadata_extrinsic__seq Definition 149. Runtime Registry Type Entry ‚Äã A registry entry contains information about a type in its portable form for serialization. The entry is a data structure of the following format: r i = ( id t , p , T , D , c ) {r}_{{i}}={\left(\text{id}_{{t}},{p},{T},{D},{c}\right)} r i ‚Äã = ( id t ‚Äã , p , T , D , c ) T = ( t 0 , ‚Ä¶ , t n ) {T}={\left({t}_{{0}},\ldots,{t}_{{n}}\right)} T = ( t 0 ‚Äã , ‚Ä¶ , t n ‚Äã ) t i = ( n , y ) {t}_{{i}}={\left({n},{y}\right)} t i ‚Äã = ( n , y ) where id t \text{id}_{{t}} id t ‚Äã is a compact integer indicating the identifier of the type. p {p} p is the path of the type, optional and based on the source file location. Encoded as a sequence ( Definition 182 ) of strings. T {T} T is a sequence ( Definition 182 ) of generic parameters (empty for non-generic types). n {n} n is the name string of the generic type parameter y {y} y is a Option type containing a type Id ( Definition 150 ). D {D} D is the type definition ( Definition 151 ). c {c} c is the documentation as sequence ( Definition 182 ) of strings. Image 9. Metadata Type ‚Äã cluster__metadata_type MetadataType cluster__param MetadataType::Param metadata_type__seq pos size type id 0 ... Scale::CompactInt id ... ... Scale::StringList path ... ... Scale::CompactInt num_params ... ... Param params repeat num_params.value times ... ... MetadataTypeDefinition definition ... ... Scale::StringList docs param__seq pos size type id 0 ... Scale::String name ... ... Scale::MaybeCompactInt type metadata_type__seq:params_type->param__seq metadata_type_definition__seq MetadataTypeDefinition metadata_type__seq:definition_type->metadata_type_definition__seq Definition 150. Runtime Type Id ‚Äã The runtime type Id is a compact integer representing the index of the entry ( Definition 149 ) in R , P {R},{P} R , P or E {E} E of the runtime metadata structure ( Section 12.1. ), depending on context (starting at 0 {0} 0 ). Definition 151. Type Variant ‚Äã The type definition D {D} D is a varying datatype ( Definition 178 ) and indicates all the possible types of encodable values a type can have. D = { 0 ‚Üí C composite type (e.g. structure or tuple) 1 ‚Üí V variant type 2 ‚Üí s v sequence type varying length 3 ‚Üí S sequence with fixed length 4 ‚Üí T tuple type 5 ‚Üí P primitive type 6 ‚Üí e compact encoded type 7 ‚Üí B sequence of bits {D}={\left\lbrace\begin{matrix}{0}&\rightarrow&{C}&\text{composite type (e.g. structure or tuple)}\\{1}&\rightarrow&{V}&\text{variant type}\\{2}&\rightarrow&{s}_{{v}}&\text{sequence type varying length}\\{3}&\rightarrow&{S}&\text{sequence with fixed length}\\{4}&\rightarrow&{T}&\text{tuple type}\\{5}&\rightarrow&{P}&\text{primitive type}\\{6}&\rightarrow&{e}&\text{compact encoded type}\\{7}&\rightarrow&{B}&\text{sequence of bits}\end{matrix}\right.} D = ‚é© ‚é® ‚éß ‚Äã 0 1 2 3 4 5 6 7 ‚Äã ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí ‚Äã C V s v ‚Äã S T P e B ‚Äã composite type (e.g. structure or tuple) variant type sequence type varying length sequence with fixed length tuple type primitive type compact encoded type sequence of bits ‚Äã where C {C} C is a sequence of the following format: C = ( f 0 , ‚Ä¶ , f n ) {C}={\left({{f}_{{0}},}\ldots,{f}_{{n}}\right)} C = ( f 0 ‚Äã , ‚Ä¶ , f n ‚Äã ) f i {f_i} f i ‚Äã is a field ( Definition 152 ). V {V} V is a sequence of the following format: V = ( v 0 , ‚Ä¶ , v n ) {V}={\left({v}_{{0}},\ldots,{v}_{{n}}\right)} V = ( v 0 ‚Äã , ‚Ä¶ , v n ‚Äã ) v i {v}_{{i}} v i ‚Äã is a variant ( Definition 153 ). s v {s}_{{v}} s v ‚Äã is a type Id ( Definition 150 ). S {S} S is of the following format: S = ( l , y ) {S}={\left({l},{y}\right)} S = ( l , y ) l {l} l is an unsigned 32-bit integer indicating the length y {y} y is a type Id ( Definition 150 ). T {T} T is a sequence ( Definition 182 ) of type Ids ( Definition 150 ). P {P} P is a varying datatype ( Definition 178 ) of the following structure: P = { 0 boolean 1 char 2 string 3 unsigned 8-bit integer 4 unsigned 16-bit integer 5 unsigned 32-bit integer 6 unsigned 64-bit integer 7 unsigned 128-bit integer 8 unsigned 256-bit integer 9 signed 8-bit integer 10 signed 16-bit integer 11 signed 32-bit integer 12 signed 64-bit integer 13 signed 128-bit integer 14 signed 256-bit integer {P}={\left\lbrace\begin{matrix}{0}&\text{boolean}\\{1}&\text{char}\\{2}&\text{string}\\{3}&\text{unsigned 8-bit integer}\\{4}&\text{unsigned 16-bit integer}\\{5}&\text{unsigned 32-bit integer}\\{6}&\text{unsigned 64-bit integer}\\{7}&\text{unsigned 128-bit integer}\\{8}&\text{unsigned 256-bit integer}\\{9}&\text{signed 8-bit integer}\\{10}&\text{signed 16-bit integer}\\{11}&\text{signed 32-bit integer}\\{12}&\text{signed 64-bit integer}\\{13}&\text{signed 128-bit integer}\\{14}&\text{signed 256-bit integer}\end{matrix}\right.} P = ‚é© ‚é® ‚éß ‚Äã 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ‚Äã boolean char string unsigned 8-bit integer unsigned 16-bit integer unsigned 32-bit integer unsigned 64-bit integer unsigned 128-bit integer unsigned 256-bit integer signed 8-bit integer signed 16-bit integer signed 32-bit integer signed 64-bit integer signed 128-bit integer signed 256-bit integer ‚Äã e {e} e is a type Id ( Definition 150 ). B {B} B is a data structure of the following format: B = ( s , o ) {B}={\left({s},{o}\right)} B = ( s , o ) s {s} s is a type Id ( Definition 150 ) representing the bit store order ( external reference ) o {o} o is a type Id ( Definition 150 ) the bit order type ( external reference ). Image 10. Metadata Type Definition ‚Äã cluster__metadata_type_definition MetadataTypeDefinition cluster__array MetadataTypeDefinition::Array cluster__sequence MetadataTypeDefinition::Sequence cluster__primitive MetadataTypeDefinition::Primitive cluster__tuple MetadataTypeDefinition::Tuple cluster__bits MetadataTypeDefinition::Bits cluster__compact MetadataTypeDefinition::Compact metadata_type_definition__seq pos size type id 0 1 u1‚ÜíType type 1 ... switch (type) details metadata_type_definition__seq:type_type->metadata_type_definition__seq:details_type metadata_type_definition__seq_details_switch case type :type_composite MetadataTypeFields :type_bits Bits :type_array Array :type_sequence Sequence :type_primitive Primitive :type_compact Compact :type_variant MetadataTypeVariants :type_tuple Tuple metadata_type_definition__seq:details_type->metadata_type_definition__seq_details_switch array__seq pos size type id 0 4 u4le length 4 ... Scale::CompactInt type metadata_type_definition__seq_details_switch:case2->array__seq sequence__seq pos size type id 0 ... Scale::CompactInt type metadata_type_definition__seq_details_switch:case3->sequence__seq primitive__seq pos size type id 0 1 u1‚ÜíPid id metadata_type_definition__seq_details_switch:case4->primitive__seq tuple__seq pos size type id 0 ... Scale::CompactInt num_types ... ... Scale::CompactInt types repeat num_types.value times metadata_type_definition__seq_details_switch:case7->tuple__seq bits__seq pos size type id 0 ... Scale::CompactInt type ... ... Scale::CompactInt order metadata_type_definition__seq_details_switch:case1->bits__seq compact__seq pos size type id 0 ... Scale::CompactInt type metadata_type_definition__seq_details_switch:case5->compact__seq metadata_type_fields__seq MetadataTypeFields metadata_type_definition__seq_details_switch:case0->metadata_type_fields__seq metadata_type_variants__seq MetadataTypeVariants metadata_type_definition__seq_details_switch:case6->metadata_type_variants__seq Definition 152. Field ‚Äã A field of a data structure of the following format: f i = ( n , y , y n , C ) {{f}_{{i}}=}{\left({n},{y},{y}_{{n}},{C}\right)} f i ‚Äã = ( n , y , y n ‚Äã , C ) where n {n} n is an Option type containing the string that indicates the field name. y {y} y is a type Id ( Definition 150 ). y n {y}_{{n}} y n ‚Äã is an Option type containing a string that indicates the name of the type as it appears in the source code. C {C} C is a sequence of varying length containing strings of documentation. Image 11. Metadata Type Fields ‚Äã cluster__metadata_type_fields MetadataTypeFields cluster__field MetadataTypeFields::Field metadata_type_fields__seq pos size type id 0 ... Scale::CompactInt num_fields ... ... Field fields repeat num_fields.value times field__seq pos size type id 0 ... Scale::MaybeString name ... ... Scale::CompactInt type ... ... Scale::MaybeString typename ... ... Scale::StringList docs metadata_type_fields__seq:fields_type->field__seq Definition 153. Variant ‚Äã A struct variant of the following format: v i = ( n , F , k , C ) {v}_{{i}}={\left({n},{F},{k},{C}\right)} v i ‚Äã = ( n , F , k , C ) where n {n} n is a string representing the name of the variant. F {F} F is a possible empty array of varying length containing field ( Definition 152 ) elements. k {k} k is an unsigned 8-bit integer indicating the index of the variant. C {C} C is a sequence of strings containing the documentation. Image 12. Metadata Type Variants ‚Äã cluster__metadata_type_variants MetadataTypeVariants cluster__variant MetadataTypeVariants::Variant metadata_type_variants__seq pos size type id 0 ... Scale::CompactInt num_variants ... ... Variant variants repeat num_variants.value times variant__seq pos size type id 0 ... Scale::String name ... ... MetadataTypeFields composite ... 1 u1 index ... ... Scale::StringList docs metadata_type_variants__seq:variants_type->variant__seq metadata_type_fields__seq MetadataTypeFields variant__seq:composite_type->metadata_type_fields__seq 12.2. Pallet Metadata ‚Äã All the metadata about a pallet, part of the main structure ( Section 12.1. ) and of the following format: p i = ( n , S , a , e , C , e , i ) {p}_{{i}}={\left({n},{S},{a},{e},{C},{e},{i}\right)} p i ‚Äã = ( n , S , a , e , C , e , i ) where n {n} n is a string representing the pallet name. S {S} S is an Option type containing the pallet storage metadata ( Definition 154 ). a {a} a is an Option type ( Definition 180 ) containing the type Id ( Definition 150 ) of pallet calls. e {e} e is an Option type ( Definition 180 ) containing the type Id ( Definition 150 ) of pallet events. C {C} C is an Sequence ( Definition 182 ) of all pallet constant metadata ( Definition 159 ). e {e} e is an Option type ( Definition 180 ) containing the type Id ( Definition 150 ) of the pallet error. i {i} i is an unsigned 8-bit integer indicating the index of the pallet, which is used for encoding pallet events and calls. Image 13. Metadata Pallet ‚Äã cluster__metadata_pallet MetadataPallet cluster__calls MetadataPallet::Calls cluster__events MetadataPallet::Events cluster__errors MetadataPallet::Errors metadata_pallet__seq pos size type id 0 ... Scale::String name ... 1 u1 has_storage ... ... PalletStorage storage ... 1 u1 has_calls ... ... Calls calls ... 1 u1 has_events ... ... Events events ... ... Scale::CompactInt num_constants ... ... PalletConstant constants repeat num_constants.value times ... 1 u1 has_errors ... ... Errors errors ... 1 u1 index calls__seq pos size type id 0 ... Scale::CompactInt type metadata_pallet__seq:calls_type->calls__seq events__seq pos size type id 0 ... Scale::CompactInt type metadata_pallet__seq:events_type->events__seq errors__seq pos size type id 0 ... Scale::CompactInt type metadata_pallet__seq:errors_type->errors__seq pallet_storage__seq PalletStorage metadata_pallet__seq:storage_type->pallet_storage__seq pallet_constant__seq PalletConstant metadata_pallet__seq:constants_type->pallet_constant__seq Definition 154. Pallet Storage Metadata ‚Äã The metadata about pallets storage. S = ( p , E ) {S}={\left({p},{E}\right)} S = ( p , E ) E = ( e 0 , ‚Ä¶ , e n ) {E}={\left({e}_{{0}},\ldots,{e}_{{n}}\right)} E = ( e 0 ‚Äã , ‚Ä¶ , e n ‚Äã ) where p {p} p is the string representing the common prefix used by all storage entries. E {E} E is an array of varying lengths containing elements of storage entries ( Definition 155 ). Definition 155. Storage Entry Metadata ‚Äã The metadata about a pallets storage entry. e i = ( n , m , y , d , C ) {e}_{{i}}={\left({n},{m},{y},{d},{C}\right)} e i ‚Äã = ( n , m , y , d , C ) C = ( c 0 , ‚Ä¶ , c n ) {C}={\left({c}_{{0}},\ldots,{c}_{{n}}\right)} C = ( c 0 ‚Äã , ‚Ä¶ , c n ‚Äã ) where n {n} n is the string representing the variable name of the storage entry. m {m} m is an enum type determining the storage entry modifier ( Definition 156 ). y {y} y is the type of the value stored in the entry ( Definition 157 ). d {d} d is a byte array containing the default value. C {C} C is an array of varying lengths of strings containing the documentation. Image 14. Pallet Storage ‚Äã cluster__pallet_storage PalletStorage cluster__item PalletStorage::Item pallet_storage__seq pos size type id 0 ... Scale::String prefix ... ... Scale::CompactInt num_items ... ... Item items repeat num_items.value times item__seq pos size type id 0 ... Scale::String name ... 1 u1‚ÜíStorageModifier modifier ... ... StorageDefinition definition ... ... Scale::Bytes fallback ... ... Scale::StringList docs pallet_storage__seq:items_type->item__seq storage_definition__seq StorageDefinition item__seq:definition_type->storage_definition__seq Definition 156. Storage Entry Modifier ‚Äã info This might be incorrect and has to be reviewed. The storage entry modifier is a varying datatype ( Definition 178 ) and indicates how the storage entry is returned and how it behaves if the entry is not present. m = { 0 optional 1 default {m}={\left\lbrace\begin{matrix}{0}&\text{optional}\\{1}&\text{default}\end{matrix}\right.} m = { 0 1 ‚Äã optional default ‚Äã where 0 indicates that the entry returns an Option type and therefore None if the storage entry is not present. 1 indicates that the entry returns the type y {y} y with default value d {d} d (in Definition 155 ) if the entry is not present. Definition 157. Storage Entry Type ‚Äã The type of the storage value is a varying datatype ( Definition 178 ) that indicates how the entry is stored. y = { 0 ‚Üí t plain type 1 ‚Üí ( H , k , v ) storage map {y}={\left\lbrace\begin{matrix}{0}&\rightarrow&{t}&\text{plain type}\\{1}&\rightarrow&{\left({H},{k},{v}\right)}&\text{storage map}\end{matrix}\right.} y = { 0 1 ‚Äã ‚Üí ‚Üí ‚Äã t ( H , k , v ) ‚Äã plain type storage map ‚Äã where t {t} t , k {k} k (key) and v {v} v (value) are all of type Ids ( Definition 150 ). H {H} H is an array of varying length containing the storage hasher ( Definition 158 ). Image 15. Storage Definition ‚Äã cluster__storage_definition StorageDefinition cluster__plain StorageDefinition::Plain cluster__map StorageDefinition::Map storage_definition__seq pos size type id 0 1 u1‚ÜíStorageType type 1 ... switch (type) details storage_definition__seq:type_type->storage_definition__seq:details_type storage_definition__seq_details_switch case type :storage_type_plain Plain :storage_type_map Map storage_definition__seq:details_type->storage_definition__seq_details_switch plain__seq pos size type id 0 ... Scale::CompactInt type storage_definition__seq_details_switch:case0->plain__seq map__seq pos size type id 0 ... Scale::CompactInt num_hasher ... 1 u1‚ÜíHasherType hasher repeat num_hasher.value times ... ... Scale::CompactInt key ... ... Scale::CompactInt value storage_definition__seq_details_switch:case1->map__seq Definition 158. Storage Hasher ‚Äã The hashing algorithm is used by storage maps. { 0 128-bit Blake2 hash 1 256-bit Blake2 hash 2 Multiple 128-bit Blake2 hashes concatenated 3 128-bit XX hash 4 256-bit XX hash 5 Multiple 64-bit XX hashes concatenated 6 Identity hashing {\left\lbrace\begin{matrix}{0}&\text{128-bit Blake2 hash}\\{1}&\text{256-bit Blake2 hash}\\{2}&\text{Multiple 128-bit Blake2 hashes concatenated}\\{3}&\text{128-bit XX hash}\\{4}&\text{256-bit XX hash}\\{5}&\text{Multiple 64-bit XX hashes concatenated}\\{6}&\text{Identity hashing}\end{matrix}\right.} ‚é© ‚é® ‚éß ‚Äã 0 1 2 3 4 5 6 ‚Äã 128-bit Blake2 hash 256-bit Blake2 hash Multiple 128-bit Blake2 hashes concatenated 128-bit XX hash 256-bit XX hash Multiple 64-bit XX hashes concatenated Identity hashing ‚Äã Definition 159. Pallet Constants ‚Äã The metadata about the pallets constants. c i = ( n , y , v , C ) {c}_{{i}}={\left({n},{y},{v},{C}\right)} c i ‚Äã = ( n , y , v , C ) where n {n} n is a string representing the name of the pallet constant. y {y} y is the type Id ( Definition 150 ) of the pallet constant. v {v} v is a byte array containing the value of the constant. C {C} C is an array of varying lengths containing a string with the documentation. Image 16. Pallet Constant ‚Äã cluster__pallet_constant PalletConstant pallet_constant__seq pos size type id 0 ... Scale::String name ... ... Scale::CompactInt type ... ... Scale::Bytes value ... ... Scale::StringList docs 12.3. Extrinsic Metadata ‚Äã The metadata about a pallets extrinsics, part of the main structure ( Section 12.1. ) and of the following format: Definition 160. Signed Extension Metadata ‚Äã The metadata about the additional, signed data required to execute an extrinsic. e i = ( n , y , a ) {e}_{{i}}={\left({n},{y},{a}\right)} e i ‚Äã = ( n , y , a ) where n {n} n is a string representing the unique signed extension identifier, which may be different from the type name. y {y} y is a type Id ( Definition 150 ) of the signed extension, with the data to be included in the extrinsic. a {a} a is the type Id ( Definition 150 ) of the additional signed data, with the data to be included in the signed payload. Image 17. Metadata Extrinsic ‚Äã cluster__metadata_extrinsic MetadataExtrinsic metadata_extrinsic__seq pos size type id 0 ... Scale::String name ... ... Scale::CompactInt type ... ... Scale::CompactInt additional Previous 11. Consensus Next Appendix A: Cryptography & Encoding 12.1. Structure 12.2. Pallet Metadata 12.3. Extrinsic Metadata 
Appendix A: Cryptography & Encoding | Polkadot Protocol Specification Appendix A: Cryptography & Encoding On this page Appendix A: Cryptography & Encoding The appendix chapter contains various protocol details. A.1. Cryptographic Algorithms ‚Äã A.1.1. Hash Functions ‚Äã A.1.1.1. BLAKE2 ‚Äã BLAKE2 is a collection of cryptographic hash functions known for their high speed. Their design closely resembles BLAKE which has been a finalist in the SHA-3 competition. Polkadot is using the Blake2b variant, which is optimized for 64-bit platforms. Unless otherwise specified, the Blake2b hash function with a 256-bit output is used whenever Blake2b is invoked in this document. The detailed specification and sample implementations of all variants of Blake2 hash functions can be found in RFC 7693 ( 1 ) . A.1.2. Randomness ‚Äã info TBH A.1.3. VRF ‚Äã A Verifiable Random Function (VRF) is a mathematical operation that takes some input and produces a random number using a secret key along with a proof of authenticity that this random number was generated using the submitter‚Äôs secret key and the given input. The proof can be verified by any challenger to ensure the random number generation is valid and has not been tampered with (for example to the benfit of submitter). In Polkadot, VRFs are used for the BABE block production lottery by Block-Production-Lottery and the parachain approval voting mechanism ( Section 8.5. ). The VRF uses a mechanism similar to algorithms introduced in the following papers: Making NSEC5 Practical for DNSSEC ( 2 ) DLEQ Proofs Verifiable Random Functions (VRFs) ( 3 ) It essentially generates a deterministic elliptic curve based on Schnorr signature as a verifiable random value. The elliptic curve group used in the VRF function is the Ristretto group specified in: ristretto.group/ Definition 161. VRF Proof ‚Äã The VRF proof proves the correctness of an associated VRF output. The VRF proof, P {P} P , is a data structure of the following format: P = ( C , S ) {P}={\left({C},{S}\right)} P = ( C , S ) S = ( b 0 , ‚Ä¶ b 31 ) {S}={\left({b}_{{0}},\ldots{b}_{{31}}\right)} S = ( b 0 ‚Äã , ‚Ä¶ b 31 ‚Äã ) where C {C} C is the challenge and S {S} S is the 32-byte Schnorr poof. Both are expressed as Curve25519 scalars as defined in Definition Definition 162 . Definition 162. DLEQ Prove ‚Äã The dleq_prove ( t , i ) \text{dleq\_prove}{\left({t},{i}\right)} dleq_prove ( t , i ) function creates a proof for a given input, i {i} i , based on the provided transcript, T {T} T . First: t 1 = append ( t , ‚Äôproto-name‚Äô , ‚ÄôDLEQProof‚Äô ) {t}_{{1}}=\text{append}{\left({t},\text{'proto-name'},\text{'DLEQProof'}\right)} t 1 ‚Äã = append ( t , ‚Äôproto-name‚Äô , ‚ÄôDLEQProof‚Äô ) t 2 = append ( t 1 , ‚Äôvrf:h‚Äô , i ) {t}_{{2}}=\text{append}{\left({t}_{{1}},\text{'vrf:h'},{i}\right)} t 2 ‚Äã = append ( t 1 ‚Äã , ‚Äôvrf:h‚Äô , i ) Then the witness scalar is calculated, s w {s}_{{w}} s w ‚Äã , where w {w} w is the 32-byte secret seed used for nonce generation in the context of sr25519. t 3 = meta-AD ( t 2 , ‚Äôproving 00‚Äô , more=False ) t 4 = meta-AD ( t 3 , w l , more=True ) t 5 = KEY ( t 4 , w , more=False ) t 6 = meta-AD ( t 5 , ‚Äôrng‚Äô , more=False ) t 7 = KEY ( t 6 , r , more=False ) t 8 = meta-AD ( t 7 , e _ ( 64 ) , more=False ) ( œï , s w ) = PRF ( t 8 , more=False ) \begin{aligned} t_3 &= \text{meta-AD}(t_2, \text{'proving\\00'}, \text{more=False}) \\ t_4 &= \text{meta-AD}(t_3, w_l, \text{more=True}) \\ t_5 &= \text{KEY}(t_4, w, \text{more=False}) \\ t_6 &= \text{meta-AD}(t_5, \text{'rng'}, \text{more=False}) \\ t_7 &= \text{KEY}(t_6, r, \text{more=False}) \\ t_8 &= \text{meta-AD}(t_7, e\_(64), \text{more=False}) \\ (\phi, s_w) &= \text{PRF}(t_8, \text{more=False}) \end{aligned} t 3 ‚Äã t 4 ‚Äã t 5 ‚Äã t 6 ‚Äã t 7 ‚Äã t 8 ‚Äã ( œï , s w ‚Äã ) ‚Äã = meta-AD ( t 2 ‚Äã , ‚Äôproving 00‚Äô , more=False ) = meta-AD ( t 3 ‚Äã , w l ‚Äã , more=True ) = KEY ( t 4 ‚Äã , w , more=False ) = meta-AD ( t 5 ‚Äã , ‚Äôrng‚Äô , more=False ) = KEY ( t 6 ‚Äã , r , more=False ) = meta-AD ( t 7 ‚Äã , e _ ( 64 ) , more=False ) = PRF ( t 8 ‚Äã , more=False ) ‚Äã where w l {w}_{{l}} w l ‚Äã is the length of the witness, encoded as a 32-bit little-endian integer. r {r} r is a 32-byte array containing the secret witness scalar. l 1 = append ( t 2 , ‚Äô vrf:R=g r ‚Äô , s w ) l 2 = append ( l 1 , ‚Äô vrf:h r ‚Äô , s i ) l 3 = append ( l 2 , ‚Äôvrf:pk‚Äô , s p ) l 4 = append ( l 3 , ‚Äô vrf:h s k ‚Äô , vrf o ) \begin{aligned} l_1 &= \text{append}(t_2, \text{'}\text{vrf:R=g}^r\text{'}, s_w) \\ l_2 &= \text{append}(l_1, \text{'}\text{vrf:h}^r\text{'}, s_i) \\ l_3 &= \text{append}(l_2, \text{'}\text{vrf:pk}\text{'}, s_p) \\ l_4 &= \text{append}(l_3, \text{'}\text{vrf:h}^{sk}\text{'}, \text{vrf}_{o}) \end{aligned} l 1 ‚Äã l 2 ‚Äã l 3 ‚Äã l 4 ‚Äã ‚Äã = append ( t 2 ‚Äã , ‚Äô vrf:R=g r ‚Äô , s w ‚Äã ) = append ( l 1 ‚Äã , ‚Äô vrf:h r ‚Äô , s i ‚Äã ) = append ( l 2 ‚Äã , ‚Äô vrf:pk ‚Äô , s p ‚Äã ) = append ( l 3 ‚Äã , ‚Äô vrf:h s k ‚Äô , vrf o ‚Äã ) ‚Äã where s i {s}_{{i}} s i ‚Äã is the compressed Ristretto point of the scalar input. s p {s}_{{p}} s p ‚Äã is the compressed Ristretto point of the public key. s w {s}_{{w}} s w ‚Äã is the compressed Ristretto point of the wittness: For the 64-byte challenge: l 5 = meta-AD ( l 4 , ‚Äôprove‚Äô , more=False ) {l}_{{5}}=\text{meta-AD}{\left({l}_{{4}},\text{'prove'},\text{more=False}\right)} l 5 ‚Äã = meta-AD ( l 4 ‚Äã , ‚Äôprove‚Äô , more=False ) l 6 = meta-AD ( l 5 , e 64 , more=True ) {l}_{{6}}=\text{meta-AD}{\left({l}_{{5}},{e}_{{{64}}},\text{more=True}\right)} l 6 ‚Äã = meta-AD ( l 5 ‚Äã , e 64 ‚Äã , more=True ) C = PRF ( l 6 , more=False ) {C}=\text{PRF}{\left({l}_{{6}},\text{more=False}\right)} C = PRF ( l 6 ‚Äã , more=False ) And the Schnorr proof: S = s w ‚àí ( C ‚ãÖ p ) {S}={s}_{{w}}-{\left({C}\cdot{p}\right)} S = s w ‚Äã ‚àí ( C ‚ãÖ p ) where p {p} p is the secret key. Definition 163. DLEQ Verify ‚Äã The dleq_verify ( i , o , P , p k ) \text{dleq\_verify}{\left({i},{o},{P},{p}_{{k}}\right)} dleq_verify ( i , o , P , p k ‚Äã ) function verifiers the VRF input, i {i} i against the output, o {o} o , with the associated proof ( Definition 161 ) and public key, p k {p}_{{k}} p k ‚Äã . t 1 = append ( t , ‚Äôproto-name‚Äô , ‚ÄôDLEQProof‚Äô ) t 2 = append ( t 1 , ‚Äôvrf:h‚Äô , s i ) t 3 = append ( t 2 , ‚Äô vrf:R=g r ‚Äô , R ) t 4 = append ( t 3 , ‚Äô vrf:h r ‚Äô , H ) t 5 = append ( t 4 , ‚Äôvrf:pk‚Äô , p k ) t 6 = append ( t 5 , ‚Äô vrf:h s k ‚Äô , o ) \begin{aligned} t_1 &= \text{append}(t, \text{'}\text{proto-name}\text{'}, \text{'}\text{DLEQProof}\text{'}) \\ t_2 &= \text{append}(t_1, \text{'}\text{vrf:h}\text{'}, s_i) \\ t_3 &= \text{append}(t_2, \text{'}\text{vrf:R=g}^r\text{'}, R) \\ t_4 &= \text{append}(t_3, \text{'}\text{vrf:h}^r\text{'}, H) \\ t_5 &= \text{append}(t_4, \text{'}\text{vrf:pk}\text{'}, p_k) \\ t_6 &= \text{append}(t_5, \text{'}\text{vrf:h}^{sk}\text{'}, o) \end{aligned} t 1 ‚Äã t 2 ‚Äã t 3 ‚Äã t 4 ‚Äã t 5 ‚Äã t 6 ‚Äã ‚Äã = append ( t , ‚Äô proto-name ‚Äô , ‚Äô DLEQProof ‚Äô ) = append ( t 1 ‚Äã , ‚Äô vrf:h ‚Äô , s i ‚Äã ) = append ( t 2 ‚Äã , ‚Äô vrf:R=g r ‚Äô , R ) = append ( t 3 ‚Äã , ‚Äô vrf:h r ‚Äô , H ) = append ( t 4 ‚Äã , ‚Äô vrf:pk ‚Äô , p k ‚Äã ) = append ( t 5 ‚Äã , ‚Äô vrf:h s k ‚Äô , o ) ‚Äã where R {R} R is calculated as: R = C ‚àà P √ó p k + S ‚àà P + B {R}={C}\in{P}\times{p}_{{k}}+{S}\in{P}+{B} R = C ‚àà P √ó p k ‚Äã + S ‚àà P + B where B {B} B is the Ristretto basepoint. H {H} H is calculated as: H = C ‚àà P √ó o + S ‚àà P √ó i {H}={C}\in{P}\times{o}+{S}\in{P}\times{i} H = C ‚àà P √ó o + S ‚àà P √ó i The challenge is valid if C ‚àà P {C}\in{P} C ‚àà P equals y {y} y : t 7 = meta-AD ( t 6 , ‚Äôprove‚Äô , more=False ) {t}_{{7}}=\text{meta-AD}{\left({t}_{{6}},\text{'prove'},\text{more=False}\right)} t 7 ‚Äã = meta-AD ( t 6 ‚Äã , ‚Äôprove‚Äô , more=False ) t 8 = meta-AD ( t 7 , e 64 , more=True ) {t}_{{8}}=\text{meta-AD}{\left({t}_{{7}},{e}_{{{64}}},\text{more=True}\right)} t 8 ‚Äã = meta-AD ( t 7 ‚Äã , e 64 ‚Äã , more=True ) y = PRF ( t 8 , more=False ) {y}=\text{PRF}{\left({t}_{{8}},\text{more=False}\right)} y = PRF ( t 8 ‚Äã , more=False ) A.1.3.1. Transcript ‚Äã A VRF transcript serves as a domain-specific separator of cryptographic protocols and is represented as a mathematical object, as defined by Merlin, which defines how that object is generated and encoded. The usage of the transcript is implementation specific, such as for certain mechanisms in the Availability & Validity chapter ( Chapter 8 ), and is therefore described in more detail in those protocols. The input value used to initiate the transcript is referred to as a context ( Definition 164 ). Definition 164. VRF Context ‚Äã The VRF context is a constant byte array used to initiate the VRF transcript. The VRF context is constant for all users of the VRF for the specific context for which the VRF function is used. Context prevents VRF values generated by the same nodes for other purposes to be reused for purposes not meant to. For example, the VRF context for the BABE Production lottery defined in Section 5.2. is set to be "substrate-babe-vrf". Definition 165. VRF Transcript ‚Äã A transcript , or VRF transcript, is a STROBE object, obj \text{obj} obj , as defined in the STROBE documentation, respectively section "5. State of a STROBE object" . obj = ( st , pos , pos begin , I 0 ) \text{obj}={\left(\text{st},\text{pos},\text{pos}_{{\text{begin}}},{I}_{{0}}\right)} obj = ( st , pos , pos begin ‚Äã , I 0 ‚Äã ) where The duplex state, st \text{st} st , is a 200-byte array created by the keccak-f1600 sponge function on the initial STROBE state . Specifically, R is of value 166 and X.Y.Z is of value 1.0.2 . pos \text{pos} pos has the initial value of 0 . pos begin \text{pos}_{{\text{begin}}} pos begin ‚Äã has the initial value of 0 . I 0 {I}_{{0}} I 0 ‚Äã has the initial value of 0 . Then, the meta-AD operation ( Definition 166 ) (where more=False ) is used to add the protocol label Merlin v1.0 to obj \text{obj} obj followed by appending ( Section A.1.3.1.1. ) label dom-step and its corresponding context, c t x {c}{t}{x} c t x , resulting in the final transcript, T {T} T . t = meta-AD ( o b j , ‚ÄôMerlin v1.0‚Äô , False ) {t}=\text{meta-AD}{\left({o}{b}{j},\text{'Merlin v1.0'},\text{False}\right)} t = meta-AD ( o b j , ‚ÄôMerlin v1.0‚Äô , False ) T = append ( t , ‚Äôdom-step‚Äô , ctx ) {T}=\text{append}{\left({t},\text{'dom-step'},\text{ctx}\right)} T = append ( t , ‚Äôdom-step‚Äô , ctx ) ctx \text{ctx} ctx serves as an arbitrary identifier/separator and its value is defined by the protocol specification individually. This transcript is treated just like a STROBE object, wherein any operations ( Definition 166 ) on it modify the values such as pos \text{pos} pos and pos begin \text{pos}_{{\text{begin}}} pos begin ‚Äã . Formally, when creating a transcript, we refer to it as Transcript ( c t x ) \text{Transcript}{\left({c}{t}{x}\right)} Transcript ( c t x ) . Definition 166. STROBE Operations ‚Äã STROBE operations are described in the STROBE specification , respectively section "6. Strobe operations" . Operations are indicated by their corresponding bitfield, as described in section "6.2. Operations and flags" and implemented as described in section "7. Implementation of operations" A.1.3.1.1. Messages ‚Äã Appending messages, or "data", to the transcript ( Definition 165 ) first requires meta-AD operations for a given label of the messages, including the size of the message, followed by an AD operation on the message itself. The size of the message is a 4-byte, little-endian encoded integer. T 0 = meta-AD ( T , l , False ) {T}_{{0}}=\text{meta-AD}{\left({T},{l},\text{False}\right)} T 0 ‚Äã = meta-AD ( T , l , False ) T 1 = meta-AD ( T 0 , m l , True ) {T}_{{1}}=\text{meta-AD}{\left({T}_{{0}},{m}_{{l}},\text{True}\right)} T 1 ‚Äã = meta-AD ( T 0 ‚Äã , m l ‚Äã , True ) T 2 = AD ( T 1 , m , False ) {T}_{{2}}=\text{AD}{\left({T}_{{1}},{m},\text{False}\right)} T 2 ‚Äã = AD ( T 1 ‚Äã , m , False ) where T {T} T is the transcript ( Definition 165 ), l {l} l is the given label and m {m} m the message, respectively m l {m}_{{l}} m l ‚Äã representing its size. T 2 {T}_{{2}} T 2 ‚Äã is the resulting transcript with the appended data. STROBE operations are described in Definition 166 . Formally, when appending a message, we refer to it as append ( T , l , m ) \text{append}{\left({T},{l},{m}\right)} append ( T , l , m ) . A.1.4. Cryptographic Keys ‚Äã Various types of keys are used in Polkadot to prove the identity of the actors involved in the Polkadot Protocols. To improve the security of the users, each key type has its own unique function and must be treated differently, as described in this Section. Definition 167. Account Key ‚Äã Account key ( s k a , p k a ) {\left({s}{k}^{{a}},{p}{k}^{{a}}\right)} ( s k a , p k a ) is a key pair of type of either of the schemes in the following table: Table 2. List of the public key scheme which can be used for an account key ‚Äã Key Scheme Description sr25519 Schnorr signature on Ristretto compressed ed25519 points as implemented in TODO ed25519 The ed25519 signature complies with ( 4 ) except for the verification process which adhere to Ed25519 Zebra variant specified in ( 5 ) . In short, the signature point is not assumed to be in the prime-ordered subgroup group. As such, the verifier must explicitly clear the cofactor during the course of verifying the signature equation. secp256k1 Only for outgoing transfer transactions. An account key can be used to sign transactions among other accounts and balance-related functions. There are two prominent subcategories of account keys, namely "stash keys" and "controller keys", each being used for a different function. Keys defined in Definition 167 , Definition 168 and Definition 169 are created and managed by the user independent of the Polkadot implementation. The user notifies the network about the used keys by submitting a transaction, as defined in Section A.1.4.2. and Section A.1.4.5. respectively. Definition 168. Stash Key ‚Äã The Stash key is a type of account key that holds funds bonded for staking (described in Section A.1.4.1. ) to a particular controller key (defined in Definition 169 ). As a result, one may actively participate with a stash key, keeping the stash key offline in a secure location. It can also be used to designate a Proxy account to vote in governance proposals, as described in Section A.1.4.2. . The Stash key holds the majority of the users‚Äô funds and should neither be shared with anyone, saved on an online device, nor used to submit extrinsics. Definition 169. Controller Key ‚Äã The Controller key is a type of account key that acts on behalf of the Stash account. It signs transactions that make decisions regarding the nomination and the validation of the other keys. It is a key that will be in direct control of a user and should mostly be kept offline, used to submit manual extrinsics. It sets preferences like payout account and commission, as described in Section A.1.4.4. . If used for a validator, it certifies the session keys, as described in Section A.1.4.5. . It only needs the required funds to pay transaction fees [ TODO: key needing fund needs to be defined ] . Definition 170. Session Keys ‚Äã Session keys are short-lived keys that are used to authenticate validator operations. Session keys are generated by the Polkadot Host and should be changed regularly due to security reasons. Nonetheless, no validity period is enforced by the Polkadot protocol on session keys. Various types of keys used by the Polkadot Host are presented in Table 3 : Table 3. List of key schemes which are used for session keys depending on the protocol ‚Äã Protocol Key scheme GRANDPA ED25519 BABE SR25519 I‚Äôm Online SR25519 Parachain SR25519 Session keys must be accessible by certain Polkadot Host APIs defined in Appendix B . Session keys are not meant to control the majority of the users‚Äô funds and should only be used for their intended purpose. A.1.4.1. Holding and staking funds ‚Äã info TBH A.1.4.2. Creating a Controller key ‚Äã info TBH A.1.4.3. Designating a proxy for voting ‚Äã info TBH A.1.4.4. Controller settings ‚Äã info TBH A.1.4.5. Certifying keys ‚Äã Due to security considerations and Runtime upgrades, the session keys are supposed to be changed regularly. As such, the new session keys need to be certified by a controller key before putting them into use. The controller only needs to create a certificate by signing a session public key and broadcasting this certificate via an extrinsic. [ TODO: spec the detail of the data structure of the certificate etc. ] A.2. Auxiliary Encodings ‚Äã Definition 171. Unix Time ‚Äã By Unix time , we refer to the unsigned, little-endian encoded 64-bit integer which stores the number of milliseconds that have elapsed since the Unix epoch, that is the time 00:00:00 UTC on 1 January 1970, minus leap seconds. Leap seconds are ignored, and every day is treated as if it contained exactly 86‚Äô400 seconds. A.2.1. Binary Enconding ‚Äã Definition 172. Sequence of Bytes ‚Äã By a sequences of bytes or a byte array , b {b} b , of length n {n} n , we refer to b = ( b 0 , b 1 , ‚Ä¶ , b n ‚àí 1 ) such that 0 ‚â§ b i ‚â§ 255 {b}\:={\left({b}_{{0}},{b}_{{1}},\ldots,{b}_{{{n}-{1}}}\right)}\ \text{ such that }\ {0}\le{b}_{{i}}\le{255} b = ( b 0 ‚Äã , b 1 ‚Äã , ‚Ä¶ , b n ‚àí 1 ‚Äã ) such that 0 ‚â§ b i ‚Äã ‚â§ 255 We define B n {\mathbb{{B}}}_{{n}} B n ‚Äã to be the set of all byte arrays of length n {n} n . Furthermore, we define: B = ‚ãÉ i = 0 ‚àû B i {\mathbb{{B}}}\:={\bigcup_{{{i}={0}}}^{\infty}}{\mathbb{{B}}}_{{i}} B = i = 0 ‚ãÉ ‚àû ‚Äã B i ‚Äã We represent the concatenation of byte arrays a = ( a 0 , ‚Ä¶ , a n ) {a}\:={\left({a}_{{0}},\ldots,{a}_{{n}}\right)} a = ( a 0 ‚Äã , ‚Ä¶ , a n ‚Äã ) and b = ( b 0 , ‚Ä¶ , b m ) {b}\:={\left({b}_{{0}},\ldots,{b}_{{m}}\right)} b = ( b 0 ‚Äã , ‚Ä¶ , b m ‚Äã ) by: a ‚à£ b : = ( a 0 , . . . , a n , b 0 , . . . , b m ) {a}{\mid} b :=(a_0, ..., a_n, b_0, ..., b_m) a ‚à£ b := ( a 0 ‚Äã , ... , a n ‚Äã , b 0 ‚Äã , ... , b m ‚Äã ) Definition 173. Bitwise Representation ‚Äã For a given byte 0 ‚â§ b ‚â§ 255 {0}\le{b}\le{255} 0 ‚â§ b ‚â§ 255 the bitwise representation in bits b i ‚àà { 0 , 1 } {b}_{{i}}\in{\left\lbrace{0},{1}\right\rbrace} b i ‚Äã ‚àà { 0 , 1 } is defined as: b = b 7 ‚Ä¶ b 0 {b}\:={b}_{{7}}\ldots{b}_{{0}} b = b 7 ‚Äã ‚Ä¶ b 0 ‚Äã where b = 2 7 b 7 + 2 6 b 6 + ‚Ä¶ + 2 0 b 0 {b}={2}^{{7}}{b}_{{7}}+{2}^{{6}}{b}_{{6}}+\ldots+{2}^{{0}}{b}_{{0}} b = 2 7 b 7 ‚Äã + 2 6 b 6 ‚Äã + ‚Ä¶ + 2 0 b 0 ‚Äã Definition 174. Little Endian ‚Äã By the little-endian representation of a non-negative integer, I {I} I , represented as I = ( B n ‚Ä¶ B 0 ) 256 {I}={\left({B}_{{n}}\ldots{B}_{{0}}\right)}_{{256}} I = ( B n ‚Äã ‚Ä¶ B 0 ‚Äã ) 256 ‚Äã in base 256, we refer to a byte array B = ( b 0 , b 1 , ‚Ä¶ , b n ) {B}={\left({b}_{{0}},{b}_{{1}},\ldots,{b}_{{n}}\right)} B = ( b 0 ‚Äã , b 1 ‚Äã , ‚Ä¶ , b n ‚Äã ) such that b i = B i {b}_{{i}}\:={B}_{{i}} b i ‚Äã = B i ‚Äã Accordingly, we define the function Enc LE {\mathsf{\text{Enc}}}_{{{\mathsf{\text{LE}}}}} Enc LE ‚Äã : Enc LE : Z + ‚Üí B ; ( B n ‚Ä¶ B 0 ) 256 ‚à£ ‚Üí ( B 0 , B 1 , ‚Ä¶ , B n ) {\mathsf{\text{Enc}}}_{{{\mathsf{\text{LE}}}}}:{\mathbb{{Z}}}^{+}\rightarrow{\mathbb{{B}}};{\left({B}_{{n}}\ldots{B}_{{0}}\right)}_{{256}}{\mid}\rightarrow{\left({B}_{{{0},}}{B}_{{1}},\ldots,{B}_{{n}}\right)} Enc LE ‚Äã : Z + ‚Üí B ; ( B n ‚Äã ‚Ä¶ B 0 ‚Äã ) 256 ‚Äã ‚à£ ‚Üí ( B 0 , ‚Äã B 1 ‚Äã , ‚Ä¶ , B n ‚Äã ) Definition 175. UINT32 ‚Äã By UINT32 we refer to a non-negative integer stored in a byte array of length 4 {4} 4 using little-endian encoding format. A.2.2. SCALE Codec ‚Äã The Polkadot Host uses Simple Concatenated Aggregate Little-Endian‚Äù (SCALE) codec to encode byte arrays as well as other data structures. SCALE provides a canonical encoding to produce consistent hash values across their implementation, including the Merkle hash proof for the State Storage. Definition 176. Decoding ‚Äã Dec SC ( d ) \text{Dec}_{{\text{SC}}}{\left({d}\right)} Dec SC ‚Äã ( d ) refers to the decoding of a blob of data. Since the SCALE codec is not self-describing, it‚Äôs up to the decoder to validate whether the blob of data can be deserialized into the given type or data structure. It‚Äôs accepted behavior for the decoder to partially decode the blob of data. Meaning, any additional data that does not fit into a data structure can be ignored. caution Considering that the decoded data is never larger than the encoded message, this information can serve as a way to validate values that can vary in size, such as sequences ( Definition 182 ). The decoder should strictly use the size of the encoded data as an upper bound when decoding in order to prevent denial of service attacks. Definition 177. Tuple ‚Äã The SCALE codec for Tuple , T {T} T , such that: T = ( A 1 , ‚Ä¶ A n ) {T}\:={\left({A}_{{1}},\ldots{A}_{{n}}\right)} T = ( A 1 ‚Äã , ‚Ä¶ A n ‚Äã ) Where A i {A}_{{i}} A i ‚Äã ‚Äôs are values of different types , is defined as: Enc SC ( T ) = Enc SC ( A 1 ) || Enc SC ( A 2 ) || ‚Ä¶ || Enc SC ( A n ) \text{Enc}_{{\text{SC}}}{\left({T}\right)}\:=\text{Enc}_{{\text{SC}}}{\left({A}_{{1}}\right)}\text{||}\text{Enc}_{{\text{SC}}}{\left({A}_{{2}}\right)}\text{||}\ldots\text{||}\text{Enc}_{{\text{SC}}}{\left({A}_{{n}}\right)} Enc SC ‚Äã ( T ) = Enc SC ‚Äã ( A 1 ‚Äã ) || Enc SC ‚Äã ( A 2 ‚Äã ) || ‚Ä¶ || Enc SC ‚Äã ( A n ‚Äã ) In the case of a tuple (or a structure), the knowledge of the shape of data is not encoded even though it is necessary for decoding. The decoder needs to derive that information from the context where the encoding/decoding is happening. Definition 178. Varying Data Type ‚Äã We define a varying data type to be an ordered set of data types. T = { T 1 , ‚Ä¶ , T n } {\mathcal{{T}}}={\left\lbrace{T}_{{1}},\ldots,{T}_{{n}}\right\rbrace} T = { T 1 ‚Äã , ‚Ä¶ , T n ‚Äã } A value A {A} A of varying date type is a pair ( A Type , A Value ) {\left({A}_{{\text{Type}}},{A}_{{\text{Value}}}\right)} ( A Type ‚Äã , A Value ‚Äã ) where A Type = T i {A}_{{\text{Type}}}={T}_{{i}} A Type ‚Äã = T i ‚Äã for some T i ‚àà T {T}_{{i}}\in{\mathcal{{T}}} T i ‚Äã ‚àà T and A Value {A}_{{\text{Value}}} A Value ‚Äã is its value of type T i {T}_{{i}} T i ‚Äã , which can be empty. We define idx ( T i ) = i ‚àí 1 \text{idx}{\left({T}_{{i}}\right)}={i}-{1} idx ( T i ‚Äã ) = i ‚àí 1 , unless it is explicitly defined as another value in the definition of a particular varying data type. In particular, we define two specific varying data which are frequently used in various part of Polkadot protocol: Option ( Definition 180 ) and Result ( Definition 181 ). Definition 179. Encoding of Varying Data Type ‚Äã The SCALE codec for value A = ( A Type , A Value ) {A}={\left({A}_{{\text{Type}}},{A}_{{\text{Value}}}\right)} A = ( A Type ‚Äã , A Value ‚Äã ) of varying data type T = { T i , ‚Ä¶ T n } {\mathcal{{T}}}={\left\lbrace{T}_{{i}},\ldots{T}_{{n}}\right\rbrace} T = { T i ‚Äã , ‚Ä¶ T n ‚Äã } , formally referred to as Enc SC ( A ) \text{Enc}_{{\text{SC}}}{\left({A}\right)} Enc SC ‚Äã ( A ) is defined as follows: Enc SC ( A ) = Enc SC ( idx ( A Type ) || Enc SC ( A Value ) ) \text{Enc}_{{\text{SC}}}{\left({A}\right)}\:=\text{Enc}_{{\text{SC}}}{\left(\text{idx}{\left({A}_{{\text{Type}}}\right)}\text{||}\text{Enc}_{{\text{SC}}}{\left({A}_{{\text{Value}}}\right)}\right)} Enc SC ‚Äã ( A ) = Enc SC ‚Äã ( idx ( A Type ‚Äã ) || Enc SC ‚Äã ( A Value ‚Äã ) ) Where idx \text{idx} idx is a 8-bit integer determining the type of A {A} A . In particular, for the optional type defined in Definition 178 , we have: Enc SC ( None , œï ) = 0 B 1 \text{Enc}_{{\text{SC}}}{\left(\text{None},\phi\right)}\:={0}_{{{\mathbb{{B}}}_{{1}}}} Enc SC ‚Äã ( None , œï ) = 0 B 1 ‚Äã ‚Äã The SCALE codec does not encode the correspondence between the value and the data type it represents; the decoder needs prior knowledge of such correspondence to decode the data. Definition 180. Option Type ‚Äã The Option type is a varying data type of { None , T 2 } {\left\lbrace\text{None},{T}_{{2}}\right\rbrace} { None , T 2 ‚Äã } which indicates if data of T 2 {T}_{{2}} T 2 ‚Äã type is available (referred to as some state) or not (referred to as empty , none or null state). The presence of type none , indicated by idx ( T None ) = 0 \text{idx}{\left({T}_{{\text{None}}}\right)}={0} idx ( T None ‚Äã ) = 0 , implies that the data corresponding to T 2 {T}_{{2}} T 2 ‚Äã type is not available and contains no additional data. Where as the presence of type T 2 {T}_{{2}} T 2 ‚Äã indicated by idx ( T 2 ) = 1 \text{idx}{\left({T}_{{2}}\right)}={1} idx ( T 2 ‚Äã ) = 1 implies that the data is available. Definition 181. Result Type ‚Äã The Result type is a varying data type of { T 1 , T 2 } {\left\lbrace{T}_{{1}},{T}_{{2}}\right\rbrace} { T 1 ‚Äã , T 2 ‚Äã } which is used to indicate if a certain operation or function was executed successfully (referred to as "ok" state) or not (referred to as "error" state). T 1 {T}_{{1}} T 1 ‚Äã implies success, T 2 {T}_{{2}} T 2 ‚Äã implies failure. Both types can either contain additional data or are defined as empty type otherwise. Definition 182. Sequence ‚Äã The SCALE codec for sequence S {S} S such that: S = A 1 , ‚Ä¶ A n {S}\:={A}_{{1}},\ldots{A}_{{n}} S = A 1 ‚Äã , ‚Ä¶ A n ‚Äã where A i {A}_{{i}} A i ‚Äã ‚Äôs are values of the same type (and the decoder is unable to infer value of n {n} n from the context) is defined as: Enc SC ( S ) = Enc SC Len ( ‚à£ S ‚à£ ) || Enc SC ( A 2 ) || ‚Ä¶ || Enc SC ( A n ) \text{Enc}_{{\text{SC}}}{\left({S}\right)}\:={\text{Enc}_{{\text{SC}}}^{{\text{Len}}}}{\left({\left|{{S}}\right|}\right)}\text{||}\text{Enc}_{{\text{SC}}}{\left({A}_{{2}}\right)}\text{||}\ldots\text{||}\text{Enc}_{{\text{SC}}}{\left({A}_{{n}}\right)} Enc SC ‚Äã ( S ) = Enc SC Len ‚Äã ( ‚à£ S ‚à£ ) || Enc SC ‚Äã ( A 2 ‚Äã ) || ‚Ä¶ || Enc SC ‚Äã ( A n ‚Äã ) where Enc SC Len {\text{Enc}_{{\text{SC}}}^{{\text{Len}}}} Enc SC Len ‚Äã is defined in Definition 188 . In some cases, the length indicator Enc SC Len ( ‚à£ S ‚à£ ) {\text{Enc}_{{\text{SC}}}^{{\text{Len}}}}{\left({\left|{{S}}\right|}\right)} Enc SC Len ‚Äã ( ‚à£ S ‚à£ ) is omitted if the length of the sequence is fixed and known by the decoder upfront. Such cases are explicitly stated by the definition of the corresponding type. Definition 183. Dictionary ‚Äã SCALE codec for dictionary or hashtable D with key-value pairs ( k i , v i ) {\left({k}_{{i}},{v}_{{i}}\right)} ( k i ‚Äã , v i ‚Äã ) s such that: D = { ( k 1 , v 1 ) , ‚Ä¶ ( k n , v n ) } {D}\:={\left\lbrace{\left({k}_{{1}},{v}_{{1}}\right)},\ldots{\left({k}_{{n}},{v}_{{n}}\right)}\right\rbrace} D = { ( k 1 ‚Äã , v 1 ‚Äã ) , ‚Ä¶ ( k n ‚Äã , v n ‚Äã ) } is defined the SCALE codec of D {D} D as a sequence of key value pairs (as tuples): Enc SC ( D ) = Enc SC Size ( ‚à£ D ‚à£ ) || Enc SC ( k 1 , v 1 ) || ‚Ä¶ || Enc SC ( k n , v n ) \text{Enc}_{{\text{SC}}}{\left({D}\right)}\:={\text{Enc}_{{\text{SC}}}^{{\text{Size}}}}{\left({\left|{{D}}\right|}\right)}\text{||}\text{Enc}_{{\text{SC}}}{\left({k}_{{1}},{v}_{{1}}\right)}\text{||}\ldots\text{||}\text{Enc}_{{\text{SC}}}{\left({k}_{{n}},{v}_{{n}}\right)} Enc SC ‚Äã ( D ) = Enc SC Size ‚Äã ( ‚à£ D ‚à£ ) || Enc SC ‚Äã ( k 1 ‚Äã , v 1 ‚Äã ) || ‚Ä¶ || Enc SC ‚Äã ( k n ‚Äã , v n ‚Äã ) where Enc SC Size {\text{Enc}_{{\text{SC}}}^{{\text{Size}}}} Enc SC Size ‚Äã is encoded the same way as Enc SC Len {\text{Enc}_{{\text{SC}}}^{{\text{Len}}}} Enc SC Len ‚Äã but argument Size \text{Size} Size refers to the number of key-value pairs rather than the length. Definition 184. Boolean ‚Äã The SCALE codec for a boolean value b {b} b defined as a byte as follows: Enc SC : { False , True } ‚Üí B 1 \text{Enc}_{{\text{SC}}}:{\left\lbrace\text{False},\text{True}\right\rbrace}\rightarrow{\mathbb{{B}}}_{{1}} Enc SC ‚Äã : { False , True } ‚Üí B 1 ‚Äã b ‚Üí { 0 b = False 1 b = True {b}\rightarrow{\left\lbrace\begin{matrix}{0}&{b}=\text{False}\\{1}&{b}=\text{True}\end{matrix}\right.} b ‚Üí { 0 1 ‚Äã b = False b = True ‚Äã Definition 185. String ‚Äã The SCALE codec for a string value is an encoded sequence ( Definition 182 ) consisting of UTF-8 encoded bytes. Definition 186. Fixed Length ‚Äã The SCALE codec, Enc SC \text{Enc}_{{\text{SC}}} Enc SC ‚Äã , for other types such as fixed length integers not defined here otherwise, is equal to little endian encoding of those values defined in Definition 174 . Definition 187. Empty ‚Äã The SCALE codec, Enc SC \text{Enc}_{{\text{SC}}} Enc SC ‚Äã , for an empty type is defined to a byte array of zero length and depicted as œï \phi œï . A.2.2.1. Length and Compact Encoding ‚Äã SCALE Length encoding is used to encode integer numbers of variying sizes prominently in an encoding length of arrays: Definition 188. Length Encoding ‚Äã SCALE Length encoding , Enc SC Len {\text{Enc}_{{\text{SC}}}^{{\text{Len}}}} Enc SC Len ‚Äã , also known as a compact encoding , of a non-negative number n {n} n is defined as follows: Enc SC Len : N ‚Üí B {\text{Enc}_{{\text{SC}}}^{{\text{Len}}}}:{\mathbb{{N}}}\rightarrow{\mathbb{{B}}} Enc SC Len ‚Äã : N ‚Üí B n ‚Üí b = { l 1 0 ‚â§ n < 2 6 i 1 i 2 2 6 ‚â§ n < 2 14 j 1 j 2 j 3 j 4 2 14 ‚â§ n < 2 30 k 1 k 2 ‚Ä¶ k m + 1 2 30 ‚â§ n {n}\rightarrow{b}\:={\left\lbrace\begin{matrix}{l}_{{1}}&{0}\le{n}<{2}^{{6}}\\{i}_{{1}}{i}_{{2}}&{2}^{{6}}\le{n}<{2}^{{14}}\\{j}_{{1}}{j}_{{2}}{j}_{{3}}{j}_{{4}}&{2}^{{14}}\le{n}<{2}^{{30}}\\{k}_{{1}}{k}_{{2}}\ldots{k}_{{m}+{1}}&{2}^{{30}}\le{n}\end{matrix}\right.} n ‚Üí b = ‚é© ‚é® ‚éß ‚Äã l 1 ‚Äã i 1 ‚Äã i 2 ‚Äã j 1 ‚Äã j 2 ‚Äã j 3 ‚Äã j 4 ‚Äã k 1 ‚Äã k 2 ‚Äã ‚Ä¶ k m + 1 ‚Äã ‚Äã 0 ‚â§ n < 2 6 2 6 ‚â§ n < 2 14 2 14 ‚â§ n < 2 30 2 30 ‚â§ n ‚Äã in where the least significant bits of the first byte of byte array b are defined as follows: l 1 1 l 1 0 = 00 {{l}_{{1}}^{{1}}}{{l}_{{1}}^{{0}}}={00} l 1 1 ‚Äã l 1 0 ‚Äã = 00 i 1 1 i 1 0 = 01 {{i}_{{1}}^{{1}}}{{i}_{{1}}^{{0}}}={01} i 1 1 ‚Äã i 1 0 ‚Äã = 01 j 1 1 j 1 0 = 10 {{j}_{{1}}^{{1}}}{{j}_{{1}}^{{0}}}={10} j 1 1 ‚Äã j 1 0 ‚Äã = 10 k 1 1 k 1 0 = 11 {{k}_{{1}}^{{1}}}{{k}_{{1}}^{{0}}}={11} k 1 1 ‚Äã k 1 0 ‚Äã = 11 and the rest of the bits of b {b} b store the value of n {n} n in little-endian format in base-2 as follows: n = { l 1 7 ‚Ä¶ l 1 3 l 1 2 n < 2 6 i 2 7 ‚Ä¶ i 2 0 i 1 7 . . i 1 2 2 6 ‚â§ n < 2 14 j 4 7 ‚Ä¶ j 4 0 j 3 7 ‚Ä¶ j 1 7 ‚Ä¶ j 1 2 2 14 ‚â§ n < 2 30 k 2 + k 3 2 8 + k 4 2 2 √ó 8 + ‚Ä¶ + k m + 1 2 ( m ‚àí 1 ) 8 2 30 ‚â§ n {n}\:={\left\lbrace\begin{matrix}{{l}_{{1}}^{{7}}}\ldots{{l}_{{1}}^{{3}}}{{l}_{{1}}^{{2}}}&{n}<{2}^{{6}}\\{{i}_{{2}}^{{7}}}\ldots{{i}_{{2}}^{{0}}}{{i}_{{1}}^{{7}}}..{{i}_{{1}}^{{2}}}&{2}^{{6}}\le{n}<{2}^{{14}}\\{{j}_{{4}}^{{7}}}\ldots{{j}_{{4}}^{{0}}}{{j}_{{3}}^{{7}}}\ldots{{j}_{{1}}^{{7}}}\ldots{{j}_{{1}}^{{2}}}&{2}^{{14}}\le{n}<{2}^{{30}}\\{k}_{{2}}+{k}_{{3}}{2}^{{8}}+{k}_{{4}}{2}^{{{2}\times{8}}}+\ldots+{k}_{{m}+{1}}{2}^{{{\left({m}-{1}\right)}{8}}}&{2}^{{30}}\le{n}\end{matrix}\right.} n = ‚é© ‚é® ‚éß ‚Äã l 1 7 ‚Äã ‚Ä¶ l 1 3 ‚Äã l 1 2 ‚Äã i 2 7 ‚Äã ‚Ä¶ i 2 0 ‚Äã i 1 7 ‚Äã .. i 1 2 ‚Äã j 4 7 ‚Äã ‚Ä¶ j 4 0 ‚Äã j 3 7 ‚Äã ‚Ä¶ j 1 7 ‚Äã ‚Ä¶ j 1 2 ‚Äã k 2 ‚Äã + k 3 ‚Äã 2 8 + k 4 ‚Äã 2 2 √ó 8 + ‚Ä¶ + k m + 1 ‚Äã 2 ( m ‚àí 1 ) 8 ‚Äã n < 2 6 2 6 ‚â§ n < 2 14 2 14 ‚â§ n < 2 30 2 30 ‚â§ n ‚Äã such that: k 1 7 ‚Ä¶ k 1 3 k 1 2 = m ‚àí 4 {{k}_{{1}}^{{7}}}\ldots{{k}_{{1}}^{{3}}}{{k}_{{1}}^{{2}}}\:={m}-{4} k 1 7 ‚Äã ‚Ä¶ k 1 3 ‚Äã k 1 2 ‚Äã = m ‚àí 4 Note that m {m} m denotes the length of the original integer being encoded and does not include the extra-byte describing the length. The encoding can be used for integers up to 2 ( 63 + 4 ) 8 ‚àí 1 = 2 536 ‚àí 1 2^{(63+4)8} -1 = 2^{536} -1 2 ( 63 + 4 ) 8 ‚àí 1 = 2 536 ‚àí 1 . A.2.3. Hex Encoding ‚Äã Practically, it is more convenient and efficient to store and process data which is stored in a byte array. On the other hand, the trie keys are broken into 4-bits nibbles. Accordingly, we need a method to encode sequences of 4-bits nibbles into byte arrays canonically. To this aim, we define hex encoding function Enc ( HE ) ( PK ) \text{Enc}{\left(\text{HE}\right)}{\left(\text{PK}\right)} Enc ( HE ) ( PK ) as follows: Definition 189. Hex Encoding ‚Äã Suppose that PK = ( k 1 , ‚Ä¶ k n ) \text{PK}={\left({k}_{{1}},\ldots{k}_{{n}}\right)} PK = ( k 1 ‚Äã , ‚Ä¶ k n ‚Äã ) is a sequence of nibbles, then: Enc HE ( PK ) = { Nibbles 4 ‚Üí B PK = ( k 1 , ‚Ä¶ k n ) ‚Üí { ( 16 k 1 + k 2 , ‚Ä¶ , 16 k 2 i ‚àí 1 + k 2 i ) n = 2 i ( k 1 , 16 k 2 + k 3 , ‚Ä¶ , 16 k 2 i + k 2 i + 1 ) n = 2 i + 1 \text{Enc}_{{\text{HE}}}{\left(\text{PK}\right)}\:={\left\lbrace\begin{matrix}\text{Nibbles}_{{4}}&\rightarrow&{\mathbb{{B}}}\\\text{PK}={\left({k}_{{1}},\ldots{k}_{{n}}\right)}&\rightarrow&{\left\lbrace\begin{matrix}{\left({16}{k}_{{1}}+{k}_{{2}},\ldots,{16}{k}_{{{2}{i}-{1}}}+{k}_{{{2}{i}}}\right)}&{n}={2}{i}\\{\left({k}_{{1}},{16}{k}_{{2}}+{k}_{{3}},\ldots,{16}{k}_{{{2}{i}}}+{k}_{{{2}{i}+{1}}}\right)}&{n}={2}{i}+{1}\end{matrix}\right.}\end{matrix}\right.} Enc HE ‚Äã ( PK ) = ‚é© ‚é® ‚éß ‚Äã Nibbles 4 ‚Äã PK = ( k 1 ‚Äã , ‚Ä¶ k n ‚Äã ) ‚Äã ‚Üí ‚Üí ‚Äã B { ( 16 k 1 ‚Äã + k 2 ‚Äã , ‚Ä¶ , 16 k 2 i ‚àí 1 ‚Äã + k 2 i ‚Äã ) ( k 1 ‚Äã , 16 k 2 ‚Äã + k 3 ‚Äã , ‚Ä¶ , 16 k 2 i ‚Äã + k 2 i + 1 ‚Äã ) ‚Äã n = 2 i n = 2 i + 1 ‚Äã ‚Äã A.3. Chain Specification ‚Äã Chain Specification (chainspec) is a collection of information that describes the blockchain network. It includes information required for a host to connect and sync with the Polakdot network, for example, the initial nodes to communicate with, protocol identifier, initial state that the hosts agree, etc. There are a set of core fields required by the Host and a set of extensions which are used by optionally implemented features of the Host. The fields of chain specification are categorised in three parts: ChainSpec ChainSpec Extensions Genesis State which is the only mandatory part of the chainspec. A.3.1. Chain Spec ‚Äã Chain specification contains information used by the Host to communicate with network participants and optionally send data to telemetry endpoints. The client specification contains the fields below. The values for Polkadot chain are specified: name : The human readable name of the chain. "name": "Polkadot" id : The id of the chain. "id": "polkadot" chainType : Possible values are Live , Development , Local . "chainType": "Live" bootNodes : A list of MultiAddress that belong to boot nodes of the chain. The list of boot nodes for Polkadot can be found here telemetryEndpoints : Optional list of "( multiaddress , verbosity )" pairs of telemetry endpoints. The verbosity goes from 0 to 9 . With 0 being the mode with the lowest verbosity. forkId : Optional fork id. Should most likely be left empty. Can be used to signal a fork on the network level when two chains have the same genesis hash. "forkId": {} properties : Optional additional properties of the chain as subfields including token symbol, token decimals and address formats. "properties": { "ss58Format": 0, "tokenDecimals": 10, "tokenSymbol": "DOT" } A.3.2. Chain Spec Extensions ‚Äã ChainSpec Extensions are additional parameters customisable from the chainspec and correspond to optional features implemented in the Host. Definition 190. Bad Blocks Header ‚Äã BadBlocks describes a list of block header hashes that are known apriori to be bad (not belonging to canonical chain) by the host, so that the host can explicitly avoid importing them. These block headers are always considered invalid and filtered out before importing the block: b a d B l o c k s = ( b 0 , ‚Ä¶ b n ) {badBlocks}={\left({b}_{{0}},\ldots{b}_{{n}}\right)} ba d Bl oc k s = ( b 0 ‚Äã , ‚Ä¶ b n ‚Äã ) where b i {b_i} b i ‚Äã is a known invalid block header hash . Definition 191. Fork Blocks ‚Äã ForkBlocks describes a list of expected block header hashes at certain block heights. They are used to set trusted checkpoints, i.e., the host will refuse to import a block with a different hash at the given height. Forkblocks are useful mechanism to guide the Host to the right fork in instances where the chain is bricked (possibly due to issues in runtime upgrades). f o r k B l o c k s = ( < b 0 , H 0 > , ‚Ä¶ < b n , H n > ) {forkBlocks}={\left(<{b}_{{0}},{H}_{{0}}>,\ldots<{b}_{{n}},{H}_{{n}} >\right)} f or k Bl oc k s = ( < b 0 ‚Äã , H 0 ‚Äã > , ‚Ä¶ < b n ‚Äã , H n ‚Äã > ) where b i {b_i} b i ‚Äã is an apriori known valid block header hash at block height H i {H_i} H i ‚Äã . The host is expected to accept no other block except b i {b_i} b i ‚Äã at height H i {H_i} H i ‚Äã . info lightSyncState describes a check-pointing format for light clients. Its specification is currently Work-In-Progress. A.3.3. Genesis State ‚Äã The genesis state is a set of key-value pairs representing the initial state of the Polkadot state storage. It can be retrieved from the Polkadot repository . While each of those key-value pairs offers important identifiable information to the Runtime, to the Polkadot Host they are a transparent set of arbitrary chain- and network-dependent keys and values. The only exception to this are the :code ( Section 2.6.2. ) and :heappages ( Section 2.6.3.1. ) keys, which are used by the Polkadot Host to initialize the WASM environment and its Runtime. The other keys and values are unspecified and solely depend on the chain and respectively its corresponding Runtime. On initialization the data should be inserted into the state storage with the Host API ( Section B.2.1. ). As such, Polkadot does not define a formal genesis block. Nonetheless for the compatibility reasons in several algorithms, the Polkadot Host defines the genesis header ( Definition 192 ). By the abuse of terminology, "genesis block" refers to the hypothetical parent of block number 1 which holds genesis header as its header. Definition 192. Genesis Header ‚Äã The Polkadot genesis header is a data structure conforming to block header format ( Definition 10 ). It contains the following values: Table 4. Table of Genesis Header Values ‚Äã Block header field Genesis Header Value parent_hash 0 number 0 state_root Merkle hash of the state storage trie ( Definition 29 ) after inserting the genesis state in it. extrinsics_root 0 digest 0 Definition 193. Code Substitutes ‚Äã Code Substitutes is a list of pairs of block number and wasm_code . The given WASM code will be used to substitute the on-chain wasm code starting with the given block number until the spec_version on-chain changes. The substitute code should be as close as possible to the on-chain wasm code. A substitute should be used to fix a bug that can not be fixed with a runtime upgrade, if for example the runtime is constantly panicking. Introducing new runtime apis isn't supported, because the node will read the runtime version from the on-chain wasm code. Use this functionality only when there is no other way around and to only patch the problematic bug, the rest should be done with a on-chain runtime upgrade. A.4. Erasure Encoding ‚Äã A.4.1. Erasure Encoding ‚Äã info Erasure Encoding has not been documented yet. Bibliography ‚Äã 1. Saarinen MJ, Aumasson J-P. The BLAKE2 cryptographic hash and message authentication code (MAC) [Internet]. https://tools.ietf.org/html/rfc7693: -; 2015. Report No.: 7693. Available from: https://tools.ietf.org/html/rfc7693 2. Papadopoulos D, Wessels D, Huque S, Naor M, Vƒçel√°k J, Reyzin L, et al. Making NSEC5 Practical for DNSSEC [Internet]. Cryptology ePrint Archive, Paper 2017/099; 2017. Available from: https://eprint.iacr.org/2017/099 3. Goldberg S, Papadopoulos D, Vcelak J. Internet Draft - Verifiable Random Functions (VRFs) [Internet]. draft-goldbe-vrf-01. 2017. Available from: https://tools.ietf.org/id/draft-goldbe-vrf-01.4. Josefsson S, Liusvaara I. Edwards-curve digital signature algorithm (EdDSA). In: Internet Research Task Force, Crypto Forum Research Group, RFC. 2017. 5. de Valence H. Explicitly Defining and Modifying Ed25519 Validation Rules [Internet]. 2020. Available from: https://github.com/zcash/zips/blob/master/zip-0215.rst Previous 12. Metadata Next Appendix B: Host API A.1. Cryptographic Algorithms A.1.1. Hash Functions A.1.2. Randomness A.1.3. VRF A.1.4. Cryptographic Keys A.2. Auxiliary Encodings A.2.1. Binary Enconding A.2.2. SCALE Codec A.2.3. Hex Encoding A.3. Chain Specification A.3.1. Chain Spec A.3.2. Chain Spec Extensions A.3.3. Genesis State A.4. Erasure Encoding A.4.1. Erasure Encoding Bibliography 
Appendix B: Host API | Polkadot Protocol Specification Appendix B: Host API On this page Appendix B: Host API Description of the expected environment available for import by the Polkadot Runtime B.1. Preliminaries ‚Äã The Polkadot Host API is a set of functions that the Polkadot Host exposes to Runtime to access external functions needed for various reasons, such as the Storage of the content, access and manipulation, memory allocation, and also efficiency. The encoding of each data type is specified or referenced in this section. If the encoding is not mentioned, then the default Wasm encoding is used, such as little-endian byte ordering for integers. Definition 194. Exposed Host API ‚Äã By RE B \text{RE}_{{B}} RE B ‚Äã we refer to the API exposed by the Polkadot Host, which interacts, manipulates, and responds based on the state storage whose state is set at the end of the execution of block B {B} B . Definition 195. Runtime Pointer ‚Äã The Runtime pointer type is an unsigned 32-bit integer representing a pointer to data in memory. This pointer is the primary way to exchange data of fixed/known size between the Runtime and Polkadot Host. Definition 196. Runtime Pointer Size ‚Äã The Runtime pointer-size type is an unsigned 64-bit integer representing two consecutive integers. The least significant is Runtime pointer ( Definition 195 ). The most significant provides the size of the data in bytes. This representation is the primary way to exchange data of arbitrary/dynamic sizes between the Runtime and the Polkadot Host. Definition 197. Lexicographic ordering ‚Äã Lexicographic ordering refers to the ascending ordering of bytes or byte arrays, such as: [ 0 , 0 , 2 ] < [ 0 , 1 , 1 ] < [ 1 ] < [ 1 , 1 , 0 ] < [ 2 ] < [ ‚Ä¶ ] {\left[{0},{0},{2}\right]}<{\left[{0},{1},{1}\right]}<{\left[{1}\right]}<{\left[{1},{1},{0}\right]}<{\left[{2}\right]}<{\left[\ldots\right]} [ 0 , 0 , 2 ] < [ 0 , 1 , 1 ] < [ 1 ] < [ 1 , 1 , 0 ] < [ 2 ] < [ ‚Ä¶ ] The functions are specified in each subsequent subsection for each category of those functions. B.2. Storage ‚Äã Interface for accessing the storage from within the runtime. danger As of now, the storage API should silently ignore any keys that start with the :child_storage:default: prefix. This applies to reading and writing. If the function expects a return value, then None ( Definition 180 ) should be returned. See substrate issue # 12461 . Definition 198. State Version ‚Äã The state version, v {v} v , dictates how a Merkle root should be constructed. The data structure is a varying type of the following format: v = { 0 full values 1 node hashes {v}={\left\lbrace\begin{matrix}{0}&\text{full values}\\{1}&\text{node hashes}\end{matrix}\right.} v = { 0 1 ‚Äã full values node hashes ‚Äã where 0 {0} 0 indicates that the values of the keys should be inserted into the trie directly, and 1 {1} 1 makes use of "node hashes" when calculating the Merkle proof ( Definition 28 ). B.2.1. ext_storage_set ‚Äã Sets the value under a given key into storage. B.2.1.1. Version 1 - Prototype ‚Äã (func $ext_storage_set_version_1 (param $key i64) (param $value i64)) Arguments key : a pointer-size ( Definition 196 ) containing the key. value : a pointer-size ( Definition 196 ) containing the value. B.2.2. ext_storage_get ‚Äã Retrieves the value associated with the given key from storage. B.2.2.1. Version 1 - Prototype ‚Äã (func $ext_storage_get_version_1 (param $key i64) (result i64)) Arguments key : a pointer-size ( Definition 196 ) containing the key. result : a pointer-size ( Definition 196 ) returning the SCALE encoded Option value ( Definition 180 ) containing the value. B.2.3. ext_storage_read ‚Äã Gets the given key from storage, placing the value into a buffer and returning the number of bytes that the entry in storage has beyond the offset. B.2.3.1. Version 1 - Prototype ‚Äã (func $ext_storage_read_version_1 (param $key i64) (param $value_out i64) (param $offset i32) (result i64)) Arguments key : a pointer-size ( Definition 196 ) containing the key. value_out : a pointer-size ( Definition 196 ) containing the buffer to which the value will be written to. This function will never write more then the length of the buffer, even if the value‚Äôs length is bigger. offset : an u32 integer (typed as i32 due to wasm types) containing the offset beyond the value should be read from. result : a pointer-size ( Definition 196 ) pointing to a SCALE encoded Option value ( Definition 180 ) containing an unsigned 32-bit integer representing the number of bytes left at supplied offset . Returns None if the entry does not exist. B.2.4. ext_storage_clear ‚Äã Clears the storage of the given key and its value. Non-existent entries are silently ignored. B.2.4.1. Version 1 - Prototype ‚Äã (func $ext_storage_clear_version_1 (param $key_data i64)) Arguments key : a pointer-size ( Definition 196 ) containing the key. B.2.5. ext_storage_exists ‚Äã Checks whether the given key exists in storage. B.2.5.1. Version 1 - Prototype ‚Äã (func $ext_storage_exists_version_1 (param $key_data i64) (return i32)) Arguments key : a pointer-size ( Definition 196 ) containing the key. return : an i32 integer value equal to 1 if the key exists or a value equal to 0 if otherwise. B.2.6. ext_storage_clear_prefix ‚Äã Clear the storage of each key/value pair where the key starts with the given prefix. B.2.6.1. Version 1 - Prototype ‚Äã (func $ext_storage_clear_prefix_version_1 (param $prefix i64)) Arguments prefix : a pointer-size ( Definition 196 ) containing the prefix. B.2.6.2. Version 2 - Prototype ‚Äã (func $ext_storage_clear_prefix_version_2 (param $prefix i64) (param $limit i64) (return i64)) Arguments prefix : a pointer-size ( Definition 196 ) containing the prefix. limit : a pointer-size ( Definition 196 ) to an Option type ( Definition 180 ) containing an unsigned 32-bit integer indicating the limit on how many keys should be deleted. No limit is applied if this is None . Any keys created during the current block execution do not count toward the limit. return : a pointer-size ( Definition 196 ) to the following variant, k {k} k : k = { 0 ‚Üí c 1 ‚Üí c {k}={\left\lbrace\begin{matrix}{0}&\rightarrow{c}\\{1}&\rightarrow{c}\end{matrix}\right.} k = { 0 1 ‚Äã ‚Üí c ‚Üí c ‚Äã where 0 indicates that all keys of the child storage have been removed, followed by the number of removed keys, c {c} c . The variant 1 indicates that there are remaining keys, followed by the number of removed keys. B.2.7. ext_storage_append ‚Äã Append the SCALE encoded value to a SCALE encoded sequence ( Definition 182 ) at the given key. This function assumes that the existing storage item is either empty or a SCALE-encoded sequence and that the value to append is also SCALE encoded and of the same type as the items in the existing sequence. To improve performance, this function is allowed to skip decoding the entire SCALE encoded sequence and instead can just append the new item to the end of the existing data and increment the length prefix Enc SC Len {\text{Enc}_{{\text{SC}}}^{{\text{Len}}}} Enc SC Len ‚Äã . caution If the storage item does not exist or is not SCALE encoded, the storage item will be set to the specified value, represented as a SCALE-encoded byte array. B.2.7.1. Version 1 - Prototype ‚Äã (func $ext_storage_append_version_1 (param $key i64) (param $value i64)) Arguments key : a pointer-size ( Definition 196 ) containing the key. value : a pointer-size ( Definition 196 ) containing the value to be appended. B.2.8. ext_storage_root ‚Äã Compute the storage root. B.2.8.1. Version 1 - Prototype ‚Äã (func $ext_storage_root_version_1 (return i64)) Arguments return : a pointer-size ( Definition 196 ) to a buffer containing the 256-bit Blake2 storage root. B.2.8.2. Version 2 - Prototype ‚Äã (func $ext_storage_root_version_2 (param $version i32) (return i64)) Arguments version : the state version ( Definition 198 ). return : a pointer-size ( Definition 196 ) to the buffer containing the 256-bit Blake2 storage root. B.2.9. ext_storage_changes_root ‚Äã info This function is not longer used and only exists for compatibility reasons. B.2.9.1. Version 1 - Prototype ‚Äã (func $ext_storage_changes_root_version_1 (param $parent_hash i64) (return i64)) Arguments parent_hash : a pointer-size ( Definition 196 ) to the SCALE encoded block hash. return : a pointer-size ( Definition 196 ) to an Option type ( Definition 180 ) that‚Äôs always None . B.2.10. ext_storage_next_key ‚Äã Get the next key in storage after the given one in lexicographic order ( Definition 197 ). The key provided to this function may or may not exist in storage. B.2.10.1. Version 1 - Prototype ‚Äã (func $ext_storage_next_key_version_1 (param $key i64) (return i64)) Arguments key : a pointer-size ( Definition 196 ) to the key. return : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the next key in lexicographic order. B.2.11. ext_storage_start_transaction ‚Äã Start a new nested transaction. This allows to either commit or roll back all changes that are made after this call. For every transaction, there must be a matching call to either ext_storage_rollback_transaction ( Section B.2.12. ) or ext_storage_commit_transaction ( Section B.2.13. ). This is also effective for all values manipulated using the child storage API ( Section B.3. ). It‚Äôs legal to call this function multiple times in a row. caution This is a low-level API that is potentially dangerous as it can easily result in unbalanced transactions. Runtimes should use high-level storage abstractions. B.2.11.1. Version 1 - Prototype ‚Äã (func $ext_storage_start_transaction_version_1) Arguments None. B.2.12. ext_storage_rollback_transaction ‚Äã Rollback the last transaction started by ext_storage_start_transaction ( Section B.2.11. ). Any changes made during that transaction are discarded. It‚Äôs legal to call this function multiple times in a row. caution Panics if ext_storage_start_transaction ( Section B.2.11. ) was not called. B.2.12.1. Version 1 - Prototype ‚Äã (func $ext_storage_rollback_transaction_version_1) Arguments None. B.2.13. ext_storage_commit_transaction ‚Äã Commit the last transaction started by ext_storage_start_transaction ( Section B.2.11. ). Any changes made during that transaction are committed to the main state. It‚Äôs legal to call this function multiple times in a row. caution Panics if ext_storage_start_transaction ( Section B.2.11. ) was not called. B.2.13.1. Version 1 - Prototype ‚Äã (func $ext_storage_commit_transaction_version_1) Arguments None. B.3. Child Storage ‚Äã Interface for accessing the child storage from within the runtime. Definition 199. Child Storage ‚Äã Child storage key is an unprefixed location of the child trie in the main trie. B.3.1. ext_default_child_storage_set ‚Äã Sets the value under a given key into the child storage. B.3.1.1. Version 1 - Prototype ‚Äã (func $ext_default_child_storage_set_version_1 (param $child_storage_key i64) (param $key i64) (param $value i64)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). key : a pointer-size ( Definition 196 ) to the key. value : a pointer-size ( Definition 196 ) to the value. B.3.2. ext_default_child_storage_get ‚Äã Retrieves the value associated with the given key from the child storage. B.3.2.1. Version 1 - Prototype ‚Äã (func $ext_default_child_storage_get_version_1 (param $child_storage_key i64) (param $key i64) (result i64)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). key : a pointer-size ( Definition 196 ) to the key. result : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the value. B.3.3. ext_default_child_storage_read ‚Äã Gets the given key from storage, placing the value into a buffer and returning the number of bytes that the entry in storage has beyond the offset. B.3.3.1. Version 1 - Prototype ‚Äã (func $ext_default_child_storage_read_version_1 (param $child_storage_key i64) (param $key i64) (param $value_out i64) (param $offset i32) (result i64)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). key : a pointer-size ( Definition 196 ) to the key. value_out : a pointer-size ( Definition 196 ) to the buffer to which the value will be written to. This function will never write more then the length of the buffer, even if the value‚Äôs length is bigger. offset : an u32 integer (typed as i32 due to wasm types) containing the offset beyond the value should be read from. result : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the number of bytes written into the value_out buffer. Returns if the entry does not exists. B.3.4. ext_default_child_storage_clear ‚Äã Clears the storage of the given key and its value from the child storage. Non-existent entries are silently ignored. B.3.4.1. Version 1 - Prototype ‚Äã (func $ext_default_child_storage_clear_version_1 (param $child_storage_key i64) (param $key i64)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). key : a pointer-size ( Definition 196 ) to the key. B.3.5. ext_default_child_storage_storage_kill ‚Äã Clears an entire child storage. B.3.5.1. Version 1 - Prototype ‚Äã (func $ext_default_child_storage_storage_kill_version_1 (param $child_storage_key i64)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). B.3.5.2. Version 2 - Prototype ‚Äã (func $ext_default_child_storage_storage_kill_version_2 (param $child_storage_key i64) (param $limit i64) (return i32)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). limit : a pointer-size ( Definition 196 ) to an Option type ( Definition 180 ) containing an unsigned 32-bit integer indicating the limit on how many keys should be deleted. No limit is applied if this is None . Any keys created during the current block execution do not count toward the limit. return : a value equal to 1 if all the keys of the child storage have been deleted or a value equal to 0 if there are remaining keys. B.3.5.3. Version 3 - Prototype ‚Äã (func $ext_default_child_storage_storage_kill_version_3 (param $child_storage_key i64) (param $limit i64) (return i64)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). limit : a pointer-size ( Definition 196 ) to an Option type ( Definition 180 ) containing an unsigned 32-bit integer indicating the limit on how many keys should be deleted. No limit is applied if this is None . Any keys created during the current block execution do not count toward the limit. return : a pointer-size ( Definition 196 ) to the following variant, k {k} k : k = { 0 ‚Üí c 1 ‚Üí c {k}={\left\lbrace\begin{matrix}{0}&\rightarrow{c}\\{1}&\rightarrow{c}\end{matrix}\right.} k = { 0 1 ‚Äã ‚Üí c ‚Üí c ‚Äã where 0 indicates that all keys of the child storage have been removed, followed by the number of removed keys, c {c} c . The variant 1 indicates that there are remaining keys, followed by the number of removed keys. B.3.6. ext_default_child_storage_exists ‚Äã Checks whether the given key exists in the child storage. B.3.6.1. Version 1 - Prototype ‚Äã (func $ext_default_child_storage_exists_version_1 (param $child_storage_key i64) (param $key i64) (return i32)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). key : a pointer-size ( Definition 196 ) to the key. return : an i32 integer value equal to 1 if the key exists or a value equal to 0 if otherwise. B.3.7. ext_default_child_storage_clear_prefix ‚Äã Clears the child storage of each key/value pair where the key starts with the given prefix. B.3.7.1. Version 1 - Prototype ‚Äã (func $ext_default_child_storage_clear_prefix_version_1 (param $child_storage_key i64) (param $prefix i64)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). prefix : a pointer-size ( Definition 196 ) to the prefix. B.3.7.2. Version 2 - Prototype ‚Äã (func $ext_default_child_storage_clear_prefix_version_2 (param $child_storage_key i64) (param $prefix i64) (param $limit i64) (return i64)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). prefix : a pointer-size ( Definition 196 ) to the prefix. limit : a pointer-size ( Definition 196 ) to an Option type ( Definition 180 ) containing an unsigned 32-bit integer indicating the limit on how many keys should be deleted. No limit is applied if this is None . Any keys created during the current block execution do not count towards the limit. return : a pointer-size ( Definition 196 ) to the following variant, k {k} k : k = { 0 ‚Üí c 1 ‚Üí c {k}={\left\lbrace\begin{matrix}{0}&\rightarrow{c}\\{1}&\rightarrow{c}\end{matrix}\right.} k = { 0 1 ‚Äã ‚Üí c ‚Üí c ‚Äã where 0 indicates that all keys of the child storage have been removed, followed by the number of removed keys, c {c} c . The variant 1 indicates that there are remaining keys, followed by the number of removed keys. B.3.8. ext_default_child_storage_root ‚Äã Commits all existing operations and computes the resulting child storage root. B.3.8.1. Version 1 - Prototype ‚Äã (func $ext_default_child_storage_root_version_1 (param $child_storage_key i64) (return i64)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). return : a pointer-size ( Definition 196 ) to the SCALE encoded storage root. B.3.8.2. Version 2 - Prototype ‚Äã (func $ext_default_child_storage_root_version_2 (param $child_storage_key i64) (param $version i32) (return i64)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). version : the state version ( Definition 198 ). return : a pointer ( Definition 195 ) to the buffer containing the 256-bit Blake2 storage root. B.3.9. ext_default_child_storage_next_key ‚Äã Gets the next key in storage after the given one in lexicographic order ( Definition 197 ). The key provided to this function may or may not exist in storage. B.3.9.1. Version 1 - Prototype ‚Äã (func $ext_default_child_storage_next_key_version_1 (param $child_storage_key i64) (param $key i64) (return i64)) Arguments child_storage_key : a pointer-size ( Definition 196 ) to the child storage key ( Definition 199 ). key : a pointer-size ( Definition 196 ) to the key. return : a pointer-size ( Definition 196 ) to the SCALE encoded as defined in Definition 180 containing the next key in lexicographic order. Returns if the entry cannot be found. B.4. Crypto ‚Äã Interfaces for working with crypto related types from within the runtime. Definition 200. Key Type Identifier ‚Äã Cryptographic keys are stored in separate key stores based on their intended use case. The separate key stores are identified by a 4-byte ASCII key type identifier . The following known types are available: Table 5. Table of known key type identifiers ‚Äã Id Description acco Key type for the controlling accounts babe Key type for the Babe module gran Key type for the Grandpa module imon Key type for the ImOnline module audi Key type for the AuthorityDiscovery module para Key type for the Parachain Validator Key asgn Key type for the Parachain Assignment Key Definition 201. ECDSA Verify Error ‚Äã EcdsaVerifyError is a varying data type ( Definition 178 ) that specifies the error type when using ECDSA recovery functionality. The following values are possible: Table 6. Table of error types in ECDSA recovery ‚Äã Id Description 0 Incorrect value of R or S 1 Incorrect value of V 2 Invalid signature B.4.1. ext_crypto_ed25519_public_keys ‚Äã Returns all ed25519 public keys for the given key identifier from the keystore. B.4.1.1. Version 1 - Prototype ‚Äã (func $ext_crypto_ed25519_public_keys_version_1 (param $key_type_id i32) (return i64)) Arguments key_type_id : a pointer ( Definition 195 ) to the key type identifier ( Definition 200 ). return : a pointer-size ( Definition 196 ) to an SCALE encoded 256-bit public keys. B.4.2. ext_crypto_ed25519_generate ‚Äã Generates an ed25519 key for the given key type using an optional BIP-39 seed and stores it in the keystore. caution Panics if the key cannot be generated, such as when an invalid key type or invalid seed was provided. B.4.2.1. Version 1 - Prototype ‚Äã (func $ext_crypto_ed25519_generate_version_1 (param $key_type_id i32) (param $seed i64) (return i32)) Arguments key_type_id : a pointer ( Definition 195 ) to the key type identifier ( Definition 200 ). seed : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the BIP-39 seed which must be valid UTF8. return : a pointer ( Definition 195 ) to the buffer containing the 256-bit public key. B.4.3. ext_crypto_ed25519_sign ‚Äã Signs the given message with the ed25519 key that corresponds to the given public key and key type in the keystore. B.4.3.1. Version 1 - Prototype ‚Äã (func $ext_crypto_ed25519_sign_version_1 (param $key_type_id i32) (param $key i32) (param $msg i64) (return i64)) Arguments key_type_id : a pointer ( Definition 195 ) to the key type identifier ( Definition 200 ). key : a pointer to the buffer containing the 256-bit public key. msg : a pointer-size ( Definition 196 ) to the message that is to be signed. return : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the 64-byte signature. This function returns if the public key cannot be found in the key store. B.4.4. ext_crypto_ed25519_verify ‚Äã Verifies an ed25519 signature. B.4.4.1. Version 1 - Prototype ‚Äã (func $ext_crypto_ed25519_verify_version_1 (param $sig i32) (param $msg i64) (param $key i32) (return i32)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 64-byte signature. msg : a pointer-size ( Definition 196 ) to the message that is to be verified. key : a pointer to the buffer containing the 256-bit public key. return : a i32 integer value equal to 1 if the signature is valid or a value equal to 0 if otherwise. B.4.5. ext_crypto_ed25519_batch_verify ‚Äã Registers an ed25519 signature for batch verification. Batch verification is enabled by calling ext_crypto_start_batch_verify ( Section B.4.20. ). The result of the verification is returned by ext_crypto_finish_batch_verify ( Section B.4.21. ). If batch verification is not enabled, the signature is verified immediately. B.4.5.1. Version 1 ‚Äã (func $ext_crypto_ed25519_batch_verify_version_1 (param $sig i32) (param $msg i64) (param $key i32) (return i32)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 64-byte signature. msg : a pointer-size ( Definition 196 ) to the message that is to be verified. key : a pointer to the buffer containing the 256-bit public key. return : an i32 integer value equal to 1 if the signature is valid or batched or a value equal 0 to if otherwise. B.4.6. ext_crypto_sr25519_public_keys ‚Äã Returns all sr25519 public keys for the given key id from the keystore. B.4.6.1. Version 1 - Prototype ‚Äã (func $ext_crypto_sr25519_public_keys_version_1 (param $key_type_id i32) (return i64)) Arguments key_type_id : a pointer ( Definition 195 ) to the key type identifier ( Definition 200 ). return : a pointer-size ( Definition 196 ) to the SCALE encoded 256-bit public keys. B.4.7. ext_crypto_sr25519_generate ‚Äã Generates an sr25519 key for the given key type using an optional BIP-39 seed and stores it in the keystore. caution Panics if the key cannot be generated, such as when an invalid key type or invalid seed was provided. B.4.7.1. Version 1 - Prototype ‚Äã (func $ext_crypto_sr25519_generate_version_1 (param $key_type_id i32) (param $seed i64) (return i32)) Arguments key_type_id : a pointer ( Definition 195 ) to the key identifier ( Definition 200 ). seed : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the BIP-39 seed which must be valid UTF8. return : a pointer ( Definition 195 ) to the buffer containing the 256-bit public key. B.4.8. ext_crypto_sr25519_sign ‚Äã Signs the given message with the sr25519 key that corresponds to the given public key and key type in the keystore. B.4.8.1. Version 1 - Prototype ‚Äã (func $ext_crypto_sr25519_sign_version_1 (param $key_type_id i32) (param $key i32) (param $msg i64) (return i64)) Arguments key_type_id : a pointer ( Definition 195 ) to the key identifier ( Definition 200 ). key : a pointer to the buffer containing the 256-bit public key. msg : a pointer-size ( Definition 196 ) to the message that is to be signed. return : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the 64-byte signature. This function returns None if the public key cannot be found in the key store. B.4.9. ext_crypto_sr25519_verify ‚Äã Verifies an sr25519 signature. B.4.9.1. Version 1 - Prototype ‚Äã (func $ext_crypto_sr25519_verify_version_1 (param $sig i32) (param $msg i64) (param $key i32) (return i32)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 64-byte signature. msg : a pointer-size ( Definition 196 ) to the message that is to be verified. key : a pointer to the buffer containing the 256-bit public key. return : a i32 integer value equal to 1 if the signature is valid or a value equal to 0 if otherwise. B.4.9.2. Version 2 - Prototype ‚Äã (func $ext_crypto_sr25519_verify_version_2 (param $sig i32) (param $msg i64) (param $key i32) (return i32)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 64-byte signature. msg : a pointer-size ( Definition 196 ) to the message that is to be verified. key : a pointer to the buffer containing the 256-bit public key. return : a i32 integer value equal to 1 if the signature is valid or a value equal to 0 if otherwise. B.4.10. ext_crypto_sr25519_batch_verify ‚Äã Registers a sr25519 signature for batch verification. Batch verification is enabled by calling ext_crypto_start_batch_verify ( Section B.4.20. ). The result of the verification is returned by ext_crypto_finish_batch_verify ( Section B.4.21. ). If batch verification is not enabled, the signature is verified immediately. B.4.10.1. Version 1 ‚Äã (func $ext_crypto_sr25519_batch_verify_version_1 (param $sig i32) (param $msg i64) (param $key i32) (return i32)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 64-byte signature. msg : a pointer-size ( Definition 196 ) to the message that is to be verified. key : a pointer to the buffer containing the 256-bit public key. return : an i32 integer value equal to 1 if the signature is valid or batched or a value equal 0 to if otherwise. B.4.11. ext_crypto_ecdsa_public_keys ‚Äã Returns all ecdsa public keys for the given key id from the keystore. B.4.11.1. Version 1 - Prototype ‚Äã (func $ext_crypto_ecdsa_public_key_version_1 (param $key_type_id i64) (return i64)) Arguments key_type_id : a pointer ( Definition 195 ) to the key type identifier ( Definition 200 ). return : a pointer-size ( Definition 196 ) to the SCALE encoded 33-byte compressed public keys. B.4.12. ext_crypto_ecdsa_generate ‚Äã Generates an ecdsa key for the given key type using an optional BIP-39 seed and stores it in the keystore. caution Panics if the key cannot be generated, such as when an invalid key type or invalid seed was provided. B.4.12.1. Version 1 - Prototype ‚Äã (func $ext_crypto_ecdsa_generate_version_1 (param $key_type_id i32) (param $seed i64) (return i32)) Arguments key_type_id : a pointer ( Definition 195 ) to the key identifier ( Definition 200 ). seed : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the BIP-39 seed which must be valid UTF8. return : a pointer ( Definition 195 ) to the buffer containing the 33-byte compressed public key. B.4.13. ext_crypto_ecdsa_sign ‚Äã Signs the hash of the given message with the ecdsa key that corresponds to the given public key and key type in the keystore. B.4.13.1. Version 1 - Prototype ‚Äã (func $ext_crypto_ecdsa_sign_version_1 (param $key_type_id i32) (param $key i32) (param $msg i64) (return i64)) Arguments key_type_id : a pointer ( Definition 195 ) to the key identifier ( Definition 200 ). key : a pointer to the buffer containing the 33-byte compressed public key. msg : a pointer-size ( Definition 196 ) to the message that is to be signed. return : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the signature. The signature is 65-bytes in size, where the first 512-bits represent the signature and the other 8 bits represent the recovery ID. This function returns if the public key cannot be found in the key store. B.4.14. ext_crypto_ecdsa_sign_prehashed ‚Äã Signs the prehashed message with the ecdsa key that corresponds to the given public key and key type in the keystore. B.4.14.1. Version 1 - Prototype ‚Äã (func $ext_crypto_ecdsa_sign_prehashed_version_1 (param $key_type_id i32) (param $key i32) (param $msg i64) (return i64)) Arguments key_type_id : a pointer-size ( Definition 195 ) to the key identifier ( Definition 200 ). key : a pointer to the buffer containing the 33-byte compressed public key. msg : a pointer-size ( Definition 196 ) to the message that is to be signed. return : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the signature. The signature is 65-bytes in size, where the first 512-bits represent the signature and the other 8 bits represent the recovery ID. This function returns if the public key cannot be found in the key store. B.4.15. ext_crypto_ecdsa_verify ‚Äã Verifies the hash of the given message against an ECDSA signature. B.4.15.1. Version 1 - Prototype ‚Äã This function allows the verification of non-standard, overflowing ECDSA signatures, an implementation specific mechanism of the Rust libsecp256k1 library , specifically the parse_overflowing function. (func $ext_crypto_ecdsa_verify_version_1 (param $sig i32) (param $msg i64) (param $key i32) (return i32)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 65-byte signature. The signature is 65-bytes in size, where the first 512-bits represent the signature and the other 8 bits represent the recovery ID. msg : a pointer-size ( Definition 196 ) to the message that is to be verified. key : a pointer to the buffer containing the 33-byte compressed public key. return : a i32 integer value equal 1 to if the signature is valid or a value equal to 0 if otherwise. B.4.15.2. Version 2 - Prototype ‚Äã Does not allow the verification of non-standard, overflowing ECDSA signatures. (func $ext_crypto_ecdsa_verify_version_2 (param $sig i32) (param $msg i64) (param $key i32) (return i32)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 65-byte signature. The signature is 65-bytes in size, where the first 512-bits represent the signature and the other 8 bits represent the recovery ID. msg : a pointer-size ( Definition 196 ) to the message that is to be verified. key : a pointer to the buffer containing the 33-byte compressed public key. return : a i32 integer value equal 1 to if the signature is valid or a value equal to 0 if otherwise. B.4.16. ext_crypto_ecdsa_verify_prehashed ‚Äã Verifies the prehashed message against a ECDSA signature. B.4.16.1. Version 1 - Prototype ‚Äã (func $ext_crypto_ecdsa_verify_prehashed_version_1 (param $sig i32) (param $msg i32) (param $key i32) (return i32)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 65-byte signature. The signature is 65-bytes in size, where the first 512-bits represent the signature and the other 8 bits represent the recovery ID. msg : a pointer to the 32-bit prehashed message to be verified. key : a pointer to the 33-byte compressed public key. return : a i32 integer value equal 1 to if the signature is valid or a value equal to 0 if otherwise. B.4.17. ext_crypto_ecdsa_batch_verify ‚Äã Registers a ECDSA signature for batch verification. Batch verification is enabled by calling ext_crypto_start_batch_verify ( Section B.4.20. ). The result of the verification is returned by ext_crypto_finish_batch_verify ( Section B.4.21. ). If batch verification is not enabled, the signature is verified immediately. B.4.17.1. Version 1 ‚Äã (func $ext_crypto_ecdsa_batch_verify_version_1 (param $sig i32) (param $msg i64) (param $key i32) (return i32)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 64-byte signature. msg : a pointer-size ( Definition 196 ) to the message that is to be verified. key : a pointer to the buffer containing the 256-bit public key. return : a i32 integer value equal to 1 if the signature is valid or batched or a value equal 0 to if otherwise. B.4.18. ext_crypto_secp256k1_ecdsa_recover ‚Äã Verify and recover a secp256k1 ECDSA signature. B.4.18.1. Version 1 - Prototype ‚Äã This function can handle non-standard, overflowing ECDSA signatures, an implemenation specific mechanism of the Rust libsecp256k1 library , specifically the parse_overflowing function. (func $ext_crypto_secp256k1_ecdsa_recover_version_1 (param $sig i32) (param $msg i32) (return i64)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 65-byte signature in RSV format. V should be either 0/1 or 27/28 . msg : a pointer ( Definition 195 ) to the buffer containing the 256-bit Blake2 hash of the message. return : a pointer-size ( Definition 196 ) to the SCALE encoded Result ( Definition 181 ). On success it contains the 64-byte recovered public key or an error type ( Definition 201 ) on failure. B.4.18.2. Version 2 - Prototype ‚Äã Does not handle non-standard, overflowing ECDSA signatures. (func $ext_crypto_secp256k1_ecdsa_recover_version_2 (param $sig i32) (param $msg i32) (return i64)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 65-byte signature in RSV format. V should be either or . msg : a pointer ( Definition 195 ) to the buffer containing the 256-bit Blake2 hash of the message. return : a pointer-size ( Definition 196 ) to the SCALE encoded Result ( Definition 181 ). On success it contains the 64-byte recovered public key or an error type ( Definition 201 ) on failure. B.4.19. ext_crypto_secp256k1_ecdsa_recover_compressed ‚Äã Verify and recover a secp256k1 ECDSA signature. B.4.19.1. Version 1 - Prototype ‚Äã This function can handle non-standard, overflowing ECDSA signatures, an implemenation specific mechanism of the Rust libsecp256k1 library , specifically the parse_overflowing function. (func $ext_crypto_secp256k1_ecdsa_recover_compressed_version_1 (param $sig i32) (param $msg i32) (return i64)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 65-byte signature in RSV format. V should be either 0/1 or 27/28 . msg : a pointer ( Definition 195 ) to the buffer containing the 256-bit Blake2 hash of the message. return : a pointer-size ( Definition 196 ) to the SCALE encoded Result value ( Definition 181 ). On success it contains the 33-byte recovered public key in compressed form on success or an error type ( Definition 201 ) on failure. B.4.19.2. Version 2 - Prototype ‚Äã Does not handle non-standard, overflowing ECDSA signatures. (func $ext_crypto_secp256k1_ecdsa_recover_compressed_version_2 (param $sig i32) (param $msg i32) (return i64)) Arguments sig : a pointer ( Definition 195 ) to the buffer containing the 65-byte signature in RSV format. V should be either 0/1 or 27/28 . msg : a pointer ( Definition 195 ) to the buffer containing the 256-bit Blake2 hash of the message. return : a pointer-size ( Definition 196 ) to the SCALE encoded Result value ( Definition 181 ). On success it contains the 33-byte recovered public key in compressed form on success or an error type ( Definition 201 ) on failure. B.4.20. ext_crypto_start_batch_verify ‚Äã Starts the verification extension. The extension is a separate background process and is used to parallel-verify signatures which are pushed to the batch with ext_crypto_ed25519_batch_verify ( Section B.4.5. ), ext_crypto_sr25519_batch_verify ( Section B.4.10. ) or ext_crypto_ecdsa_batch_verify ( Section B.4.17. ). Verification will start immediately and the Runtime can retrieve the result when calling ext_crypto_finish_batch_verify ( Section B.4.21. ). B.4.20.1. Version 1 - Prototype ‚Äã (func $ext_crypto_start_batch_verify_version_1) Arguments None. B.4.21. ext_crypto_finish_batch_verify ‚Äã Finish verifying the batch of signatures since the last call to this function. Blocks until all the signatures are verified. caution Panics if ext_crypto_start_batch_verify ( Section B.4.20. ) was not called. B.4.21.1. Version 1 - Prototype ‚Äã (func $ext_crypto_finish_batch_verify_version_1 (return i32)) Arguments return : an i32 integer value equal to 1 if all the signatures are valid or a value equal to 0 if one or more of the signatures are invalid. B.5. Hashing ‚Äã Interface that provides functions for hashing with different algorithms. B.5.1. ext_hashing_keccak_256 ‚Äã Conducts a 256-bit Keccak hash. B.5.1.1. Version 1 - Prototype ‚Äã (func $ext_hashing_keccak_256_version_1 (param $data i64) (return i32)) Arguments data : a pointer-size ( Definition 196 ) to the data to be hashed. return : a pointer ( Definition 195 ) to the buffer containing the 256-bit hash result. B.5.2. ext_hashing_keccak_512 ‚Äã Conducts a 512-bit Keccak hash. B.5.2.1. Version 1 - Prototype ‚Äã (func $ext_hashing_keccak_512_version_1 (param $data i64) (return i32)) Arguments data : a pointer-size ( Definition 196 ) to the data to be hashed. return : a pointer ( Definition 195 ) to the buffer containing the 512-bit hash result. B.5.3. ext_hashing_sha2_256 ‚Äã Conducts a 256-bit Sha2 hash. B.5.3.1. Version 1 - Prototype ‚Äã (func $ext_hashing_sha2_256_version_1 (param $data i64) (return i32)) Arguments data : a pointer-size ( Definition 196 ) to the data to be hashed. return : a pointer ( Definition 195 ) to the buffer containing the 256-bit hash result. B.5.4. ext_hashing_blake2_128 ‚Äã Conducts a 128-bit Blake2 hash. B.5.4.1. Version 1 - Prototype ‚Äã (func $ext_hashing_blake2_128_version_1 (param $data i64) (return i32)) Arguments data : a pointer-size ( Definition 196 ) to the data to be hashed. return : a pointer ( Definition 195 ) to the buffer containing the 128-bit hash result. B.5.5. ext_hashing_blake2_256 ‚Äã Conducts a 256-bit Blake2 hash. B.5.5.1. Version 1 - Prototype ‚Äã (func $ext_hashing_blake2_256_version_1 (param $data i64) (return i32)) Arguments data : a pointer-size ( Definition 196 ) to the data to be hashed. return : a pointer ( Definition 195 ) to the buffer containing the 256-bit hash result. B.5.6. ext_hashing_twox_64 ‚Äã Conducts a 64-bit xxHash hash. B.5.6.1. Version 1 - Prototype ‚Äã (func $ext_hashing_twox_64_version_1 (param $data i64) (return i32)) Arguments data : a pointer-size ( Definition 196 ) to the data to be hashed. return : a pointer ( Definition 195 ) to the buffer containing the 64-bit hash result. B.5.7. ext_hashing_twox_128 ‚Äã Conducts a 128-bit xxHash hash. B.5.7.1. Version 1 - Prototype ‚Äã (func $ext_hashing_twox_128 (param $data i64) (return i32)) Arguments data : a pointer-size ( Definition 196 ) to the data to be hashed. return : a pointer ( Definition 195 ) to the buffer containing the 128-bit hash result. B.5.8. ext_hashing_twox_256 ‚Äã Conducts a 256-bit xxHash hash. B.5.8.1. Version 1 - Prototype ‚Äã (func $ext_hashing_twox_256 (param $data i64) (return i32)) Arguments data : a pointer-size ( Definition 196 ) to the data to be hashed. return : a pointer ( Definition 195 ) to the buffer containing the 256-bit hash result. B.6. Offchain ‚Äã The Offchain Workers allow the execution of long-running and possibly non-deterministic tasks (e.g. web requests, encryption/decryption and signing of data, random number generation, CPU-intensive computations, enumeration/aggregation of on-chain data, etc.) which could otherwise require longer than the block execution time. Offchain Workers have their own execution environment. This separation of concerns is to make sure that the block production is not impacted by the long-running tasks. All data and results generated by Offchain workers are unique per node and nondeterministic. Information can be propagated to other nodes by submitting a transaction that should be included in the next block. As Offchain workers runs on their own execution environment they have access to their own separate storage. There are two different types of storage available which are defined in Definition 202 and Definition 203 . Definition 202. Persisted Storage ‚Äã Persistent storage is non-revertible and not fork-aware. It means that any value set by the offchain worker is persisted even if that block (at which the worker is called) is reverted as non-canonical (meaning that the block was surpassed by a longer chain). The value is available for the worker that is re-run at the new (different block with the same block number) and future blocks. This storage can be used by offchain workers to handle forks and coordinate offchain workers running on different forks. Definition 203. Local Storage ‚Äã Local storage is revertible and fork-aware. It means that any value set by the offchain worker triggered at a certain block is reverted if that block is reverted as non-canonical. The value is NOT available for the worker that is re-run at the next or any future blocks. Definition 204. HTTP Status Code ‚Äã HTTP status codes that can get returned by certain Offchain HTTP functions. 0 : the specified request identifier is invalid. 10 : the deadline for the started request was reached. 20 : an error has occurred during the request, e.g. a timeout or the remote server has closed the connection. On returning this error code, the request is considered destroyed and must be reconstructed again. 100 - 999 : the request has finished with the given HTTP status code. Definition 205. HTTP Error ‚Äã HTTP error, E {E} E , is a varying data type ( Definition 178 ) and specifies the error types of certain HTTP functions. Following values are possible: E = { 0 The deadile was reached 1 There was an IO error while processing the request 2 The Id of the request is invalid {E}={\left\lbrace\begin{matrix}{0}&\text{The deadile was reached}\\{1}&\text{There was an IO error while processing the request}\\{2}&\text{The Id of the request is invalid}\end{matrix}\right.} E = ‚é© ‚é® ‚éß ‚Äã 0 1 2 ‚Äã The deadile was reached There was an IO error while processing the request The Id of the request is invalid ‚Äã B.6.1. ext_offchain_is_validator ‚Äã Check whether the local node is a potential validator. Even if this function returns 1 , it does not mean that any keys are configured or that the validator is registered in the chain. B.6.1.1. Version 1 - Prototype ‚Äã (func $ext_offchain_is_validator_version_1 (return i32)) Arguments return : a i32 integer which is equal to 1 if the local node is a potential validator or a integer equal to 0 if it is not. B.6.2. ext_offchain_submit_transaction ‚Äã Given a SCALE encoded extrinsic, this function submits the extrinsic to the Host‚Äôs transaction pool, ready to be propagated to remote peers. B.6.2.1. Version 1 - Prototype ‚Äã (func $ext_offchain_submit_transaction_version_1 (param $data i64) (return i64)) Arguments data : a pointer-size ( Definition 196 ) to the byte array storing the encoded extrinsic. return : a pointer-size ( Definition 196 ) to the SCALE encoded Result value ( Definition 181 ). Neither on success or failure is there any additional data provided. The cause of a failure is implementation specific. B.6.3. ext_offchain_network_state ‚Äã Returns the SCALE encoded, opaque information about the local node‚Äôs network state. Definition 206. Opaque Network State ‚Äã The Opaque network state structure , S {S} S , is a SCALE encoded blob holding information about the the libp2p PeerId , P id {P}_{{\text{id}}} P id ‚Äã , of the local node and a list of libp2p Multiaddresses , ( M 0 , ‚Ä¶ M n ) {\left({M}_{{0}},\ldots{M}_{{n}}\right)} ( M 0 ‚Äã , ‚Ä¶ M n ‚Äã ) , the node knows it can be reached at: S = ( P id , ( M 0 , ‚Ä¶ M n ) ) {S}={\left({P}_{{\text{id}}},{\left({M}_{{0}},\ldots{M}_{{n}}\right)}\right)} S = ( P id ‚Äã , ( M 0 ‚Äã , ‚Ä¶ M n ‚Äã ) ) where P id = ( b 0 , ‚Ä¶ b n ) {P}_{{\text{id}}}={\left({b}_{{0}},\ldots{b}_{{n}}\right)} P id ‚Äã = ( b 0 ‚Äã , ‚Ä¶ b n ‚Äã ) M = ( b 0 , ‚Ä¶ b n ) {M}={\left({b}_{{0}},\ldots{b}_{{n}}\right)} M = ( b 0 ‚Äã , ‚Ä¶ b n ‚Äã ) The information contained in this structure is naturally opaque to the caller of this function. B.6.3.1. Version 1 - Prototype ‚Äã (func $ext_offchain_network_state_version_1 (result i64)) Arguments result : a pointer-size ( Definition 196 ) to the SCALE encoded Result value ( Definition 181 ). On success it contains the Opaque network state structure ( Definition 206 ). On failure, an empty value is yielded where its cause is implementation specific. B.6.4. ext_offchain_timestamp ‚Äã Returns the current timestamp. B.6.4.1. Version 1 - Prototype ‚Äã (func $ext_offchain_timestamp_version_1 (result i64)) Arguments result : an u64 integer (typed as i64 due to wasm types) indicating the current UNIX timestamp ( Definition 171 ). B.6.5. ext_offchain_sleep_until ‚Äã Pause the execution until the deadline is reached. B.6.5.1. Version 1 - Prototype ‚Äã (func $ext_offchain_sleep_until_version_1 (param $deadline i64)) Arguments deadline : an u64 integer (typed as i64 due to wasm types) specifying the UNIX timestamp ( Definition 171 ). B.6.6. ext_offchain_random_seed ‚Äã Generates a random seed. This is a truly random non deterministic seed generated by the host environment. B.6.6.1. Version 1 - Prototype ‚Äã (func $ext_offchain_random_seed_version_1 (result i32)) Arguments result : a pointer ( Definition 195 ) to the buffer containing the 256-bit seed. B.6.7. ext_offchain_local_storage_set ‚Äã Sets a value in the local storage. This storage is not part of the consensus, it‚Äôs only accessible by the offchain worker tasks running on the same machine and is persisted between runs. B.6.7.1. Version 1 - Prototype ‚Äã (func $ext_offchain_local_storage_set_version_1 (param $kind i32) (param $key i64) (param $value i64)) Arguments kind : an i32 integer indicating the storage kind. A value equal to 1 is used for a persistent storage ( Definition 202 ) and a value equal to 2 for local storage ( Definition 203 ). key : a pointer-size ( Definition 196 ) to the key. value : a pointer-size ( Definition 196 ) to the value. B.6.8. ext_offchain_local_storage_clear ‚Äã Remove a value from the local storage. B.6.8.1. Version 1 - Prototype ‚Äã (func $ext_offchain_local_storage_clear_version_1 (param $kind i32) (param $key i64)) Arguments kind : an i32 integer indicating the storage kind. A value equal to 1 is used for a persistent storage ( Definition 202 ) and a value equal to 2 for local storage ( Definition 203 ). key : a pointer-size ( Definition 196 ) to the key. B.6.9. ext_offchain_local_storage_compare_and_set ‚Äã Sets a new value in the local storage if the condition matches the current value. B.6.9.1. Version 1 - Prototype ‚Äã (fund $ext_offchain_local_storage_compare_and_set_version_1 (param $kind i32) (param $key i64) (param $old_value i64) (param $new_value i64) (result i32)) Arguments kind : an i32 integer indicating the storage kind. A value equal to 1 is used for a persistent storage ( Definition 202 ) and a value equal to 2 for local storage ( Definition 203 ). key : a pointer-size ( Definition 196 ) to the key. old_value : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the old key. new_value : a pointer-size ( Definition 196 ) to the new value. result : an i32 integer equal to 1 if the new value has been set or a value equal to 0 if otherwise. B.6.10. ext_offchain_local_storage_get ‚Äã Gets a value from the local storage. B.6.10.1. Version 1 - Prototype ‚Äã (func $ext_offchain_local_storage_get_version_1 (param $kind i32) (param $key i64) (result i64)) Arguments kind : an i32 integer indicating the storage kind. A value equal to 1 is used for a persistent storage ( Definition 202 ) and a value equal to 2 for local storage ( Definition 203 ). key : a pointer-size ( Definition 196 ) to the key. result : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the value or the corresponding key. B.6.11. ext_offchain_http_request_start ‚Äã Initiates a HTTP request given by the HTTP method and the URL. Returns the Id of a newly started request. B.6.11.1. Version 1 - Prototype ‚Äã (func $ext_offchain_http_request_start_version_1 (param $method i64) (param $uri i64) (param $meta i64) (result i64)) Arguments method : a pointer-size ( Definition 196 ) to the HTTP method. Possible values are ‚ÄúGET‚Äù and ‚ÄúPOST‚Äù. uri : a pointer-size ( Definition 196 ) to the URI. meta : a future-reserved field containing additional, SCALE encoded parameters. Currently, an empty array should be passed. result : a pointer-size ( Definition 196 ) to the SCALE encoded Result value ( Definition 181 ) containing the i16 ID of the newly started request. On failure no additionally data is provided. The cause of failure is implementation specific. B.6.12. ext_offchain_http_request_add_header ‚Äã Append header to the request. Returns an error if the request identifier is invalid, http_response_wait has already been called on the specified request identifier, the deadline is reached or an I/O error has happened (e.g. the remote has closed the connection). B.6.12.1. Version 1 - Prototype ‚Äã (func $ext_offchain_http_request_add_header_version_1 (param $request_id i32) (param $name i64) (param $value i64) (result i64)) Arguments request_id : an i32 integer indicating the ID of the started request. name : a pointer-size ( Definition 196 ) to the HTTP header name. value : a pointer-size ( Definition 196 ) to the HTTP header value. result : a pointer-size ( Definition 196 ) to the SCALE encoded Result value ( Definition 181 ). Neither on success or failure is there any additional data provided. The cause of failure is implementation specific. B.6.13. ext_offchain_http_request_write_body ‚Äã Writes a chunk of the request body. Returns a non-zero value in case the deadline is reached or the chunk could not be written. B.6.13.1. Version 1 - Prototype ‚Äã (func $ext_offchain_http_request_write_body_version_1 (param $request_id i32) (param $chunk i64) (param $deadline i64) (result i64)) Arguments request_id : an i32 integer indicating the ID of the started request. chunk : a pointer-size ( Definition 196 ) to the chunk of bytes. Writing an empty chunk finalizes the request. deadline : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the UNIX timestamp ( Definition 171 ). Passing None blocks indefinitely. result : a pointer-size ( Definition 196 ) to the SCALE encoded Result value ( Definition 181 ). On success, no additional data is provided. On error it contains the HTTP error type ( Definition 205 ). B.6.14. ext_offchain_http_response_wait ‚Äã Returns an array of request statuses (the length is the same as IDs). Note that if deadline is not provided the method will block indefinitely, otherwise unready responses will produce DeadlineReached status. B.6.14.1. Version 1 - Prototype ‚Äã (func $ext_offchain_http_response_wait_version_1 (param $ids i64) (param $deadline i64) (result i64)) Arguments ids : a pointer-size ( Definition 196 ) to the SCALE encoded array of started request IDs. deadline : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the UNIX timestamp ( Definition 171 ). Passing None blocks indefinitely. result : a pointer-size ( Definition 196 ) to the SCALE encoded array of request statuses ( Definition 204 ). B.6.15. ext_offchain_http_response_headers ‚Äã Read all HTTP response headers. Returns an array of key/value pairs. Response headers must be read before the response body. B.6.15.1. Version 1 - Prototype ‚Äã (func $ext_offchain_http_response_headers_version_1 (param $request_id i32) (result i64)) Arguments request_id : an i32 integer indicating the ID of the started request. result : a pointer-size ( Definition 196 ) to a SCALE encoded array of key/value pairs. B.6.16. ext_offchain_http_response_read_body ‚Äã Reads a chunk of body response to the given buffer. Returns the number of bytes written or an error in case a deadline is reached or the server closed the connection. If 0 is returned it means that the response has been fully consumed and the request_id is now invalid. This implies that response headers must be read before draining the body. B.6.16.1. Version 1 - Prototype ‚Äã (func $ext_offchain_http_response_read_body_version_1 (param $request_id i32) (param $buffer i64) (param $deadline i64) (result i64)) Arguments request_id : an i32 integer indicating the ID of the started request. buffer : a pointer-size ( Definition 196 ) to the buffer where the body gets written to. deadline : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the UNIX timestamp ( Definition 171 ). Passing None will block indefinitely. result : a pointer-size ( Definition 196 ) to the SCALE encoded Result value ( Definition 181 ). On success it contains an i32 integer specifying the number of bytes written or a HTTP error type ( Definition 205 ) on failure. B.7. Offchain Index ‚Äã Interface that provides functions to access the Offchain DB through offchain indexing. B.7.1. Offchain_index_set ‚Äã Write a key-value pair to the Offchain DB in a buffered fashion. B.7.1.1. Version 1 - Prototype ‚Äã (func $ext_offchain_index_set_version_1 (param $key i64) (param $value i64)) Arguments key : a pointer-size ( Definition 196 ) containing the key. value : a pointer-size ( Definition 196 ) containing the value. B.7.2. Offchain_index_clear ‚Äã Remove a key and its associated value from the Offchain DB. B.7.2.1. Version 1 - Prototype ‚Äã (func $ext_offchain_index_set_version_1 (param $key i64)) Arguments key : a pointer-size ( Definition 196 ) containing the key. B.8. Trie ‚Äã Interface that provides trie related functionality. B.8.1. ext_trie_blake2_256_root ‚Äã Compute a 256-bit Blake2 trie root formed from the iterated items. B.8.1.1. Version 1 - Prototype ‚Äã (func $ext_trie_blake2_256_root_version_1 (param $data i64) (result i32)) Arguments data : a pointer-size ( Definition 196 ) to the iterated items from which the trie root gets formed. The items consist of a SCALE encoded array containing arbitrary key/value pairs (tuples). result : a pointer ( Definition 195 ) to the buffer containing the 256-bit trie root. B.8.1.2. Version 2 - Prototype ‚Äã (func $ext_trie_blake2_256_root_version_2 (param $data i64) (param $version i32) (result i32)) Arguments data : a pointer-size ( Definition 196 ) to the iterated items from which the trie root gets formed. The items consist of a SCALE encoded array containing arbitrary key/value pairs (tuples). version : the state version ( Definition 198 ). result : a pointer ( Definition 195 ) to the buffer containing the 256-bit trie root. B.8.2. ext_trie_blake2_256_ordered_root ‚Äã Compute a 256-bit Blake2 trie root formed from the enumerated items. B.8.2.1. Version 1 - Prototype ‚Äã (func $ext_trie_blake2_256_ordered_root_version_1 (param $data i64) (result i32)) Arguments data : a pointer-size ( Definition 196 ) to the enumerated items from which the trie root gets formed. The items consist of a SCALE encoded array containing only values, where the corresponding key of each value is the index of the item in the array, starting at 0. The keys are compact encoded integers ( Definition 188 ). result : a pointer ( Definition 195 ) to the buffer containing the 256-bit trie root result. B.8.2.2. Version 2 - Prototype ‚Äã (func $ext_trie_blake2_256_ordered_root_version_2 (param $data i64) (param $version i32) (result i32)) Arguments data : a pointer-size ( Definition 196 ) to the enumerated items from which the trie root gets formed. The items consist of a SCALE encoded array containing only values, where the corresponding key of each value is the index of the item in the array, starting at 0. The keys are compact encoded integers ( Definition 188 ). version : the state version ( Definition 198 ). result : a pointer ( Definition 195 ) to the buffer containing the 256-bit trie root result. B.8.3. ext_trie_keccak_256_root ‚Äã Compute a 256-bit Keccak trie root formed from the iterated items. B.8.3.1. Version 1 - Prototype ‚Äã (func $ext_trie_keccak_256_root_version_1 (param $data i64) (result i32)) Arguments data : a pointer-size ( Definition 196 ) to the iterated items from which the trie root gets formed. The items consist of a SCALE encoded array containing arbitrary key/value pairs. result : a pointer ( Definition 195 ) to the buffer containing the 256-bit trie root. B.8.3.2. Version 2 - Prototype ‚Äã (func $ext_trie_keccak_256_root_version_2 (param $data i64) (param $version i32) (result i32)) Arguments data : a pointer-size ( Definition 196 ) to the iterated items from which the trie root gets formed. The items consist of a SCALE encoded array containing arbitrary key/value pairs. version : the state version ( Definition 198 ). result : a pointer ( Definition 195 ) to the buffer containing the 256-bit trie root. B.8.4. ext_trie_keccak_256_ordered_root ‚Äã Compute a 256-bit Keccak trie root formed from the enumerated items. B.8.4.1. Version 1 - Prototype ‚Äã (func $ext_trie_keccak_256_ordered_root_version_1 (param $data i64) (result i32)) Arguments data : a pointer-size ( Definition 196 ) to the enumerated items from which the trie root gets formed. The items consist of a SCALE encoded array containing only values, where the corresponding key of each value is the index of the item in the array, starting at 0. The keys are compact encoded integers ( Definition 188 ). result : a pointer ( Definition 195 ) to the buffer containing the 256-bit trie root result. B.8.4.2. Version 2 - Prototype ‚Äã (func $ext_trie_keccak_256_ordered_root_version_2 (param $data i64) (param $version i32) (result i32)) Arguments data : a pointer-size ( Definition 196 ) to the enumerated items from which the trie root gets formed. The items consist of a SCALE encoded array containing only values, where the corresponding key of each value is the index of the item in the array, starting at 0. The keys are compact encoded integers ( Definition 188 ). version : the state version ( Definition 198 ). result : a pointer ( Definition 195 ) to the buffer containing the 256-bit trie root result. B.8.5. ext_trie_blake2_256_verify_proof ‚Äã Verifies a key/value pair against a Blake2 256-bit merkle root. B.8.5.1. Version 1 - Prototype ‚Äã (func $ext_trie_blake2_256_verify_proof_version_1 (param $root i32) (param $proof i64) (param $key i64) (param $value i64) (result i32)) Arguments root : a pointer to the 256-bit merkle root. proof : a pointer-size ( Definition 196 ) to an array containing the node proofs. key : a pointer-size ( Definition 196 ) to the key. value : a pointer-size ( Definition 196 ) to the value. return : a value equal to 1 if the proof could be successfully verified or a value equal to 0 if otherwise. B.8.5.2. Version 2 - Prototype ‚Äã (func $ext_trie_blake2_256_verify_proof_version_2 (param $root i32) (param $proof i64) (param $key i64) (param $value i64) (param $version i32) (result i32)) Arguments root : a pointer to the 256-bit merkle root. proof : a pointer-size ( Definition 196 ) to an array containing the node proofs. key : a pointer-size ( Definition 196 ) to the key. value : a pointer-size ( Definition 196 ) to the value. version : the state version ( Definition 198 ). return : a value equal to 1 if the proof could be successfully verified or a value equal to 0 if otherwise. B.8.6. ext_trie_keccak_256_verify_proof ‚Äã Verifies a key/value pair against a Keccak 256-bit merkle root. B.8.6.1. Version 1 - Prototype ‚Äã (func $ext_trie_keccak_256_verify_proof_version_1 (param $root i32) (param $proof i64) (param $key i64) (param $value i64) (result i32)) Arguments root : a pointer to the 256-bit merkle root. proof : a pointer-size ( Definition 196 ) to an array containing the node proofs. key : a pointer-size ( Definition 196 ) to the key. value : a pointer-size ( Definition 196 ) to the value. return : a value equal to 1 if the proof could be successfully verified or a value equal to 0 if otherwise. B.8.6.2. Version 2 - Prototype ‚Äã (func $ext_trie_keccak_256_verify_proof_version_2 (param $root i32) (param $proof i64) (param $key i64) (param $value i64) (param $version i32) (result i32)) Arguments root : a pointer to the 256-bit merkle root. proof : a pointer-size ( Definition 196 ) to an array containing the node proofs. key : a pointer-size ( Definition 196 ) to the key. value : a pointer-size ( Definition 196 ) to the value. version : the state version ( Definition 198 ). return : a value equal to 1 if the proof could be successfully verified or a value equal to 0 if otherwise. B.9. Miscellaneous ‚Äã Interface that provides miscellaneous functions for communicating between the runtime and the node. B.9.1. ext_misc_print_num ‚Äã Print a number. B.9.1.1. Version 1 - Prototype ‚Äã (func $ext_misc_print_num_version_1 (param $value i64)) Arguments value : the number to be printed. B.9.2. ext_misc_print_utf8 ‚Äã Print a valid UTF8 encoded buffer. B.9.2.1. Version 1 - Prototype ‚Äã (func $ext_misc_print_utf8_version_1 (param $data i64)) Arguments : : a pointer-size ( Definition 196 ) to the valid buffer to be printed. B.9.3. ext_misc_print_hex ‚Äã Print any buffer in hexadecimal representation. B.9.3.1. Version 1 - Prototype ‚Äã (func $ext_misc_print_hex_version_1 (param $data i64)) Arguments : data : a pointer-size ( Definition 196 ) to the buffer to be printed. B.9.4. ext_misc_runtime_version ‚Äã Extract the Runtime version of the given Wasm blob by calling Core_version ( Section C.4.1. ). Returns the SCALE encoded runtime version or None ( Definition 180 ) if the call fails. This function gets primarily used when upgrading Runtimes. caution Calling this function is very expensive and should only be done very occasionally. For getting the runtime version, it requires instantiating the Wasm blob ( Section 2.6.2. ) and calling the Core_version function ( Section C.4.1. ) in this blob. B.9.4.1. Version 1 - Prototype ‚Äã (func $ext_misc_runtime_version_version_1 (param $data i64) (result i64)) Arguments data : a pointer-size ( Definition 196 ) to the Wasm blob. result : a pointer-size ( Definition 196 ) to the SCALE encoded Option value ( Definition 180 ) containing the Runtime version of the given Wasm blob which is encoded as a byte array. B.10. Allocator ‚Äã The Polkadot Runtime does not include a memory allocator and relies on the Host API for all heap allocations. The beginning of this heap is marked by the __heap_base symbol exported by the Polkadot Runtime. No memory should be allocated below that address, to avoid clashes with the stack and data section. The same allocator made accessible by this Host API should be used for any other WASM memory allocations and deallocations outside the runtime e.g. when passing the SCALE-encoded parameters to Runtime API calls. B.10.1. ext_allocator_malloc ‚Äã Allocates the given number of bytes and returns the pointer to that memory location. B.10.1.1. Version 1 - Prototype ‚Äã (func $ext_allocator_malloc_version_1 (param $size i32) (result i32)) Arguments size : the size of the buffer to be allocated. result : a pointer ( Definition 195 ) to the allocated buffer. B.10.2. ext_allocator_free ‚Äã Free the given pointer. B.10.2.1. Version 1 - Prototype ‚Äã (func $ext_allocator_free_version_1 (param $ptr i32)) Arguments ptr : a pointer ( Definition 195 ) to the memory buffer to be freed. B.11. Logging ‚Äã Interface that provides functions for logging from within the runtime. Definition 207. Log Level ‚Äã The Log Level , L {L} L , is a varying data type ( Definition 178 ) and implies the emergency of the log. Possible log levels and the corresponding identifier is as follows: L = { 0 Error = 1 1 Warn = 2 2 Info = 3 3 Debug = 4 4 Trace = 5 {L}={\left\lbrace\begin{matrix}{0}&\text{Error = 1}\\{1}&\text{Warn = 2}\\{2}&\text{Info = 3}\\{3}&\text{Debug = 4}\\{4}&\text{Trace = 5}\end{matrix}\right.} L = ‚é© ‚é® ‚éß ‚Äã 0 1 2 3 4 ‚Äã Error = 1 Warn = 2 Info = 3 Debug = 4 Trace = 5 ‚Äã B.11.1. ext_logging_log ‚Äã Request to print a log message on the host. Note that this will be only displayed if the host is enabled to display log messages with given level and target. B.11.1.1. Version 1 - Prototype ‚Äã (func $ext_logging_log_version_1 (param $level i32) (param $target i64) (param $message i64)) Arguments level : the log level ( Definition 207 ). target : a pointer-size ( Definition 196 ) to the string which contains the path, module or location from where the log was executed. message : a pointer-size ( Definition 196 ) to the UTF-8 encoded log message. B.11.2. ext_logging_max_level ‚Äã Returns the max logging level used by the host. B.11.2.1. Version 1 - Prototype ‚Äã (func $ext_logging_max_level_version_1 (result i32)) Arguments None Returns result : the max log level ( Definition 207 ) used by the host. B.12. Abort Handler ‚Äã Interface for aborting the execution of the runtime. B.12.1. ext_panic_handler_abort_on_panic ‚Äã Aborts the execution of the runtime with a given message. Note that the message will be only displayed if the host is enabled to display those types of messages, which is implementation specific. B.12.1.1. Version 1 - Prototype ‚Äã (func $ext_panic_handler_abort_on_panic_version_1 (param $message i64)) Arguments message : a pointer-size ( Definition 196 ) to the UTF-8 encoded message. Previous Appendix A: Cryptography & Encoding Next Appendix C: Runtime API B.1. Preliminaries B.2. Storage B.2.1. ext_storage_set B.2.2. ext_storage_get B.2.3. ext_storage_read B.2.4. ext_storage_clear B.2.5. ext_storage_exists B.2.6. ext_storage_clear_prefix B.2.7. ext_storage_append B.2.8. ext_storage_root B.2.9. ext_storage_changes_root B.2.10. ext_storage_next_key B.2.11. ext_storage_start_transaction B.2.12. ext_storage_rollback_transaction B.2.13. ext_storage_commit_transaction B.3. Child Storage B.3.1. ext_default_child_storage_set B.3.2. ext_default_child_storage_get B.3.3. ext_default_child_storage_read B.3.4. ext_default_child_storage_clear B.3.5. ext_default_child_storage_storage_kill B.3.6. ext_default_child_storage_exists B.3.7. ext_default_child_storage_clear_prefix B.3.8. ext_default_child_storage_root B.3.9. ext_default_child_storage_next_key B.4. Crypto B.4.1. ext_crypto_ed25519_public_keys B.4.2. ext_crypto_ed25519_generate B.4.3. ext_crypto_ed25519_sign B.4.4. ext_crypto_ed25519_verify B.4.5. ext_crypto_ed25519_batch_verify B.4.6. ext_crypto_sr25519_public_keys B.4.7. ext_crypto_sr25519_generate B.4.8. ext_crypto_sr25519_sign B.4.9. ext_crypto_sr25519_verify B.4.10. ext_crypto_sr25519_batch_verify B.4.11. ext_crypto_ecdsa_public_keys B.4.12. ext_crypto_ecdsa_generate B.4.13. ext_crypto_ecdsa_sign B.4.14. ext_crypto_ecdsa_sign_prehashed B.4.15. ext_crypto_ecdsa_verify B.4.16. ext_crypto_ecdsa_verify_prehashed B.4.17. ext_crypto_ecdsa_batch_verify B.4.18. ext_crypto_secp256k1_ecdsa_recover B.4.19. ext_crypto_secp256k1_ecdsa_recover_compressed B.4.20. ext_crypto_start_batch_verify B.4.21. ext_crypto_finish_batch_verify B.5. Hashing B.5.1. ext_hashing_keccak_256 B.5.2. ext_hashing_keccak_512 B.5.3. ext_hashing_sha2_256 B.5.4. ext_hashing_blake2_128 B.5.5. ext_hashing_blake2_256 B.5.6. ext_hashing_twox_64 B.5.7. ext_hashing_twox_128 B.5.8. ext_hashing_twox_256 B.6. Offchain B.6.1. ext_offchain_is_validator B.6.2. ext_offchain_submit_transaction B.6.3. ext_offchain_network_state B.6.4. ext_offchain_timestamp B.6.5. ext_offchain_sleep_until B.6.6. ext_offchain_random_seed B.6.7. ext_offchain_local_storage_set B.6.8. ext_offchain_local_storage_clear B.6.9. ext_offchain_local_storage_compare_and_set B.6.10. ext_offchain_local_storage_get B.6.11. ext_offchain_http_request_start B.6.12. ext_offchain_http_request_add_header B.6.13. ext_offchain_http_request_write_body B.6.14. ext_offchain_http_response_wait B.6.15. ext_offchain_http_response_headers B.6.16. ext_offchain_http_response_read_body B.7. Offchain Index B.7.1. Offchain_index_set B.7.2. Offchain_index_clear B.8. Trie B.8.1. ext_trie_blake2_256_root B.8.2. ext_trie_blake2_256_ordered_root B.8.3. ext_trie_keccak_256_root B.8.4. ext_trie_keccak_256_ordered_root B.8.5. ext_trie_blake2_256_verify_proof B.8.6. ext_trie_keccak_256_verify_proof B.9. Miscellaneous B.9.1. ext_misc_print_num B.9.2. ext_misc_print_utf8 B.9.3. ext_misc_print_hex B.9.4. ext_misc_runtime_version B.10. Allocator B.10.1. ext_allocator_malloc B.10.2. ext_allocator_free B.11. Logging B.11.1. ext_logging_log B.11.2. ext_logging_max_level B.12. Abort Handler B.12.1. ext_panic_handler_abort_on_panic 
Appendix C: Runtime API | Polkadot Protocol Specification Appendix C: Runtime API On this page Appendix C: Runtime API Description of how to interact with the Runtime through its exported functions C.1. General Information ‚Äã The Polkadot Host assumes that at least the constants and functions described in this Chapter are implemented in the Runtime Wasm blob. It should be noted that the API can change through the Runtime updates. Therefore, a host should check the API versions of each module returned in the api field by Core_version ( Section C.4.1. ) after every Runtime upgrade and warn if an updated API is encountered and that this might require an update of the host. This section describes all Runtime API functions alongside their arguments and the return values. The functions are organized into modules, with each being versioned independently. C.1.1. JSON-RPC API for external services ‚Äã Polkadot Host implementers are encouraged to implement an API in order for external, third-party services to interact with the node. The JSON-RPC Interface for Polkadot Nodes (PSP6) is a Polkadot Standard Proposal for such an API and makes it easier to integrate the node with existing tools available in the Polkadot ecosystem, such as polkadot.js.org . The Runtime API has a few modules designed specifically for use in the official RPC API. C.2. Runtime Constants ‚Äã C.2.1. __heap_base ‚Äã This constant indicates the beginning of the heap in memory. The space below is reserved for the stack and the data section. For more details please refer to Section 2.6.3.1. . C.3. Runtime Call Convention ‚Äã Definition 208. Runtime API Call Convention ‚Äã The Runtime API Call Convention describes that all functions receive and return SCALE-encoded data and, as a result, have the following prototype signature: (func $generic_runtime_entry (param $ptr i32) (parm $len i32) (result i64)) where ptr points to the SCALE encoded tuple of the parameters passed to the function and len is the length of this data, while result is a pointer-size (Definition Definition 196 ) to the SCALE-encoded return data. See Section 2.6.3. for more information about the behavior of the Wasm Runtime. Also, note that any storage changes must be fork-aware ( Section 2.4.5. ). C.4. Module Core ‚Äã note This section describes Version 3 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. C.4.1. Core_version ‚Äã note For newer Runtimes, the version identifiers can be read directly from the Wasm blob in the form of custom sections ( Section 2.6.3.4. ). That method of retrieving this data should be preferred since it involves significantly less overhead. Returns the version identifiers of the Runtime. This function can be used by the Polkadot Host implementation when it seems appropriate, such as for the JSON-RPC API as described in Section C.1.1. . Arguments None Return A data structure of the following format: Table 7. Details of the version that the data type returns from the Runtime function. ‚Äã Name Type Description spec_name String Runtime identifier impl_name String Name of the implementation (e.g. C++) authoring_version Unsigned 32-bit integer Version of the authorship interface spec_version Unsigned 32-bit integer Version of the Runtime specification impl_version Unsigned 32-bit integer Version of the Runtime implementation apis ApiVersions ( Definition 209 ) List of supported APIs along with their version transaction_version Unsigned 32-bit integer Version of the transaction format state_version Unsigned 8-bit integer Version of the trie format Definition 209. ApiVersions ‚Äã ApiVersions is a specialized type for the ( Section C.4.1. ) function entry. It represents an array of tuples, where the first value of the tuple is an array of 8-bytes containing the Blake2b hash of the API name. The second value of the tuple is the version number of the corresponding API. A p i V e r s i o n s : = ( T 0 , ‚Ä¶ , T n ) T : = ( ( b 0 , ‚Ä¶ , b 7 ) , U I N T 32 ) \begin{aligned} \mathrm{ApiVersions} :=& (T_0, \ldots, T_n) \\ T :=& ((b_0, \ldots, b_7), \mathrm{UINT32}) \end{aligned} ApiVersions := T := ‚Äã ( T 0 ‚Äã , ‚Ä¶ , T n ‚Äã ) (( b 0 ‚Äã , ‚Ä¶ , b 7 ‚Äã ) , UINT32 ) ‚Äã Requires Core_initialize_block to be called beforehand. C.4.2. Core_execute_block ‚Äã This function executes a full block and all its extrinsics and updates the state accordingly. Additionally, some integrity checks are executed, such as validating if the parent hash is correct and that the transaction root represents the transactions. Internally, this function performs an operation similar to the process described in Build-Block , by calling Core_initialize_block , BlockBuilder_apply_extrinsics and BlockBuilder_finalize_block . This function should be called when a fully complete block is available that is not actively being built on, such as blocks received from other peers. State changes resulting from calling this function are usually meant to persist when the block is imported successfully. Additionally, the seal digest in the block header, as described in Definition 11 , must be removed by the Polkadot host before submitting the block. Arguments A block represented as a tuple consisting of a block header, as described in Definition 10 , and the block body, as described in Definition 13 . Return None. C.4.3. Core_initialize_block ‚Äã Sets up the environment required for building a new block as described in Build-Block . Arguments The header of the new block as defined in Definition 10 . The values H r {H}_{{r}} H r ‚Äã , H e {H}_{{e}} H e ‚Äã and H d {H}_{{d}} H d ‚Äã are left empty. Return None. C.5. Module Metadata ‚Äã note This section describes Version 1 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. C.5.1. Metadata_metadata ‚Äã Returns native Runtime metadata in an opaque form. This function can be used by the Polkadot Host implementation when it seems appropriate, such as for the JSON-RPC API as described in Section C.1.1. , and returns all the information necessary to build valid transactions. Arguments None. Return The scale-encoded ( Section A.2.2. ) runtime metadata as described in Chapter 12 . C.5.2. Metadata_metadata_at_version ‚Äã Returns native Runtime metadata in an opaque form at a particular version. Arguments Metadata version represented by an unsigned 32-bit integer. Return The scale-encoded ( Section A.2.2. ) runtime metadata as described in Chapter 12 at the particular version. C.5.3. Metadata_metadata_versions ‚Äã Returns supported metadata versions. Arguments None. Return A vector of supported metadata versions of type vec<u32> . C.6. Module BlockBuilder ‚Äã note This section describes Version 4 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. All calls in this module require Core_initialize_block ( Section C.4.3. ) to be called beforehand. C.6.1. BlockBuilder_apply_extrinsic ‚Äã Apply the extrinsic outside of the block execution function. This does not attempt to validate anything regarding the block, but it builds a list of transaction hashes. Arguments A byte array of varying sizes containing the opaque extrinsic. Return Returns the varying datatype ApplyExtrinsicResult as defined in Definition 210 . This structure lets the block builder know whether an extrinsic should be included in the block or rejected. Definition 210. ApplyExtrinsicResult ‚Äã ApplyExtrinsicResult is a varying data type as defined in Definition 181 . This structure can contain multiple nested structures, indicating either module dispatch outcomes or transaction invalidity errors. Table 8. Possible values of varying data type ApplyExtrinsicResult . ‚Äã Id Description Type 0 Outcome of dispatching the extrinsic. DispatchOutcome ( Definition 211 ) 1 Possible errors while checking the validity of a transaction. TransactionValidityError ( Definition 214 ) info As long as a DispatchOutcome ( Definition 211 ) is returned, the extrinsic is always included in the block, even if the outcome is a dispatch error. Dispatch errors do not invalidate the block and all state changes are persisted. Definition 211. DispatchOutcome ‚Äã DispatchOutcome is the varying data type as defined in Definition 181 . Table 9. Possible values of varying data type DispatchOutcome . ‚Äã Id Description Type 0 Extrinsic is valid and was submitted successfully. None 1 Possible errors while dispatching the extrinsic. DispatchError ( Definition 212 ) Definition 212. DispatchError ‚Äã DispatchError is a varying data type as defined in Definition 178 . Indicates various reasons why a dispatch call failed. Table 10. Possible values of varying data type DispatchError . ‚Äã Id Description Type 0 Some unknown error occurred. SCALE encoded byte array containing a valid UTF-8 sequence. 1 Failed to look up some data. None 2 A bad origin. None 3 A custom error in a module. CustomModuleError ( Definition 213 ) Definition 213. CustomModuleError ‚Äã CustomModuleError is a tuple appended after a possible error in as defined in Definition 212 . Table 11. Possible values of varying data type CustomModuleError . ‚Äã Name Description Type Index Module index matching the metadata module index. Unsigned 8-bit integer. Error Module-specific error value. Unsigned 8-bit integer Message Optional error message. Varying data type Option ( Definition 180 ). The optional value is a SCALE-encoded byte array containing a valid UTF-8 sequence. info Whenever TransactionValidityError ( Definition 214 ) is returned, the contained error type will indicate whether an extrinsic should be outright rejected or requested for a later block. This behavior is clarified further in Definition 215 and respectively Definition 216 . Definition 214. TransactionValidityError ‚Äã TransactionValidityError is a varying data type as defined in Definition 178 . It indicates possible errors that can occur while checking the validity of a transaction. Table 12. Possible values of varying data type TransactionValidityError . ‚Äã Id Description Type 0 Transaction is invalid. InvalidTransaction ( Definition 215 ) 1 Transaction validity can‚Äôt be determined. UnknownTransaction ( Definition 216 ) Definition 215. InvalidTransaction ‚Äã InvalidTransaction is a varying data type as defined in Definition 178 and specifies the invalidity of the transaction in more detail. Table 13. Possible values of varying data type InvalidTransaction . ‚Äã Id Description Type Reject 0 Call of the transaction is not expected. None Yes 1 General error to do with the inability to pay some fees (e.g., account balance too low). None Yes 2 General error to do with the transaction not yet being valid (e.g., nonce too high). None No 3 General error to do with the transaction being outdated (e.g., nonce too low). None Yes 4 General error to do with the transactions‚Äô proof (e.g., signature) None Yes 5 The transaction birth block is ancient. None Yes 6 The transaction would exhaust the resources of the current block. None No 7 Some unknown error occurred. Unsigned 8-bit integer Yes 8 An extrinsic with mandatory dispatch resulted in an error. None Yes 9 A transaction with a mandatory dispatch (only inherents are allowed to have mandatory dispatch). None Yes Definition 216. UnknownTransaction ‚Äã UnknownTransaction is a varying data type as defined in Definition 178 and specifies the unknown invalidity of the transaction in more detail. Table 14. Possible values of varying data type UnknownTransaction . ‚Äã Id Description Type Reject 0 Could not look up some information that is required to validate the transaction. None Yes 1 No validator found for the given unsigned transaction. None Yes 2 Any other custom unknown validity that is not covered by this type. Unsigned 8-bit integer Yes C.6.2. BlockBuilder_finalize_block ‚Äã Finalize the block - it is up to the caller to ensure that all header fields are valid except for the state root. State changes resulting from calling this function are usually meant to persist upon successful execution of the function and appending of the block to the chain. Arguments None. Return The header of the new block as defined in Definition 10 . C.6.3. BlockBuilder_inherent_extrinisics : ‚Äã Generates the inherent extrinsics, which are explained in more detail in Section 2.3.3. . This function takes a SCALE-encoded hash table as defined in Definition 182 and returns an array of extrinsics. The Polkadot Host must submit each of those to the BlockBuilder_apply_extrinsic , described in Section C.6.1. . This procedure is outlined in Build-Block . Arguments A Inherents-Data structure as defined in Definition 15 . Return A byte array of varying sizes containing extrinisics. Each extrinsic is a byte array of varying size. C.6.4. BlockBuilder_check_inherents ‚Äã Checks whether the provided inherent is valid. This function can be used by the Polkadot Host when deemed appropriate, e.g., during the block-building process. Arguments A block represented as a tuple consisting of a block header as described in Definition 10 and the block body as described in Definition 13 . A Inherents-Data structure as defined in Definition 15 . Return A data structure of the following format: ( o , f e , e ) {\left({o},{{f}_{{e}},}{e}\right)} ( o , f e ‚Äã , e ) where o {o} o is a boolean indicating whether the check was successful. f e {f_e} f e ‚Äã is a boolean indicating whether a fatal error was encountered. e {e} e is a Inherents-Data structure as defined in Definition 15 containing any errors created by this Runtime function. C.7. Module TaggedTransactionQueue ‚Äã note This section describes Version 2 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. All calls in this module require Core_initialize_block ( Section C.4.3. ) to be called beforehand. C.7.1. TaggedTransactionQueue_validate_transaction ‚Äã This entry is invoked against extrinsics submitted through a transaction network message ( Section 4.8.5. ) or by an off-chain worker through the Host API ( Section B.6.2. ). It indicates if the submitted blob represents a valid extrinsics, the order in which it should be applied and if it should be gossiped to other peers. Furthermore, this function gets called internally when executing blocks with the runtime function as described in Section C.4.2. . Arguments The source of the transaction as defined in Definition 217 . A byte array that contains the transaction. The hash of the parent of the block that the transaction is included in. Definition 217. TransactionSource ‚Äã TransactionSource is an enum describing the source of a transaction and can have one of the following values: Table 15. The TransactionSource enum ‚Äã Id Name Description 0 InBlock Transaction is already included in a block. 1 Local Transaction is coming from a local source, e.g. off-chain worker. 2 External Transaction has been received externally, e.g. over the network. Return This function returns a Result as defined in Definition 181 which contains the type ValidTransaction as defined in Definition 218 on success and the type TransactionValidityError as defined in Definition 214 on failure. Definition 218. ValidTransaction ‚Äã ValidTransaction is a tuple that contains information concerning a valid transaction. Table 16. The tuple provided by in the case the transaction is judged to be valid. ‚Äã Name Description Type Priority Determines the ordering of two transactions that have all their dependencies (required tags) are satisfied. Unsigned 64-bit integer Requires List of tags specifying extrinsics which should be applied before the current extrinsics can be applied. Array containing inner arrays Provides Informs Runtime of the extrinsics depending on the tags in the list that can be applied after current extrinsics are being applied. Describes the minimum number of blocks for the validity to be correct. Array containing inner arrays Longevity After this period, the transaction should be removed from the pool or revalidated. Unsigned 64-bit integer Propagate A flag indicating if the transaction should be gossiped to other peers. Boolean ::: info If Propagate is set to false the transaction will still be considered for inclusion in blocks that are authored on the current node, but should not be gossiped to other peers. info If this function gets called by the Polkadot Host in order to validate a transaction received from peers, the Polkadot Host disregards and rewinds state changes resulting in such a call. C.8. Module OffchainWorkerApi ‚Äã note This section describes Version 2 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. Does not require Core_initialize_block ( Section C.4.3. ) to be called beforehand. C.8.1. OffchainWorkerApi_offchain_worker ‚Äã Starts an off-chain worker and generates extrinsics. [ To do: when is this called? ] Arguments The block header as defined in Definition 10 . Return None. C.9. Module ParachainHost ‚Äã note This section describes Version 1 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. C.9.1. ParachainHost_validators ‚Äã Returns the validator set at the current state. The specified validators are responsible for backing parachains for the current state. Arguments None. Return An array of public keys representing the validators. C.9.2. ParachainHost_validator_groups ‚Äã Returns the validator groups ( Definition 126 ) used during the current session. The validators in the groups are referred to by the validator set Id ( Definition 69 ). Arguments None Return An array of tuples, T {T} T , of the following format: T = ( I , G ) {T}={\left({I},{G}\right)} T = ( I , G ) I = ( v n , ‚Ä¶ v m ) {I}={\left({v}_{{n}},‚Ä¶{v}_{{m}}\right)} I = ( v n ‚Äã , ‚Ä¶ v m ‚Äã ) G = ( B s , f , B c ) {G}={\left({B}_{{s}},{f},{B}_{{c}}\right)} G = ( B s ‚Äã , f , B c ‚Äã ) where I {I} I is an array of the validator set Ids ( Definition 69 ). B s {B}_{{s}} B s ‚Äã indicates the block number where the session started. f {f} f indicates how often groups rotate. 0 means never. B c {B}_{{c}} B c ‚Äã indicates the current block number. C.9.3. ParachainHost_availability_cores ‚Äã Returns information on all availability cores ( Definition 125 ). Arguments None Return An array of core states, S, of the following format: S = { 0 ‚Üí C o 1 ‚Üí C s 2 ‚Üí œï {S}={\left\lbrace\begin{matrix}{0}&\rightarrow&{C}_{{o}}\\{1}&\rightarrow&{C}_{{s}}\\{2}&\rightarrow&\phi\end{matrix}\right.} S = ‚é© ‚é® ‚éß ‚Äã 0 1 2 ‚Äã ‚Üí ‚Üí ‚Üí ‚Äã C o ‚Äã C s ‚Äã œï ‚Äã C o = ( n u , B o , B t , n t , b , G i , C h , C d ) {C}_{{o}}={\left({n}_{{u}},{B}_{{o}},{B}_{{t}},{n}_{{t}},{b},{G}_{{i}},{C}_{{h}},{C}_{{d}}\right)} C o ‚Äã = ( n u ‚Äã , B o ‚Äã , B t ‚Äã , n t ‚Äã , b , G i ‚Äã , C h ‚Äã , C d ‚Äã ) C s = ( P i d , C i ) {C}_{{s}}={\left({P}_{{i}}{d},{C}_{{i}}\right)} C s ‚Äã = ( P i ‚Äã d , C i ‚Äã ) where S {S} S specifies the core state. 0 indicates that the core is occupied, 1 implies it‚Äôs currently free but scheduled and given the opportunity to occupy and 2 implies it‚Äôs free and there‚Äôs nothing scheduled. n u {n}_{{u}} n u ‚Äã is an Option value ( Definition 180 ) which can contain a C s {C}_{{s}} C s ‚Äã value if the core was freed by the Runtime and indicates the assignment that is next scheduled on this core. An empty value indicates there is nothing scheduled. B o {B}_{{o}} B o ‚Äã indicates the relay chain block number at which the core got occupied. B t {B}_{{t}} B t ‚Äã indicates the relay chain block number the core will time-out at, if any. n t {n}_{{t}} n t ‚Äã is an Option value ( Definition 180 ) which can contain a C s {C}_{{s}} C s ‚Äã value if the core is freed by a time-out and indicates the assignment that is next scheduled on this core. An empty value indicates there is nothing scheduled. b {b} b is a bitfield array ( Definition 131 ). A > 2 3 >\frac{{2}}{{3}} > 3 2 ‚Äã majority of assigned validators voting with 1 {1} 1 values means that the core is available. G i {G}_{{i}} G i ‚Äã indicates the assigned validator group index ( Definition 126 ) is to distribute availability pieces of this candidate. C h {C}_{{h}} C h ‚Äã indicates the hash of the candidate occupying the core. C d {C}_{{d}} C d ‚Äã is the candidate descriptor ( Definition 96 ). C i {C}_{{i}} C i ‚Äã is an Option value ( Definition 180 ) which can contain the collators public key indicating who should author the block. C.9.4. ParachainHost_persisted_validation_data ‚Äã Returns the persisted validation data for the given parachain Id and a given occupied core assumption. Arguments The parachain Id ( Definition 124 ). An occupied core assumption ( Definition 219 ). Return An Option value ( Definition 180 ) which can contain the persisted validation data ( Definition 220 ). The value is empty if the parachain Id is not registered or the core assumption is of index 2 {2} 2 , meaning that the core was freed. Definition 219. Occupied Core Assumption ‚Äã An occupied core assumption is used for fetching certain pieces of information about a parachain by using the relay chain API. The assumption indicates how the Runtime API should compute the result. The assumptions, A, is a varying datatype of the following format: A = { 0 ‚Üí œï 1 ‚Üí œï 2 ‚Üí œï {A}={\left\lbrace\begin{matrix}{0}&\rightarrow&\phi\\{1}&\rightarrow&\phi\\{2}&\rightarrow&\phi\end{matrix}\right.} A = ‚é© ‚é® ‚éß ‚Äã 0 1 2 ‚Äã ‚Üí ‚Üí ‚Üí ‚Äã œï œï œï ‚Äã where 0 indicates that the candidate occupying the core was made available and included to free the core, 1 indicates that it timed-out and freed the core without advancing the parachain and 2 indicates that the core was not occupied to begin with. Definition 220. Persisted Validation Data ‚Äã The persisted validation data provides information about how to create the inputs for the validation of a candidate by calling the Runtime. This information is derived from the parachain state and will vary from parachain to parachain, although some of the fields may be the same for every parachain. This validation data acts as a way to authorize the additional data (such as messages) the collator needs to pass to the validation function. The persisted validation data, D p v {D}_{{{p}{v}}} D p v ‚Äã , is a datastructure of the following format: D p v = ( P h , H i , H r , m b ) {D}_{{{p}{v}}}={\left({P}_{{h}},{H}_{{i}},{H}_{{r}},{m}_{{b}}\right)} D p v ‚Äã = ( P h ‚Äã , H i ‚Äã , H r ‚Äã , m b ‚Äã ) where P h {P}_{{h}} P h ‚Äã is the parent head data ( Definition 123 ). H i {H}_{{i}} H i ‚Äã is the relay chain block number this is in the context of. H r {H}_{{r}} H r ‚Äã is the relay chain storage root this is in the context of. m b {m}_{{b}} m b ‚Äã is the maximum legal size of the PoV block, in bytes. The persisted validation data is fetched via the Runtime API ( Section C.9.4. ). C.9.5. ParachainHost_assumed_validation_data ‚Äã Returns the persisted validation data for the given parachain Id along with the corresponding Validation Code Hash. Instead of accepting validation about para, matches the validation data hash against an expected one and yields None if they are unequal. Arguments The Parachain Id ( Definition 124 ). Expected Persistent Validation Data Hash ( Definition 220 ). Return An Option value ( Definition 180 ) which can contain the pair of persisted validation data ( Definition 220 ) and Validation Code Hash. The value is None if the parachain Id is not registered or the validation data hash does not match the expected one. C.9.6. ParachainHost_check_validation_outputs ‚Äã Checks if the given validation outputs pass the acceptance criteria. Arguments The parachain Id ( Definition 124 ). The candidate commitments ( Definition 97 ). Return A boolean indicating whether the candidate commitments pass the acceptance criteria. C.9.7. ParachainHost_session_index_for_child ‚Äã Returns the session index that is expected at the child of a block. caution TODO clarify session index Arguments None Return A unsigned 32-bit integer representing the session index. C.9.8. ParachainHost_validation_code ‚Äã Fetches the validation code (Runtime) of a parachain by parachain Id. Arguments The parachain Id ( Definition 124 ). The occupied core assumption ( Definition 219 ). Return An Option value ( Definition 180 ) containing the full validation code in a byte array. This value is empty if the parachain Id cannot be found or the assumption is wrong. C.9.9. ParachainHost_validation_code_by_hash ‚Äã Returns the validation code (Runtime) of a parachain by its hash. Arguments The hash value of the validation code. Return An Option value ( Definition 180 ) containing the full validation code in a byte array. This value is empty if the parachain Id cannot be found or the assumption is wrong. C.9.10. ParachainHost_validation_code_hash ‚Äã Returns the validation code hash of a parachain. Arguments The parachain Id ( Definition 124 ). An occupied core assumption ( Definition 219 ). Return An Option value ( Definition 180 ) containing the hash value of the validation code. This value is empty if the parachain Id cannot be found or the assumption is wrong. C.9.11. ParachainHost_candidate_pending_availability ‚Äã Returns the receipt of a candidate pending availability for any parachain assigned to an occupied availability core. Arguments The parachain Id ( Definition 124 ). Return An Option value ( Definition 180 ) containing the committed candidate receipt ( Definition 94 ). This value is empty if the given parachain Id is not assigned to an occupied availability core. C.9.12. ParachainHost_candidate_events ‚Äã Returns an array of candidate events that occurred within the latest state. Arguments None Return An array of single candidate events, E, of the following format: E = { 0 ‚Üí d 1 ‚Üí d 2 ‚Üí ( C r , h , I c ) {E}={\left\lbrace\begin{matrix}{0}&\rightarrow&{d}\\{1}&\rightarrow&{d}\\{2}&\rightarrow&{\left({C}_{{r}},{h},{I}_{{c}}\right)}\end{matrix}\right.} E = ‚é© ‚é® ‚éß ‚Äã 0 1 2 ‚Äã ‚Üí ‚Üí ‚Üí ‚Äã d d ( C r ‚Äã , h , I c ‚Äã ) ‚Äã d = ( C r , h , I c , G i ) {d}={\left({C}_{{r}},{h},{I}_{{c}},{G}_{{i}}\right)} d = ( C r ‚Äã , h , I c ‚Äã , G i ‚Äã ) where E {E} E specifies the event type of the candidate. 0 indicates that the candidate receipt was backed in the latest relay chain block, 1 indicates that it was included and became a parachain block at the latest relay chain block and 2 indicates that the candidate receipt was not made available and timed out. C r {C}_{{r}} C r ‚Äã is the candidate receipt ( Definition 94 ). h {h} h is the parachain head data ( Definition 123 ). I c {I}_{{c}} I c ‚Äã is the index of the availability core as can be retrieved in Section C.9.3. that the candidate is occupying. If E {E} E is of variant 2 {2} 2 , then this indicates the core index the candidate was occupying. G i {G}_{{i}} G i ‚Äã is the group index ( Definition 126 ) that is responsible of backing the candidate. C.9.13. ParachainHost_session_info ‚Äã Get the session info of the given session, if available. Arguments The unsigned 32-bit integer indicating the session index. Return An Option type ( Definition 180 ) which can contain the session info structure, S {S} S , of the following format: S = ( A , D , K , G , c , z , s , d , x , a ) {S}={\left({A},{D},{K},{G},{c},{z},{s},{d},{x},{a}\right)} S = ( A , D , K , G , c , z , s , d , x , a ) A = ( v n , ‚Ä¶ v m ) {A}={\left({v}_{{n}},‚Ä¶{v}_{{m}}\right)} A = ( v n ‚Äã , ‚Ä¶ v m ‚Äã ) D = ( v n , ‚Ä¶ v m ) {D}={\left({v}_{{_{n}}},‚Ä¶{v}_{{m}}\right)} D = ( v n ‚Äã ‚Äã , ‚Ä¶ v m ‚Äã ) K = ( v n , ‚Ä¶ v m ) {K}={\left({v}_{{n}},‚Ä¶{v}_{{m}}\right)} K = ( v n ‚Äã , ‚Ä¶ v m ‚Äã ) G = ( g n , ‚Ä¶ g m ) {G}={\left({{g}_{{n}},}‚Ä¶{g}_{{m}}\right)} G = ( g n ‚Äã , ‚Ä¶ g m ‚Äã ) g = ( A n , ‚Ä¶ A m ) {g}={\left({A}_{{n}},‚Ä¶{A}_{{m}}\right)} g = ( A n ‚Äã , ‚Ä¶ A m ‚Äã ) where A {A} A indicates the validators of the current session in canonical order. There might be more validators in the current session than validators participating in parachain consensus, as returned by the Runtime API ( Section C.9.1. ). D {D} D indicates the validator authority discovery keys for the given session in canonical order. The first couple of validators are equal to the corresponding validators participating in the parachain consensus, as returned by the Runtime API ( Section C.9.1. ). The remaining authorities are not participating in the parachain consensus. K {K} K indicates the assignment keys for validators. There might be more authorities in the session that validators participating in parachain consensus, as returned by the Runtime API ( Section C.9.1. ). G {G} G indicates the validator groups in shuffled order. v n {v}_{{n}} v n ‚Äã is public key of the authority. A n {A}_{{n}} A n ‚Äã is the authority set Id ( Definition 69 ). c {c} c is an unsigned 32-bit integer indicating the number of availability cores used by the protocol during the given session. z {z} z is an unsigned 32-bit integer indicating the zeroth delay tranche width. s {s} s is an unsigned 32-bit integer indicating the number of samples an assigned validator should do for approval voting. d {d} d is an unsigned 32-bit integer indicating the number of delay tranches in total. x {x} x is an unsigned 32-bit integer indicating how many BABE slots must pass before an assignment is considered a ‚Äúno-show‚Äù. a {a} a is an unsigned 32-bit integer indicating the number of validators needed to approve a block. C.9.14. ParachainHost_dmq_contents ‚Äã Returns all the pending inbound messages in the downward message queue for a given parachain. Arguments The parachain Id ( Definition 124 ). Return An array of inbound downward messages ( Definition 128 ). C.9.15. ParachainHost_inbound_hrmp_channels_contents ‚Äã Returns the contents of all channels addressed to the given recipient. Channels that have no messages in them are also included. Arguments The parachain Id ( Definition 124 ). Return An array of inbound HRMP messages ( Definition 130 ). C.9.16. ParachainHost_on_chain_votes ‚Äã Returns disputes relevant from on-chain, backing votes, and resolved disputes. Arguments None Return An Option ( Definition 180 ) type which can contain the scraped on-chain votes data ( Definition 221 ). Definition 221. Scraped On Chain Vote ‚Äã Contains the scraped runtime backing votes and resolved disputes. The scraped on-chain votes data, S O C V SOCV SOC V , is a data structure of the following format: S O C V = ( S i , B V , d ) B V = [ C r , [ ( i , a ) ] ] SOCV = (S_i,BV,d) \\ BV = [C_r, [(i,a)]] SOC V = ( S i ‚Äã , B V , d ) B V = [ C r ‚Äã , [( i , a )]] where : S i S_i S i ‚Äã is the u32 integer representing the session index in which the block was introduced. B V BV B V is the set of backing validators for each candidate, represented by its candidate receipt ( Definition 94 ). Each candidate C r C_r C r ‚Äã has a list of ( i , a ) (i,a) ( i , a ) , the pair of validator index and validation attestations ( Definition 93 ). d d d is a set of dispute statements ( Section 8.7.2.1. ). Note that the above B V BV B V is unrelated to the backers of the dispute candidates. caution PVF Pre-Checker subsystem is still Work-in-Progress, hence the below APIs are subject to change. C.9.17. ParachainHost_pvfs_require_precheck ‚Äã This runtime API fetches all PVFs that require pre-checking voting. The PVFs are identified by their code hashes. As soon as the PVF gains the required support, the runtime API will not return the PVF anymore. Arguments None Return A list of validation code hashes that require prechecking of votes by validators in the active set. C.9.18. ParachainHost_submit_pvf_check_statement ‚Äã This runtime API submits the judgment for a PVF, whether it is approved or not. The voting process uses unsigned transactions. The check is circulated through the network via gossip, similar to a normal transaction. At some point, the validator will include the statement in the block, where it will be processed by the runtime. If that was the last vote before gaining the super-majority, this PVF would not be returned by pvfs_require_precheck ( Section C.9.17. ) anymore. Arguments A PVF pre checking statement ( Definition 222 ) to be submitted into the transaction pool. Validator Signature ( Definition 93 ). Return None Definition 222. PVF Check Statement ‚Äã This is a statement by the validator who ran the pre-checking process for a PVF. A PVF is identified by the ValidationCodeHash . The statement is valid only during a single session, specified in the session_index . The PVF Check Statement S p v f S_{pvf} S p v f ‚Äã , is a datastructure of the following format: S p v f = ( b , V C H , S i , V i ) S_{pvf} = (b,VC_H,S_i,V_i) S p v f ‚Äã = ( b , V C H ‚Äã , S i ‚Äã , V i ‚Äã ) where : b b b is a boolean denoting if the subject passed pre-checking. V C H VC_H V C H ‚Äã is the validation code hash. S i S_i S i ‚Äã is a u32 integer representing the session index. V i V_i V i ‚Äã is the validator index ( Definition 93 ). C.9.19. ParachainHost_disputes ‚Äã This runtime API fetches all on-chain disputes. Arguments None Return A list of (SessionIndex, CandidateHash, DisputeState). caution TODO clarify DisputeState C.9.20. ParachainHost_executor_params ‚Äã This runtime API returns execution parameters for the session. Arguments Session Index caution TODO clarify session index Return Option type of Executor Parameters. caution TODO clarify Executor Parameters C.10. Module GrandpaApi ‚Äã note This section describes Version 2 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. All calls in this module require Core_initialize_block ( Section C.4.3. ) to be called beforehand. C.10.1. GrandpaApi_grandpa_authorities ‚Äã This entry fetches the list of GRANDPA authorities according to the genesis block and is used to initialize an authority list at genesis, defined in Definition 33 . Any future authority changes get tracked via Runtime-to-consensus engine messages, as described in Section 3.3.2. . Arguments None. Return An authority list as defined in Definition 33 . C.10.2. GrandpaApi_current_set_id ‚Äã This entry fetches the list of GRANDPA authority set IDs ( Definition 69 ). Any future authority changes get tracked via Runtime-to-consensus engine messages, as described in Section 3.3.2. . Arguments None. Return An authority set ID as defined in Definition 69 . C.10.3. GrandpaApi_submit_report_equivocation_unsigned_extrinsic ‚Äã A GRANDPA equivocation occurs when a validator votes for multiple blocks during one voting subround, as described further in Definition 76 . The Polkadot Host is expected to identify equivocators and report those to the Runtime by calling this function. Arguments The equivocation proof of the following format: G E p = ( i d V , e , r , A i d , B h 1 , B n 1 , A s i g 1 , B h 2 , B n 2 , A s i g 2 ) e = { 0 Equivocation at prevote stage 1 Equivocation at precommit stage \begin{aligned} G_{\mathrm{Ep}} =& (\mathrm{id}_{\mathbb{V}}, e, r, A_{\mathrm{id}}, B^1_h, B^1_n, A^1_{\mathrm{sig}}, B^2_h, B^2_n, A^2_{\mathrm{sig}}) \\ e =& \begin{cases} 0 & \quad \textrm{Equivocation at prevote stage} \\ 1 & \quad \textrm{Equivocation at precommit stage} \end{cases} \end{aligned} G Ep ‚Äã = e = ‚Äã ( id V ‚Äã , e , r , A id ‚Äã , B h 1 ‚Äã , B n 1 ‚Äã , A sig 1 ‚Äã , B h 2 ‚Äã , B n 2 ‚Äã , A sig 2 ‚Äã ) { 0 1 ‚Äã Equivocation at prevote stage Equivocation at precommit stage ‚Äã ‚Äã where m a t h r m { i d } V {m}{a}{t}{h}{r}{m}{\left\lbrace{i}{d}\right\rbrace}_{{{\mathbb{{{V}}}}}} m a t h r m { i d } V ‚Äã is the authority set id as defined in Definition 69 . e {e} e indicates the stage at which the equivocation occurred. r {r} r is the round number the equivocation occurred. A m a t h r m { i d } {A}_{{{m}{a}{t}{h}{r}{m}{\left\lbrace{i}{d}\right\rbrace}}} A m a t h r m { i d } ‚Äã is the public key of the equivocator. B h 1 {B}^{{1}}_{h} B h 1 ‚Äã is the block hash of the first block the equivocator voted for. B n 1 {B}^{{1}}_{n} B n 1 ‚Äã is the block number of the first block the equivocator voted for. A { m a t h r m { s i g } } 1 {A}^{{1}}_{\left\lbrace{m}{a}{t}{h}{r}{m}{\left\lbrace{s}{i}{g}\right\rbrace}\right\rbrace} A { m a t h r m { s i g } } 1 ‚Äã is the equivocators signature of the first vote. B h 2 {B}^{{2}}_{h} B h 2 ‚Äã is the block hash of the second block the equivocator voted for. B n 2 {B}^{{2}}_{n} B n 2 ‚Äã is the block number of the second block the equivocator voted for. A { m a t h r m { s i g } } 2 {A}^{{2}}_{\left\lbrace{m}{a}{t}{h}{r}{m}{\left\lbrace{s}{i}{g}\right\rbrace}\right\rbrace} A { m a t h r m { s i g } } 2 ‚Äã is the equivocators signature of the second vote. A proof of the key owner in an opaque form as described in Section C.10.4. . Return A SCALE encoded Option as defined in Definition 180 containing an empty value on success. C.10.4. GrandpaApi_generate_key_ownership_proof ‚Äã Generates proof of the membership of a key owner in the specified block state. The returned value is used to report equivocations as described in Section C.10.3. . Arguments The authority set id as defined in Definition 69 . The 256-bit public key of the authority. Return A SCALE encoded Option as defined in Definition 180 containing the proof in an opaque form. C.11. Module BabeApi ‚Äã note This section describes Version 2 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. All calls in this module require Core_initialized_block ( Section C.4.3. ) to be called beforehand. C.11.1. BabeApi_configuration ‚Äã This entry is called to obtain the current configuration of the BABE consensus protocol. Arguments None. Return A tuple containing configuration data used by the Babe consensus engine. Table 17. The tuple provided by BabeApi_configuration . ‚Äã Name Description Type SlotDuration The slot duration in milliseconds. Currently, only the value provided by this type at genesis will be used. Dynamic slot duration may be supported in the future. Unsigned 64bit integer EpochLength The duration of epochs in slots. Unsigned 64bit integer Constant A constant value that is used in the threshold calculation formula as defined in Definition 55 . Tuple containing two unsigned 64bit integers GenesisAuthorities The authority list for the genesis epoch as defined in Definition 33 . Array of tuples containing a 256-bit byte array and an unsigned 64bit integer Randomness The randomness for the genesis epoch 32-byte array SecondarySlot Whether this chain should run with a round-robin-style secondary slot and if this secondary slot requires the inclusion of an auxiliary VRF output ( Section 5.2. ). A one-byte enum as defined in Definition 54 as 2 nd {2}_{{\text{nd}}} 2 nd ‚Äã . C.11.2. BabeApi_current_epoch_start ‚Äã Finds the start slot of the current epoch. Arguments None. Return A unsigned 64-bit integer indicating the slot number. C.11.3. BabeApi_current_epoch ‚Äã Produces information about the current epoch. Arguments None. Return A data structure of the following format: ( e i , s s , d , A , r ) {\left({e}_{{i}},{s}_{{s}},{d},{A},{r}\right)} ( e i ‚Äã , s s ‚Äã , d , A , r ) where e i {e}_{{i}} e i ‚Äã is a unsigned 64-bit integer representing the epoch index. s s {s}_{{s}} s s ‚Äã is an unsigned 64-bit integer representing the starting slot of the epoch. d {d} d is an unsigned 64-bit integer representing the duration of the epoch. A {A} A is an authority list as defined in Definition 33 . r {r} r is a 256-bit array containing the randomness for the epoch as defined in Definition 67 . C.11.4. BabeApi_next_epoch ‚Äã Produces information about the next epoch. Arguments None. Return Returns the same data structure as described in Section C.11.3. . C.11.5. BabeApi_generate_key_ownership_proof ‚Äã Generates proof of the membership of a key owner in the specified block state. The returned value is used to report equivocations as described in Section C.11.6. . Arguments The unsigned 64-bit integer indicating the slot number. The 256-bit public key of the authority. Return A SCALE encoded Option as defined in Definition Definition 180 containing the proof in an opaque form. C.11.6. BabeApi_submit_report_equivocation_unsigned_extrinsic ‚Äã A BABE equivocation occurs when a validator produces more than one block at the same slot. The proof of equivocation are the given distinct headers that were signed by the validator and which include the slot number. The Polkadot Host is expected to identify equivocators and report those to the Runtime using this function. info If there are more than two blocks that cause an equivocation, the equivocation only needs to be reported once i.e. no additional equivocations must be reported for the same slot. Arguments The equivocation proof of the following format: B m a t h r m { E p } = ( A m a t h r m { i d } , s , h 1 , h 2 ) {B}_{{{m}{a}{t}{h}{r}{m}{\left\lbrace{E}{p}\right\rbrace}}}={\left({A}_{{{m}{a}{t}{h}{r}{m}{\left\lbrace{i}{d}\right\rbrace}}},{s},{h}_{{1}},{h}_{{2}}\right)} B m a t h r m { E p } ‚Äã = ( A m a t h r m { i d } ‚Äã , s , h 1 ‚Äã , h 2 ‚Äã ) where A m a t h r m { i d } {A}_{{{m}{a}{t}{h}{r}{m}{\left\lbrace{i}{d}\right\rbrace}}} A m a t h r m { i d } ‚Äã is the public key of the equivocator. s {s} s is the slot as described in Definition 50 at which the equivocation occurred. h 1 {h}_{{1}} h 1 ‚Äã is the block header of the first block produced by the equivocator. h 2 {h}_{{2}} h 2 ‚Äã is the block header of the second block produced by the equivocator. Unlike during block execution, the Seal in both block headers is not removed before submission. The block headers are submitted in its full form. An proof of the key owner in an opaque form as described in Section C.11.5. . Return A SCALE encoded Option as defined in Definition 180 containing an empty value on success. C.12. Module AuthorityDiscoveryApi ‚Äã note This section describes Version 1 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. All calls in this module require (Section Section C.4.3. ) to be called beforehand. C.12.1. AuthorityDiscoveryApi_authorities ‚Äã A function that helps to discover authorities. Arguments None. Return A byte array of varying size containing 256-bit public keys of the authorities. C.13. Module SessionKeys ‚Äã note This section describes Version 1 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. All calls in this module require Core_initialize_block ( Section C.4.3. ) to be called beforehand. C.13.1. SessionKeys_generate_session_keys ‚Äã Generates a set of session keys with an optional seed. The keys should be stored within the keystore exposed by the Host API. The seed needs to be valid and UTF-8 encoded. Arguments A SCALE-encoded Option as defined in Definition 180 containing an array of varying sizes indicating the seed. Return A byte array of varying size containing the encoded session keys. C.13.2. SessionKeys_decode_session_keys ‚Äã Decodes the given public session keys. Returns a list of raw public keys, including their key type. Arguments An array of varying size containing the encoded public session keys. Return An array of varying size containing tuple pairs of the following format: ( k , k m a t h r m { i d } ) {\left({k},{k}_{{{m}{a}{t}{h}{r}{m}{\left\lbrace{i}{d}\right\rbrace}}}\right)} ( k , k m a t h r m { i d } ‚Äã ) where k {k} k is an array of varying sizes containing the raw public key and k m a t h r m { i d } {k}_{{{m}{a}{t}{h}{r}{m}{\left\lbrace{i}{d}\right\rbrace}}} k m a t h r m { i d } ‚Äã is a 4-byte array indicating the key type. C.14. Module AccountNonceApi ‚Äã note This section describes Version 1 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. All calls in this module require Core_initialize_block ( Section C.4.3. ) to be called beforehand. C.14.1. AccountNonceApi_account_nonce ‚Äã Get the current nonce of an account. This function can be used by the Polkadot Host implementation when it seems appropriate, such as for the JSON-RPC API as described in Section C.1.1. . Arguments The 256-bit public key of the account. Return A 32-bit unsigned integer indicating the nonce of the account. C.15. Module TransactionPaymentApi ‚Äã note This section describes Version 2 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. All calls in this module require Core_initialize_block ( Section C.4.3. ) to be called beforehand. C.15.1. TransactionPaymentApi_query_info ‚Äã Returns information of a given extrinsic. This function is not aware of the internals of an extrinsic, but only interprets the extrinsic as some encoded value and accounts for its weight and length, the Runtime‚Äôs extrinsic base weight, and the current fee multiplier. This function can be used by the Polkadot Host implementation when it seems appropriate, such as for the JSON-RPC API as described in Section C.1.1. . Arguments A byte array of varying sizes containing the extrinsic. The length of the extrinsic. [ To do: why is this needed? ] Return A data structure of the following format: ( w , c , f ) {\left({w},{c},{f}\right)} ( w , c , f ) where w {w} w is the weight of the extrinsic. c {c} c is the "class" of the extrinsic, where a class is a varying data ( Definition 178 ) type defined as: c = { 0 Normal extrinsic 1 Operational extrinsic 2 Mandatory extrinsic, which is always included c = \left\{ \begin{array}{l} 0 \quad \textrm{Normal extrinsic} \\ 1 \quad \textrm{Operational extrinsic} \\ 2 \quad \textrm{Mandatory extrinsic, which is always included} \end{array} \right. c = ‚é© ‚é® ‚éß ‚Äã 0 Normal extrinsic 1 Operational extrinsic 2 Mandatory extrinsic, which is always included ‚Äã f {f} f is the inclusion fee of the extrinsic. This does not include a tip or anything else that depends on the signature. C.15.2. TransactionPaymentApi_query_fee_details ‚Äã Query the detailed fee of a given extrinsic. This function can be used by the Polkadot Host implementation when it seems appropriate, such as for the JSON-RPC API as described in Section C.1.1. . Arguments A byte array of varying sizes containing the extrinsic. The length of the extrinsic. Return A data structure of the following format: ( f , t ) {\left({f},{t}\right)} ( f , t ) where f {f} f is a SCALE encoded as defined in Definition 180 containing the following data structure: f = ( f b , f l , f a ) {f}={\left({{f}_{{b}},}{{f}_{{l}},}{f}_{{a}}\right)} f = ( f b ‚Äã , f l ‚Äã , f a ‚Äã ) where f b {f_b} f b ‚Äã is the minimum required fee for an extrinsic. f l {f_l} f l ‚Äã is the length fee, the amount paid for the encoded length (in bytes) of the extrinsic. f a {f_a} f a ‚Äã is the ‚Äúadjusted weight fee,‚Äù which is a multiplication of the fee multiplier and the weight fee. The fee multiplier varies depending on the usage of the network. t {t} t is the tip for the block author. C.16. Module TransactionPaymentCallApi ‚Äã All calls in this module require Core_initialize_block ( Section C.4.3. ) to be called beforehand. caution TODO clarify differences between RuntimeCall and Extrinsics C.16.1. TransactionPaymentCallApi_query_call_info ‚Äã Query information of a dispatch class, weight, and fee of a given encoded Call . Arguments A byte array of varying sizes containing the Call . The length of the Call. Return A data structure of the following format: ( w , c , f ) (w, c, f) ( w , c , f ) where : w w w is the weight of the call. c c c is the "class" of the call, where a class is a varying data ( Definition 178 ) type defined as: c = { 0 Normal dispatch 1 Operational dispatch 2 Mandatory dispatch, which is always included regardless of their weight c = \left\{\begin{array}{l} 0 \quad \textrm{Normal dispatch}\\ 1 \quad \textrm{Operational dispatch}\\ 2 \quad \textrm{Mandatory dispatch, which is always included regardless of their weight} \end{array}\right. c = ‚é© ‚é® ‚éß ‚Äã 0 Normal dispatch 1 Operational dispatch 2 Mandatory dispatch, which is always included regardless of their weight ‚Äã f f f is the partial-fee of the call. This does not include a tip or anything else that depends on the signature. C.16.2. TransactionPaymentCallApi_query_call_fee_details ‚Äã Query the fee details of a given encoded Call including tip. Arguments A byte array of varying sizes containing the Call . The length of the Call . Return A data structure of the following format: ( f , t ) (f, t) ( f , t ) where : f f f is a SCALE encoded as defined in Definition 180 containing the following data structure: f = ( f b , f l , f a ) f = (f_b, f_l, f_a) f = ( f b ‚Äã , f l ‚Äã , f a ‚Äã ) where : f b f_b f b ‚Äã is the minimum required fee for the Call . f l f_l f l ‚Äã is the length fee, the amount paid for the encoded length (in bytes) of the Call . f a f_a f a ‚Äã is the " adjusted weight fee ", which is a multiplication of the fee multiplier and the weight fee. The fee multiplier varies depending on the usage of the network. t t t is the tip for the block author. C.17. Module Nomination Pools ‚Äã note This section describes Version 1 of this API. Please check Core_version ( Section C.4.1. ) to ensure compatibility. Currently supports only one RPC endpoint. C.17.1. NominationPoolsApi_pending_rewards ‚Äã Runtime API for accessing information about the nomination pools. Returns the pending rewards for the member that the Account ID was given for. Arguments The account ID as a SCALE encoded 32-byte address of the sender ( Definition 134 ). Return The SCALE encoded balance of type u128 representing the pending reward of the account ID. The default value is Zero in case of errors in fetching the rewards. C.17.2. NominationPoolsApi_points_to_balance ‚Äã Runtime API to convert the number of points to balances given the current pool state, which is often used for unbonding. Arguments An unsigned 32-bit integer representing Pool Identifier An unsigned 32-bit integer Points Return An unsigned 32-bit integer Balance C.17.3. NominationPoolsApi_balance_to_points ‚Äã Runtime API to convert the given amount of balances to points for the current pool state, which is often used for bonding and issuing new funds in to the pool. Arguments An unsigned 32-bit integer representing Pool Identifier An unsigned 32-bit integer Balance Return An unsigned 32-bit integer Points Previous Appendix B: Host API Next Glossary C.1. General Information C.1.1. JSON-RPC API for external services C.2. Runtime Constants C.2.1. __heap_base C.3. Runtime Call Convention C.4. Module Core C.4.1. Core_version C.4.2. Core_execute_block C.4.3. Core_initialize_block C.5. Module Metadata C.5.1. Metadata_metadata C.5.2. Metadata_metadata_at_version C.5.3. Metadata_metadata_versions C.6. Module BlockBuilder C.6.1. BlockBuilder_apply_extrinsic C.6.2. BlockBuilder_finalize_block C.6.3. BlockBuilder_inherent_extrinisics : C.6.4. BlockBuilder_check_inherents C.7. Module TaggedTransactionQueue C.7.1. TaggedTransactionQueue_validate_transaction C.8. Module OffchainWorkerApi C.8.1. OffchainWorkerApi_offchain_worker C.9. Module ParachainHost C.9.1. ParachainHost_validators C.9.2. ParachainHost_validator_groups C.9.3. ParachainHost_availability_cores C.9.4. ParachainHost_persisted_validation_data C.9.5. ParachainHost_assumed_validation_data C.9.6. ParachainHost_check_validation_outputs C.9.7. ParachainHost_session_index_for_child C.9.8. ParachainHost_validation_code C.9.9. ParachainHost_validation_code_by_hash C.9.10. ParachainHost_validation_code_hash C.9.11. ParachainHost_candidate_pending_availability C.9.12. ParachainHost_candidate_events C.9.13. ParachainHost_session_info C.9.14. ParachainHost_dmq_contents C.9.15. ParachainHost_inbound_hrmp_channels_contents C.9.16. ParachainHost_on_chain_votes C.9.17. ParachainHost_pvfs_require_precheck C.9.18. ParachainHost_submit_pvf_check_statement C.9.19. ParachainHost_disputes C.9.20. ParachainHost_executor_params C.10. Module GrandpaApi C.10.1. GrandpaApi_grandpa_authorities C.10.2. GrandpaApi_current_set_id C.10.3. GrandpaApi_submit_report_equivocation_unsigned_extrinsic C.10.4. GrandpaApi_generate_key_ownership_proof C.11. Module BabeApi C.11.1. BabeApi_configuration C.11.2. BabeApi_current_epoch_start C.11.3. BabeApi_current_epoch C.11.4. BabeApi_next_epoch C.11.5. BabeApi_generate_key_ownership_proof C.11.6. BabeApi_submit_report_equivocation_unsigned_extrinsic C.12. Module AuthorityDiscoveryApi C.12.1. AuthorityDiscoveryApi_authorities C.13. Module SessionKeys C.13.1. SessionKeys_generate_session_keys C.13.2. SessionKeys_decode_session_keys C.14. Module AccountNonceApi C.14.1. AccountNonceApi_account_nonce C.15. Module TransactionPaymentApi C.15.1. TransactionPaymentApi_query_info C.15.2. TransactionPaymentApi_query_fee_details C.16. Module TransactionPaymentCallApi C.16.1. TransactionPaymentCallApi_query_call_info C.16.2. TransactionPaymentCallApi_query_call_fee_details C.17. Module Nomination Pools C.17.1. NominationPoolsApi_pending_rewards C.17.2. NominationPoolsApi_points_to_balance C.17.3. NominationPoolsApi_balance_to_points 
Glossary | Polkadot Protocol Specification Glossary Glossary P n {P}_{{n}} P n ‚Äã A path graph or a path of n {n} n nodes. ( b 0 , b 1 , ‚Ä¶ , b n ‚àí 1 ) {\left({b}_{{0}},{b}_{{1}},\ldots,{b}_{{{n}-{1}}}\right)} ( b 0 ‚Äã , b 1 ‚Äã , ‚Ä¶ , b n ‚àí 1 ‚Äã ) A sequence of bytes or byte array of length n {n} n ùîπ n ùîπ_{{n}} B n ‚Äã A set of all byte arrays of length n {n} n I = ( B n ‚Ä¶ B 0 ) 256 {I}={\left({B}_{{n}}‚Ä¶{B}_{{0}}\right)}_{{{256}}} I = ( B n ‚Äã ‚Ä¶ B 0 ‚Äã ) 256 ‚Äã A non-negative integer in base 256 B = ( b 0 , b 1 , ‚Ä¶ , b n ) {B}={\left({b}_{{0}},{b}_{{1}},‚Ä¶,{b}_{{n}}\right)} B = ( b 0 ‚Äã , b 1 ‚Äã , ‚Ä¶ , b n ‚Äã ) The little-endian representation of a non-negative interger I = ( B n ‚Ä¶ B 0 ) 256 {I}={\left({B}_{{n}}‚Ä¶{B}_{{0}}\right)}_{{{256}}} I = ( B n ‚Äã ‚Ä¶ B 0 ‚Äã ) 256 ‚Äã such that b i ‚âî B i {b}_{{i}}‚âî{B}_{{i}} b i ‚Äã : = B i ‚Äã Enc L E \textrm{Enc}_{LE} Enc L E ‚Äã The little-endian encoding function. C {C} C A blockchain is defined as a directed path graph. Block A node of the directed path graph (blockchain) C Genesis Block The unique sink of blockchain C Head The source of blockchain C P ( B ) {P}{\left({B}\right)} P ( B ) The parent of block B {B} B UNIX time The number of milliseconds that have elapsed since the Unix epoch as a 64-bit integer B T {BT} BT The block tree of a blockchain G {G} G The genesis block, the root of the block tree BT CHAIN ( B ) \textrm{CHAIN}({B}) CHAIN ( B ) The path graph from G {G} G to B {B} B in B T {BT} BT . H e a d ( C ) {Head}{\left({C}\right)} He a d ( C ) The head of chain C. ‚à£ C ‚à£ {\left|{C}\right|} ‚à£ C ‚à£ The length of chain C {C} C as a path graph SubChain ( B ‚Ä≤ , B ) \textrm{SubChain}{\left({B}',{B}\right)} SubChain ( B ‚Ä≤ , B ) The subgraph of C h a i n ( B ) {Chain}{\left({B}\right)} C hain ( B ) path graph containing both B {B} B and B ‚Ä≤ {B}' B ‚Ä≤ . C B ( B T ) ‚ÑÇ_{{B}}{\left({BT}\right)} C B ‚Äã ( BT ) The set of all subchains of B T {BT} BT rooted at block B {B} B . C , C ( B T ) ‚ÑÇ,‚ÑÇ{\left({BT}\right)} C , C ( BT ) C G ( B T ) ‚ÑÇ_{{G}}{\left({BT}\right)} C G ‚Äã ( BT ) i.e. the set of all chains of B T {BT} BT rooted at genesis block Longest-Chain ( B T ) \textrm{Longest-Chain}{\left({BT}\right)} Longest-Chain ( BT ) The longest sub path graph of B T {BT} BT i.e. C : ‚à£ C ‚à£ = max ‚Å° C i ‚àà C ‚à£ C i ‚à£ {C}:{\left|{C}\right|}=\max_{{{C}_{{i}}‚àà‚ÑÇ}}{\left|{C}_{{i}}\right|} C : ‚à£ C ‚à£ = max C i ‚Äã ‚àà C ‚Äã ‚à£ C i ‚Äã ‚à£ Longest-Path ( B T ) \textrm{Longest-Path}{\left({BT}\right)} Longest-Path ( BT ) The longest sub path graph of ( P ) B T {\left({P}\right)}{BT} ( P ) BT with earliest block arrival time Deepest-Leaf ( B T ) \textrm{Deepest-Leaf}{\left({BT}\right)} Deepest-Leaf ( BT ) HeadLongest-Path ( B T ) \textrm{Head}{\textrm{Longest-Path}}{(BT)} Head Longest-Path ( BT ) i.e. the head of Longest-Path ( B T ) \textrm{Longest-Path}{\left({BT}\right)} Longest-Path ( BT ) B > B ‚Ä≤ {B}>{B}' B > B ‚Ä≤ B {B} B is a descendant of B ‚Ä≤ {B}' B ‚Ä≤ in the block tree StoredValue ( k ) \textrm{StoredValue}{\left({k}\right)} StoredValue ( k ) The function to retrieve the value stored under a specific key in the state storage. State trie, trie The Merkle radix-16 Tree, which stores hashes of storage entries. KeyEncode ( k ) \textrm{KeyEncode}{\left({k}\right)} KeyEncode ( k ) The function to encode keys for labeling branches of the trie. ùí© ùí© N The set of all nodes in the Polkadot state trie. N {N} N An individual node in the trie. ùí© b ùí©_{{b}} N b ‚Äã A branch node of the trie which has at least one and at most 16 children ùí© l ùí©_{{l}} N l ‚Äã A childless leaf node of the trie p k N A g r {pk}_{{N}}^{{{Agr}}} p k N A g r ‚Äã The aggregated prefix key of node N p k N {pk}_{{N}} p k N ‚Äã The (suffix) partial key of node N Index N \textrm{Index}_{{N}} Index N ‚Äã A function returning an integer in range of {0, . . . ,15} representing the index of a child node of node N {N} N among the children of N {N} N v N {v}_{{N}} v N ‚Äã Node value containing the header of node N {N} N , its partial key and the digest of its childern values H e a d N {Head}_{{N}} He a d N ‚Äã The node header of trie node N {N} N storing information about the node‚Äôs type and kay H ( N ) {H}{\left({N}\right)} H ( N ) The Merkle value of node N {N} N . ChildrenBitmap \textrm{ChildrenBitmap} ChildrenBitmap The binary function indicates which child of a given node is present in the trie. s v N {sv}_{{N}} s v N ‚Äã The subvalue of a trie node N {N} N . Child storage A sub storage of the state storage which has the same structure, although being stored separately Child trie State trie of a child storage Transaction Queue See Definition 14 . H p {H}_{{p}} H p ‚Äã The 32-byte Blake2b hash of the header of the parent of the block. H i , H i ( B ) {H}_{{i}},{H}_{{i}}{\left({B}\right)} H i ‚Äã , H i ‚Äã ( B ) Block number, the incremental integer index of the current block in the chain. H r {H}_{{r}} H r ‚Äã The hash of the root of the Merkle trie of the state storage at a given block H e {H}_{{e}} H e ‚Äã An auxiliary field in the block header used by Runtime to validate the integrity of the extrinsics composing the block body. H d {H}_{{d}} H d ‚Äã , H d ( B ) {H}_{{d}}{\left({B}\right)} H d ‚Äã ( B ) A block header used to store any chain-specific auxiliary data. H h ( B ) {H}_{{h}}{\left({B}\right)} H h ‚Äã ( B ) The hash of the header of block B {B} B B o d y ( B ) {Body}{\left({B}\right)} B o d y ( B ) The body of block B {B} B consisting of a set of extrinsics M v r , s t a g e {M}^{{r},{stage}}_{v} M v r , s t a g e ‚Äã Vote message broadcasted by the voter v as part of the finality protocol M v r , F i n ( B ) {M}_{{v}}^{{{r},{Fin}}}{\left({B}\right)} M v r , F in ‚Äã ( B ) The commit message broadcasted by voter v {v} v indicating that they have finalized bock B {B} B in round r {r} r v {v} v GRANDPA voter node, which casts votes in the finality protocol k v p r {{k}_{{v}}^{pr}} k v p r ‚Äã The private key of voter v {v} v v i d {v}_{{{i}{d}}} v i d ‚Äã The public key of voter v {v} v ùïç B , ùïç ùïç_{{B}},ùïç V B ‚Äã , V The set of all GRANDPA voters for at block B {B} B G S {GS} GS GRANDPA protocol state consisting of the set of voters, the number of times voters set has changed, and the current round number. r {r} r The voting round counter in the finality protocol V B {V}_{{{B}}} V B ‚Äã A GRANDPA vote casted in favor of block B V v r , p v {{V}_{{v}}^{{{r},{pv}}}} V v r , p v ‚Äã A GRANDPA vote casted by voter v {v} v during the pre-vote stage of round r {r} r V v r , p c {{V}_{{v}}^{{{r},{pc}}}} V v r , p c ‚Äã A GRANDPA vote casted by voter v {v} v during the pre-commit stage of round r {r} r J r , s t a g e ( B ) {J}^{{{r},{stage}}}{\left({B}\right)} J r , s t a g e ( B ) The justification for pre-committing or committing to block B {B} B in round r {r} r of finality protocol S i g n { v i } r , s t a g e ( B ) {Sign}^{{{r},{stage}}}_{\left\lbrace{v}_{{i}}\right\rbrace}{\left({B}\right)} S i g n { v i ‚Äã } r , s t a g e ‚Äã ( B ) The signature of voter v {v} v on their vote to block B, broadcasted during the specified stage of finality round r {r} r E r , s t a g e ‚Ñ∞^{{{r},{stage}}} E r , s t a g e The set of all equivocator voters in sub-round ‚Äò‚Äòstage'' of round r {r} r E { o b s ( v ) } r , s t a g e ‚Ñ∞^{{{r},{stage}}}_{\left\lbrace{o}{b}{s}{\left({v}\right)}\right\rbrace} E { o b s ( v ) } r , s t a g e ‚Äã The set of all equivocator voters in sub-round ‚Äò‚Äòstage'' of round r {r} r observed by voter v {v} v V D { o b s ( v ) } r , s t a g e ( B ) {V}{D}^{{{r},{stage}}}_{\left\lbrace{o}{b}{s}{\left({v}\right)}\right\rbrace}{\left({B}\right)} V D { o b s ( v ) } r , s t a g e ‚Äã ( B ) The set of observed direct votes for block B in round r {r} r V { o b s ( v ) } r , s t a g e {V}^{{{r},{stage}}}_{\left\lbrace{o}{b}{s}{\left({v}\right)}\right\rbrace} V { o b s ( v ) } r , s t a g e ‚Äã The set of total votes observed by voter v in sub-round ‚Äò‚Äòstage'' of round r V { o b s ( v ) } r , s t a g e ( B ) {V}^{{{r},{stage}}}_{\left\lbrace{o}{b}{s}{\left({v}\right)}\right\rbrace}{\left({B}\right)} V { o b s ( v ) } r , s t a g e ‚Äã ( B ) The set of all observed votes by v {v} v in the sub-round ‚Äústage‚Äù of round r {r} r (directly or indirectly) for block B {B} B B v r , p v {B}^{{{r},{pv}}}_{v} B v r , p v ‚Äã The currently pre-voted block in round r {r} r . The GRANDPA GHOST of round r {r} r Account key , ( s k a , p k a ) {\left({sk}^{{a}},{pk}^{{a}}\right)} ( s k a , p k a ) A key pair of types accepted by the Polkadot protocol which can be used to sign transactions E n c S C ( A ) {Enc}_{{{SC}}}{\left({A}\right)} E n c SC ‚Äã ( A ) SCALE encoding of value A {A} A T ‚âî ( A 1 , ‚Ä¶ , A n ) {T}‚âî{\left({A}_{{1}},\ldots,{A}_{{n}}\right)} T : = ( A 1 ‚Äã , ‚Ä¶ , A n ‚Äã ) A tuple of values A i {A}_{{i}} A i ‚Äã 's each of different type Varying Data Types ùíØ = { T 1 , ‚Ä¶ , T n } ùíØ={\left\lbrace{T}_{{1}},‚Ä¶,{T}_{{n}}\right\rbrace} T = { T 1 ‚Äã , ‚Ä¶ , T n ‚Äã } A data type representing any of varying types T 1 , ‚Ä¶ , T n {T}_{{1}},‚Ä¶,{T}_{{n}} T 1 ‚Äã , ‚Ä¶ , T n ‚Äã . S ‚âî A 1 , ‚Ä¶ , A n {S}‚âî{A}_{{1}},‚Ä¶,{A}_{{n}} S : = A 1 ‚Äã , ‚Ä¶ , A n ‚Äã Sequence of values A i {A}_{{i}} A i ‚Äã of the same type E n c { S C } L e n ( n ) {Enc}^{{{Len}}}_{\left\lbrace{S}{C}\right\rbrace}{\left({n}\right)} E n c { S C } L e n ‚Äã ( n ) SCALE length encoding, aka. compact encoding of non-negative interger n {n} n of arbitrary size. E n c H E ( P K ) {Enc}_{{{HE}}}{\left({P}{K}\right)} E n c H E ‚Äã ( P K ) Hex encoding Previous Appendix C: Runtime API 
Quick start Quick start The Quick start assumes that you are a new developer without any prior experience working with Substrate or FRAME, but that you are aware that these tools exist. The Quick start also assumes that you want a simplified entry point for setting up a development environment and starting a blockchain node on your local computer. In the Quick start , you'll get a brief overview of what Substrate is, learn how to compile and start a node, and take a short tour through the node template code. The following topics are intended to be read in order as a fast path to learning to build with Substrate: Substrate at a glance Start a node Explore the code Modify the runtime If the Quick start isn't your route of choice, there are other ways to approach the documentation for learning to build with Substrate. For example, you can dive directly into core concepts in the Learn section or work through hands-on exercises in the Tutorials section. To help you find your way, think of Substrate documentation as being divided into three parts: the developer journey, the tutorials, and an API and reference section. Although the parts are interrelated, each part presents information in a different way and with a different purpose: The Developer journey presents information in the form of a narrative arc: Learn covers core blockchain and Substrate concepts and operations. Install provides platorm-specific installation instructions and troubleshooting tips for installation-related issues. Build exposes the tools and techniques for building custom blockchain applications. Test highlights approaches for unit testing and benchmarking code before deploying on a test network or in a production environment. Deploy describes different options for deploying nodes, how to prepare for deployment, and how to move from a test network to a production network. Maintain provides general information about maintaining your network with hardware and software upgrades, managing new releases, and maintaining your infrastructure. The Tutorials provide hands-on learning experiences that cover different aspects of building and managing a Substrate-based blockchain: Build a blockchain tutorials are focused on network basics from starting a single node to creating a private four node blockchain, monitoring node operations, and upgrading a running network. Build application logic tutorials are focused on how to implement application-specific logic working with existing and custom pallets. Build a parachain tutorials are focused on how to move from a standalone chain to a parachain, connect a parachain to a relay chain, and send messages between chains. The Reference provides direct access to technical information such as the Rust API and help for command-line tools. Last edit: June 12, 2023 Next ‚Üí Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Substrate at a glance Substrate at a glance Substrate at a glance This article offers a bare bones overview of the Substrate framework for building application-optimized blockchains. All of the topics mentioned here are covered in more detail in other parts of the documentation. What is Substrate? Substrate is a Software Development Kit (SDK) that uses Rust-based libraries and tools to enable you to build application-specific blockchains from modular and extensible components. Application-specific blockchains that are built with Substrate can run as standalone services or in parallel with other chains to take advantage of the shared security provided by the Polkadot ecosystem. Substrate includes default implementations of the core components of the blockchain infrastructure to allow you to focus on the application logic. What is FRAME? FRAME provides the core modular and extensible components that make the Substrate software development kit flexible and adaptable to different use cases. FRAME include Rust-based programs and libraries that simplify and streamline the development of application-specific logic. Most of the functionality that FRAME provides takes the form of plug-in modules called pallets that you can add and configure to suit your requirements. Why use Substrate and FRAME? By using Substrate and FRAME, you can build proof-of-concept application-specific blockchains without the complexity of building a blockchain from scratch or the limitations of building on a general-purpose blockchain. With Substrate and FRAME, you can focus on crafting the business logic that makes your chain unique and innovative with the additional benefits of flexibility, upgradeability, open source licensing, and cross-consensus interoperability. What is a Substrate node? Every blockchain platform relies on a decentralized network of computers‚Äîcalled nodes‚Äîthat communicate with each other about transactions and blocks. In general, a node in this context is the software running on the connected devices rather than the physical or virtual machine in the network. As software, Substrate nodes consist of two main parts with separate responsibilities: A core client with outer node services to handle network and blockchain infrastructure activity. A runtime with the business logic for state transitions and the current state of the blockchain. Why build a custom runtime? The separation of responsibilities into client-driven activity and runtime-driven activity is a critical part of what makes Substrate nodes upgradeable. The application logic is what makes your chain unique and it's stored on-chain in the form of a WebAssembly binary. If you make changes to the application logic, you simply compile a new WebAssembly binary. You can then submit a transaction to update the WebAssembly binary currently stored on-chain with your updated binary. Because the custom runtime is a self-contained object that's stored as part of the chain state, you can easily iterate on the application design and evolve your project as your community evolves. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE What is Substrate? What is FRAME? Why use Substrate and FRAME? What is a Substrate node? Why build a custom runtime? Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Start a node Start a node Start a node All of the Substrate tutorials and how-to guides require you to build and run a Substrate node in your development environment. To help you set up a working environment quickly, the Substrate Developer Hub maintains templates for you to use. For example, the substrate-node-template is a snapshot of the main Substrate node-template binary that includes a core set of features to get you started. After you start the node, you can connect to it using a web browser and a simple application that allows you to look up a balance for a predefined account. Before you begin Before you begin, verify the following: You have an internet connection and access to an interactive shell terminal on your local computer. You are generally familiar with software development and using command-line interfaces. You have the Rust compiler and toolchain installed. You can check whether you have Rust installed by running the rustup show command. If Rust is installed, this command displays version information for the toolchain and compiler. If Rust is not installed, the command doesn't return any output. For information about installing Rust, see Install . Build the node template Clone the node template repository by running the following command: git clone https://github.com/substrate-developer-hub/substrate-node-template This command clones the main branch. Optionally, you can use the --branch command-line option and a tag to specify a version of Polkadot you want the node to be compatible with. Change to the root of the cloned directory: cd substrate-node-template Create a new branch to save your work by running a command similar to the following: git switch -c my-learning-branch-yyyy-mm-dd You can name the branch using any identifying information you choose. In most cases, you should include information about the year-month-day you cloned the branch in the name. For example: git switch -c my-learning-branch-2023-03-31 Compile the node template: cargo build --package node-template --release The first time you compile the node, it can take some time to complete. When compiling is complete, you should see a line like this: Finished release [ optimized ] target ( s ) in 11m 23s View information for the node Verify that your node is ready to use and see information about the command-line options available by running the following command: ./target/release/node-template --help The usage information displays the command-line options you can use to: start the node work with accounts and keys modify node operations View account information for the predefined Alice development account by running the following command: ./target/release/node-template key inspect //Alice The command displays the following account and address information: Secret Key URI `//Alice` is account: Network ID: substrate Secret seed: 0xe5be9a5092b81bca64be81d212e7f2f9eba183bb7a90954f7b76361f6edb5c0a Public key (hex): 0xd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d Account ID: 0xd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d Public key (SS58): 5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY SS58 Address: 5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY Predefined development accounts like Alice and Bob are configured in a chain specification file. You'll learn more about node template files in Explore the code and more specifically about the chain specification file in Chain specification . For now, it's enough to be aware that development accounts exist to enable testing of simple transactions like balance transfers. Start the blockchain Start the node in development mode by running the following command: ./target/release/node-template --dev In development mode, the chain doesn't require any peer computers to finalize blocks. As the node starts, the terminal displays output about the operations performed. If you see messages that blocks are being proposed and finalized, you have a running node. ... Idle (0 peers), best: #3 (0xcc78‚Ä¶5cb1), finalized #1 ... ... Starting consensus session on top of parent ... ... Prepared block for proposing at 4 (0 ms) ... Connect to the node Now that your node is running, you can connect to it to check the balance of the predefined Alice account. For this simple application, you can create one index.file that uses JavaScript and the Polkadot-JS API to interact with the blockchain. For example, this sample index.demonstrates how to use JavaScript, the Polkadot-JS API, and to do the following: Take an account address as input. Look up the account balance using an onClick event. Display the balance for the account as output. To connect to the node and check an account balance: Copy and paste the sample code for the Quick start: Get Balance application into a new file in your code editor and save the file on your local computer. Open the index.file in a web browser. Copy and paste the SS58 Address for the Alice account in the input field, then click Get Balance . Stop the node Go to the terminal that displays blockchain operations. Stop the local blockchain and clear all state by pressing the control-c key combination. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Before you begin Build the node template View information for the node Start the blockchain Connect to the node Stop the node Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Explore the code Explore the code Explore the code In Start a node , you compiled and started a local Substrate node in development mode. This particular node‚Äîthe substrate-node-template ‚Äîprovides a simplified environment with only a few common modules to get you started. Without going too deeply into the details, there's a lot you can learn from exploring the basic building blocks of the node template code. About the node template The node template includes some default blockchain essentials, like peer-to-peer networking, a simple consensus mechanism, and transaction handling. The node template also includes some basic functionality for working with accounts, balances, and transaction fees and performing administrative actions. This core set of functionality is provided through several predefined modules‚Äîcalled pallets ‚Äîthat implement specific features. For example, the following core modules are predefined in the node template: pallet_balances for managing account assets and transfers between accounts. pallet_transaction_payment for managing transaction fees for the transactions performed. pallet_sudo for performing operations that require administrative permissions. The node template also provides a starter pallet_template that illustrates how to implement features in custom pallets. Now that you have an overview of the features included in the node template, let's take a closer look at the code in the substrate-node-template directory and its subdirectories. Manifest files Because Substrate is a Rust-based framework, each package has a manifest file‚Äîthe Cargo.toml file‚Äîthat contains information required to compile the package. If you open the Cargo.toml file located in the root directory for the substrate-node-template , you can see that it describes the member packages that make up the node template workspace. For example: [ workspace ] members = [ "node" , "pallets/template" , "runtime" , ] [ profile.release ] panic = "unwind" From this manifest, you see that the node template workspace includes three packages: The node package provides Rust modules for many core blockchain services like peer-to-peer networking, block authoring, block finalization, and transaction pool management. The template package in the pallets subdirectory is the starter template that illustrates how to implement features when building your own custom modules. The runtime package provides all of the application logic for handling accounts, balances, transaction fees, and other features that have been included in the node template. Each member package also has its own manifest‚Äîits own Cargo.toml file‚Äîthat contains package-specific information, including dependencies and configuration settings, that are required to compile that member package. For example, the Cargo.toml file for the node member of the workspace specifies the name of the package is node-template and lists the core libraries and primitives that enable the node template to provide essential blockchain services. You'll learn more about libraries and primitives in Architecture and Rust libraries . For now, it's enough to understand the importance of the manifest in describing dependencies and other critical information for each package. If you open the runtime/Cargo.toml file and the pallets/template/Cargo.toml , you‚Äôll see different libraries and primitives as dependencies, but you‚Äôll get a general sense of what‚Äôs required to compile these packages. For example, the manifest for the runtime lists all of the pallets‚Äîincluding the frame_system , frame_support and previously-mentioned pallet_balances , pallet_transaction_payment , and pallet_sudo modules‚Äîthat comprise the default runtime for the node template. Core client source code One of the most important aspects of Substrate is that nodes consist of two main parts: the core client and the runtime . The node template also consists of separate packages for core client services in the the node/src directory and the runtime in the runtime/src directory. By default, the node/src directory includes the following Rust modules: benchmarking.rs chain_spec.rs cli.rs command.rs lib.rs main.rs rpc.rs service.rs Most core client service are encapsulated in the node/src/service.rs Rust module. It's rare that you need to modify this file or the other Rust modules in the node/src directory. A file you are likely to modify is the chain_spec.rs file. The chain_spec.rs file describes the configuration of the default Development and Local Testnet chains, including information about default pre-funded development accounts and the nodes that are preconfigured with the authority to produce blocks. If you create a custom chain, you use this file to identify the network that a node connects to and the other nodes that the local node communicates with. Default node template runtime Because Substrate provides a modular and flexible framework for building blockchains, you can make changes to any package in the workspace. However, most application development work is done in the runtime and in the modules‚Äîthe pallets‚Äîused to construct the runtime. Before you start to customize the runtime for your own project, you should spend a little time exploring what‚Äôs in the default node template. Default manifest You've already seen how the default manifest for the runtime lists the default dependencies and features for the runtime in lines similar to the following: pallet - balances = { version = "4.0.0-dev" , default - features = false , git = "https://github.com/paritytech/substrate.git" , branch = "polkadot-vX.Y.Z" } pallet - sudo = { version = "4.0.0-dev" , default - features = false , git = "https://github.com/paritytech/substrate.git" , branch = "polkadot-vX.Y.Z" } pallet - transaction - payment = { version = "4.0.0-dev" , default - features = false , git = "https://github.com/paritytech/substrate.git" , branch = "polkadot-vX.Y.Z" } There are also dependencies on core packages‚Äîsuch as frame-system , frame-support , and frame-executive . You'll learn more about these core services in Core FRAME services . For now, just notice that these and other modules are required to compile the runtime for the node template. Default source code The main source code for the runtime is located in the runtime/src/lib.rs file. If you open this file in your code editor, it might seen complicated at first. There are some nuances that are covered in other parts of the documentation, but in essence, the source code does the following: Imports the frame system and frame support core services. Specifies version information for the runtime. Declares the pallets to include. Declares the types and parameters for each pallet included. Sets constant and variable values for each pallet included. Implements the Config trait for each pallet included. Constructs the runtime from the pallets included. Prepares the benchmarking framework for evaluating pallet performance. Implements the interfaces that enable the core client to call into the runtime. You‚Äôll learn more about constructing the runtime, defining benchmarks, and using runtime interfaces in topics in the Build and Test sections. For now, you only need to have a general sense of how the runtime is composed and how the default pallets are implemented using the Config trait. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE About the node template Manifest files Core client source code Default node template runtime Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Modify the runtime Modify the runtime Modify the runtime In Explore the code , you learned about the manifest files and Rust modules that make up the default node template. Now that you have a general idea of what the runtime source code looks like, let's look at how easy it is for you to make a few simple changes to customize the runtime. For this simple demonstration, you are going to do the following: Add a pallet that has some functionality you want to use. Change some constant values. Update the runtime version. Recompile the runtime to include your changes. Submit a transaction to update the runtime stored on-chain. You'll also see another application that uses the Polkadot-JS API and how you can use the hosted version of that application to view the chain state and submit transactions. Before you begin When you run a node in development mode using the --dev command-line option, it starts in a clean state with the first block. To best illustrate how to modify and update the runtime, you should restart the default node template with its default runtime so that it starts producing blocks. To restart the node with the default runtime: Open a terminal shell on your computer. Change to the root directory where you compiled the Substrate node template. Start the local node in development mode by running the following command: cargo run --release -- --dev After you start the node, you can connect to it using a browser-based application built using the Polkadot-JS API. To connect to the running node: Open the Polkadot/Substrate Portal in a Chrome or a Chromium-based browser. If you use a more restrictive browser‚Äîsuch as Firefox‚Äîyou might find that connections between the Polkadot/Substrate Portal and the node are blocked. Connect to the Development network and the default local node endpoint 127.0.0.1:9944 , if necessary. In most cases, the Polkadot/Substrate Portal initializes the connection to the running local node automatically. If required, click Unknown to display the network selection menu, then select Development and Local Node , then click Switch . Notice that under Development, the node template version is the default version 100. Add a pallet The most common way to start building with Substrate and FRAME involves adding pallets, either by importing one from the existing library or by creating your own. Creating your own pallet from scratch isn't difficult, but it requires more work designing the application logic, storage requirements, error handling, and so on. To keep things simple, let's add a pallet by importing one from the existing library. By default, the node template doesn't include the Utility pallet . If this pallet contains functions you want to use, you can add it to the default runtime. To add the Utility pallet: Open a second terminal shell on your computer and change to the node template root directory. Open the runtime manifest‚Äî runtime/Cargo.toml in your code editor. Locate the [dependencies] section and add the Utility pallet as a dependency. For example, you should add a single line similar to the following. pallet-utility = { version = "4.0.0-dev" , default-features = false , git = "https://github.com/paritytech/substrate.git" , branch = "polkadot-vX.Y.Z" } Be sure to replace branch = "polkadot-vX.Y.Z" with the Polkadot branch used for other pallets. You can copy any existing pallet dependency as a model to ensure that the branch setting for the pallet-utility dependency is the same as the branch setting for all other pallets. Locate the [features] section and add the Utility pallet to the list of default features for the standard binary. For example: [ features ] default = [ "std" ] std = [ . . . "pallet-utility/std" , . . . ] You'll learn more about building features for the standard and WebAssembly binaries in Rust and WebAssembly . Save your changes and close the Cargo.toml file. Open the runtime/src/lib.rs file in your code editor. Add the implementation for the Config trait for the Utility pallet. For example: impl pallet_utility :: Config for Runtime { type RuntimeEvent = RuntimeEvent ; type RuntimeCall = RuntimeCall ; type PalletsOrigin = OriginCaller ; type WeightInfo = pallet_utility :: weights :: SubstrateWeight < Runtime > ; } Every pallet has a Config trait for the specific parameters and types it requires. You can always look at the Rust documentation for a pallet to learn more about its configuration requirements. For example, you can view the Rust documentation for the pallet-utility . Add the Utility pallet inside the construct_runtime! macro. For example: construct_runtime! ( pub struct Runtime where Block = Block , NodeBlock = opaque :: Block , UncheckedExtrinsic = UncheckedExtrinsic { System : frame_system , RandomnessCollectiveFlip : pallet_randomness_collective_flip , Timestamp : pallet_timestamp , Aura : pallet_aura , ... Utility : pallet_utility , // Add this line ... } You can learn more about how the construct_runtime macro works in FRAME macros and Runtime construction macros . Change constant values By default, the Balances pallet in the node template defines an EXISTENTIAL_DEPOSIT constant. The EXISTENTIAL_DEPOSIT represents the minimum balance that an account must have to be considered a valid active account. By default, the constant is defined as a 128-bit unsigned integer type with a value of 500. To keep things simple, you're going to change the value of this constant from 500 to 1000. To update a constant value: Open the runtime/src/lib.rs file in your code editor. Locate the EXISTENTIAL_DEPOSIT for the Balances pallet. /// Existential deposit. pub const EXISTENTIAL_DEPOSIT: u128 = 500; Update the value for the EXISTENTIAL_DEPOSIT. pub const EXISTENTIAL_DEPOSIT : u128 = 1000 // Update this value. Update the runtime version By default, the node template identifies the default runtime version in the VERSION constant using the spec_version and a value of 100. To indicate that you've made changes to the default runtime, you're going to change the spec_version from 100 to 101. Note that updating the spec_version isn't strictly required for the changes you've made to the default runtime in the Quick start . However, by updating the version you can see the basic steps involved in performing a forkless upgrade. To update the runtime version: Open the runtime/src/lib.rs file in your code editor. Locate the runtime_version macro. #[sp_version::runtime_version] pub const VERSION: RuntimeVersion = RuntimeVersion { spec_name: create_runtime_str!("node-template"), impl_name: create_runtime_str!("node-template"), authoring_version: 1, spec_version: 100, impl_version: 1, apis: RUNTIME_API_VERSIONS, transaction_version: 1, state_version: 1, }; Update the spec_version to specify the new runtime version. spec_version : 101 , // Change the spec_version from 100 to 101 Save your changes and close the runtime/src/lib.rs file. At this point, you've modified the runtime code and changed the version information. However, the running node is still using the previously-compiled version of the runtime. If you are still connected to the running node using the Polkadot/Substrate Portal , you can see the node template version is still the default version 100 and the chain state for the balances constant existentialDeposit is still 500. Recompile the runtime Before you can update the node template to use your modified runtime, you must recompile the runtime. To recompile the runtime package: Open a second terminal shell and change to the root directory where you compiled the node template. Recompile the runtime by running the following command: cargo build --release --package node-template-runtime The --release command-line option requires a longer compile time. However, it generates a smaller build artifact that is better suited for submitting to the blockchain network. Storage optimization is critical for any blockchain. With this command, the build artifacts are output to the target/release directory. The WebAssembly build artifacts are in the target/release/wbuild/node-template-runtime directory. For example, if you list the contents of the target/release/wbuild/node-template-runtime directory, you should see the following WebAssembly artifacts: node_template_runtime.compact.compressed.wasm node_template_runtime.compact.wasm node_template_runtime.wasm Submit a transaction You now have an updated WebAssembly object that describes the modified runtime. However, the running node isn't using the upgraded runtime yet. To update the runtime stored on-chain, you must submit a transaction that changes the WebAssembly object to use. To update the runtime: In the Polkadot/Substrate Portal , click Developer and select Extrinsics . Select the administrative Alice account. Select the sudo pallet and the sudoUncheckedWeight(call, weight) function. Select system and setCode(code) as the call to make using the Alice account. Click file upload , then select or drag and drop the compact and compressed WebAssembly file‚Äî node_template_runtime.compact.compressed.wasm ‚Äîthat you generated for the updated runtime. For example, navigate to the target/release/wbuild/node-template-runtime directory and select node_template_runtime.compact.compressed.wasm as the file to upload. Leave both of the weight parameters set to the default value of 0 . Click Submit Transaction . Review the authorization, then click Sign and Submit . Verify the modified runtime After the transaction is included in a block, you can verify that you're using the modified runtime. To verify your changes: In the Polkadot/Substrate Portal , click Network and select Explorer to see that there has been a successful sudo.Sudid event. Check that the node template version is now 101 . For example: Click Developer and select Extrinsics . Click submit the following extrinsic and scroll to the bottom of the list to verify that the utility pallet is available as an option. Click Developer , select Chain state , then click Constants . Select the balances pallet, select existentialDeposit , then click + to query the constant value. Where to go next After verifying the changes, you know that you have a customized version of the node template running and have successfully upgraded your local node to use your modified runtime. That's quite an achievement, but there's a lot more you can do. To dig deeper into concepts and core components, review topics in the Learn section or start building on what you've learned so far by exploring topics in the Build section. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Before you begin Add a pallet Change constant values Update the runtime version Recompile the runtime Submit a transaction Verify the modified runtime Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Learn Substrate is a Software Development Kit (SDK) that uses Rust-based libraries and tools to enable you to build application-specific blockchains from modular and extensible components. The topics in this section explain many of the core principles and unique features of the Substrate development environment. These topics are intended to help you learn what's possible when you build a Substrate-based blockchain and how Substrate can help you build a blockchain that best serves your specific project requirements or business model. Welcome to Substrate highlights the key benefits of developing with Substrate that most blockchain and smart contract platforms can't provide. Blockchain basics provides context about the complexity associated with blockchain development and introduces common blockchain concepts, components, and terminology. Architecture and Rust libraries describes the Substrate architecture and the relationship between the architecture and the core Rust libraries. Networks and nodes defines the different types of network topologies that can be built with Substrate and the different roles that nodes can fill. What can you build introduces the advantages and limitations of different development options and why you might select one approach over another. Runtime development highlights the importance of the Substrate runtime and introduces the core application interfaces and primitives required for Substrate runtime development. Transactions and block basics introduces transaction types and the components that make up a block. Transaction lifecycle explains how transactions are received, queued, and executed to be included in a block. State transitions and storage describes how the state changes processed in the runtime are stored and managed using trie data structures and a key-value database. Accounts, addresses, and keys explains the relationship between accounts, addresses, and keys and how they are used. Rust for Substrate highlights the specific Rust features‚Äîincluding traits, generics, associated types, and macros‚Äîthat you should be most familiar with to build a Substrate-based blockchain. Offchain operations explores reasons for handling some operations offchain and alternatives for performing those offchain operations. Light clients in Substrate Connect describes how to use Substrate Connect to integrate a light client into your applications and enable interaction with any Substrate-based chain. Cryptography provides an overview of the hashing algorithms and signature schemes used for cryptography in Substrate. Consensus describes the most common consensus models and the types of consensus you can implement for a Substrate blockchain. Cross-consensus messaging provides an overview of cross-consensus communication and the cross-consensus messaging (XCM) format. After you digest the information in these introductory sections, you'll be ready to start designing, building, and testing your own custom blockchain solution. Last edit: April 3, 2023 ‚Üê Previous Next ‚Üí Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Welcome to Substrate Welcome to Substrate Welcome to Substrate Substrate is a Software Development Kit (SDK) that allows you to build application-specific blockchains that can run as standalone services or in parallel with other chains with the shared security provided by the Polkadot ecosystem. Simplicity with the freedom to innovate With Substrate, you have complete creative control over the application you want to build. You can select predefined application logic from a large library of open source modules and templates to speed your development time. Not finding what you need in the current library? That‚Äôs not a problem‚Äîit‚Äôs an opportunity to build custom modules from reusable Rust macros and scaffolding code. If you‚Äôre feeling more adventurous or have a novel idea, you‚Äôre free to innovate on the blockchain design by using low-level primitives. Building with templates and modules Most projects start with templates to reduce complexity and development time, then progress by modifying existing modules and adding new ones. The modules, macros, and libraries are the core components of the FRAME development environment. The main purpose of FRAME is to provide a modular and flexible collection of components for building a customized Substrate runtime. The FRAME development environment enables you to select and configure the specific modules‚Äîcalled pallets ‚Äîthat you want to use in your runtime. Pallets provide customizable business logic for common use cases like managing account balances and voting on proposals. Pallets also provide the business logic for blockchain operations such as staking and consensus. Composing a runtime Each pallet defines specific types, storage items, and functions to implement a specific set of features or functionality for a runtime. You select and combine the pallets that suit your application to compose a custom runtime. For example, if your application needs to manage account balances, you can simply include the Balances pallet in the configuration of your runtime logic. You can then modify the configuration of the pallet in your custom runtime to suit your application. In the following diagram, the runtime is composed of nine pallets to implement consensus, include timestamps for blocks, manage assets and balances, and prepare a framework for governance and managing pooled funds. In addition to the pallets that provide the functionality you choose to use in the runtime, FRAME relies on a few underlying system services to construct and enable the client outer node services to interact with the runtime. These underlying services are provided by the following required modules: FRAME system crate frame_system provides low-level types, storage, and functions for the runtime. FRAME support crate frame_support is a collection of Rust macros, types, traits, and modules that simplify the development of Substrate pallets. FRAME executive pallet frame_executive orchestrates the execution of incoming function calls to the respective pallets in the runtime. There are a lot of pallets available for you to use as building blocks for your runtime. You can see the list of pallets available in the Substrate repository or in the Rust documentation. For a summary with a brief description of the most common pallets, see FRAME pallets. If you can‚Äôt find a pallet that provides the functionality you need, you can use FRAME to create your own custom pallet, then add that custom pallet to your customized runtime. Building with custom pallets The FRAME development environment includes libraries that make it relatively easy to build your own custom pallets. With custom pallets, you have the flexibility to define the runtime behavior that best suits your application. Because each pallet has its own discrete logic, you can combine existing open source pallets with custom pallets to provide the specific features or functionality your application requires. For example, you might include the Balances pallet in your runtime to use its functions and storage items for managing account balances, but add a custom pallet to send notifications to a service when an account balance changes. Why you should build with Substrate Substrate is a fully modular and flexible framework that lets you compose a chain by selecting and customizing the infrastructure components best suited to your project. For example, you can change the network stack, consensus model, transaction format, or governance method to deploy a blockchain that's uniquely designed for your application, but that can also evolve with your changing needs. In addition to being composable and adaptable, Substrate is designed to be upgradeable . The state transition logic‚Äîthe Substrate runtime‚Äîis a self-contained WebAssembly object that you can change completely whenever you need to introduce new features or update existing features. Because the runtime is a self-contained object, you can introduce runtime upgrades across the network without disrupting service or requiring nodes to be taken offline. In most cases, no action is required for nodes to operate with a new runtime, so you can evolve your network protocols seamlessly over time to meet the needs of your users. Substrate is also an open source project and all of the Substrate libraries and tools are available under open-source licensing. In addition, core components of the Substrate framework use open protocols such as libp2p and jsonRPC while empowering you to decide how much you want to customize your blockchain architecture. Substrate also has a large, active, and helpful builder community contributing to the ecosystem. Contributions from the community enhance the capabilities available for you to incorporate into your own blockchain as it evolves. Substrate supports cross-consensus messaging (XCM) to enable disparate systems to pass messages to each other. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Simplicity with the freedom to innovate Building with templates and modules Composing a runtime Building with custom pallets Why you should build with Substrate Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Blockchain basics Blockchain basics Blockchain basics A blockchain is a decentralized ledger that records information in a sequence of blocks. The information contained in a block is an ordered set of instructions that might result in a change in state. In a blockchain network, individual computers‚Äîcalled nodes‚Äîcommunicate with each other to form a decentralized peer-to-peer (P2P) network. There is no central authority that controls the network and, typically, each node that participates in block production stores a copy of the blocks that make up the canonical chain. In most cases, users interact with a blockchain by submitting a request that might result in a change in state, for example, a request to change the owner of a file or to transfer funds from one account to another. These transactions requests are gossiped to other nodes on the network and assembled into a block by a block author. To ensure the security of the data on the chain and the ongoing progress of the chain, the nodes use some form of consensus to agree on the state of the data in each block and on the order of transactions executed. What is a blockchain node? At a high level, all blockchain nodes require the following core components: Data storage for the state changes recorded as a result of transactions. Peer-to-peer networking for decentralized communication between nodes. Consensus methodology to protect against malicious activity and ensure the ongoing progress of the chain. Logic for ordering and processing incoming transactions. Cryptography for generating hash digests for blocks and for signing and verifying the signatures associated with transactions. Because of the complexity involved in building the core components a blockchain requires, most blockchain projects start with a complete copy of an existing blockchain code base so that developers can modify existing code to add new features instead of writing everything from scratch. For example, the Bitcoin repository was forked to create Litecoin, ZCash, Namecoin and Bitcoin Cash. Similarly, the Ethereum repository was forked to create Quorum, POA Network, KodakCoin, and Musicoin. However, most blockchain platforms are not designed to allow for modification or customization. As a result, building a new blockchain by forking has serious limitations, including limitations such as scalability that are inherent in the originating blockchain code. Before you explore how Substrate alleviates many of the limitations associated with other blockchain projects, it's important to understand some of the common properties that most blockchains share. By learning about how most blockchains operate, you'll be better prepared to see how Substrate provides alternatives and capabilities for building a blockchain best suited to your needs. State transitions and conflicts A blockchain is essentially a state machine . At any point in time, the blockchain has a current internal state. As inbound transactions are executed, they result in changes to state so the blockchain must transition from its current state to a new state. However, there can be multiple valid transitions that would result in different future states, and the blockchain must select a single state transition that can be agreed upon. To agree on the state after a transition, all operations within a blockchain must be deterministic. For the chain to progress successfully, a majority of the nodes must agree on all of the state transitions, including: The initial state of the chain, called the genesis state or genesis block. The series of state transitions that result from executed transactions that are recorded in each block. A final state for the block to be included in the chain. In centralized networks, a central authority can choose between mutually exclusive state transitions. For example, a server configured as the primary authority might record changes to state transition in the order it sees them or use a weighting process to choose between competing alternatives when a conflict arises. In a decentralized network, the nodes see transactions in different orders, so they must use a more elaborate method to select transactions and choose between conflicting state transition. The method that a blockchain uses to batch transactions into blocks and to select which node can submit a block to the chain is called the blockchain's consensus model or consensus algorithm. The most commonly-used consensus model is called the proof-of-work consensus model. With the proof-of-work consensus model, the node that completes a computational problem first has the right to submit a block to the chain. For a blockchain to be fault tolerant and provide a consistent view of state even if some nodes are compromised by malicious actors or network outages, some consensus models require at least two-thirds of the nodes to agree on state at all time. This two-thirds majority ensures that the network is fault tolerant and can withstand some network participants behaving badly, regardless of whether the behavior is intentional or accidental. Blockchain economics All blockchains require resources‚Äîprocessors, memory, storage, and network bandwidth‚Äîto perform operations. The computers that participate in the network‚Äîthe nodes that produce blocks‚Äîprovide these resources to blockchain users. The nodes create a distributed, decentralized network that serves the needs of a community of participants. To support a community and make a blockchain sustainable, most blockchains require users to pay for the network resources they use in the form of transaction fees. The payment of transaction fees requires user identities to be associated with accounts that hold assets of some type. Blockchains typically use tokens to represent the value of assets in an account and network participants purchase tokens outside of the chain through an exchange. Network participants can then deposit the tokens to enable them to pay for transactions. Blockchain governance Some blockchains allow network participants to submit and vote on proposals that affect network operations or the blockchain community. By submitting and voting on proposals‚Äîreferenda‚Äîthe blockchain community can determine how the blockchain evolves in an essentially democratic process. On-chain governance is relatively rare, however, and to participate, a blockchain might require users to maintain a significant stake of tokens in an account or to be selected as a representative for other users. Applications running on a blockchain Applications that run on a blockchain‚Äîoften referred to as decentralized applications or dApps‚Äîare typically web applications that are written using front-end frameworks but with backend smart contracts for changing the blockchain state. A smart contract is a program that runs on a blockchain and executes transactions on behalf of users under specific conditions. Developers can write smart contracts to ensure that the outcome of programmatically-executed transactions is recorded and can't be tampered with. Yet, with smart contracts alone, developers don't have access to some underlying blockchain functionality‚Äîsuch as the consensus, storage, or transaction layers‚Äîand instead, abide by a chain's fixed rules and restrictions. Smart contract developers often accept these limitations as a tradeoff that enables faster development time with fewer core design decisions to make. Where to go next All blockchains share some common characteristics. Substrate‚Äîwhile not a blockchain itself‚Äîis a blockchain builders' toolkit with a modular framework of components to create a custom blockchain. With Substrate, you can take the common blockchain components‚Äîlike storage and consensus and cryptography‚Äîand combine them to use the functions they provide as-is or modify them to suit the purpose of your project. You can explore the following resources to learn more. Tell me Learn Welcome to Substrate Architecture and Rust libraries Networks and blockchains Guide me Build a local blockchain Simulate a network Add trusted nodes If you prefer to explore code directly, you can start building in the Substrate Playground or consult the API reference to get details about the Rust crates you use. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE What is a blockchain node? State transitions and conflicts Blockchain economics Blockchain governance Applications running on a blockchain Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Architecture and Rust libraries Architecture Architecture and Rust libraries As noted in Blockchain basics , a blockchain relies on a decentralized network of computers‚Äîcalled nodes‚Äîthat communicate with each other. Because the node is a core component of any blockchain, it‚Äôs important to understand what makes a Substrate node unique, including the core services and libraries that are provided by default and how the node can be customized and extended to suit different project goals. Client and runtime At a high level, a Substrate node consists of two main parts: A core client with outer node services that handles network activity such as peer discovery, managing transaction requests, reaching consensus with peers, and responding to RPC calls. A runtime that contains all of the business logic for executing the state transition function of the blockchain. The following diagram illustrates this separation of responsibilities in simplified form to help you visualize the architecture and how Substrate provides a modular framework for building blockchains. Client outer node services The core client includes several outer node services that are responsible for activity that takes place outside of the runtime. For example, the outer node service in the core client handle peer discovery, manage the transaction pool, communicate with other nodes to reach consensus, and respond to RPC requests from the outside world. Some of the most important activities that are handled by core client services involve the following components: Storage : The outer node persists the evolving state of a Substrate blockchain using a simple and highly efficient key-value storage layer. Peer-to-peer networking : The outer node uses the Rust implementation of the libp2p network stack to communicate with other network participants. Consensus : The outer node communicates with other network participants to ensure they agree on the state of the blockchain. Remote procedure call (RPC) API : The outer node accepts inbound HTTP and WebSocket requests to allow blockchain users to interact with the network. Telemetry : The outer node collects and provides access to node metrics through an embedded Prometheus server. Execution environment : The outer node is responsible for selecting the execution environment‚ÄîWebAssembly or native Rust‚Äîfor the runtime to use then dispatching calls to the runtime selected. Substrate provides default implementations for handling these activities through its core blockchain components. In principle, you can modify or replace the default implementation of any component with your own code. In practice, it's rare for an application to require changes to any of the underlying blockchain features, but Substrate allows you to make changes so you are free to innovate where you see fit. Performing these tasks often requires the client node services to communicate with the runtime. This communication is handled by calling specialized runtime APIs . Runtime The runtime determines whether transactions are valid or invalid and is responsible for handling changes to the blockchain state. Requests coming from the outside come through the client into the runtime, and the runtime is responsible for the state transition functions and storing the resulting state. Because the runtime executes the functions it receives, it controls how transactions are included in blocks and how blocks are returned to the outer node for gossiping or importing to other nodes. In essence, the runtime is responsible for handling everything that happens on-chain. It is also the core component of the node for building Substrate blockchains. The Substrate runtime is designed to compile to WebAssembly (Wasm) byte code. This design decision enables: Support for forkless upgrades. Multi-platform compatibility. Runtime validity checking. Validation proofs for relay chain consensus mechanisms. Similar to how the outer node has a way to provide information to the runtime, the runtime uses specialized host functions to communicate with the outer node or the outside world. Core libraries To keep things simple in the node template, many aspects of the blockchain are configured with a default implementation. For example, there are default implementations of the networking layer, database, and consensus mechanism that you can use as-is to get your blockchain running without a lot of customization. However, the libraries underlying the basic architecture provide a great deal of flexibility for defining your own blockchain components. Much like the node consists of two main parts‚Äîthe core client and the runtime‚Äîthat provide different services, the Substrate libraries are divided into three main areas of responsibility: Core client libraries for outer node services. FRAME libraries for the runtime. Primitive libraries for underlying functions and interfaces for communication between the libraries. The following diagram illustrates how the libraries mirror the core client outer node and runtime responsibilities and how the library of primitives provides the communication layer between the two. Core client libraries The libraries that enable a Substrate node to handle its network responsibilities, including consensus and block execution are Rust crates that use the sc_ prefix in the crate name. For example, the sc_service library is responsible for building the networking layer for Substrate blockchains, managing the communication between the network participants and the transaction pool. FRAME libraries for the runtime The libraries that enable you to build the runtime logic and to encode and decode the information passed into and out of the runtime are Rust crates that use the frame_ prefix in the crate name. The frame_* libraries provide the infrastructure for the runtime. For example, the frame_system library provides a basic set of functions for interacting with other Substrate components and frame_support enables you to declare runtime storage items, errors, and events. In addition to the infrastructure provided by the frame_* libraries, the runtime can include one or more pallet_* libraries. Each Rust crate that uses the pallet_ prefix represents a single FRAME module. In most cases, you use the pallet_* libraries to assemble the functionality you want to incorporate in the blockchain to suit your project. You can build a Substrate runtime without using the frame_* or pallet_* libraries using the primitives libraries. However, the frame_* or pallet_* libraries provide the most efficient path to composing a Substrate runtime. Primitive libraries At the lowest level of the Substrate architecture, there are primitive libraries that give you control over underlying operations and enable communication between the core client services and the runtime. The primitive libraries are Rust crates that use the sp_ prefix in the crate name. The primitive libraries provide the lowest level of abstraction to expose interfaces that the core client or the runtime can use to perform operations or interact with each other. For example: The sp_arithmetic library defines fixed point arithmetic primitives and types for the runtime to use. The sp_core library provides a set of shareable Substrate types. The sp_std library exports primitives from the Rust standard library to make them usable with any code that depends on the runtime. Modular architecture The separation of the core Substrate libraries provides a flexible and modular architecture for writing the blockchain logic. The primitives library provides a foundation that both the core client and the runtime can build on without communicating directly with each other. Primitive types and traits are exposed in their own separate crates, so they are available to the outer node services and runtime components without introducing cyclic dependency issues. Where to go next Now that you are familiar with the architecture and libraries used to build and interact with Substrate nodes, you might want to explore the libraries in more depth. To learn more about the technical details for any library, you should review the Rust API documentation for that library. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Client and runtime Client outer node services Runtime Core libraries Modular architecture Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Networks and nodes Networks and nodes Networks and nodes When thinking about building a blockchain, it's useful to consider that boundaries are what define a network. For example, a set of computers connected to a single router can be considered a home network. A firewall might be the boundary that defines an enterprise network. Smaller, isolated networks can be connected to wider area networks through a common communication protocol. Similarly, you can think of a blockchain network as being defined by its boundaries and its isolation from or communication with other blockchains. As a blockchain builder's toolkit, Substrate enables you to develop any type of blockchain you can imagine and to define its boundaries based on your application-specific requirements. With this flexibility in mind, one of the decisions you need to make is the type of network you want to build and the role that different nodes might play in that network. Network types Substrate-based blockchains can be used in different types of network architecture. For example, Substrate blockchains are used to build the following network types: Private networks that limit access to a restricted set of nodes. Solo chains that implement their own security protocol and don't connect or communicate with any other chains. Bitcoin and Ethereum are examples of non-Substrate based solo chains. Relay chains that provide decentralized security and communication for other chains that connect to them. Kusama and Polkadot are examples of relay chains. Parachains that are built to connect to a relay chain and have the ability to communicate with other chains that use the same relay chain. Because parachains depend on the relay chain to finalize the blocks produced, parachains must implement the same consensus protocol as the relay chain they target. Node types Blockchains require network nodes to be synchronised to present a consistent and up-to-date view of the blockchain state. Each synchronised node stores a copy of the blockchain and keeps track of incoming transactions. However, keeping a full copy of an entire blockchain requires a lot of storage and computing resources and downloading all of the blocks from genesis to the most recent isn‚Äôt practical for most use cases. To make it easier to maintain the security and integrity of the chain but reduce the resource requirements for clients wanting access to blockchain data, there are different types of nodes that can interact with the chain: Full nodes Archive nodes Light client nodes Full nodes Full nodes are a critical part of the blockchain network infrastructure and are the most common node type. Full nodes store blockchain data and, typically, participate in common blockchain operations, such as authoring and validating blocks, receiving and verifying transactions, and serving data in response to user requests. By default, full nodes are configured to store only the most recent 256 blocks and to discard state older than that‚Äîwith the exception of the genesis block‚Äîto prevent the full node from growing indefinitely and consuming all available disk space. You can configure the number of blocks a full node retains. Although older blocks are discarded, full nodes retain all of the block headers from the genesis block to the most recent block to validate that the state is correct. Because the full node has access to all of the block headers, it can be used to rebuild the state of the entire blockchain by executing all of the blocks from the genesis block. Thus it requires much more computation to retrieve information about some previous state, and an archive should generally be used instead. Full nodes allow you to read the current state of the chain and to submit and validate transactions directly on the network. By discarding state from older blocks, a full node requires much less disk space than an archive node. However, a full node requires far more computational resources to query and retrieve information about some previous state. If you need to query historical blocks, you should purge the full node then restart it as an archive node. Archive nodes Archive nodes are similar to full nodes except that they store all past blocks with complete state available for every block. Archive nodes are most often used by utilities‚Äîsuch as block explorers, wallets, discussion forums, and similar applications‚Äîthat need access to historical information. Because archive nodes retain historical state, they require a lot of disk space. Because of the disk space required to run them, archive nodes are less common than full nodes. However, archive nodes make it convenient to query the past state of the chain at any point in time. For example, you can query an archive node to look up an account balance in a certain block or to see details about a transaction resulted in a specific state change. These types of queries are faster and more efficient when you run them on the data in an archive node. Light client nodes Light client nodes enable you to connect to a Substrate network with minimal hardware requirements. Because light client nodes require minimal system resources, they can be embedded into web-based applications, browser extensions, mobile device applications, or internet of things (IoT) devices. Light client nodes provide a runtime and access to the current state through RPC endpoints. The RPC endpoints for light client nodes can be written in Rust, JavaScript, or other languages and used to read block headers, submit transactions, and view the results of transactions. Light client nodes don't participate in blockchain or network operations. For example, light client nodes aren't responsible for block authoring or validation, gossipping transactions or reaching consensus. The light client node doesn't store any past blocks, so it can't read historical data without requesting it from a node that has it. Node roles Depending on the command-line options you specify when you start a node, nodes can play different roles in the progression of the chain and can provide different levels of access to the on-chain state. For example, you can limit the nodes that are authorized to author new blocks and which nodes can communicate with peers. Peer nodes that aren't authorized as block producers can import new blocks, receive transactions, and send and receive gossip about new transactions to other nodes. Nodes can also be prevented from connecting to the broader network and restricted to communicating with specific nodes. Where to go next You can use Substrate to build virtually any type of network‚Äîfrom a completely self-contained and private solo-chain to your own relay chain ecosystem or compatible parachains. To take a deeper dive into networks and nodes types, explore the following topics. Build a local blockchain Simulate a network Add trusted nodes Authorize specific nodes Last edit: April 3, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Network types Node types Node roles Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
What you can build What can you build What you can build One of the first decisions you need to make in designing your application is the approach you want to use. For example, you need to decide whether your project is best suited to be delivered in the form of a smart contract, an individual pallet, a custom runtime, or a parachain. The decision about what to build will affect almost all of other decisions you'll need to make. To help you make that initial determination about what to build, this section highlights the options available, the differences between them, and the reasons you might select one approach over another. Smart contracts Many developers are familiar with smart contracts and are naturally inclined to think that their project is well-suited to the smart contract model. However, there are both benefits and drawbacks to consider in determining whether the smart contract approach is right for your project. Smart contracts must adhere to blockchain rules Smart contracts are instructions that are deployed on a specific chain and run on a specific chain address. Because smart contracts run on an underlying blockchain that they don't control, they must comply with any rules or limitations that the underlying chain imposes. For example, the underlying blockchain might restrict access to storage or prevent certain types of transactions. In addition, blockchains that accept smart contracts typically treat the code as coming from an untrusted source‚Äîpotentially a malicious actor or inexperienced developer. To prevent untrusted code from disrupting blockchain operations, the underlying blockchain implements native safeguards to limit what a malicious or faulty smart contract can do. For example, the underlying chain might charge fees or enforce metering to ensure that contract developers or users are charged for the computation and storage the contract consumes. The fees and rules for contract execution are at the discretion of the underlying chain. Smart contracts and state You can think of smart contracts as executing in a sandboxed environment. They don't modify the underlying blockchain storage or the storage of other contracts directly. In general, smart contracts only modify their own state and don't make calls to other contracts or to runtime functions. There's typically some additional overhead for running smart contracts to ensure that the underlying blockchain can revert transactions to prevent state from being updated if errors in a contract cause execution to fail. Scenarios for using smart contracts Although there are limitations to smart contracts, there are scenarios where your project might benefit from using smart contracts. For example, smart contracts have lower barrier to entry and can often be built and deployed in a short period of time. The reduced development time might give you an advantage in determining product-to-market fit and iterating quickly. Similarly, if you're familiar with building smart contracts using a language like Solidity, you can reduce the learning curve and time-to-market for your project. Because smart contracts adhere to the functionality of the chain where they are deployed, you can focus more narrowly on implementing the application logic of the contract without worrying about blockchain infrastructure or economics. If you are planning to build a parachain, you can also use smart contracts to prototype features or functionality in an isolated way that doesn't affect the underlying network before investing in a more full-scale solution. If you are a runtime developer, you can incorporate contracts to allow your community to extend and develop features for your runtime without granting them access to the underlying runtime logic. You can also use smart contracts to test future runtime changes. In general, you should consider the following characteristics when deciding whether to build your project using smart contracts: They are inherently safer for the network because safeguards are built into the underlying chain, but you have no control over any restrictions, limitations, or computational overhead imposed by those safeguards. The underlying chains provides the built-in economic incentives against abuse, but the fee and metering system is defined by the underlying chain. They have a lower barrier to entry in terms of code complexity and time to deployment. They can provide an isolated environment for prototyping, testing, and community engagement. They have lower deployment and maintenance overhead because you take advantage of an existing network. The following examples illustrate use cases for smart contracts: Add a derivative to an existing decentralized exchange (DEX). Implement a custom trading algorithm. Define logic for a contract between specific parties. Prototype and test an application before converting it to parachain. Introduce layer-2 tokens and custom assets on an existing chain. Support for smart contracts The Polkadot relay chain doesn't support smart contracts. However, the parachains that connect to Polkadot can support arbitrary state transitions, so any parachain can be a potential platform for smart contract deployment. For example, there are several parachains in the current Polkadot ecosystem that support different types of smart contract deployment. If you plan to develop a smart contract for the Polkadot ecosystem, you must first decide on the type of smart contract you want to build and identify a parachain that supports that type of smart contract. Substrate provides tools to support two types of smart contracts: The contracts pallet in the FRAME library enables a Substrate-based chain to execute smart contracts compiled to WebAssembly regardless of the language used to write the smart contract. The evm pallet in the Frontier project enables a Substrate-based chain to run Ethereum virtual machine (EVM) contracts written in Solidity. Explore smart contracts If your project seems well-suited to be a smart contracts, you can see some simple examples to get you started in the following tutorials: Develop smart contracts Access EVM accounts Individual pallets In some cases, you might want to implement application logic as a standalone pallet and make the functionality available to the community as a library rather than building your own custom runtime. For example, if you don‚Äôt want to deploy and manage an application-specific blockchain, you might build one or more individual pallets to provide features that are broadly useful across all Substrate-based chains, that improve existing functionality, or that define a standard for the Polkadot ecosystem. Individual pallets are typically easy to develop by using FRAME and easy for Substrate chains to integrate. Writing correct code It's worth noting that pallets don't inherently provide any of type of protection or safeguards that smart contracts provide. With pallets, you control the logic available for runtime developers to implement. You provide the methods, storage items, events, and errors that your module requires. Pallets don't inherently introduce a fee or metering system. It is up to you to ensure that your pallet logic doesn't allow bad behavior or leave the network where your pallet is used vulnerable to attacks. This lack of built-in safeguards implies that you have a great deal of responsibility to write code that avoids mistakes. Pallets outside of runtime development Often, writing a pallet is the gateway to runtime development, giving you the opportunity to experiment with existing pallets and coding patterns without building a compete blockchain application. Individual pallets also provide an alternative way you can contribute to a project without writing your own application. Although writing and testing pallets is typically a stepping stone to building larger scale application, there are many examples of the value individual pallets can have to the ecosystem as a whole. Even if you are building a single pallet, you'll need to test it in the context of a runtime. The main disadvantage of developing individual pallets is that you don't have control over any other part of the runtime where they are used. If you treat your pallet as isolated code, you might miss opportunities to enhance or improve it. In addition, changes to FRAME or Substrate can create maintenance issues for your individual pallets if you don't update your code to stay synchronized with those changes. Explore building pallets If your project seems well-suited to be an individual pallet, you can see some simple examples to get you started in the following sections: Custom pallets Build application logic Collectibles workshop Custom runtime In most cases, deciding to build a custom runtime is the critical step toward building and deploying an application-specific parallel blockchain‚Äîa parachain‚Äîas part of the Polkadot ecosystem. By building with Substrate and FRAME, you can develop a fully-customized runtime. With a custom runtime, you have complete control over all aspects of your application, including economic incentives, governance, consensus, and resource management. There are pallets that provide pluggable modules for many of these features. However, it is up to you to decide which modules to use, how to modify them for your needs, and where custom modules are required. Because you control all of the underlying logic that each node in your network runs, there's a higher barrier to entry in terms of coding skill and experience than there is for writing a smart contract or an individual pallet. As with individual pallets, a custom runtime doesn't provide any built-in safeguards to prevent bad actors or incorrect code from causing harm. It is up to you to correctly assess the resource consumption and how to apply transaction fees in the runtime logic to adequately protect the network and your user community. Unlike a smart contract or an individual pallet, a custom runtime is a fully-functioning blockchain. Making the custom runtime accessible and secure for others to use involves acquiring physical or cloud computing resources, building a community that finds value in your service, and managing a network infrastructure. With smart contracts, your application runs on top of an existing execution model, limiting what your application can do. With a custom runtime, you control that underlying execution model and can choose to extend it to support smart contract execution for other developers. With a custom runtime, you can also deliver more complex functionality and user interactions than smart contracts or individual pallets can provide. Explore building a custom runtime If you want to build a more complete custom runtime rather than an individual pallet, you can start with a simple example like the Collectibles workshop . However, if you want to build a custom runtime as a proof-of-concept for a solo chain or a parachain, you'll want a broader and deeper understanding of runtime components and FRAME pallets. The most relevant topics are under Build and Test and in the following sections: Runtime storage structures Transactions, weights, and fees Application development FRAME pallets FRAME macros Parachain A custom runtime can exist on its own as the business logic for a private network or a solo chain, but if you want your project to be a viable production chain, there are several advantages to deploying the business logic and state transition function for your application as a parachain or parathread. Parachains and parathreads act as independent Layer-1 blockchains. Each parachain has its own logic and runs in parallel with other chains in their ecosystem. All of the chains in the ecosystem benefit from the shared security, governance, scalability, and interoperability of the network. Parachains provide maximum flexibility By developing your project as a parachain, you have a great deal of freedom and flexibility in the design and functionality of the chain. What you decide to build is entirely up to you. For example, you can define what data to store on-chain or off. You can define your own economic primitives, transaction requirements, fee policies, governance model, treasury accounts, and access control rules. Your parachain can have as little‚Äîor as much‚Äîoverhead per transaction as you decide, and your parachain can evolve with upgrades and optimization over time. The only requirement is that your parachain or parathread must be compatible with the Polkadot API. Planning parachain resource requirements As a parachain, your project can offer functionality to a broader community in a more secure way that a private chain or a solo chain. However, if you want to build a production-ready parachain, you should keep the following additional requirements in mind: You'll need a development team with sufficient skills and experience, whether that means programming in Rust or a background in UX design. Parachain development might require more resources than other options. You'll need to build your community through marketing, outreach, or ecosystem development programs. You'll need resources for your infrastructure and network maintenance. A parachain is a whole blockchain. Although the relay chain provides security and consensus for your project, you must maintain your chain and network infrastructure. In addition to developer operations (DevOps), you need to secure a parachain slot, design a crowdloan or auction strategy, and accumulate enough resources to extend the slot. You'll need sufficient time for testing and validating your chain operations, in a sandbox or simulated network and on a fully-functioning test network. Parachain use cases In general rule, you should build your project as a parachain if it requires complex operations because parachains provide faster and more efficient execution of transactions. For example, building a parachain might be the best option for the following use cases: Decentralized finance (DeFi) applications Digital wallets Internet of things (IOT) applications Gaming applications Web 3.0 infrastructureM Explore building a parachain If you have a custom runtime that you want to deploy as a parachain to take advantage of the security, governance, and interoperability of the relay chain and the Polkadot or Kusama ecosystem, you can start by building locally and setting up your own test network for initial testing. For some examples to get you started, see the following sections: Connect parachains to a network Simulate parachains in a test network Parachains To learn more about what you can build, explore the following resources: Build with Polkadot Parachain development Smart contracts Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Smart contracts Individual pallets Custom runtime Parachain Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Runtime development Runtime development Runtime development As discussed in Architecture , the runtime for a Substrate node contains all of the business logic for executing transactions, saving state transitions, and interacting with the outer node. Substrate provides all of the tools required to build common blockchain components so you can focus on developing the runtime logic that defines the blockchain behavior. State transitions and the runtime At the most basic level, every blockchain is essentially a ledger or record of each change that takes place on-chain. In Substrate-based chains, these changes to state are recorded in the runtime. Because the runtime handles this operation, the runtime is sometimes described as providing the state transition function . Because state transitions occur in the runtime, the runtime is where you define the storage items that represent the blockchain state and the transactions that allow blockchain users to make changes to this state. The Substrate runtime determines which transactions are valid and invalid and how the chain state is changed in response to transactions. Runtime interfaces As you learned in Architecture , the outer node is responsible for handling peer discovery, transaction pooling, block and transaction gossiping, consensus, and answering RPC calls from the outside world. These tasks frequently require the outer node to query the runtime for information or to provide information to the runtime. The runtime API facilitates this kind of communication between the outer node and the runtime. In Substrate, the sp_api crate provides an interface to implement a runtime API. It is designed to give you flexibility in defining your own custom interfaces using the impl_runtime_apis macro. However, every runtime must implement the Core and Metadata interfaces. In addition to these required interfaces, most Substrate nodes‚Äîlike the node template‚Äîimplement the following runtime interfaces: BlockBuilder for the functionality required to build a block. TaggedTransactionQueue for validating transactions. OffchainWorkerApi for enabling offchain operations. AuraApi for block authoring and validation using a round-robin method of consensus. SessionKeys for generating and decoding session keys. GrandpaApi for block finalization into the runtime. AccountNonceApi for querying transaction indices. TransactionPaymentApi for querying information about transactions. Benchmark for estimating and measuring execution time required to complete transactions. Core primitives Substrate also defines the core primitives that the runtime must implement. The Substrate framework makes minimal assumptions about what your runtime must provide to the other layers of Substrate. However, there are a few data types that must be defined and must fulfill a particular interface to work within the Substrate framework. These core primitives are: Hash : A type which encodes a cryptographic digest of some data. Typically just a 256-bit quantity. DigestItem : A type which must be able to encode one of a number of "hard-wired" alternatives relevant to consensus and change-tracking as well as any number of "soft-coded" variants, relevant to specific modules within the runtime. Digest : A series of DigestItems. This encodes all information that is relevant for a light-client to have on hand within the block. Extrinsic : A type to represent a single piece of data external to the blockchain that is recognized by the blockchain. This typically involves one or more signatures, and some sort of encoded instructions (e.g. for transferring ownership of funds or calling into a smart contract). Header : A type which is representative (cryptographically or otherwise) of all information relevant to a block. It includes the parent hash, the storage root and the extrinsics trie root, the digest and a block number. Block : Essentially just a combination of Header and a series of Extrinsics , together with a specification of the hashing algorithm to be used. BlockNumber : A type which encodes the total number of ancestors any valid block has. Typically a 32-bit quantity. FRAME FRAME is one of the most powerful tools available to you as a runtime developer. As mentioned in Substrate empowers developers , FRAME is an acronym for Framework for Runtime Aggregation of Modularized Entities and it encompasses a significant number of modules and support libraries that simplify runtime development. In Substrate, these modules‚Äîcalled pallets ‚Äîoffer customizable business logic for different use cases and features that you might want to include in your runtime. For example, there are pallets that provide a framework of business logic for staking, consensus, governance, and other common activities. For a summary of the pallets available, see FRAME pallets . In addition to pallets, FRAME provides services to interact with the runtime through the following libraries and modules FRAME system crate frame_system provides low-level types, storage, and functions for the runtime. FRAME support crate frame_support is a collection of Rust macros, types, traits, and modules that simplify the development of Substrate pallets. FRAME executive pallet frame_executive orchestrates the execution of incoming function calls to the respective pallets in the runtime. The following diagram illustrates how FRAME and its system, support, and executives modules provide services for the runtime environment. Composing a runtime with pallets You can build a Substrate-based blockchain without using FRAME. However, FRAME pallets enable you to compose custom runtime logic using predefined components as a starting point. Each pallet defines specific types, storage items, and functions to implement a specific set of features or functionality for a runtime. The following diagram illustrates how you can select and combine FRAME pallets to compose a runtime. Building custom pallets In addition to the library of pre-built FRAME pallets, you can use the FRAME libraries and services to build your own custom pallets. With custom pallets, you have the flexibility to define the runtime behavior that best suits your purposes. Because each pallet has its own discrete logic, you can combine pre-built and custom pallets to control the features and functionality your blockchain provides and achieve the results you want. For example, you might include the Balances pallet in your runtime to use its cryptocurrency-related storage items and functions for managing tokens, but add custom logic to call a pallet you write when an account balance changes. Most pallets are composed with some combination of the following sections: Imports and dependencies Pallet type declaration Runtime configuration trait Runtime storage Runtime events Hooks for logic that should be executed in a specific context Function calls that can be used to execute transactions For example, if you wanted to define a custom pallet, you might start with a skeleton structure for the pallet similar to the following: // Add required imports and dependencies pub use pallet :: * ; #[frame_support::pallet] pub mod pallet { use frame_support :: pallet_prelude :: * ; use frame_system :: pallet_prelude :: * ; // Declare the pallet type // This is a placeholder to implement traits and methods. #[pallet::pallet] #[pallet::generate_store(pub(super) trait Store)] pub struct Pallet < T > ( _ ) ; // Add the runtime configuration trait // All types and constants go here. #[pallet::config] pub trait Config : frame_system :: Config { ... } // Add runtime storage to declare storage items. #[pallet::storage] #[pallet::getter(fn something)] pub type MyStorage < T : Config > = StorageValue < _ , u32 > ; // Add runtime events #[pallet::event] #[pallet::generate_deposit(pub(super) fn deposit_event)] pub enum Event < T : Config > { ... } // Add hooks to define some logic that should be executed // in a specific context, for example on_initialize. #[pallet::hooks] impl < T : Config > Hooks < BlockNumberFor < T >> for Pallet < T > { ... } // Add functions that are callable from outside the runtime. #[pallet::call] impl < T : Config > Pallet < T > { ... } } You can compose pallets with some or all of the sections, as needed. As you start to design and build your custom runtime, you'll learn more about FRAME libraries and the runtime primitives used to define configuration traits, storage items, events, and errors, and how to write the function calls that are dispatched to the runtime for execution. Where to go next Now that you are familiar with the basics of Substrate runtime development and working with pallets, explore the following topics and tutorials to learn more. Frame pallets Add a module to the runtime Rust for Substrate Macro reference Use macros in a custom pallet Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE State transitions and the runtime Runtime interfaces Core primitives FRAME Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Transactions and block basics Transaction types Transactions and block basics In this article, you'll learn about the different types of transactions that you can create and how you can use them in a runtime. Broadly-speaking, transactions determine the data that makes its way into the blocks in your blockchain. By learning how different transaction types are used, you'll be better prepared to select the appropriate type for your needs. What is a transaction? In general, transactions provide a mechanism for making changes to state that can be included in a block. There are three distinct transaction types in Substrate: Signed transactions Unsigned transactions Inherent transactions In Substrate, all three transaction types are often more broadly referred to as extrinsics . The term extrinsic is generally used to mean any information that originates outside of the runtime. However, for practical purposes, it is more useful to consider each transaction type independently and identify scenarios where each type would be most applicable. Signed transactions Signed transactions must include the signature of an account sending an inbound request to execute some runtime call. Typically, the request is signed using the private key for the account that is submitting the request. In most cases, the account submitting the request also pays a transaction fee. However, transaction fees and other elements of transaction processing depend on how the runtime logic is defined. Signed transactions are the most common type of transaction. As an example, assume you have an account with some number of tokens. If you want to transfer tokens to Alice, you can call the pallet_balances::Call::transfer function in the Balances pallet. Because your account is used to make this call, your account key is used to sign the transaction. As the requester, you would typically be responsible for paying a fee to have your request processed. Optionally, you could also tip the block author to give your transaction higher priority. Unsigned transactions Unsigned transactions don't require a signature and don't include any information about who submitted the transaction. With an unsigned transaction, there's no economic deterrent to prevent spam or replay attacks. You must define the conditions for validating unsigned transactions and the logic required to protect the network from misuse and attacks. Because unsigned transactions require custom validation, this transaction type consumes more resources than a signed transaction. The pallet_im_online::Call::heartbeat function uses unsigned transactions to enable validator nodes to send a signal to the network to indicate that the node is online. This function can only be called by a node that's registered as a validator in the network. The function includes internal logic to verify that the node is a validator, allowing the node to call the function using an unsigned transaction to avoid paying any fees. Inherent transactions Inherent transactions‚Äîsometimes referred to as inherents‚Äîare a special type of unsigned transaction. With this type of transaction, block authoring nodes can add information directly to a block. Inherent transactions can only be inserted into a block by the block authoring node that calls them. Typically, this type of transaction is not gossiped to other nodes or stored in the transaction queue. The data inserted using an inherent transaction is assumed to be valid without requiring specific validation. For example, if a block authoring node inserts a timestamp into a block, there is no way to prove that a timestamp is accurate. Instead, validators might accept or reject the block based on whether the timestamp it is within some acceptable range of their own system clocks. As an example, the pallet_timestamp::Call::now function enables a block authoring node to insert a current timestamp in each block the node produces. Similarly, the paras_inherent::Call::enter function enables a parachain collator node to send its relay chain the validation data the relay chain expects. What is a block? In Substrate, a block consists of a header and an array of transactions. The header contains the following properties: Block height Parent hash Transaction root State root Digest All of the transactions are bundled together as a series to be executed as defined in the runtime. You'll learn more about transaction ordering in Transaction lifecycle . The transaction root is a cryptographic digest of this series. This cryptographic digest serves two purposes: It prevents any alterations to the series of transactions after the header has been built and distributed. It enables light clients to succinctly verify that any given transaction exists in a block given only knowledge of the header. Where to go next Now that you are familiar with transaction types and the information that constitutes a block, explore the following topics to learn more. Transaction lifecycle State transitions and storage Transactions, weights, and fees Transaction format Block reference Last edit: April 3, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE What is a transaction? What is a block? Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Transaction lifecycle Transaction lifecycle Transaction lifecycle In Substrate, transactions contain data to be included in a block. Because the data in transactions originates outside of the runtime, transactions are sometimes more broadly referred to as extrinsic data or as extrinsics . However, the most common extrinsics are signed transactions . Therefore, this discussion of the transaction lifecycle focuses on how signed transactions are validated and executed. You've already learned that signed transactions include the signature of the account sending the request to execute some runtime call. Typically, the request is signed using the private key for the account that is submitting the request. In most cases, the account submitting the request also pays a transaction fee. However, transaction fees and other elements of transaction processing depend on how the runtime logic is defined. Where transactions are defined As discussed in Runtime development , the Substrate runtime contains the business logic that defines transaction properties, including: What constitutes a valid transaction. Whether the transactions are sent as signed or unsigned. How transactions change the state of the chain. Typically, you use pallets to compose the runtime functions and to implement the transactions that you want your chain to support. After you compile the runtime, users interact with the blockchain to submit requests that are processed as transactions. For example, a user might submit a request to transfer funds from one account to another. The request becomes a signed transaction that contains the signature for that user account and if there are sufficient funds in the user's account to pay for the transaction, the transaction executes successfully, and the transfer is made. How transactions are processed on a block authoring node Depending on the configuration of your network, you might have a combination of nodes that are authorized to author blocks and nodes that are not authorized for block authoring. If a Substrate node is authorized to produce blocks, it can process the signed and unsigned transactions it receives. The following diagram illustrates the lifecycle of a transaction that's submitted to a network and processed by an authoring node. Any signed or unsigned transaction that's sent to a non-authoring node is gossiped to other nodes in the network and enter their transaction pool until it is received by an authoring node. Validating and queuing transactions As discussed in Consensus , a majority of nodes in the network must agree on the order of transactions in a block to agree on the state of the blockchain and to continue securely adding blocks. To reach consensus, two-thirds of the nodes must agree on the order of the transactions executed and the resulting state change. To prepare for consensus, transactions are first validated and queued on the local node in a transaction pool . Validating transactions in the transaction pool Using rules that are defined in the runtime, the transaction pool checks the validity of each transaction. The checks ensure that only valid transactions that meet specific conditions are queued to be included in a block. For example, the transaction pool might perform the following checks to determine whether a transaction is valid: Is the transaction index‚Äîalso referred to as the transaction nonce‚Äîcorrect? Does the account used to sign the transaction have enough funds to pay the associated fees? Is the signature used to sign the transaction valid? After the initial validity check, the transaction pool periodically checks whether existing transactions in the pool are still valid. If a transaction is found to be invalid or has expired, it is dropped from the pool. The transaction pool only deals with the validity of the transaction and the ordering of valid transactions placed in a transaction queue. Specific details on how the validation mechanism works‚Äîincluding handling for fees, accounts, or signatures‚Äîcan be found in the validate_transaction method. Adding valid transactions to a transaction queue If a transaction is identified as valid, the transaction pool moves the transaction into a transaction queue. There are two transaction queues for valid transactions: The ready queue contains transactions that can be included in a new pending block. If the runtime is built with FRAME, transactions must follow the exact order that they are placed in the ready queue. The future queue contains transactions that might become valid in the future. For example, if a transaction has a nonce that is too high for its account, it can wait in the future queue until the appropriate number of transactions for the account have been included in the chain. Invalid transaction handling If a transaction is invalid‚Äîfor example, because it is too large or doesn't contain a valid signature‚Äîit is rejected and won't be added to a block. A transaction might be rejected for any of the following reasons: The transaction has already been included in a block so it is dropped from the verifying queue. The transaction's signature is invalid, so it is immediately be rejected. The transaction is too large to fit in the current block, so it is be put back in a queue for a new verification round. Transactions ordered by priority If a node is the next block author, the node uses a priority system to order the transactions for the next block. The transactions are ordered from high to low priority until the block reaches the maximum weight or length. Transaction priority is calculated in the runtime and provided to the outer node as a tag on the transaction. In a FRAME runtime, a special pallet is used to calculate priority based on the weights and fees associated with the transaction. This priority calculation applies to all types of transactions with the exception of inherents. Inherents are always placed first using the EnsureInherentsAreFirst trait. Account-based transaction ordering If your runtime is built with FRAME, every signed transaction contains a nonce that is incremented every time a new transaction is made by a specific account. For example, the first transaction from a new account has nonce = 0 and the second transaction for the same account has nonce = 1 . The block authoring node can use the nonce when ordering the transactions to include in a block. For transactions that have dependencies, the ordering takes into account the fees that the transaction pays and any dependency on other transactions it contains. For example: If there is an unsigned transaction with TransactionPriority::max_value() and another signed transaction, the unsigned transaction is placed first in the queue. If there are two transactions from different senders, the priority determines which transaction is more important and should be included in the block first. If there are two transactions from the same sender with an identical nonce : only one transaction can be included in the block, so only the transaction with the higher fee is included in the queue. Executing transactions and producing blocks After valid transactions are placed in the transaction queue, a separate executive module orchestrates how transactions are executed to produce a block. The executive module calls functions in the runtime modules and executes those functions in specific order. As a runtime developer, it's important to understand how the executive module interacts with the system pallet and the other pallets that compose the business logic for your blockchain because you can insert logic for the executive module to perform as part of the following operations: Initializing a block Executing the transactions to be included in a block Finalizing block building Initialize a block To initialize a block, the executive module first calls the on_initialize function in the system pallet and then in all other runtime pallets. The on_initialize function enables you to define business logic that should be completed before transactions are executed. The system pallet on_initialize function is always executed first. The remaining pallets are called in the order they are defined in the construct_runtime! macro. After all of on_initialize functions have been executed, the executive module checks the parent hash in the block header and the trie root to verify that the information is correct. Executing transactions After the block has been initialized, each valid transaction is executed in order of transaction priority. It is important to remember that the state is not cached prior to execution. Instead, state changes are written directly to storage during execution. If a transaction were to fail mid-execution, any state changes that took place before the failure would not be reverted, leaving the block in an unrecoverable state. Before committing any state changes to storage, the runtime logic should perform all necessary checks to ensure the extrinsic will succeed. Note that events are also written to storage. Therefore, the runtime logic should not emit an event before performing the complementary actions. If a transaction fails after an event is emitted, the event is not be reverted. Finalizing a block After all queued transactions have been executed, the executive module calls into each pallet's on_idle and on_finalize functions to perform any final business logic that should take place at the end of the block. The modules are again executed in the order that they are defined in the construct_runtime! macro, but in this case, the on_finalize function in the system pallet is executed last. After all of the on_finalize functions have been executed, the executive module checks that the digest and storage root in the block header match what was calculated when the block was initialized. The on_idle function also passes through the remaining weight of the block to allow for execution based on the usage of the blockchain. Block authoring and block imports So far, you have seen how transactions are included in a block produced by the local node. If the local node is authorized to produce blocks, the transaction lifecycle follows a path like this: The local node listens for transactions on the network. Each transaction is verified. Valid transactions are placed in the transaction pool. The transaction pool orders the valid transactions in the appropriate transaction queue and the executive module calls into the runtime to begin the next block. Transactions are executed and state changes are stored in local memory. The constructed block is published to the network. After the block is published to the network, it is available for other nodes to import. The block import queue is part of the outer node in every Substrate node. The block import queue listens for incoming blocks and consensus-related messages and adds them to a pool. In the pool, incoming information is checked for validity and discarded if it isn't valid. After verifying that a block or message is valid, the block import queue imports the incoming information into the local node's state and adds it to the database of blocks that the node knows about. In most cases, you don't need to know details about how transactions are gossiped or how blocks are imported by other nodes on the network. However, if you plan to write any custom consensus logic or want to know more about the implementation of the block import queue, you can find details in the Rust API documentation. ImportQueue Link BasicQueue Verifier BlockImport Where to go next Seminar: Lifecycle of a transaction Accounts, addresses, and keys Last edit: April 3, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Where transactions are defined How transactions are processed on a block authoring node Validating and queuing transactions Transactions ordered by priority Executing transactions and producing blocks Block authoring and block imports Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
State transitions and storage State transitions and storage State transitions and storage Substrate uses a simple key-value data store implemented as a database-backed, modified Merkle tree. All of Substrate's higher-level storage abstractions are built on top of this simple key-value store. Key-Value database Substrate implements its storage database with RocksDB , a persistent key-value store for fast storage environments. It also supports an experimental Parity DB . The DB is used for all the components of Substrate that require persistent storage, such as: Substrate clients Substrate light-clients Off-chain workers Trie abstraction One advantage of using a simple key-value store is that you are able to easily abstract storage structures on top of it. Substrate uses a Base-16 Modified Merkle Patricia tree ("trie") from paritytech/trie to provide a trie structure whose contents can be modified and whose root hash is recalculated efficiently. Tries allow efficient storing and sharing of the historical block state. The trie root is a representation of the data within the trie; that is, two tries with different data will always have different roots. Thus, two blockchain nodes can easily verify that they have the same state by simply comparing their trie roots. Accessing trie data is costly. Each read operation takes O(log N) time, where N is the number of elements stored in the trie. To mitigate this, we use a key-value cache. All trie nodes are stored in the DB and part of the trie state can get pruned, i.e. a key-value pair can be deleted from storage when it is out of pruning range for non-archive nodes. We do not use reference counting for performance reasons. State trie Substrate-based chains have a single main trie, called the state trie, whose root hash is placed in each block header. This is used to easily verify the state of the blockchain and provide a basis for light clients to verify proofs. This trie only stores content for the canonical chain, not forks. There is a separate state_db layer that maintains the trie state with references counted in memory for all that is non-canonical. Child trie Substrate also provides an API to generate new child tries with their own root hashes that can be used in the runtime. Child tries are identical to the main state trie, except that a child trie's root is stored and updated as a node in the main trie instead of the block header. Since their headers are a part of the main state trie, it is still easy to verify the complete node state when it includes child tries. Child tries are useful when you want your own independent trie with a separate root hash that you can use to verify the specific content in that trie. Subsections of a trie do not have a root-hash-like representation that satisfy these needs automatically; thus a child trie is used instead. Querying storage Blockchains that are built with Substrate expose a remote procedure call (RPC) server that can be used to query runtime storage. When you use the Substrate RPC to access a storage item, you only need to provide the key associated with that item. Substrate's runtime storage APIs expose a number of storage item types; keep reading to learn how to calculate storage keys for the different types of storage items. Storage value keys To calculate the key for a simple Storage Value , take the TwoX 128 hash of the name of the pallet that contains the Storage Value and append to it the TwoX 128 hash of the name of the Storage Value itself. For example, the Sudo pallet exposes a Storage Value item named Key : twox_128 ( "Sudo" ) = "0x5c0d1176a568c1f92944340dbfed9e9c" twox_128 ( "Key" ) = "0x530ebca703c85910e7164cb7d1c9e47b" twox_128 ( "Sudo" ) + twox_128 ( "Key" ) = "0x5c0d1176a568c1f92944340dbfed9e9c530ebca703c85910e7164cb7d1c9e47b" If the familiar Alice account is the sudo user, an RPC request and response to read the Sudo pallet's Key Storage Value could be represented as: state_getStorage ( "0x5c0d1176a568c1f92944340dbfed9e9c530ebca703c85910e7164cb7d1c9e47b" ) = "0xd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d" In this case, the value that is returned ( "0xd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d" ) is Alice's SCALE -encoded account ID ( 5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY ). You may have noticed that the non-cryptographic TwoX 128 hash algorithm is used to generate Storage Value keys. This is because it is not necessary to pay the performance costs associated with a cryptographic hash function since the input to the hash function (the names of the pallet and storage item) are determined by the runtime developer and not by potentially malicious users of your blockchain. Storage map keys Like Storage Values, the keys for Storage Maps are equal to the TwoX 128 hash of the name of the pallet that contains the map prepended to the TwoX 128 hash of the name of the Storage Map itself. To retrieve an element from a map, append the hash of the desired map key to the storage key of the Storage Map. For maps with two keys (Storage Double Maps), append the hash of the first map key followed by the hash of the second map key to the Storage Double Map's storage key. Like Storage Values, Substrate uses the TwoX 128 hashing algorithm for the pallet and Storage Map names, but you will need to make sure to use the correct hashing algorithm (the one that was declared in the #[pallet::storage] macro ) when determining the hashed keys for the elements in a map. Here is an example that illustrates querying a Storage Map named FreeBalance from a pallet named Balances for the balance of the Alice account. In this example, the FreeBalance map is using the transparent Blake2 128 Concat hashing algorithm : twox_128 ( "Balances" ) = "0xc2261276cc9d1f8598ea4b6a74b15c2f" twox_128 ( "FreeBalance" ) = "0x6482b9ade7bc6657aaca787ba1add3b4" scale_encode ( "5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY" ) = "0xd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d" blake2_128_concat ( "0xd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d" ) = "0xde1e86a9a8c739864cf3cc5ec2bea59fd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d" state_getStorage ( "0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4de1e86a9a8c739864cf3cc5ec2bea59fd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d" ) = "0x0000a0dec5adc9353600000000000000" The value that is returned from the storage query ( "0x0000a0dec5adc9353600000000000000" in the example above) is the SCALE -encoded value of Alice's account balance ( "1000000000000000000000" in this example). Notice that before hashing Alice's account ID it has to be SCALE-encoded. Also notice that the output of the blake2_128_concat function consists of 32 hexadecimal characters followed by the function's input. This is because the Blake2 128 Concat is a transparent hashing algorithm . Although the above example may make this characteristic seem superfluous, its utility becomes more apparent when the goal is to iterate over the keys in a map (as opposed to retrieving the value associated with a single key). The ability to iterate over the keys in a map is a common requirement in order to allow people to use the map in a way that seems natural (such as UIs): first, a user is presented with a list of elements in the map, then, that user can select the element that they are interested in and query the map for more details about that particular element. Here is another example that uses the same example Storage Map (a map named FreeBalances that uses a Blake2 128 Concat hashing algorithm in a pallet named Balances that demonstrates using the Substrate RPC to query a Storage Map for its list of keys via the state_getKeys RPC endpoint: twox_128 ( "Balances" ) = "0xc2261276cc9d1f8598ea4b6a74b15c2f" twox_128 ( "FreeBalance" ) = "0x6482b9ade7bc6657aaca787ba1add3b4" state_getKeys ( "0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4" ) = [ "0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4de1e86a9a8c739864cf3cc5ec2bea59fd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d" , "0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b432a5935f6edc617ae178fef9eb1e211fbe5ddb1579b72e84524fc29e78609e3caf42e85aa118ebfe0b0ad404b5bdd25f" , ... ] Each element in the list that is returned by the Substrate RPC's state_getKeys endpoint can be directly used as input for the RPC's state_getStorage endpoint. In fact, the first element in the example list above is equal to the input used for the state_getStorage query in the previous example (the one used to find the balance for Alice ). Because the map that these keys belong to uses a transparent hashing algorithm to generate its keys, it is possible to determine the account associated with the second element in the list. Notice that each element in the list is a hexadecimal value that begins with the same 64 characters; this is because each list element represents a key in the same map, and that map is identified by concatenating two TwoX 128 hashes, each of which are 128-bits or 32 hexadecimal characters. After discarding this portion of the second element in the list, you are left with 0x32a5935f6edc617ae178fef9eb1e211fbe5ddb1579b72e84524fc29e78609e3caf42e85aa118ebfe0b0ad404b5bdd25f . You saw in the previous example that this represents the Blake2 128 Concat hash of some SCALE -encoded account ID. The Blake 128 Concat hashing algorithm consists of appending (concatenating) the hashing algorithm's input to its Blake 128 hash. This means that the first 128 bits (or 32 hexadecimal characters) of a Blake2 128 Concat hash represents a Blake2 128 hash, and the remainder represents the value that was passed to the Blake 2 128 hashing algorithm. In this example, after you remove the first 32 hexadecimal characters that represent the Blake2 128 hash (i.e. 0x32a5935f6edc617ae178fef9eb1e211f ) what is left is the hexadecimal value 0xbe5ddb1579b72e84524fc29e78609e3caf42e85aa118ebfe0b0ad404b5bdd25f , which is a SCALE -encoded account ID. Decoding this value yields the result 5GNJqTPyNqANBkUVMN1LPPrxXnFouWXoe2wNSmmEoLctxiZY , which is the account ID for the familiar Alice_Stash account. Runtime storage API Substrate's FRAME Support crate provides utilities for generating unique, deterministic keys for your runtime's storage items. These storage items are placed in the state trie and are accessible by querying the trie by key . Where to go next Runtime storage Type encoding (SCALE) Last edit: April 3, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Key-Value database Trie abstraction Querying storage Runtime storage API Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Accounts, addresses, and keys Accounts addresses keys Accounts, addresses, and keys An account represents an identity‚Äîusually of a person or an organization‚Äîthat is capable of making transactions or holding funds. Although accounts are most often used to represent a person, that doesn't have to be the case. An account can be used to perform operations on behalf of a user or another entity, or to perform operations autonomously. In addition, any single person or entity could have multiple accounts for different purposes. For example, Polkadot is a Substrate-based blockchain that has specialized accounts for holding funds that are separate from accounts used for making transactions. How you implement and use accounts is entirely up to you as a blockchain or parachain developer. Public and private keys In general, every account has an owner who possesses a public and private key pair. The private key is a cryptographically-secure sequence of randomly-generated numbers. For human readability, the private key generates a random sequence of words called a secret seed phrase or mnemonic . The secret seed phrase is important because it can be used to recover access to an account if the private key is lost. For most networks, the public key associated with an account is how that account is identified on the network and some form of it is used as the destination address for transactions. However, Substrate-based chains use the underlying public key to derive one or more public addresses . Instead of using the public key directly, Substrate allows you generate multiple addresses and address formats for an account. Address encoding and chain-specific addresses By deriving multiple addresses from a single public key, you can interact with multiple chains without creating separate public and private key pairs for each network. By default, the addresses associated with the public key for an account use the Substrate SS58 address format . The SS58 address format is an enhanced version of base-58 encoding . The important characteristics of the SS58 address format include: Encoded addresses consist of 58 alphanumeric characters, resulting in a shorter and more identifiable address than a hex-encoded address. Addresses don't use characters that can be difficult to distinguish from each other in a string. For example, the characters 0 , O , I , and l aren't used in SS58 addresses. Addresses can include a network-specific prefixes so you can use the same public key to derive addresses for different chains. Addresses can use derivation paths to create multiple addresses from the same public key so you can use different addresses for different purposes. For example, you can create sub-accounts for separating funds or executing specific types of transactions. Addresses can be verified using a checksum to prevent input errors. Inspecting network-specific addresses Because a single public key can be used to derive addresses for different Substrate chains, a single account can have multiple chain-specific addresses. For example, if you inspect the addresses for the alice account public key 0xd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d depends on the chain-specific address type. Chain address type Address Polkadot (SS58) 15oF4uVJwmo4TdGW7VfQxNLavjCXviqxT9S1MgbjMNHr6Sp5 Kusama (SS58) HNZata7iMYWmk5RvZRTiAsSDhV8366zq2YGb3tLH5Upf74F Generic Substrate chain (SS58) 5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY Registering for a network-specific address Each Substrate-based blockchain can register a custom prefix to create a network-specific address type. For example, all Polkadot addresses start with 1 . All Kusama addresses start with a capital letter. All unregistered Substrate chains start with 5 . You can look up the network-specific address for a public key using the subkey inspect command and --network command-line option or by using Subscan . For information about generating public and private key pairs and inspecting addresses, see subkey . For information about registering for a chain-specific address, see the instructions in the SS58 repository . Account information in FRAME Conceptually, accounts represent identities that have a public/private key pair with one or more public addresses. However, in a runtime built with FRAME, an account is defined as a storage map with a 32-byte address identifier and corresponding account information, such as the number of transactions the account has made, the number of modules that depend on the account, and the account balance. The account properties‚Äîsuch as the AccountId ‚Äîcan be defined generically in the frame_system module. The generic type is then resolved as a specific type in the runtime implementation, and eventually assigned a specific value. For example, the Account type in FRAME relies on an associated AccountId type. The AccountId type remains a generic type until it is assigned a type in the runtime implementation for a pallet that needs this information. For more information about how accounts are defined in the frame_system pallet and the account properties in the Account storage map, see Account data structures . For more information about working with generic types, see Rust for Substrate . Specialized accounts As a flexible and module framework for blockchain development, Substrate itself doesn't require you define or use any specific type of accounts. However, different chains can implement different rules for how accounts and the keys that control them are used. For example, you might implement specialized accounts if your application requires: custom cryptographic schemes complex or multi-user signing rules restricted access to specific functions restricted access to specific pallets In most cases, specialized accounts are implemented in the context of a specific FRAME pallet, either in a prebuilt pallet like Staking or Multisig or in custom pallets that you design. For example, the Staking pallet takes an originating FRAME system account that wants to put up a bond and generates the stash and controller account abstractions to identify the account required to perform specific operations. You can see the implementation of these account abstractions in the Polkadot ecosystem. However, you can use the same framework to implement different account rules or account types or as inspiration for a custom pallet with its own account abstractions. Multi-signature accounts Typically, an account has one and only one owner and that owner holds the private key for signing transactions. The Multisig pallet enables you to configure a specialized account for executing transactions that multiple account owners must approve. The multisig account is an address that has a public key, but no private key. The public address for the multisig account is derived from a deterministic list of the authorized account signatories and an associated transaction request block height and extrinsic index identifier. The Multisig pallet enables multiple parties to share responsibility for executing certain transactions. Any account holder can specify the accounts that are allowed to approve a multi-signature transaction and the minimum number of approvals required for a call to be dispatched to the runtime. Proxy and keyless accounts The Proxy pallet provides another way you can configure specialized accounts for a Substrate-based chain using FRAME. With proxy accounts, primary account owners can designate one or more other accounts to act on their behalf. Proxy accounts can be used to add a layer of security by isolating primary account funds from accounts assigned to specific roles that can complete tasks on behalf of the primary account. By configuring one or more proxy account, an account owner can do the following: Specify up to a maximum number of proxy accounts that are allowed to submit transactions on behalf of a primary account owner. Configure time delays for transactions to be executed by each proxy. Set restrictions on the types of transactions that each proxy can issue. Announce transactions that are to be executed by a proxy before the transactions are executed. Cancel or reject announced transactions that are to be executed by a proxy. Create anonymous‚Äîpure proxy‚Äîaccounts that have no private key and can act without account ownership through their own configured proxies. Runtime implementation Although the Proxy pallet provides this framework for configuring proxy accounts, the implementation details are up to you as a runtime developer. For example, the default Proxy pallet filters the calls a proxy account can dispatch based on the proxy type. However, the runtime implementation defines the proxy types and the transactions that each proxy type is allowed to execute. Polkadot enables you to restrict transactions for a proxy account using the following proxy types: Any NonTransfer Governance Staking IdentityJudgement CancelProxy Auction The enumerated list of proxy types and the logic for matching proxy types to transaction is defined in the Polkadot runtime . Anonymous proxy account The anonymous or pure proxy account is a special type of proxy account with a randomly-generated address and no corresponding private key. Typically, you create this type of proxy account if you want to delegate permissions to an account that can dispatch function calls without your intervention and without access to your keys. After the new account with the delegated permissions is created, the account can be used as a recipient to burn funds or to hold tokens awaiting the execution of a transfer. Where to go next In Substrate, accounts require a public key and a private key to receive funds, sign transactions, and execute transactions. At a high level, there are three types of accounts: User accounts that enable end users to interact with the blockchain. Network accounts that provide additional security for validators and nominators by restricting some operations for staking and governance. Pallet accounts that are executed by valid origins to perform pallet-specific operations. For more information about working with accounts, addresses, and keys, see the following resources: SS58 trait implementation SS58 registry Command reference: subkey Account data structures Cryptography Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Public and private keys Address encoding and chain-specific addresses Account information in FRAME Specialized accounts Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Rust for Substrate Rust basics Rust for Substrate Much of what makes Substrate a flexible and extensible framework for creating mission-critical software is owed to Rust . Being the language of choice for Substrate, Rust is a highly performant programming language and a first choice for the following reasons: Rust is fast: it's statically typed at compile time, making it possible for the compiler to optimize the code for speed and for developers to optimize for a specific compilation target. Rust is portable: it's designed to run on embedded devices with support for any type of operating system. Rust is memory safe: it has no garbage collector and it checks each and every variable you use and every memory address you reference to avoid any memory leaks. Rust is Wasm first: it has first class support for compiling to WebAssembly. Rust in Substrate In the Architecture section, you will learn that Substrate is made of two distinct architectural components: the outer node and the runtime. While more complex features in Rust such as multithreading and asynchronous Rust are used in the outer node code, they aren't directly exposed to runtime engineers, making it easier for runtime engineers to focus on the business logic of their node. Generally, depending on their focus, developers should expect to know: Basic Rust idioms , working with no_std and what macros are used and why (for runtime engineering). Asynchronous Rust (for more advanced developers working with outer node (client) code). Although general familiarity with Rust is essential before diving into Substrate‚Äîand there are many resources available for learning Rust, including the Rust Language Programming Book and Rust by Example ‚Äîthe remainder of this section highlights ways Substrate uses some of Rust's core features for developers getting started with runtime engineering. Compilation target When building a Substrate node, we use the wasm32-unknown-unknown compilation target which means that Substrate runtime engineers are constrained to writing runtimes that must compile to Wasm. This implies that you can't rely on some typical standard library types and functions and must only use no_std compatible crates for the majority of runtime code. Substrate has a lot of its of own primitive types and associated traits that make it possible to work around the no_std requirement. Macros As you learn how to use and write FRAME pallets, you'll see there are many macros available as reusable code to abstract common tasks or enforce runtime-specific requirements. These macros allow you to focus on writing idiomatic Rust and application-specific logic instead of the common code required to interact with a runtime. Rust macros are a powerful tool to help ensure certain requirements are met (without re-writing code) such as the logic to be formatted in a specific way, specific checks are made, or some logic consists of specific data structures. This is especially useful to help developers write code that can integrate with the complexity of a Substrate runtime. For example, the #[frame_system::pallet] macro is required in all FRAME pallets to help you correctly implement certain required attributes-such as storage items or externally callable functions-and make it compatible with the build process in construct_runtime . Developing Substrate runtimes involves heavy use of Rust's attribute macros, which come in two flavors: derive attributes and custom attributes. When you're getting started with Substrate, it isn't so important to know exactly how they work, but rather to know that they exist that they empower you to write correct runtime code. Derive attributes are useful for custom runtime types that need to satisfy certain traits, for instance, to have types be decodable by a node during runtime execution. Other attribute like macros are also common throughout Substrate's codebase for: Specifying whether a code snippet only compiles to no_std or can use the std library. Building custom FRAME pallets. Specifying the way the runtime is built. Generics and configuration traits Often compared to interfaces in languages like Java, traits in Rust provide ways to give advanced functionality to a type. If you've read about pallets, you've probably noticed that every pallet has a Config trait which allows you to define the types and interfaces a pallet depends on. This trait itself inherits a number of core runtime types from the frame_system::pallet::Config trait, making it easy to access common types when writing runtime logic. In addition, in any FRAME pallet the Config trait is generic over T (more on generics in the next section). Some common examples of these core runtime types could be T::AccountId , the common type for identifying user accounts in the runtime or T::BlockNumber , the block number type used by the runtime. For more information about generic types and traits in Rust, see the sections on Generic Types , Traits and Advanced traits from the Rust book. With Rust generics, Substrate runtime developers can write pallets that are completely agnostic to specific implementation details and therefore make full use of Substrate's flexibility, extensibility and modularity. All types in the Config trait are defined generically using trait bounds and made concrete in the runtime implementation. This not only means that you can write pallets that support different specifications for the same type (e.g. addresses for both Substrate and Ethereum chains), but you can also customize generic implementations to your needs with minimal overhead (e.g. change block number to u32 ). This gives developers the flexibility of writing code that makes no assumptions about the core blockchain architecture decisions you have made. Substrate maximizes the use of generic types to provide maximum flexibility. You define how the generic types are resolved to suit your purpose. For more information about generic types and traits in Rust, see the sections on Generic Types from the Rust book. Where to go next Now that you know how Substrate relies on a few key Rust features‚Äîlike traits, generic types, and macros‚Äîyou can explore the following resources to learn more. Rust book Why Rust? (blog by Parity) Cargo and crates.io Why Rust for smart contracts? (ink! documentation) Last edit: April 3, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Rust in Substrate Compilation target Macros Generics and configuration traits Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Offchain operations Offchain operations Offchain operations There are many use cases where you might want to query data from an offchain source or process data without using on-chain resources before updating the on-chain state. The conventional way of incorporating offchain data involves connecting to oracles to supply the data from some traditional source. Although using oracles is one approach to working with offchain data sources, there are limitations to the security, scalability, and infrastructure efficiency that oracles can provide. To make the offchain data integration more secure and efficient, Substrate supports offchain operations through the following features: Offchain workers are a subsystem of components that enable the execution of long-running and possibly non-deterministic tasks, such as: website service requests encryption, decryption, and signing of data random number generation CPU-intensive computations enumeration or aggregation of on-chain data Offchain workers enable you to move tasks that might require more time to execute than allowed out of the block processing pipeline. Any task that might take longer than the maximum block execution time allowed is a reasonable candidate for offchain processing. Offchain storage is storage that is local to a Substrate node and can be accessed by both offchain workers and on-chain logic: Offchain workers have both read and write access to offchain storage. On-chain logic has write access through offchain indexing but doesn't have read access. The offchain storage allows different worker threads to communicate with each other and to store user-specific or node-specific data that does not require consensus over the whole network. Offchain indexing is an optional service that allows the runtime to write directly to the offchain storage independently from offchain workers. The offchain index provides temporary storage for on-chain logic and complements the on-chain state. Off-chain workers Offchain workers run in their own Wasm execution environment outside of the Substrate runtime. This separation of concerns makes sure that block production is not impacted by long-running offchain tasks. However, because offchain workers are declared in the same code as the runtime, they can easily access on-chain state for their computations. Offchain workers have access to extended APIs for communicating with the external world: Ability to submit transactions ‚Äîeither signed or unsigned‚Äîto the chain to publish computation results. A fully-featured HTTP client allowing the worker to access and fetch data from external services. Access to the local keystore to sign and verify statements or transactions. An additional, local key-value database shared between all offchain workers. A secure, local entropy source for random number generation. Access to the node's precise local time . The ability to sleep and resume work. Note that the results from offchain workers are not subject to regular transaction verification. Therefore, you should ensure the offchain operation includes a verification method to determine what information gets into the chain. For example, you might verify offchain transactions by implementing a mechanism for voting, averaging, or checking sender signatures. You should also note that offchain workers don't have any specific privileges or permissions by default and, therefore, represent a potential attack vector that a malicious user could exploit. In most cases, checking whether a transaction was submitted by an offchain worker before writing to storage isn't sufficient to protect the network. Instead of assuming that the offchain worker can be trusted without safeguards, you should intentionally set restrictive permissions that limit access to the process and what it can do. Offchain workers are spawned during each block import. However, they aren't executed during initial blockchain synchronization. Offchain storage Offchain storage is always local to a Substrate node and is not shared on-chain with any other blockchain nodes or subject to consensus. You can access the data stored in the offchain storage using offchain worker threads that have read and write access or through the on-chain logic using offchain indexing. Because an offchain worker thread is spawned during each block import, there can be multiple offchain worker threads running at any given time. As with any multi-threaded programming environment, there are utilities to mutex lock the offchain storage when offchain worker threads access it to ensure data consistency. Offchain storage serves as a bridge for offchain worker threads to communicate to each other and for communication between offchain and on-chain logic. It can also be read using remote procedure calls (RPC) so it fits the use case of storing indefinitely growing data without over-consuming the on-chain storage. Offchain indexing In the context of a blockchain, storage is most often concerned with the on-chain state. However, on-chain state is expensive because it must be agreed upon and populated to multiple nodes in the network. Therefore, you shouldn't store historical or user-generated data‚Äîwhich grow indefinitely over time‚Äîusing on-chain storage. To address the need to access historical or user-generated data, Substrate provides access to the offchain storage using offchain indexing. Offchain indexing allows the runtime to write directly to the offchain storage without using offchain worker threads. You can enable this functionality to persist data by starting a Substrate node with the --enable-offchain-indexing command-line option. Unlike offchain workers, offchain indexing populates the offchain storage every time a block is processed. By populating the data at every block, offchain indexing ensures that the data is always consistent and is exactly the same for every node running with indexing enabled. Where to go next Now that you are familiar with how offchain workers, offchain storage, and offchain indexing enable you to work with data not stored on-chain, you might want to explore the following examples of offchain workers and how to use them in runtime development: Example: Offchain worker Example: Submit transactions Example: Use HTTP requests to fetch data Example: Offchain storage Example: Offchain indexing Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Off-chain workers Offchain storage Offchain indexing Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Light clients in Substrate Connect Light clients in Substrate connect Light clients in Substrate Connect Typically, the nodes that provide peer-to-peer networking for a blockchain require significant resources, including powerful high-speed processors and high-capacity storage devices. By contrast, light client nodes can synchronize data from the blockchain while running in resource-constrained environments and embedded in applications. With a light client node, you can interact with a blockchain in a secure and decentralized way without investing in the high-powered hardware and network capacity that running a full node would require. Light clients in the JavaScript ecosystem For Substrate-based chains, the light client node is implemented as a WebAssembly client‚Äîcalled smoldot ‚Äîthat can run in a browser and interact with the chain using JSON-RPC calls. To make the smoldot WebAssembly light client easier to integrate with JavaScript and TypeScript applications, there's a JavaScript package that's built on top of the smoldot source called Substrate Connect. Substrate Connect is available as a Node.js package that can be installed with the npm package manager. The Substrate Connect package enables the light client node to be integrated with applications in the JavaScript ecosystem. After adding Substrate Connect to an application, the application can communicate with the light client‚Äîand access blockchain data‚Äîthrough JSON-RPC messages. Connect to the blockchain directly from a browser With Substrate Connect, your applications can be configured to run a light client node inside a browser running locally on your computer. From the browser, application users can interact with blockchains directly‚Äîwithout connecting to any third-party nodes or other servers. By eliminating the need for intermediary servers, Substrate Connect provides benefits to blockchain builders, application developers, and end users. A few of the key benefits include: improved security simplified network infrastructure lowered maintenance costs easier onboarding for novice blockchain users faster path to adoption for Web3 applications Blockchain networks with well-known names You can use Substrate Connect to connect to any Substrate-based blockchain. However, you must specify the correct name of the chain that you want to connect to. There are a few well-known chain names that are defined for the WellKnownChain enumeration type. You can connect to the following public blockchain networks using the name listed: To connect to this chain Use this chain identifier Polkadot polkadot Kusama ksmcc3 Westend westend2 Rococo rococo_v2_2 Note that you must use the chain identifier as it appears in the chain specification for a specific network rather than the more commonly-used network name. For example, you must specify ksmcc3 as the chain identifier to connect to Kusama. Using the correct name is especially important for chains that have been forked. For example, rococo_v2 and rococo_v2_2 are two different chains. Integrate in apps that use Polkadot-JS API If you have built applications that use the existing Polkadot-JS API, the @polkadot/rpc-provider package already contains the substrate-connect RPC provider. To add substrate-connect to your application: Install the @polkadot/rpc-provider package by running the appropriate command for the package manager you use. For example, if you use yarn , run the following command: yarn add @polkadot/rpc-provider If you use npm as your package manager, run the following command: npm i @polkadot/rpc-provider Install the @polkadot/api package by running the appropriate command for the package manager you use. For example, if you use yarn , run the following command: yarn add @polkadot/api If you use npm as your package manager, run the following command: npm i @polkadot/api Use the RPC provider to connect to a well-known network The following example illustrates how you can use the rpc-provider to connect to a well-known network such as Polkadot, Kusama, Westend, or Rococo. import { ScProvider , WellKnownChain } from "@polkadot/rpc-provider/substrate-connect" ; import { ApiPromise } from "@polkadot/api" ; // Create the provider for a known chain const provider = new ScProvider ( WellKnownChain . westend2 ) ; // Stablish the connection (and catch possible errors) await provider . connect ( ) ; // Create the PolkadotJS api instance const api = await ApiPromise . create ( { provider } ) ; await api . rpc . chain . subscribeNewHeads ( lastHeader => { console . log ( lastHeader . hash ) ; } ) ; await api . disconnect ( ) ; Use the RPC provider to connect to a custom network The following example illustrates how you can use the rpc-provider to connect to a custom network by specifying its chain specification. import { ScProvider } from "@polkadot/rpc-provider/substrate-connect" ; import { ApiPromise } from "@polkadot/api" ; import jsonCustomSpec from "./jsonCustomSpec.json" ; // Create the provider for the custom chain const customSpec = JSON . stringify ( jsonCustomSpec ) ; const provider = new ScProvider ( customSpec ) ; // Stablish the connection (and catch possible errors) await provider . connect ( ) ; // Create the PolkadotJS api instance const api = await ApiPromise . create ( { provider } ) ; await api . rpc . chain . subscribeNewHeads ( lastHeader => { console . log ( lastHeader . hash ) ; } ) ; await api . disconnect ( ) ; Use the RPC provider to connect to a parachain The following example illustrates how you can use the rpc-provider to connect to a parachain by specifying its chain specification. import { ScProvider , WellKnownChain } from "@polkadot/rpc-provider/substrate-connect" ; import { ApiPromise } from "@polkadot/api" ; import jsonParachainSpec from "./jsonParachainSpec.json" ; // Create the provider for the relay chain const relayProvider = new ScProvider ( WellKnownChain . westend2 ) ; // Create the provider for the parachain. Notice that // we must pass the provider of the relay chain as the // second argument const parachainSpec = JSON . stringify ( jsonParachainSpec ) ; const provider = new ScProvider ( parachainSpec , relayProvider ) ; // Stablish the connection (and catch possible errors) await provider . connect ( ) ; // Create the PolkadotJS api instance const api = await ApiPromise . create ( { provider } ) ; await api . rpc . chain . subscribeNewHeads ( lastHeader => { console . log ( lastHeader . hash ) ; } ) ; await api . disconnect ( ) ; Use Substrate Connect with other libraries The previous section demonstrated how to integrate the Substrate Connect provider into applications that use the Polkadot-JS API. With this provider, you create applications that enable users to interact with the chain through the browser using calls to the Polkadot-JS API methods. However, you can install and use @substrate-connect in applications that don't depend on the Polkadot-JS API. For example, if you are building your own application library or programming interfaces, you can install the Substrate Connect dependencies by running the appropriate command for the package manager you use. For example, if you use yarn , run the following command: yarn add @substrate/connect If you use npm as your package manager, run the following command: npm i @substrate/connect Connect to a well-known chain The following example illustrates how you can use Substrate Connect to connect to a well-known network such as Polkadot, Kusama, Westend, or Rococo. import { WellKnownChain , createScClient } from "@substrate/connect" ; // Create the client const client = createScClient ( ) ; // Create the chain connection, while passing the `jsonRpcCallback` function. const chain = await client . addWellKnownChain ( WellKnownChain . polkadot , function jsonRpcCallback ( response ) { console . log ( "response" , response ) ; } ) ; // send a RpcRequest chain . sendJsonRpc ( '{"jsonrpc":"2.0","id":"1","method":"system_health","params":[]}' ) ; Connect to a parachain The following example illustrates how you can use Substrate Connect to connect to a parachain. import { WellKnownChain , createScClient } from "@substrate/connect" ; import jsonParachainSpec from "./jsonParachainSpec.json" ; // Create the client const client = createScClient ( ) ; // Create the relay chain connection. There is no need to pass a callback // function because we will sending and receiving messages through // the parachain connection. await client . addWellKnownChain ( WellKnownChain . westend2 ) ; // Create the parachain connection. const parachainSpec = JSON . stringify ( jsonParachainSpec ) ; const chain = await client . addChain ( parachainSpec , function jsonRpcCallback ( response ) { console . log ( "response" , response ) ; } ) ; // send a request chain . sendJsonRpc ( '{"jsonrpc":"2.0","id":"1","method":"system_health","params":[]}' ) ; API Documentation For more information about the substrate-connect API, see Substrate Connect . Browser extension The Substrate Connect Browser Extension is using Substrate Connect and Smoldot light client node modules and upon browser initiation updates and synchronizes in the well known substrate chain specs ( Polkadot, Kusama, Rococo, Westend ), keeping them to the latest state inside the extension, for faster chain sync. When a dApp that integrates Substrate Connect (e.g. polkadotJS/apps ) starts in a browser's tab, then it receives the latest specs from the Extension instead of wrap-synching from the last imported inside the dApp; At the same time, the dApp will appear inside the Extension as "connected" - meaning that it is using the Extension's bootnodes and specs; You can download the Chrome and Firefox extensions from Substrate Connect or find more information on the Github repo . Example projects Burnr Insecure redeemable wallet: A light-client-based, in-browser wallet for Substrate. It's meant to be quick and easy to use but less secure than other solutions. Github Multi-demo Simple demo that covers multichain and parachain examples. Github Brave browser WebSocket issue As of Brave v1.36 , extensions and web pages are limited to a maximum of 10 active WebSocket connections to prevent side-channel attacks. You can find more information about this change in Partition WebSockets Limits to prevent side channels . If you're using the Brave browser and are unable to connect because you've opened the maximum number of WebSocket connections allowed, you can disable this restriction. To disable the WebSocket limit: Open a new tab in the Brave browser. Copy the URL brave://flags/#restrict-websockets-pool . Paste the URL in the Address bar to select the Restrict WebSockets pool setting. Click the setting list and select Disabled . Relaunch the browser. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Light clients in the JavaScript ecosystem Connect to the blockchain directly from a browser Blockchain networks with well-known names Integrate in apps that use Polkadot-JS API Use Substrate Connect with other libraries API Documentation Browser extension Example projects Brave browser WebSocket issue Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Cryptography is what provides the mathematical verifiableness behind consensus systems, data integrity, and user security. While understanding the fundamental overarching applications of cryptography as they relate to blockchain is essential for the average developer, the underlying mathematical processes themselves are not necessarily relevant. This page provides the base context for the various implementations of cryptography across Parity and the ecosystem more broadly. Hash functions Hashing is a mathematical process that creates a one-to-one mapping between any piece of data and a 32 byte reference, using 2 random and unique numerical inputs between zero and the 255 power. Using hash functions, any data, including simple text, images, or any other form of file is given a distinct and entirely unique identifier. Hashing is used for verifying data integrity, creating digital signatures, and providing a secure way to store passwords. This form of mapping is known as the 'pigeonhole principle', and it is primarily implemented to efficiently and verifiably identify data from large sets. These functions are deterministic , meaning that the same input will always produce the same output. This is important for ensuring that two different computers can agree on the same data. They can be designed to be fast or slow, depending on the purpose. Fast hash functions are used when speed is important, while slow hash functions are used when security is the priority. Slow hash functions are also used to mitigate the success of brute force attacks by increasing the amount of work required to find the data. Collision resistance In blockchain, hash functions are also used to provide collision resistance . These are performed by an attacker that calculates or controls both numerical inputs, attempting to find two identical values in order to gain access to an encrypted object. With partial collisions, a similar method is applied, but is only attempting to find two values that share the first few bits instead of the entirety. While only implementing partial collision resistance is computationally lighter weight and provides fairly strong protections against the possibility of collisions, it is a less-secure option when facing well resourced adversaries such as nation-states, as it is significantly easier to brute-force past the first few digits with a significant amount of computational power. That said, it is acceptable with an average attack vector (i.e, rogue actors). Blake2 When engineering a new blockchain protocol or ecosystem, it is important to consider the computational costs of the cryptography method being used. Prioritising efficiency and processor loads, Substrate utilises Blake2. Blake2 is a relatively recent hashing method that provides equal or greater security than SHA2, while also being significantly faster than other comparable algorithms. While determining the exact benchmark of it's speed improvements over other hashing methods is highly dependent on hardware specifications, the biggest positive implication for Substrate is how it heavily reduces the amount of time and resources a new node will need in order to sync with the chain, and to a lesser extent, lower required time for validating. For a comprehensive view of Blake2, see their official document . Types of cryptography There are two different ways that cryptographic algorithms are implemented: symmetric cryptography , and asymmetric cryptography . Symmetric cryptography Symmetric encryption is a branch of cryptography that is not based on one-way functions, unlike asymmetric cryptography. It uses the same cryptographic key for both the encryption of plain text and the decryption of the resulting ciphertext. Symmetric cryptography is the type of encryption that has been used throughout history, such as the Enigma Cipher and the Caesar Cipher. It is still widely used today, and can be found in web2 and web3 applications alike. There is only one single key, and requires a recipient to also have access to it in order to access the contained information. Asymmetric cryptography Asymmetric encryption is a type of cryptography which uses two different keys, known as a keypair: a public key, used to encrypt plain text, and a private counterpart, used to decrypt the cipher text. The public key is used to encrypt a fixed length message that can only be decrypted with the recipient's private key and, at times, a set password. The public key can be used to cryptographically verify that the corresponding private key was used to create a piece of data without compromising the private key itself, such as with digital signatures . This has obvious implications for identity, ownership and properties, and is used in many different protocols across both web2 and web3. Trade-offs and compromises Symmetric cryptography is faster and requires fewer bits in the key to achieve the same level of security that asymmetric cryptography provides. However, it requires a shared secret before communication can take place, which poses issues to it's integrity and a potential compromise point. Asymmetric cryptography, on the other hand, does not require the secret to be shared ahead of time, allowing for far better end-user security. Hybrid symmetric and asymmetric cryptography is often used to overcome the engineering issues of asymmetric cryptography, as it is slower and requires more bits in the key to achieve the same level of security. It is used to encrypt a key, and then use the comparatively lightweight symmetric cipher to do the 'heavy lifting' with the message. Digital signatures Digital signatures are a way of verifying the authenticity of a document or message using asymmetric keypairs. They are used to ensure that a sender or signer's document or message has not been tampered with in transit, and for recipients to verify said data is accurate and from the expected sender. Signing digital signatures only requires a low level understanding of mathematics and cryptography. For a conceptual example -- when signing a check, it is expected that the check cannot be cashed multiple times. This is not a feature of the signature system, but rather the check serialization system. The bank will check that the serial number on the check has not already been used. Digital signatures essentially combines these two concepts, allowing the signature itself to provide the serialization via a unique cryptographic fingerprint that cannot be reproduced. Unlike with a pen and paper signatures, knowledge of the digital signature cannot be used to create other signatures. Digital signatures are often used in bureaucratic processes, as they are more secure than simply scanning in a signature and pasting it onto a document. Substrate provides multiple different cryptographic schemes and is generic such that it can support anything which implements the Pair trait . Elliptic Curve Blockchain technology requires the ability to have multiple keys creating a signature for block proposal and validation. To this end, Elliptic Curve Digital Signature Algorithm (ECDSA) and Schnorr signatures are two of the most commonly used methods. While ECDSA are a far simpler implementation, Schnorr signatures are more efficient when it comes to multi-signatures. Schnorr signatures bring some noticeable features over the ECDSA /EdDSA schemes: It is better for hierarchical deterministic key derivations. It allows for native multi-signature through signature aggregation . It is generally more resistant to misuse. One sacrifice that is made when using Schnorr signatures over ECDSA is that both require 64 bytes, but only ECDSA signatures communicate their public key. Various implementations ECDSA Substrate provides an ECDSA signature scheme using the secp256k1 curve. This is the same cryptographic algorithm used to secure Bitcoin and Ethereum . Ed25519 Ed25519 is an EdDSA signature scheme using Curve25519 . It is carefully engineered at several levels of design and implementation to achieve very high speeds without compromising security. SR25519 SR25519 is based on the same underlying curve as Ed25519 . However, it uses Schnorr signatures instead of the EdDSA scheme. Where to go next Cryptography on Polkadot . Research at W3F: Cryptography . Hash trait for implementing new hashing algorithms. Pair trait for implementing new cryptographic schemes. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Hash functions Types of cryptography Digital signatures Elliptic Curve Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Consensus Consensus Consensus All blockchains require some type of consensus mechanism to agree on the blockchain state. Because Substrate provides a modular framework for building blockchains, it supports a few different models for nodes to reach consensus. In general, different consensus models have different trade-offs, so selecting the type of consensus you want to use for your chain is an important consideration. The consensus models that Substrate supports by default require minimal configuration, but it's also possible to build a custom consensus model, if needed. Consensus in two phases Unlike some blockchains, Substrate splits the requirement to reach consensus into two separate phases: Block authoring is the process nodes use to create new blocks. Block finalization is the process used to handle forks and choose the canonical chain. Block authoring Before you can reach consensus, some nodes in the blockchain network must be able to produce new blocks. How the blockchain decides the nodes that are authorized to author blocks depends on which consensus model you're using. For example, in a centralized network, a single node might author all the blocks. In a completely decentralized network without any trusted nodes, an algorithm must select the block author at each block height. For a Substrate-based blockchain, you can choose one of the following block authoring algorithms or create your own: Authority-based round-robin scheduling ( Aura ). Blind assignment of blockchain extension ( BABE ) slot-based scheduling. Proof of work computation-based scheduling. The Aura and BABE consensus models require you to have a known set of validator nodes that are permitted to produce blocks. In both of these consensus models, time is divided up into discrete slots. During each slot only some of the validators can produce a block. In the Aura consensus model, validators that can author blocks rotate in a round-robin fashion. In the BABE consensus model, validators are selected based on a verifiable random function (VRF) as opposed to the round-robin selection method. In proof-of-work consensus models, any node can produce a block at any time if the node has solved a computationally-intensive problem. Solving the problem takes CPU time, and thus nodes can only produce blocks in proportion with their computing resources. Substrate provides a proof-of-work block production engine. Finalization and forks As a primitive, a block contains a header and transactions . Each block header contains a reference to its parent block, so you can trace the chain back to its genesis. Forks occur when two blocks reference the same parent. Block finalization is a mechanism that resolves forks such that only the canonical chain exists. A fork choice rule is an algorithm that selects the best chain that should be extended. Substrate exposes this fork choice rule through the SelectChain trait. You can use the trait to write your custom fork choice rule, or use GRANDPA , the finality mechanism used in Polkadot and similar chains. In the GRANDPA protocol, the longest chain rule simply says that the best chain is the longest chain. Substrate provides this chain selection rule with the LongestChain struct. GRANDPA uses the longest chain rule in its voting mechanism. One disadvantage of the longest chain rule is that an attacker could create a chain of blocks that outpaces the network and effectively hijack the main chain. The Greedy Heaviest Observed SubTree (GHOST) rule says that, starting at the genesis block, each fork is resolved by choosing the heaviest branch that has the most blocks built on it. In this diagram, the heaviest chain is the fork that has accumulated most blocks built on top of it. If you are using the GHOST rule for chain selection, this fork would be selected as the main chain even though it has fewer blocks than the longest chain fork. Deterministic finality It's natural for users to want to know when transactions are finalized and signaled by some event such as a receipt delivered or papers signed. However, with the block authoring and fork choice rules described so far, transactions are never entirely finalized. There is always a chance that a longer or heavier chain might revert a transaction. However, the more blocks are built on top of a particular block, the less likely it is to ever be reverted. In this way, block authoring along with a proper fork choice rule provides probabilistic finality . If your blockchain requires deterministic finality, you can add a finality mechanism to the blockchain logic. For example, you can have members of a fixed authority set cast finality votes. When enough votes have been cast for a certain block, the block is deemed final. In most blockchains, this threshold is two-thirds. Blocks that have been finalized cannot be reverted without external coordination such as a hard fork. In some consensus models, block production and block finality are combined, and a new block N+1 cannot be authored until block N is finalized. As you've seen, in Substrate, the two processes are isolated from one another. By separating block authoring from block finalization, Substrate enables you to use any block authoring algorithm with probabilistic finality or combine it with a finality mechanism to achieve deterministic finality. If your blockchain uses a finality mechanism, you must modify the fork choice rule to consider the results of the finality vote. For example, instead of taking the longest chain period, a node would take the longest chain that contains the most recently finalized block. Default consensus models Although you can implement your own consensus mechanism, the Substrate node template includes Aura for block authoring and GRANDPA finalization by default. Substrate also provides implementations of BABE and proof-of-work consensus models. Aura Aura provides a slot-based block authoring mechanism. In Aura a known set of authorities take turns producing blocks. BABE BABE provides slot-based block authoring with a known set of validators and is typically used in proof-of-stake blockchains. Unlike Aura, slot assignment is based on the evaluation of a Verifiable Random Function (VRF). Each validator is assigned a weight for an epoch. This epoch is broken up into slots and the validator evaluates its VRF at each slot. For each slot that the validator's VRF output is below its weight, it is allowed to author a block. Because multiple validators might be able to produce a block during the same slot, forks are more common in BABE than they are in Aura, even in good network conditions. Substrate's implementation of BABE also has a fallback mechanism for when no authorities are chosen in a given slot. These secondary slot assignments allow BABE to achieve a constant block time. Proof of work Proof-of-work block authoring is not slot-based and does not require a known authority set. In proof of work, anyone can produce a block at any time, so long as they can solve a computationally challenging problem (typically a hash preimage search). The difficulty of this problem can be tuned to provide a statistical target block time. GRANDPA GRANDPA provides block finalization. It has a known weighted authority set like BABE. However, GRANDPA does not author blocks. It just listens to gossip about blocks that have been produced by block authoring nodes. GRANDPA validators vote on chains, not blocks, . GRANDPA validators vote on a block that they consider best and their votes are applied transitively to all previous blocks. After two-thirds of the GRANDPA authorities have voted for a particular block, it is considered final. All deterministic finality algorithms, including GRANDPA, require at least 2f + 1 non-faulty nodes, where f is the number of faulty or malicious nodes. Learn more about where this threshold comes from and why it is ideal in the seminal paper Reaching Agreement in the Presence of Faults or on Wikipedia: Byzantine Fault . Not all consensus protocols define a single, canonical chain. Some protocols validate directed acyclic graphs (DAG) when two blocks with the same parent do not have conflicting state changes. See AlephBFT for such an example. Where to go next BABE research GRANDPA research Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Consensus in two phases Block authoring Finalization and forks Deterministic finality Default consensus models Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Cross-consensus messaging Xcm communication Cross-consensus messaging Cross-consensus communication relies on a message format‚ÄîXCM‚Äîthat is designed to provide a generalized and extensible set of instructions for completing transactions across boundaries created by different consensus systems, transaction formats, and transport protocols. The XCM format expresses the content of the message. Each message consists of a set of instructions being requested by a sender that can be accepted or rejected by a message recipient . The message format is completely independent of the message protocol used to send and receive messages. Message protocols In the Polkadot ecosystem, there are three main communication channels‚Äîthe message protocols‚Äîused to transport messages between chains: Upward message passing (UMP) enables a parachain to pass messages up to its relay chain. Downward message passing (DMP) enables the relay chain to pass messages down to a parachain. Cross-consensus message passing (XCMP) enables parachains to exchange messages with other parachains that are connected to the same relay chain. The upward and downward message passing protocols provide a vertical message passing channel. Cross-consensus message passing can be thought of as a horizontal‚Äîparachain-to-parachain‚Äîtransport protocol. Because the full cross-consensus message passing (XCMP) protocol is still under development, horizontal relay-routed message passing (HRMP) provides an interim solution for routing messages intended for a parachain through the relay chain. Horizontal relay-routed message passing (HRMP) is intended to be a temporary solution that will be deprecated when XCMP is released to production. Although these message passing protocols are the primary means of communication between chains in the Polkadot ecosystem, XCM itself isn't limited by these transport protocols. Instead, you can use XCM to express many common types of transactions regardless of where the message originates and its destination. For example, you can construct messages that are routed from smart contracts or pallets, over bridges, or using transport protocols that aren't part of the Polkadot ecosystem at all. Because XCM is specifically designed to communicate what should be done by a receiving system, it can provide a flexible and agnostic transaction format for many common types of transactions. Messages in the XCM format There are four important principles you should understand about messages that use the XCM format: Messages are asynchronous . After you send a message, there's no expectation that sending system should wait for a response that indicates the message was delivered or executed. Messages are absolute in that they are guaranteed to be delivered and interpreted in order and executed efficiently . Messages are asymmetric and don't return any results back to the sender. You can only communicate results back to the sender separately using an additional message. Messages are agnostic and make no assumptions about the consensus systems between which messages are passed. With these basic principles in mind, you can start constructing messages using XCM. In Rust, a message is defined like this: pub struct Xcm < Call > ( pub Vec < Instruction < Call >> ) ; As this definition indicates, the message is simply a call to execute an ordered set of instructions. The Instruction type is an enumeration data type and the order in which the variants are defined reflects generally the order in which they are used when constructing a message. For example, the WithdrawAsset is the first variant because it is typically be executed before other instructions‚Äîsuch as BuyExecution or DepositAsset ‚Äîin the ordered list of instructions. Most of the XCM instructions enable you to perform common tasks such as transfer an asset to a new location or deposit an asset in a different account. The instructions that perform these types of tasks allow you to construct consistent messages that do what you expect them to do regardless of how the consensus system you communicate with is configured. However, you also have the flexibility to customize how instructions are executed or to use the Transact instruction. The Transact instruction allow you to execute any callable function exposed by the recipient of a message. By using the Transact instruction, you can make a generic call to any function on the receiving system, but it requires you to know something about how that system is configured. For example, if you want to call a specific pallet of another parachain, you must know how the receiving runtime is configured to construct the correct message to reach the correct pallet. This information can vary from chain to chain because every runtime can be configured differently. Execution in a virtual machine The cross-consensus virtual machine (XCVM) is a high level virtual machine with an XCM executor program that executes the XCM instructions it receives. The program executes the instructions in order until it runs to the end or encounters an error and stops execution. As XCM instructions are executed, the XCVM maintains its internal state through the use of several specialized registers. The XCVM also has access to the state of the underlying consensus system where the instructions are executing. Depending on the operations performed, the XCM instructions might change a register, the state of the consensus system, or both. For example, the TransferAsset instruction specifies an asset to transfer and where the asset is to be transferred. When this instruction is executed, the origin register is automatically set to reflect where the message came from and, from that information, identify where assets to be transferred should be taken from. Another register that is manipulated when executing XCM instructions is the holding register . The holding register is used to store assets temporarily while waiting for additional instructions for what should be done with them. There are several other registers in the XCVM to handle specific tasks. For example, there's a surplus weight register to store any overestimation of fees and a refunded weight register to store the portion of surplus weight that has been refunded. In general, you can't modify the values stored in the registers directly. Instead, values are set when the XCM executor program starts and are manipulated by specific instructions, under certain circumstances, or according to certain rules. FOr more information about what's contained in each register, see XCM reference . Configuration Like other components in Substrate and FRAME-based chains, the XCM executor is modular and configurable. You can configure many aspects of the XCM executor program using the Config trait and customize the implementation to handle XCM instructions in different ways. For example, the Config trait provides the following type definitions: /// The trait to parameterize the `XcmExecutor`. pub trait Config { /// The outer call dispatch type. type Call : Parameter + Dispatchable < PostInfo = PostDispatchInfo > + GetDispatchInfo ; /// How to send an onward XCM message. type XcmSender : SendXcm ; /// How to withdraw and deposit an asset. type AssetTransactor : TransactAsset ; /// How to get a call origin from a `OriginKind` value. type OriginConverter : ConvertOrigin << Self :: Call as Dispatchable > :: Origin > ; /// Combinations of (Location, Asset) pairs trusted as reserves. type IsReserve : FilterAssetLocation ; /// Combinations of (Location, Asset) pairs trusted as teleporters. type IsTeleporter : FilterAssetLocation ; /// Means of inverting a location. type LocationInverter : InvertLocation ; /// Whether to execute the given XCM at all. type Barrier : ShouldExecute ; /// Handler for estimating weight for XCM execution. type Weigher : WeightBounds < Self :: Call > ; /// Handler for purchasing weight credit for XCM execution. type Trader : WeightTrader ; /// Handler for the response to a query. type ResponseHandler : OnResponse ; /// Handler for assets in the Holding register after execution. type AssetTrap : DropAssets ; /// Handler for when there is an instruction to claim assets. type AssetClaims : ClaimAssets ; /// Handler version subscription requests. type SubscriptionService : VersionChangeNotifier ; } The configuration settings and the XCM set of instructions‚Äîthe message or, more accurately, the program to be executed on the receiving system‚Äîact as input to the XCM executor. With additional types and functions provided by XCM builder modules, the XCM executor interprets and executes the operations contained in the instructions one at a time in the order provided. The following diagram provides a simplified overview of the workflow. Locations Because XCM is a language for communicating between different consensus systems, it must have an abstract way to express locations in a general, flexible, and unambiguous way. For example, XCM must be able to identify the location for the following types of activity: where an instruction should be executed. where an asset should be withdrawn from. where an account to receive assets can be found. For any of these activities, the location might be in the context of a relay chain, a parachain, a foreign chain, an account on a specific chain, a specific smart contract, or an individual pallet. For example, XCM must be able to identify the following types of locations: A layer-0 chain such as the Polkadot or Kusama relay chain. A layer-1 chain such as the Bitcoin or Ethereum mainnet or a parachain. A layer-2 smart contract such as an ERC-20 on Ethereum. An address on a parachain or Ethereum. An account on a relay chain or parachain. A specific pallet on a Frame-based Substrate chain. A single instance of a pallet on a Frame-based Substrate chain. To describe the location within the context of a consensus system, XCM uses the MultiLocation type. The MultiLocation type expresses a location that is relative to the current location and consists of two parameters: parents: u8 to describe the number of levels to travel up from the current consensus location before interpreting the interior parameter. interior: InteriorMultiLocation to describe a location interior to the outer consensus system after ascending the relative path as specified using the parents parameter. The InteriorMultiLocation identifies a consensus system interior to the local consensus system using the concept of junctions , with each junction specifying a location further internal to the previous. An InteriorMultiLocation with no junctions simply refers to the local consensus system (Here). You can use junctions to specify an interior context for XCM instructions as a parachain, an account, or a pallet instance relative to the outer consensus. For example, the following parameters refer to a parachain with the unique identifier 1000 from the context of the relay chain: { "parents" : 1 , "interior" : { "X1" : [ { "Parachain" : 1000 } ] } } In this example, the parents parameter ascends one level to the parent chain and interior specifies an interior location with a junction type of Parachain and an index of 1000 . In text, a MultiLocation follows the convention used to describe file system paths. For example, the MultiLocation expressed as ../PalletInstance(3)/GeneralIndex(42) describes a MultiLocation with one parent (..) and two junctions ( PalletInstance{index: 3} ) and ( GeneralIndex{index: 42} ). For more information about specifying locations and junctions, see Universal consensus location identifiers . Assets Most blockchains depend on some type of digital asset to provide economic incentives that are crucial to the security of the network. Some blockchains support a single native asset. Other blockchains allow multiple assets to be managed on-chain, for example, as assets defined in smart contracts or non-native tokens. There are also blockchains that support non-fungible digital assets for one-of-a-kind collectibles. For XCM to support these different types of assets, it must be able to express assets in a general, flexible, and unambiguous way. To describe on-chain assets, XCM uses the MultiAsset type. The MultiAsset type specifies the asset identity and whether the asset is fungible or non-fungible. Typically, the asset identity is specified using a concrete location. If the asset is fungible, the definition includes an amount. Although it's possible to identify an asset using an abstract identifier, concrete identifiers are an unambiguous way of identifying an asset without a global registry of asset names. Concrete identifiers specifically identify a single asset through its location in a consensus system relative to the context interpreting. However, it's worth noting that the concrete asset identifier can't just be copied between consensus systems. Instead, the asset is moved using the relative path for each consensus system. The relative paths must be constructed to be read from the point of view of the receiving system. For native assets‚Äîsuch as DOT on the Polkadot relay chain‚Äîthe asset identifier is typically the chain that mints the asset or up one level ( .. ) from the context of one its parachains. If an assets is managed from within a pallet, the asset identifier specifies a location using the pallet instance identifier and the index within that pallet. For example, the Karura parachain might refer to an asset on the Statemine parachain with the location ../Parachain(1000)/PalletInstance(50)/GeneralIndex(42) . For more information about specifying locations and junctions, see Universal asset identifiers . Instructions Most of the XCM instructions enable you to construct consistent messages that do what you expect regardless of how the consensus system you communicate with is configured. However, you also have the flexibility to use the Transact instruction to execute any callable function exposed by the recipient of the message. By using the Transact instruction, you can make a generic call to any function on the receiving system, but it requires you to know something about how that system is configured. For example, if you want to call a specific pallet of another parachain, you must know how the receiving runtime is configured to construct the correct message to reach the correct pallet. This information can vary from chain to chain because every runtime can be configured differently. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Message protocols Messages in the XCM format Execution in a virtual machine Locations Assets Instructions Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Install Install Before you can start developing a Substrate-based blockchain, you need to prepare your development environment with the required compiler and tools. Because Substrate‚Äîand most of the developer tools for working with Substrate‚Äîare written in the Rust programming language, the first step in preparing your computer is to install Rust. The steps for installing Rust depend on the operating system of the computer you are using for Substrate development. For installation instructions, see the appropriate topic for your operating system. Linux macOS Windows If you already have Rust installed, you can skip the installation instructions. For additional information about installing and upgrading the Rust compiler and toolchain or troubleshooting installation issues, see the following topics: Rust toolchain Developer tools Troubleshoot Rust issues Last edit: August 17, 2022 ‚Üê Previous Next ‚Üí Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Rust toolchain Rust toolchain Rust toolchain Rust is a modern, type sound, and performant programming language that provides a rich feature set for building complex systems. The language also has an active developer community and a growing ecosystem of sharable libraries called crates . Learning Rust Rust is the core language used to build Substrate-based blockchains, so if you intend to do Substrate development, you need to be familiar with the Rust programming language, compiler, and toolchain management. If you are just getting started with Rust, you should bookmark The Rust Programming Language and refer to other Learn Rust resources on the Rust website to guide you. However, there are a few important points to be aware of as you prepare your development environment. About the Rust toolchain The core tools in the Rust toolchain are the rustc compiler, the cargo build and package manager, and the rustup toolchain manager. At any given point in time, there can multiple versions of Rust available. For example, there are release channels for stable, beta, and nightly builds. You use the rustup program to manage the builds available in your environment and the versions of the toolchain programs that are used with different Rust builds. The rustc compiler enables you to build binaries for different architectures, referred to as targets . Targets are identified by a string that specifies the kind of output the compiler should produce. This feature is important because Substrate is compiled to both a native Rust binary and a WebAssembly target. WebAssembly is a portable binary format that can be executed on any modern computer hardware and through any browser accessing the internet. The WebAssembly (Wasm) target enables Substrate to produce portable blockchain runtimes. For more information about how these binaries are used, see Build process . Last edit: ‚Üê Previous Next ‚Üí ON THIS PAGE Learning Rust About the Rust toolchain Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Linux development environment Linux Linux development environment Rust supports most Linux distributions. Depending on the specific distribution and version of the operating system you use, you might need to add some software dependencies to your environment. In general, your development environment should include a linker or C-compatible compiler such as clang and an appropriate integrated development environment (IDE). Before you begin Check the documentation for your operating system for information about the packages that are installed and how to download and install any additional packages you might need. For example, if you use Ubuntu, you can use the Ubuntu Advanced Packaging Tool ( apt ) to install the build-essential package: sudo apt install build-essential At a minimum, you need the following packages before you install Rust: clang curl git make Because the blockchain requires standard cryptography to support the generation of public/private key pairs and the validation of transaction signatures, you must also have a package that provides cryptography, such as libssl-dev or openssl-devel . Install required packages and Rust To install the Rust toolchain on Linux: Log on to your computer and open a terminal shell. Check the packages you have installed on the local computer by running an appropriate package management command for your Linux distribution. Add any package dependencies you are missing to your local development environment by running an appropriate package management command for your Linux distribution. For example, on Ubuntu Desktop or Ubuntu Server, you might run a command similar to the following: sudo apt install --assume-yes git clang curl libssl-dev protobuf-compiler Click the tab titles to see examples for other Linux operating systems: Debian Arch Fedora Opensuse Remember that different distributions might use different package managers and bundle packages in different ways. For example, depending on your installation selections, Ubuntu Desktop and Ubuntu Server might have different packages and different requirements. However, the packages listed in the command-line examples are applicable for many common Linux distributions, including Debian, Linux Mint, MX Linux, and Elementary OS. Download the rustup installation program and use it to install Rust by running the following command: curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh Follow the prompts displayed to proceed with a default installation. Update your current shell to include Cargo by running the following command: source $HOME /.cargo/env Verify your installation by running the following command: rustc --version Configure the Rust toolchain to default to the latest stable version by running the following commands: rustup default stable rustup update Add the nightly release and the nightly WebAssembly (wasm) targets to your development environment by running the following commands: rustup update nightly rustup target add wasm32-unknown-unknown --toolchain nightly Verify the configuration of your development environment by running the following command: rustup show rustup +nightly show The command displays output similar to the following: # rustup show active toolchain ---------------- stable-x86_64-unknown-linux-gnu ( default ) rustc 1.62 .1 ( e092d0b6b 2022 -07-16 ) # rustup +nightly show active toolchain ---------------- nightly-x86_64-unknown-linux-gnu ( overridden by +toolchain on the command line ) rustc 1.65 .0-nightly ( 34a6cae28 2022 -08-09 ) Compile a Substrate node Now that you have Rust installed and the Rust toolchains configured for Substrate development, you are ready to finish setting up your development environment by cloning the Substrate node template files and compiling a Substrate node. The node template provides a working environment that includes all of the most common features you need to build a blockchain without any extraneous modules or tools. To ensure that the node template offers a relatively stable working environment for you to experiment with, the recommended best practice is to clone Substrate node template from the Substrate Developer Hub repository, rather than from the core Substrate repository. To compile the Substrate node template: Clone the node template repository by running the following command: git clone https://github.com/substrate-developer-hub/substrate-node-template In most cases, you can clone the main branch to get the latest code. However, you can use the --branch command-line option if you want to work with a Substrate branch that is compatible with a specific Polkadot version. Click Tags to see the list of branches that are compatible with specific Polkadot versions. Change to the root of the node template directory by running the following command: cd substrate-node-template If you want to save your changes and make this branch easy to identify, you can create a new branch by running a command similar to the following: git switch -c my-wip-branch Compile the node template by running the following command: cargo build --release Because of the number of packages required, compiling the node can take several minutes. After the build completes successfully, your local computer is ready for Substrate development activity. Where to go next The Substrate Developer Hub acts as a central portal for access to the many resources available to the community. Depending on your interests and learning style, you might prefer one avenue over another. For example, if you prefer to read source code and are familiar with Rust, you might want to start by digging into the Rust API . Tell me Architecture Networks and blockchains Build process Guide me Build a local blockchain Simulate a network Add trusted nodes Last edit: June 22, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Before you begin Install required packages and Rust Compile a Substrate node Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
macOS development environment Macos macOS development environment You can install Rust and set up a Substrate development environment on Apple macOS computers with either Intel or an Apple M1 processors. Before you begin Before you install Rust and set up your development environment on macOS, verify that your computer meets the following basic requirements: Operating system version is 10.7 Lion, or later. Processor speed of at least 2Ghz, 3Ghz recommended. Memory of at least 8 GB RAM, 16 GB recommended. Storage of at 10 GB available space. Broadband Internet connection. Support for Apple Silicon Protobuf must be installed before the build process can begin. To install it, run the following command: brew install protobuf Install Homebrew In most cases, you should use Homebrew to install and manage packages on macOS computers. If you don't already have Homebrew installed on your local computer, you should download and install it before continuing. To install Homebrew: Open the Terminal application. Download and install Homebrew by running the following command: /bin/bash -c " $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) " Verify Homebrew has been successfully installed by running the following command: brew --version The command displays output similar to the following: Homebrew 3.3 .1 Homebrew/homebrew-core ( git revision c6c488fbc0f ; last commit 2021 -10-30 ) Homebrew/homebrew-cask ( git revision 66bab33b26 ; last commit 2021 -10-30 ) Installation Because the blockchain requires standard cryptography to support the generation of public/private key pairs and the validation of transaction signatures, you must also have a package that provides cryptography, such as openssl . To install openssl and the Rust toolchain on macOS: Open the Terminal application. Ensure you have an updated version of Homebrew by running the following command: brew update Install the openssl package by running the following command: brew install openssl Download the rustup installation program and use it to install Rust by running the following command: curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh Follow the prompts displayed to proceed with a default installation. Update your current shell to include Cargo by running the following command: source ~/.cargo/env Verify your installation by running the following command: rustc --version Configure the Rust toolchain to default to the latest stable version by running the following commands: rustup default stable rustup update rustup target add wasm32-unknown-unknown Add the nightly release and the nightly WebAssembly (wasm) targets to your development environment by running the following commands: rustup update nightly rustup target add wasm32-unknown-unknown --toolchain nightly Verify the configuration of your development environment by running the following command: rustup show rustup +nightly show The command displays output similar to the following: # rustup show active toolchain ---------------- stable-x86_64-apple-darwin ( default ) rustc 1.61 .0 ( fe5b13d68 2022 -05-18 ) # rustup +nightly show active toolchain ---------------- nightly-x86_64-apple-darwin ( overridden by +toolchain on the command line ) rustc 1.63 .0-nightly ( e71440575 2022 -06-02 ) Install cmake using the following command: brew install cmake Compile a Substrate node Now that you have Rust installed and the Rust toolchains configured for Substrate development, you are ready to finish setting up your development environment by cloning the Substrate node template files and compiling a Substrate node. The node template provides a working environment that includes all of the most common features you need to build a blockchain without any extraneous modules or tools. To ensure that the node template offers a relatively stable working environment for you to experiment with, the recommended best practice is to clone Substrate node template from the Substrate Developer Hub repository, rather than from the core Substrate repository. To compile the Substrate node template: Clone the node template repository by running the following command: git clone https://github.com/substrate-developer-hub/substrate-node-template In most cases, you can clone the main branch to get the latest code. However, you can use the --branch command-line option if you want to work with a Substrate branch that is compatible with a specific Polkadot version. Click Tags to see the list of branches that are compatible with specific Polkadot versions. Change to the root of the node template directory by running the following command: cd substrate-node-template If you want to save your changes and make this branch easy to identify, you can create a new branch by running a command similar to the following: git switch -c my-wip-branch Compile the node template by running the following command: cargo build --release Because of the number of packages required, compiling the node can take several minutes. After the build completes successfully, your local computer is ready for Substrate development activity. Where to go next The Substrate Developer Hub acts as a central portal for access to the many resources available to the community. Depending on your interests and learning style, you might prefer one avenue over another. For example, if you prefer to read source code and are familiar with Rust, you might want to start by digging into the Rust API . Here are a few additional suggestions for where you can learn more. Tell me Architecture Networks and blockchains Build process Guide me Build a local blockchain Simulate a network Add trusted nodes Last edit: May 29, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Before you begin Installation Compile a Substrate node Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Windows development environment Windows Windows development environment In general, UNIX-based operating systems‚Äîlike macOS or Linux‚Äîprovide a better development environment for building Substrate-based blockchains. All of the code examples and command-line instructions in Substrate Tutorials and How-to guides illustrate how to interact with Substrate using UNIX-compatible commands in a terminal. However, if your local computer uses Microsoft Windows instead of a UNIX-based operating system, you can configure it with additional software to make it a suitable development environment for building Substrate-based blockchains. To prepare a development environment on a computer running Microsoft Windows, you can use Windows Subsystem for Linux (WSL) to emulate a UNIX operating environment. Before you begin Before installing on Microsoft Windows, verify the following basic requirements: You have a computer running a supported version of the Microsoft Windows operating system. You must be running Microsoft Windows 10, version 2004 or later, or Microsoft Windows 11 to install Windows Subsystem for Linux on a computer with the Windows desktop operating system. You must be running Microsoft Windows Server 2019, or later, to install Windows Subsystem for Linux on a computer with the Windows server operating system. You have good internet connection and access to a shell terminal on your local computer. Set up Windows Subsystem for Linux Windows Subsystem for Linux (WSL) enables you to emulate a Linux environment on a computer that uses the Windows operating system. The primary advantage of this approach for Substrate development is that you can use all of the code and command-line examples as described in the Substrate documentation. For example, you can run common commands‚Äîsuch as ls and ps ‚Äîunmodified. By using Windows Subsystem for Linux, you can avoid configuring a virtual machine image or a dual-boot operating system. To prepare a development environment using Windows Subsystem for Linux: Check your Windows version and build number to see if Windows Subsystem for Linux is enabled by default. If you have Microsoft Windows 10, version 2004 (Build 19041 and higher), or Microsoft Windows 11, Windows Subsystem for Linux is available by default and you can continue to the next step. If you have an older version of Microsoft Windows installed, see WSL manual installation steps for older versions . If you are installing on an older version of Microsoft Windows, you can download and install WLS 2 if your computer has Windows 10, version 1903 or higher. Select Windows PowerShell or Command Prompt from the Start menu, right-click, then Run as administrator . In the PowerShell or Command Prompt terminal, run the following command: wsl --install This command enables the required WSL 2 components that are part of the Windows operating system, downloads the latest Linux kernel, and installs the Ubuntu Linux distribution by default. If you want to review the other Linux distributions available, run the following command: wsl --list --online After the distribution is downloaded, close the terminal. Click the Start menu, select Shut down or sign out , then click Restart to restart the computer. Restarting the computer is required to start the installation of the Linux distribution. It can take a few minutes for the installation to complete after you restart. For more information about setting up WSL as a development environment, see Set up a WSL development environment . Install required packages and Rust To install the Rust toolchain on WSL: Click the Start menu, then select Ubuntu . Type a UNIX user name to create user account. Type a password for your UNIX user, then retype the password to confirm it. Download the latest updates for the Ubuntu distribution using the Ubuntu Advanced Packaging Tool ( apt ) by running the following command: sudo apt update Add the required packages for the Ubuntu distribution by running the following command: sudo apt install --assume-yes git clang curl libssl-dev llvm libudev-dev make protobuf-compiler Download the rustup installation program and use it to install Rust for the Ubuntu distribution by running the following command: curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh Follow the prompts displayed to proceed with a default installation. Update your current shell to include Cargo by running the following command: source ~/.cargo/env Verify your installation by running the following command: rustc --version Configure the Rust toolchain to use the latest stable version as the default toolchain by running the following commands: rustup default stable rustup update Add the nightly version of the toolchain and the nightly WebAssembly ( wasm ) target to your development environment by running the following commands: rustup update nightly rustup target add wasm32-unknown-unknown --toolchain nightly Verify the configuration of your development environment by running the following command: rustup show rustup +nightly show The command displays output similar to the following: # rustup show active toolchain ---------------- stable-x86_64-unknown-linux-gnu ( default ) rustc 1.61 .0 ( fe5b13d68 2022 -05-18 ) # rustup +nightly show active toolchain ---------------- nightly-x86_64-unknown-linux-gnu ( overridden by +toolchain on the command line ) rustc 1.63 .0-nightly ( e71440575 2022 -06-02 ) Compile a Substrate node Now that you have Rust installed and the Rust toolchains configured for Substrate development, you are ready to finish setting up your development environment by cloning the Substrate node template files and compiling a Substrate node. The node template provides a working environment that includes all of the most common features you need to build a blockchain without any extraneous modules or tools. To ensure that the node template offers a relatively stable working environment for you to experiment with, the recommended best practice is to clone Substrate node template from the Substrate Developer Hub repository, rather than from the core Substrate repository. To compile the Substrate node template: Clone the node template repository by running the following command: git clone https://github.com/substrate-developer-hub/substrate-node-template In most cases, you can clone the main branch to get the latest code. However, you can use the --branch command-line option if you want to work with a Substrate branch that is compatible with a specific Polkadot version. Click Tags to see the list of branches that are compatible with specific Polkadot versions. Change to the root of the node template directory by running the following command: cd substrate-node-template If you want to save your changes and make this branch easy to identify, you can create a new branch by running a command similar to the following: git switch -c my-wip-branch Compile the node template by running the following command: cargo build --release Because of the number of packages required, compiling the node can take several minutes. After the build completes successfully, your local computer is ready for Substrate development activity. Where to go next The Substrate Developer Hub acts as a central portal for access to the many resources available to the community. Depending on your interests and learning style, you might prefer one avenue over another. For example, if you prefer to read source code and are familiar with Rust, you might want to start by digging into the Rust API . Here are a few additional suggestions for where you can learn more. Tell me Architecture Networks and blockchains Build process Guide me Build a local blockchain Simulate a network Add trusted nodes Last edit: April 3, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Before you begin Set up Windows Subsystem for Linux Install required packages and Rust Compile a Substrate node Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Developer tools Developer tools Developer tools Because you are going to be writing code in Rust, be sure your development environment has the appropriate extensions and plug-ins for working with Rust. As you begin working with the Substrate node template, you'll see that it includes a core set of features and tools specifically designed for runtime development. However, there are also many other specialized tools available that you can install to complement and extend your development environment or to handle specific tasks. A few of the tools you might find useful as you start developing a Substrate-based blockchain include the following: Polkadot-JS API The Polkadot-JS API provides a library of methods that enable you to query and interact with any Substrate-based chain using JavaScript. You can add the @polkadot/api package to any JavaScript or TypeScript working environment. Most of the interfaces exposed by the API are generated dynamically by connecting to a running node. Because the configuration of the node determines which interfaces are exposed, you can use the API to work with customized chains that implement different features. To work with the API, you must identify the URL for the chain to connect to. After connecting to a node for the chain, the API collects information about the chain state and its features, then populates the API with methods based on the information collected about that specific chain. Front-end template The Substrate front-end template provides a predefined front-end application that you can use to connect to a Substrate node back-end with minimal configuration. The template enables you to start experimenting with the basic features of a Substrate node without building your own custom user interface. The template is built using the Create React App starter project and the Polkadot-JS API. Submit transactions command-line interface The subxt-cli is a command-line program you can use to download complete configuration information‚Äîthe metadata ‚Äîfor a Substrate-based chain by connecting to a running node. Similar to the Polkadot-JS API, the metadata you can download with the subxt-cli program exposes information about a Substrate chain that enables you to interact with that chain. You can also use the subxt-cli program to expose information about the chain in a human-readable format. sidecar The @substrate/api-sidecar package is a RESTful service that you can use to connect to and interact with Substrate nodes that are built using the FRAME developer framework. For information about the endpoints that the service supports, see Substrate API Sidecar . You might also want to explore resources and community projects listed in Awesome Substrate . For an overview of some of the most commonly-used tools, see command-line tools . Where to go next Command-line tools node-template subkey try-runtime Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Troubleshoot Rust issues Troubleshoot Rust issues Troubleshoot Rust issues If compiling the Substrate node template fails, the problem is most likely to be caused by how Rust is configured in your development environment. This section suggests how you can diagnose and fix configuration issues. Check your current configuration To see information about the Rust toolchain that you are currently using, run the following command: rustup show This command displays output similar to the following Ubuntu example: Default host: x86_64-unknown-linux-gnu rustup home: /home/user/.rustup installed toolchains -------------------- stable-x86_64-unknown-linux-gnu (default) nightly-2020-10-06-x86_64-unknown-linux-gnu nightly-x86_64-unknown-linux-gnu installed targets for active toolchain -------------------------------------- wasm32-unknown-unknown x86_64-unknown-linux-gnu active toolchain ---------------- stable-x86_64-unknown-linux-gnu (default) rustc 1.50.0 (cb75ad5db 2021-02-10) In this example, the default toolchain is from the stable release channel for Linux running on x86 64 architecture. The sample output also indicates that the `nightly-x86 64-unknown-linux-gnu` toolchain is installed and there are two targets installed: x86_64-unknown-linux-gnu the native Rust target for Linux. wasm32-unknown-unknown the WebAssembly target. This environment also has the nightly-2020-10-06-x86_64-unknown-linux-gnu toolchain installed, but this toolchain is only used if explicitly specified as a command-line option. For an example of specifying a specific toolchain as a command line option, see Specify a nightly version . Use the nightly release channel for WebAssembly Substrate uses WebAssembly (Wasm) to produce a portable blockchain runtime. You must configure the Rust compiler to use nightly builds to allow you to compile Substrate runtime code to the Wasm target. Update the toolchain In general, you should always use the latest versions of Rust stable and nightly builds because changes in Substrate often depend on upstream changes in the Rust nightly compiler build. To ensure your Rust compiler is always up to date, you should run the following commands: rustup update rustup update nightly rustup target add wasm32-unknown-unknown --toolchain nightly Running rustup update updates both the nightly and stable toolchains to use the most recent release. If you are unable to compile the WebAssembly target after updating the nightly toolchain, you can roll back to an earlier version of the toolchain and specify that version as a command-line option. For more information about getting an earlier version of the nightly toolchain and specifying the version to use as a command-line option, downgrade the toolchain . Use a specific nightly toolchain If you want to guarantee that your build works on your computer as you update Rust and other dependencies, you should use a specific Rust nightly toolchain that you know to be compatible with the version of Substrate you are using. How you identify and communicate the specific nightly toolchain version to use for a project can vary. For example, Polkadot publishes this information in its release notes . After you identify the specific nightly toolchain version to use, you can install it in your development environment by running a command similar to the following: rustup install nightly- < yyyy-MM-dd > For example: rustup install nightly-2022-02-16 After you install a specific version of the nightly toolchain, configure the WebAssembly target to use it by running a command similar to the following: rustup target add wasm32-unknown-unknown --toolchain nightly- < yyyy-MM-dd > For example: rustup target add wasm32-unknown-unknown --toolchain nightly-2022-02-16 Specify the toolchain in an environment variable You can set the WASM_BUILD_TOOLCHAIN environment variable to specify the version of the nightly toolchain to use for compiling WebAssembly.For example: WASM_BUILD_TOOLCHAIN = nightly- < yyyy-MM-dd > cargo build --release This command builds the runtime using the specified nightly toolchain. The rest of project is compiled using the default toolchain, that is, the latest version of the stable toolchain that you have installed. Downgrade the nightly toolchain If your computer is configured to use the latest Rust nightly toolchain and you want to downgrade to a specific nightly version,you must first uninstall the latest nightly toolchain. For example, you can remove the latest nightly toolchain, then use a specific version of the nightly toolchain by running commands similar to the following: rustup uninstall nightly rustup install nightly- < yyyy-MM-dd > rustup target add wasm32-unknown-unknown --toolchain nightly- < yyyy-MM-dd > Ensure PATH is set correctly If after installing Rust the commands don't seem to work, showing errors such as command not found: rustup , make sure it your PATH is configured correctly. Currently, the rustup installer installs by default to the bash profile (on mac). If you are using another shell, make sure to add this line to your profile (e.g. .zshrc ): source " $HOME /.cargo/env" Installing cmake or protobuf for M1 macOS users Currently, there are issues compiling the Substrate node when using the packages that are pre-installed on macOS computers with the M1 chip. error: failed to run custom build command for prost-build v0.10.4 If you see this error, there are two solutions. Install cmake by running the following command: brew install cmake Install the correct pre-compiled protoc by running the following set of commands: git clone https://github.com/protocolbuffers/protobuf.git cd protobuf brew install autoconf brew install automake brew install Libtool autoreconf -i ./autogen.sh ./configure make make check sudo make install export PATH = /opt/usr/local/bin: $PATH Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Check your current configuration Use the nightly release channel for WebAssembly Update the toolchain Use a specific nightly toolchain Ensure PATH is set correctly Installing cmake or protobuf for M1 macOS users Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Build Build The topics in this section provide a more detailed exploration of the code used to construct the runtime logic, including the libraries and tools available for building and interacting with the node and a closer look at how the logic is compiled to build a Substrate node. Smart contracts summarizes how you can use build smart contracts as an approach to application development in a Substrate-based chain. Custom pallets exposes the macros and attributes that form the foundation for building custom pallets. Runtime storage offers a closer look at storage structure and how to navigate to data stored in the runtime. Transactions, weights, and fees explains the role of weights and fees in executing transactions and the mechanics of how fees are calculated and refunded. Pallet coupling describes how pallets can be tightly or loosely coupled in a runtime. Events and errors explains how to emit events and errors from the runtime. Randomness suggests ways you can include randomness in applications that run on a Substrate-based blockchain. Privileged calls and origins describes how you can use predefined or custom origins to identify the originator of a function call. Remote procedure calls summarizes how you can use remote procedure calls and RPC methods to interact with a Substrate node. Application development introduces the role of metadata and front-end libraries as tools for building applications that run on the blockchain. Chain specification discusses the use of chain specifications, including what you can and can't modify, and how to distribute customized chain specifications. Genesis configuration describes the main elements of the genesis configuration. Build process delves into the details of how the Rust code compiles to a Rust binary and a WebAssembly target and how these two targets are used to optimize node operations. Build a deterministic runtime s xplains how to use the Substrate runtime toolbox ( srtool ) and Docker to build the WebAssembly runtime for Substrate-based chains. Troubleshoot your code highlights general and Substrate-specific coding techniques for troubleshooting issues and following best practices. Last edit: April 4, 2023 ‚Üê Previous Next ‚Üí Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Smart contracts Smart contracts strategy Smart contracts As discussed in Decide what to build , smart contracts are one approach to building an application to run on a Substrate-based blockchain. Although there are certain limitations, there are also advantages to smart contracts. For example, one reason you might want to start building your project with smart contracts is because contracts typically can be developed and tested quickly, allowing you to iterate on your design decisions and release your applications to the market faster. In general, there are two types of smart contracts that you can deploy in a Substrate runtime: WebAssembly contracts. EVM-compatible contracts. The type of contract you want to build is one of the first decisions you need to make. It's also important to note that smart contracts are instructions that are instantiated and executed using a specific chain address‚Äîthe smart contract account address. This means that‚Äîin addition to selecting a language for writing the smart contract‚Äîyou need to identify the chain that will serve as the host platform for your contract. For example, you might want to deploy your application as a smart contract on an existing parachain in the Polkadot or Kusama ecosystem, work with another team to build a custom paraachain specifically for the type of contract you want to deploy, or build your own runtime with the specific features and functionality you want to deliver. About the Contracts pallet In most cases, the Contracts pallet is best option if you want to build smart contracts for a Substrate runtime. By adding the Contracts pallet to the runtime, you can deploy and execute WebAssembly-based smart contracts. If you are building your own runtime to host smart contracts, there are two main reasons for including the Contracts pallet: You want to build a general-purpose blockchain that uses smart contracts to provide the core functionality you want to deliver. For example, if you have an innovative idea that focuses on the use of smart contracts, you can use the Contracts pallet to build the base layer of your chain, then complement the base layer with only the minimal number of additional pallets required to achieve your application-specific goals. You want to build an application-specific blockchain with the flexibility to use smart contracts to interact with the chain. For example, if you are building a chain with most of the logic defined using other Substrate pallets, you can expose some parts of the chain logic to users through smart contracts. Smart contracts are ideal for this type of use case because they treat all user input as untrusted and potentially adversarial. As an example, assume you are building a decentralized exchange. Most of the logic is defined in pallets, but you want to allow users to upload their own trading algorithms through a smart contract. With the gas fees associated with executing a smart contract, users have to pay for the execution time of their trading algorithms. The Contracts pallet provides the Chain extension primitive for exactly that functionality and programming languages‚Äîlike ink! ‚Äîcan make use of the business logic primitives your chain exposes. Smart contract accounts The Contracts pallet extends accounts based on the Currency trait to have smart contract functionality. You can use these smart contract accounts to instantiate smart contracts and to make calls to other contract and non-contract accounts. The smart contract code is stored in a cache and can be retrieved using its hash. This design enables multiple smart contracts to be instantiated from the same hash without replicating the code each time. When a user interacts with a smart contract by calling one of its functions, the associated smart contract code is retrieved using the code hash and the function is executed. Calling a smart contract function can result in: Changing the storage associated wih the smart contract account. Changing the storage associated wih a non-contract account. instantiating a new smart contract. Calling another smart contract account. If a smart contract account is depleted, its associated code and storage is also be deleted. Contract execution and gas All instructions invoked by a smart contract require payment in the form of gas fees. Senders must specify a gas limit for every call. Unused gas is refunded after the call, regardless of the execution outcome. If the gas limit is reached, then all calls and state changes‚Äîincluding balance transfers‚Äîare only reverted at the current call‚Äôs contract level. For example, if contract A calls contract B and contract B runs out of gas mid-call, then all of the contract B calls and state changes are reverted. If contract A has correct error handling, any other calls and state changes make by contract A persist. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE About the Contracts pallet Smart contract accounts Contract execution and gas Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Custom pallets Custom pallets Custom pallets The most common approach to building a custom runtime is to start with existing pallets . For example, you might start building an application-specific staking pallet that uses the types exposed in existing Collective and Balances pallets, but includes custom runtime logic required by your application and its staking rules. Although FRAME pallets provides an overview of the most common pallets, the best place to find current information about existing pallets is the Rust API documentation for crates that use the naming convention pallet_* . If you don't find a pallet that meets your needs, you can use FRAME macros to build the scaffolding for a custom pallet. Pallet macros and attributes FRAME makes extensive use of Rust macros to encapsulate complex blocks of code. The most important macros for building custom pallets is the pallet macro. The pallet macro defines the core set of attributes that a pallet must provide. For example: #[pallet::pallet] is a mandatory pallet attribute that enables you to define a structure (struct) for the pallet so it can store data that can be easily retrieved. #[pallet::config] is a mandatory pallet attribute that enables you to define the configuration trait for the pallet. The pallet macro also defines the core set of attributes that pallets typically provide. For example: #[pallet::call] is the attribute that enables you to implement dispatchable function calls for the pallet. #[pallet::error] is the attribute that enables you to generate dispatchable errors. #[pallet::event] is the attribute that enables you to generate dispatchable events. #[pallet::storage] is the attribute that enables you to generate a storage instance in the runtime and its metadata. These core attributes align with the decisions you need to make when writing a custom pallet. For example, you need to consider: Storage. What data does your pallet store? Is the data stored on-chain or off-chain? Functions. What are the callable functions that your pallet exposes? Transactionality. Are your function calls designed to atomically modify storage? Hooks. Will your pallet be making calls to any runtime hooks? Macros simplify the code you need to write to implement custom runtime logic. However, some macros enforce particular requirements on function declarations. For example, the Config trait must be bound by frame_system::Config and the #[pallet::pallet] struct must be declared as pub struct Pallet<T>(_); . For an overview of the macros used in FRAME pallets, see FRAME macros . Last edit: June 12, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Pallet macros and attributes Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Runtime storage structures Runtime storage Runtime storage structures As you develop runtime logic, you'll need to make important decisions about the information you store and how to make storing information as efficient as possible. As discussed in State transitions and storage , reading and writing data to storage is expensive. In addition, storing unnecessarily large data sets can slow your network and strain system resources. Substrate is designed to provide a flexible framework that allows you to build the blockchain that suits your needs. However, you should keep a few basic guidelines in mind when designing runtime storage to ensure that you build a blockchain that is secure, performant, and maintainable in the long-term. Deciding what to store The fundamental principle for blockchain runtime storage is to minimize both the number and size of the data items you store. For example, you should only store consensus-critical information in the runtime. You shouldn't store intermediate or temporary data in the runtime or data that won't be needed if an operation fails. Use hashed data Whenever possible, use techniques like hashing to reduce the amount of data you must store. For example, many governance capabilities‚Äîsuch as the propose function in the Democracy pallet‚Äîallow network participants to vote on the hash of a dispatchable call instead of the call itself. The hash of the call is always bounded in size, whereas the call might be unbounded in length. Using the hash of a call is particularly important in the case of runtime upgrades where the dispatchable call takes an entire runtime Wasm blob as its parameter. Because these governance mechanisms are implemented on-chain , all the information that is needed to come to consensus on the state of a given proposal must also be stored on-chain - this includes what is being voted on. However, by binding an on-chain proposal to its hash, Substrate's governance mechanisms allow this to be done in a way that defers bringing all the data associated with a proposal on-chain until after it has been approved. This means that storage is not wasted on proposals that fail. Once a proposal has passed, someone can initiate the actual dispatchable call (including all its parameters), which will be hashed and compared to the hash in the proposal. Another common pattern for using hashes to minimize data that is stored on-chain is to store the pre-image associated with an object in IPFS ; this means that only the IPFS location (a hash that is bounded in size) needs to be stored on-chain. Avoid storing transient data Do not use runtime storage to store intermediate or transient data within the context of an operation that is logically atomic or data that will not be needed if the operation is to fail. This does not mean that runtime storage should not be used to track the state of actions that require multiple atomic operations, as in the case of the multi-signature capabilities from the Utility pallet . In this case, runtime storage is used to track the signatories on a dispatchable call even though a given call may never receive enough signatures to actually be invoked. In this case, each signature is considered an atomic event in the ongoing multi-signature operation. The data needed to record a single signature is not stored until after all the preconditions associated with that signature have been met. Create bounds Creating bounds on the size of storage items is an extremely effective way to control the use of runtime storage and one that is used repeatedly throughout the Substrate codebase. In general, any storage item whose size is determined by user action should have a bound on it. The multi-signature capabilities from the Multisig pallet that were described above are one such example. In this case, the list of signatories associated with a multi-signature operation is provided by the multi-signature participants. Because this signatory list is necessary to come to consensus on the state of the multi-signature operation, it must be stored in the runtime. However, to control how much space signatory list can use, the Utility pallet requires users to configure a bound on this number to be included as a precondition before anything is written to storage. Transactional storage As explained in State transitions and storage , runtime storage involves an underlying key-value database and in-memory storage overlay abstractions that keep track of keys and state changes until the values are committed to the underlying database. By default, functions in the runtime write changes to a single in-memory transactional storage layer before committing them to the main storage overlay. If an error prevents the transaction from being completed, the changes in the transactional storage layer are discarded instead of being passed on to the main storage overlay and state in the underlying database remains unchanged. Adding transactional storage layers You can extend the transactional storage layer by using the #[transactional] macro to spawn additional in-memory storage overlays. By spawning additional in-memory transactional storage overlays, you can choose whether you want to commit specific changes to the main storage overlay or not. The additional transactional storage layers give you the flexibility to isolate changes to specific function calls and select at any point which changes to commit. You can also nest transactional storage layers up to a maximum of ten nested transactional layers. With each nested transactional storage layer you create, you can choose whether you want to commit changes to the transactional layer below it, giving you a great deal of control over what is committed to the underlying database. Limiting the total number of nested transactional storage layers limits the computational overhead in resolving the changes to be committed. Dispatching transactional storage layer call If you want to dispatch a function call within its own transactional layer, you can use the dispatch_with_transactional(call) function to explicitly spawn a new transactional layer for the call and use that transactional layer context to handle the result. Committing changes without the transactional storage layer If you want to commit changes to the main storage overlay without using the default transactional storage layer, you can use the #[without_transactional] macro. The #[without_transactional] macro enables you to identify a function that is safe to be executed without its own transactional layer. For example, you might define a function like this: /// This function is safe to execute without an additional transactional storage layer. #[without_transactional] fn set_value ( x : u32 ) -> DispatchResult { Self :: check_value ( x ) ? ; MyStorage :: set ( x ) ; Ok ( ( ) ) } Calling this function doesn't spawn a transactional storage layer. However, if you use the #[without_transactional] macro, keep in mind that changes to storage will affect the values in the main in-memory storage overlay. If an error occurs after you have modified storage, those changes will persist, and potentially could result in your database being left in an inconsistent state. Accessing runtime storage In State transitions and storage , you learned how Substrate uses storage abstractions to provide read and write access to the underlying key-value database. The FRAME Storage module simplifies access to these layered storage abstractions. You can use the FRAME storage data structures to read or write any value that can be encoded by the SCALE codec . The storage module provides the following types of storage structures: StorageValue to store any single value, such as a u64 . StorageMap to store a single key to value mapping, such as a specific account key to a specific balance value. StorageDoubleMap to store values in a storage map with two keys as an optimization to efficiently remove all entries that have a common first key. StorageNMap to store values in a map with any arbitrary number of keys. You can include any of these storage structures in pallets to introduce new storage items that will become part of the blockchain state. The type of storage items you choose to implement depends entirely on how you want to use the information in the context of the runtime logic. Simple storage values You can use StorageValue storage items for values that are viewed as a single unit by the runtime. For example, you should use this type of storage for the following common use cases: Single primitive values Single struct data type objects Single collection of related items If you use this type of storage for lists of items, you should be conscious about the size of the lists you store. Large lists and structs incur storage costs and iterating over a large list or struct in the runtime can affect network performance or stop block production entirely. If iterating over storage exceeds the block production time and your project is a parachain , the blockchain will stop producing blocks and functioning. Refer to the StorageValue documentation for a comprehensive list of methods that StorageValue exposes. Single key storage maps Map data structures are ideal for managing sets of items whose elements will be accessed randomly, as opposed to iterating over them sequentially in their entirety. Single key storage maps in Substrate are similar to traditional hash maps with key-to-value mapping to perform random lookups. To give you flexibility and control, Substrate allows you to select the hashing algorithm you want to use to generate the map keys. For example, if a map stores sensitive data you might want to generate keys using a hashing algorithm with stronger encryption over a hashing algorithm with better performance but weaker encryption properties. For more information about selecting a hashing algorithm for a map to use, see Hashing algorithms . Refer to the StorageMap documentation for a comprehensive list of methods that StorageMap exposes. Double key storage maps DoubleStorageMap storage items are similar to single key storage maps except that they contain two keys. Using this type of storage structure is useful for querying values with common keys. Multi-key storage maps The StorageNMap storage structure is also similar to single key and double key storage maps, but enable you to define any number of keys. To specify the keys in a StorageNMap structure, you must provide a tuple containing the NMapKey struct as a type to the Key type parameter while declaring the StorageNMap . Refer to the StorageNMap documentation for more details about the syntax to use in declaring this type of storage structure. Iterating over storage maps You can iterate over Substrate storage maps using the map keys and values. However, it's important to keep in mind that maps are often used to track unbounded or very large sets of data, such as accounts and balances. Iterating over a large data set can consume a lot of the limited resources you have available for producing blocks. For example, if the time it takes to iterate over a data set exceeds the maximum time allocated for producing blocks, the runtime might stop producing new blocks, halting the progress of the chain. In addition, the database reads required to access the elements in a storage map far exceeds the database reads required to access the elements in a list. Therefore, it is significantly more costly‚Äîin terms of performance and execution time‚Äîto iterate over the elements in a storage map than to read the elements in a list. With the relative costs in mind, it's generally better to avoid iterating over storage maps in the runtime. However, there are no firm rules about how you use Substrate storage capabilities, and, ultimately, it's up to you to decide the best way to access runtime storage for your application. Substrate provides the following methods to enable you to iterate over storage maps: Method Description iter() Enumerates all elements in the map in no particular order. If you alter the map while doing this, you'll get undefined results. For more information, see IterableStorageMap , IterableStorageDoubleMap , or IterableStorageNMap . drain() Removes all elements from the map and iterate through them in no particular order. If you add elements to the map while doing this, you'll get undefined results. For more information, see IterableStorageMap , IterableStorageDoubleMap , IterableStorageNMap . translate() Translates all elements of the map in no particular order. To remove an element from the map, return None from the translation function. For more information, see IterableStorageMap , IterableStorageDoubleMap , IterableStorageNMap . Declaring storage items You can create runtime storage items with the #[pallet::storage] attribute macro in any FRAME-based pallet. The following examples illustrate how to declare different types of storage items. Single storage value #[pallet::storage] type SomePrivateValue < T > = StorageValue < _ , u32 , ValueQuery > ; #[pallet::storage] #[pallet::getter(fn some_primitive_value)] pub ( super ) type SomePrimitiveValue < T > = StorageValue < _ , u32 , ValueQuery > ; #[pallet::storage] pub ( super ) type SomeComplexValue < T : Config > = StorageValue < _ , T :: AccountId , ValueQuery > ; Single key storage map #[pallet::storage] #[pallet::getter(fn some_map)] pub ( super ) type SomeMap < T : Config > = StorageMap < _ , Blake2_128Concat , T :: AccountId , u32 , ValueQuery > ; Double key storage map #[pallet::storage] pub ( super ) type SomeDoubleMap < T : Config > = StorageDoubleMap < _ , Blake2_128Concat , u32 , Blake2_128Concat , T :: AccountId , u32 , ValueQuery > ; Multi-key storage map #[pallet::storage] #[pallet::getter(fn some_nmap)] pub ( super ) type SomeNMap < T : Config > = StorageNMap < _ , ( NMapKey < Blake2_128Concat , u32 > , NMapKey < Blake2_128Concat , T :: AccountId > , NMapKey < Twox64Concat , u32 > , ) , u32 , ValueQuery , > ; Notice that the map's storage items specify the hashing algorithm that will be used. Handling query return values When you declare a storage item, you can specify how queries should handle the return value if there is no value in storage for the specified key. In the storage declaration, you specify the following: OptionQuery to query an optional value from storage and return Some if storage contains a value or None if there's no value is in storage. ResultQuery to query a result value from storage and return an error if there's no value is in storage. ValueQuery to query a value from storage and return the value. You can also use ValueQuery to return the default value if you have configured a specific default for a storage item or return the value configured with the OnEmpty generic. Visibility In the examples above, all the storage items except SomePrivateValue are made public by way of the pub keyword. Blockchain storage is always publicly visible from outside of the runtime. The visibility of Substrate storage items only impacts whether or not other pallets within the runtime will be able to access a storage item. Getter methods The #[pallet::getter(..)] macro provides an optional get extension that can be used to implement a getter method for a storage item on the module that contains that storage item. The extension takes the desired name of the getter function as an argument. If you omit this optional extension, you can access the storage item value, but you will not be able to do so by way of a getter method implemented on the module; instead, you will need to use the storage item's get method . The optional getter extension only impacts the way that a storage item can be accessed from within Substrate code‚Äîyou will always be able to query the storage of your runtime to get the value of a storage item. Here is an example that implements a getter method named some_value for a Storage Value named SomeValue . This pallet would now have access to a Self::some_value() method in addition to the SomeValue::get() method: #[pallet::storage] #[pallet::getter(fn some_value)] pub ( super ) type SomeValue = StorageValue < _ , u64 , ValueQuery > ; Default values Substrate allows you to specify a default value that is returned when a storage item's value is not set. Although the default value does not actually occupy runtime storage, the runtime logic will see this value during execution. Here is an example of specifying a default value in storage: #[pallet::type_value] pub ( super ) fn MyDefault < T : Config > ( ) -> T :: Balance { 3 . into ( ) } #[pallet::storage] pub ( super ) type MyStorageValue < T : Config > = StorageValue < Value = T :: Balance , QueryKind = ValueQuery , OnEmpty = MyDefault < T >> ; Notice that for the sake of adding clarity to each storage field, the syntax above is the non-abbreviated version of declaring storage items. Accessing storage items Blockchains that are built with Substrate expose a remote procedure call (RPC) server that can be used to query runtime storage. You can use software libraries like Polkadot JS to easily interact with the RPC server from your code and access storage items. The Polkadot JS team also maintains the Polkadot Apps UI , which is a fully-featured web app for interacting with Substrate-based blockchains, including querying storage. Hashing algorithms A novel feature of Storage Maps in Substrate is that they allow developers to specify the hashing algorithm that will be used to generate a map's keys. A Rust object that is used to encapsulate hashing logic is referred to as a "hasher". Broadly speaking, the hashers that are available to Substrate developers can be described in two ways: (1) whether or not they are cryptographic; and (2) whether or not they produce a transparent output. For the sake of completeness, the characteristics of non-transparent hashing algorithms are described below, but keep in mind that any hasher that does not produce a transparent output has been deprecated for FRAME-based blockchains. Cryptographic hashing algorithms Cryptographic hashing algorithms enable us to build tools that make it extremely difficult to manipulate the input of a hashing algorithm to influence its output. For example, a cryptographic hashing algorithm would produce a wide distribution of outputs even if the inputs were the numbers 1 through 10. It is critical to use cryptographic hashing algorithms when users are able to influence the keys of a Storage Map. Failure to do so creates an attack vector that makes it easy for malicious actors to degrade the performance of your blockchain network. An example of a map that should use a cryptographic hash algorithm to generate its keys is a map used to track account balances. In this case, it is important to use a cryptographic hashing algorithm so that an attacker cannot bombard your system with many small transfers to sequential account numbers. Without the appropriate cryptographic hashing algorithm this would create an imbalanced storage structure that would suffer in performance. Read more about common hashers in Subsrate in Common Substrate hashers . Cryptographic hashing algorithms are more complex and resource-intensive than their non-cryptographic counterparts, which is why it is important for runtime engineers to understand their appropriate usages in order to make the best use of the flexibility Substrate provides. Transparent hashing algorithms A transparent hashing algorithm is one that makes it easy to discover and verify the input that was used to generate a given output. In Substrate, hashing algorithms are made transparent by concatenating the algorithm's input to its output. This makes it trivial for users to retrieve a key's original unhashed value and verify it if they'd like (by re-hashing it). The creators of Substrate have deprecated the use of non-transparent hashers within FRAME-based runtimes, so this information is provided primarily for completeness. In fact, it is necessary to use a transparent hashing algorithm if you would like to access iterable map capabilities. Common Substrate hashers This table lists some common hashers used in Substrate and denotes those that are cryptographic and those that are transparent: Hasher Cryptographic Transparent Blake2 128 Concat X X TwoX 64 Concat X Identity X The Identity hasher encapsulates a hashing algorithm that has an output equal to its input (the identity function). This type of hasher should only be used when the starting key is already a cryptographic hash. Where to go next Check out some guides covering various topics on storage: How-to: Create a storage structure StorageValue StorageMap StorageDoubleMap StorageNMap Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Deciding what to store Use hashed data Transactional storage Accessing runtime storage Simple storage values Single key storage maps Double key storage maps Multi-key storage maps Iterating over storage maps Declaring storage items Accessing storage items Hashing algorithms Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Transactions, weights, and fees Tx weights fees Transactions, weights, and fees When transactions are executed or data is stored on-chain, the activity changes the state of the chain and consumes blockchain resources. Because the resources available to a blockchain are limited, it‚Äôs important to manage how operations on-chain consume them. In addition to being limited in practical terms‚Äîsuch as storage capacity‚Äîblockchain resources represent a potential attack vector for malicious users. For example, a malicious user might attempt to overload the network with messages to stop the network from producing new blocks. To protect blockchain resources from being drained or overloaded, you need to manage how they are made available and how they are consumed. The resources to be aware of include: Memory usage Storage input and output Computation Transaction and block size State database size Substrate provides block authors with several ways to manage access to resources and to prevent individual components of the chain from consuming too much of any single resource. Two of the most important mechanisms available to block authors are weights and transaction fees . Weights are used to manage the time it takes to validate a block. In general, weights are used to characterize the time it takes to execute the calls in the body of a block. By controlling the execution time that a block can consume, weights set limits on storage input and output and computation. Some of the weight allowed for a block is consumed as part of the block's initialization and finalization. The weight might also be used to execute mandatory inherent extrinsic calls. To help ensure blocks don‚Äôt consume too much execution time‚Äîand prevent malicious users from overloading the system with unnecessary calls‚Äîweights are used in combination with transaction fees . Transaction fees provide an economic incentive to limit execution time, computation, and the number of calls required to perform operations. Transaction fees are also used to make the blockchain economically sustainable because they are typically applied to transactions initiated by users and deducted before a transaction request is executed. How fees are calculated The final fee for a transaction is calculated using the following parameters: base fee : This is the minimum amount a user pays for a transaction. It is declared as a base weight in the runtime and converted to a fee using WeightToFee . weight fee : A fee proportional to the execution time (input and output and computation) that a transaction consumes. length fee : A fee proportional to the encoded length of the transaction. tip : An optional tip to increase the priority of the transaction, giving it a higher chance to be included by the transaction queue. The base fee and proportional weight and length fees constitute the inclusion fee . The inclusion fee is the minimum fee that must be available for a transaction to be included in a block. Using the transaction payment pallet The Transaction Payment pallet provides the basic logic for calculating the inclusion fee. You can also use the Transaction Payment pallet to: Convert a weight value into a deductible fee based on a currency type using Config::WeightToFee . Update the fee for the next block by defining a multiplier, based on the final state of the chain at the end of the previous block using Config::FeeMultiplierUpdate . Manage the withdrawal, refund, and deposit of transaction fees using Config::OnChargeTransaction . You can learn more about these configuration traits in the Transaction Payment documentation. You should note that transaction fees are withdrawn before the transaction is executed. After the transaction is executed, the transaction weight can be adjusted to reflect the actual resources the transaction used. If a transaction uses fewer resources than expected, the transaction fee is corrected and the adjusted transaction fee is deposited. A closer look at the inclusion fee The formula for calculating the final fee looks like this: inclusion_fee = base_fee + length_fee + [targeted_fee_adjustment * weight_fee]; final_fee = inclusion_fee + tip; In this formula, the targeted_fee_adjustment is a multiplier that can tune the final fee based on the congestion of the network. The base_fee derived from the base weight covers inclusion overhead like signature verification. The length_fee is a per-byte fee that is multiplied by the length of the encoded extrinsic. The weight_fee fee is calculated using two parameters: The ExtrinsicBaseWeight that is declared in the runtime and applies to all extrinsics. The #[pallet::weight] annotation that accounts for an extrinsic's complexity. To convert the weight to Currency, the runtime must define a WeightToFee struct that implements a conversion function, Convert<Weight,Balance> . Note that the extrinsic sender is charged the inclusion fee before the extrinsic is invoked. The fee is deducted from the sender's balance even if the transaction fails upon execution. Accounts with an insufficient balance If an account does not have a sufficient balance to pay the inclusion fee and remain alive‚Äîthat is, enough to pay the inclusion fee and maintain the minimum existential deposit ‚Äîthen you should ensure the transaction is cancelled so that no fee is deducted and the transaction does not begin execution. Substrate does not enforce this rollback behavior. However, this scenario would be a rare occurrence because the transaction queue and block-making logic perform checks to prevent it before adding an extrinsic to a block. Fee multiplier The inclusion fee formula always results in the same fee for the same input. However, weight can be dynamic and‚Äîbased on how WeightToFee is defined‚Äîthe final fee can include some degree of variability. To account for this variability, the Transaction Payment pallet provides the FeeMultiplierUpdate configurable parameter. The default update function is inspired by the Polkadot network and implements a targeted adjustment in which a target saturation level of block weight is defined. If the previous block is more saturated, then the fees are slightly increased. Similarly, if the previous block has fewer transactions than the target, fees are decreased by a small amount. For more information about fee multiplier adjustments, see the Web3 research page . Transactions with special requirements Inclusion fees must be computable prior to execution, and therefore can only represent fixed logic. Some transactions warrant limiting resources with other strategies. For example: Bonds are a type of fee that might be returned or slashed after some on-chain event. For example, you might want to require users to place a bond to participate in a vote. The bond might then be returned at the end of the referendum or slashed if the voter attempted malicious behavior. Deposits are fees that might be returned later. For example, you might require users to pay a deposit to execute an operation that uses storage. If a subsequent operation frees up storage, the user's deposit could be returned. Burn operations are used to pay for a transaction based on its internal logic. For example, a transaction might burn funds from the sender if the transaction creates new storage items to pay for the increased the state size. Limits enable you to enforce constant or configurable limits on certain operations. For example, the default Staking pallet only allows nominators to nominate 16 validators to limit the complexity of the validator election process. It is important to note that if you query the chain for a transaction fee, it only returns the inclusion fee. Default weight annotations All dispatchable functions in Substrate must specify a weight. The way of doing that is using the annotation-based system that lets you combine fixed values for database read/write weight and/or fixed values based on benchmarks. The most basic example would look like this: #[pallet::weight(100_000)] fn my_dispatchable ( ) { // ... } Note that the ExtrinsicBaseWeight is automatically added to the declared weight to account for the costs of simply including an empty extrinsic into a block. Weights and database read/write operations To make weight annotations independent of the deployed database backend, they are defined as a constant and then used in the annotations when expressing database accesses performed by the dispatchable: #[pallet::weight(T::DbWeight::get().reads_writes(1, 2) + 20_000)] fn my_dispatchable ( ) { // ... } This dispatchable does one database read and two database writes in addition to other things that add the additional 20,000. A database access is generally every time a value that is declared inside the #[pallet::storage] block is accessed. However, only unique accesses are counted because after a value is accessed it is cached and accessing it again does not result in a database operation. That is: Multiple reads of the same value count as one read. Multiple writes of the same value count as one write. Multiple reads of the same value, followed by a write to that value, count as one read and one write. A write followed by a read only counts as one write. Dispatch classes Dispatches are broken into three classes: Normal Operational Mandatory If a dispatch is not defined as Operational or Mandatory in the weight annotation, the dispatch is identified as Normal by default. You can specify that the dispatchable uses another class like this: #[pallet::dispatch((DispatchClass::Operational))] fn my_dispatchable ( ) { // ... } This tuple notation also allows you to specify a final argument that determines whether or not the user is charged based on the annotated weight. If you don't specify otherwise, Pays::Yes is assumed: #[pallet::dispatch(DispatchClass::Normal, Pays::No)] fn my_dispatchable ( ) { // ... } Normal dispatches Dispatches in this class represent normal user-triggered transactions. These types of dispatches only consume a portion of a block's total weight limit. For information about the maximum portion of a block that can be consumed for normal dispatches, see AvailableBlockRatio . Normal dispatches are sent to the transaction pool . Operational dispatches Unlike normal dispatches, which represent usage of network capabilities, operational dispatches are those that provide network capabilities. Operational dispatches can consume the entire weight limit of a block. They are not bound by the AvailableBlockRatio . Dispatches in this class are given maximum priority and are exempt from paying the length_fee . Mandatory dispatches Mandatory dispatches are included in a block even if they cause the block to surpass its weight limit. You can only use the mandatory dispatch class for inherent transactions that are submitted by the block author. This dispatch class is intended to represent functions that are part of the block validation process. Because these dispatches are always included in a block regardless of the function weight, it is critical that the validation process prevents malicious nodes from abusing the function to craft blocks that are valid but impossibly heavy. You can typically accomplish this by ensuring that: The operation performed is always light. The operation can only be included in a block once. To make it more difficult for malicious nodes to abuse mandatory dispatches, they cannot be included in blocks that return errors. This dispatch class exists to serve the assumption that it is better to allow an overweight block to be created than to not allow any block to be created at all. Dynamic weights In addition to purely fixed weights and constants, the weight calculation can consider the input arguments of a dispatchable. The weight should be trivially computable from the input arguments with some basic arithmetic: use frame_support :: { dispatch :: { DispatchClass :: Normal , Pays :: Yes , } , weights :: Weight , #[pallet::weight(FunctionOf( |args: (&Vec<User>,)| args.0.len().saturating_mul(10_000), ) ] fn handle_users ( origin , calls : Vec < User > ) { // Do something per user } Post dispatch weight correction Depending on the execution logic, a dispatchable function might consume less weight than was prescribed pre-dispatch. To correct weight, the function declares a different return type and returns its actual weight: #[pallet::weight(10_000 + 500_000_000)] fn expensive_or_cheap ( input : u64 ) -> DispatchResultWithPostInfo { let was_heavy = do_calculation ( input ) ; if ( was_heavy ) { // None means "no correction" from the weight annotation. Ok ( None . into ( ) ) } else { // Return the actual weight consumed. Ok ( Some ( 10_000 ) . into ( ) ) } } Custom fees You can also define custom fee systems through custom weight functions or inclusion fee functions. Custom weights Instead of using the default weight annotations, you can create a custom weight calculation type using the weights module. The custom weight calculation type must implement the following traits: WeighData<T> to determine the weight of the dispatch. ClassifyDispatch<T> to determine the class of the dispatch. Pays<T> to determine whether the sender of the dispatch pays fees. Substrate then bundles the output information of the three traits into the DispatchInfo struct and provides it by implementing the GetDispatchInfo for all Call variants and opaque extrinsic types. This is used internally by the System and Executive modules. ClassifyDispatch , WeighData , and PaysFee are generic over T , which gets resolved into the tuple of all dispatch arguments except for the origin. The following example illustrates a struct that calculates the weight as m * len(args) where m is a given multiplier and args is the concatenated tuple of all dispatch arguments. In this example, the dispatch class is Operational if the transaction has more than 100 bytes of length in arguments and will pay fees if the encoded length is greater than 10 bytes. struct LenWeight ( u32 ) ; impl < T > WeighData < T > for LenWeight { fn weigh_data ( & self , target : T ) -> Weight { let multiplier = self . 0 ; let encoded_len = target . encode ( ) . len ( ) as u32 ; multiplier * encoded_len } } impl < T > ClassifyDispatch < T > for LenWeight { fn classify_dispatch ( & self , target : T ) -> DispatchClass { let encoded_len = target . encode ( ) . len ( ) as u32 ; if encoded_len > 100 { DispatchClass :: Operational } else { DispatchClass :: Normal } } } impl < T > PaysFee < T > { fn pays_fee ( & self , target : T ) -> Pays { let encoded_len = target . encode ( ) . len ( ) as u32 ; if encoded_len > 10 { Pays :: Yes } else { Pays :: No } } } A weight calculator function can also be coerced to the final type of the argument, instead of defining it as a vague type that can be encoded. The code would roughly look like this: struct CustomWeight ; impl WeighData < ( & u32 , & u64 ) > for CustomWeight { fn weigh_data ( & self , target : ( & u32 , & u64 ) ) -> Weight { ... } } // given a dispatch: #[pallet::call] impl < T : Config < I > , I : 'static > Pallet < T , I > { #[pallet::weight(CustomWeight)] fn foo ( a : u32 , b : u64 ) { ... } } In this example, the CustomWeight can only be used in conjunction with a dispatch with a particular signature (u32, u64) , as opposed to LenWeight , which can be used with anything because there aren't any assumptions about <T> . Custom inclusion fee The following example illustrates how to customize your inclusion fee. You must configure the appropriate associated types in the respective module. // Assume this is the balance type type Balance = u64 ; // Assume we want all the weights to have a `100 + 2 * w` conversion to fees struct CustomWeightToFee ; impl WeightToFee < Weight , Balance > for CustomWeightToFee { fn convert ( w : Weight ) -> Balance { let a = Balance :: from ( 100 ) ; let b = Balance :: from ( 2 ) ; let w = Balance :: from ( w ) ; a + b * w } } parameter_types! { pub const ExtrinsicBaseWeight : Weight = 10_000_000 ; } impl frame_system :: Config for Runtime { type ExtrinsicBaseWeight = ExtrinsicBaseWeight ; } parameter_types! { pub const TransactionByteFee : Balance = 10 ; } impl transaction_payment :: Config { type TransactionByteFee = TransactionByteFee ; type WeightToFee = CustomWeightToFee ; type FeeMultiplierUpdate = TargetedFeeAdjustment < TargetBlockFullness > ; } struct TargetedFeeAdjustment < T > ( sp_std :: marker :: PhantomData < T > ) ; impl < T : Get < Perquintill >> WeightToFee < Fixed128 , Fixed128 > for TargetedFeeAdjustment < T > { fn convert ( multiplier : Fixed128 ) -> Fixed128 { // Don't change anything. Put any fee update info here. multiplier } } Where to go next You now know what the weight system is, how it affects transaction fee computation, and how to specify weights for your dispatchable calls. The next step is determining the correct weight to account for the operations your dispatchable performs. You can use Substrate benchmarking functions and frame-benchmarking calls to test your functions with different parameters and empirically determine the correct weight in their worst case scenarios. Benchmark SignedExtension Custom weights for the Example pallet Web3 Foundation Research Last edit: October 24, 2022 ‚Üê Previous Next ‚Üí ON THIS PAGE How fees are calculated Using the transaction payment pallet Transactions with special requirements Default weight annotations Post dispatch weight correction Custom fees Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Pallet coupling Pallet coupling Pallet coupling The term coupling is often used to describe the degree to which two software modules depend on each other. For example, in object-oriented programming tight coupling and loose coupling are used to describe the relationship between objects classes: Tight coupling is when two groups of classes are dependent on each other. Loose coupling is when a class uses an interface that another class exposes. In Substrate, tight pallet coupling and loose pallet coupling are used to describe how a pallet can call functions in another pallet. Both techniques achieve the same thing in different ways, each having certain trade-offs. Tightly coupled pallets Because tight coupling makes working with pallets less flexible and extensible, you would only use tight pallet coupling if a pallet requires inheriting its coupled counterpart as a whole rather than specific types or methods. When writing a pallet that requires tight coupling, you explicitly specify the pallet's Config trait to be bound by the Config trait of the pallet to couple with. All FRAME pallets are tightly coupled to the frame_system pallet. The following example illustrates how to use the Config trait of a pallet called some_pallet to be tightly coupled with the frame_system pallet: pub trait Config : frame_system :: Config + some_pallet :: Config { // --snip-- } This is very similar to using class inheritance in object-oriented programming. Supplying this trait bound implies that this pallet can only be installed in a runtime that also has some_pallet pallet installed. Similar to frame_system , the tight coupling in this example would require you to specify some_pallet in the coupled pallet's Cargo.toml file. Tight coupling has several disadvantages developers should take into account: Maintainability : changes in one pallet will often result in needing to modify the other pallet. Reusability : both modules must be included for one to be used, making it more difficult to integrate a tightly coupled pallet. Loosely coupled pallets In loose pallet coupling, you can specify the traits and function interfaces that certain types need to be bound by. The actual implementation of such types happens outside of the pallet during the runtime configuration‚Äîtypically as code defined in the /runtime/src/lib.rs file. With loose coupling, you can use types and interfaces from another pallet that has implemented the traits, or you can declare a totally new struct, implement those traits, and assign it when implementing the pallet in runtime. As an example, assume you have a pallet that can access account balances and make transfers to another account. This pallet defines a Currency trait, which has an abstract function interface that will implement the actual transfer logic later. pub trait Currency < AccountId > { // -- snip -- fn transfer ( source : & AccountId , dest : & AccountId , value : Self :: Balance , // don't worry about the last parameter for now existence_requirement : ExistenceRequirement , ) -> DispatchResult ; } In a second pallet, you define the MyCurrency associated type and bind it by Currency<Self::AccountId> trait so that you can use the balance transfer logic by calling T::MyCurrency::transfer(...) . pub trait Config : frame_system :: Config { type MyCurrency : Currency < Self :: AccountId > ; } impl < T : Config > Pallet < T > { pub fn my_function ( ) { T :: MyCurrency :: transfer ( & buyer , & seller , price , ExistenceRequirement :: KeepAlive ) ? ; } } Notice that, at this point, you have not specified how the Currency::transfer() logic will be implemented. It is only agreed upon that it will be implemented somewhere. Now, you can use the runtime configuration‚Äî runtime/src/lib.rs ‚Äîto implement the pallet and specify the type to be Balances . impl my_pallet :: Config for Runtime { type MyCurrency = Balances ; } The Balances type is specified in construct_runtime! macro as part of the pallet_balances that implements the Currency trait . With the implementation provided by the runtime, you can make use of Currency<AccountId> trait in your loosely coupled pallet. Many FRAME pallets are coupled to this Currency trait in this way. Choosing a pallet coupling strategy In general, loose coupling provides more flexibility than tight coupling and is considered a better practice from a system design perspective. It guarantees better maintainability, reusability, and extensibility of your code. However, tight coupling can be useful for pallets that are less complex or that have more overlap in methods and types than differences. In FRAME, there are two pallets that are tightly coupled to pallet_treasury : Bounties pallet Tipping pallet As a general rule, the more complex a pallet is, the less desirable it is to tightly couple it. This evokes a concept in computer science called cohesion , a metric used to examine the overall quality of a software system. Where to go next How-to: Use loose coupling How-to: Use tight coupling Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Tightly coupled pallets Loosely coupled pallets Choosing a pallet coupling strategy Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Events and errors Events and errors Events and errors A pallet can emit events when it wants to send notification about changes or conditions in the runtime to external entities like users, chain explorers, or dApps. In custom pallets, you can define: what type of events you want to be emitted what information is contained within those events when those events are emitted Declaring an event Events are created using the #[pallet::event] macro. For example: #[pallet::event] #[pallet::generate_deposit(pub(super) fn deposit_event)] pub enum Event < T : Config > { /// Set a value. ValueSet { value : u32 , who : T :: AccountId } , } Then, the RuntimeEvent type is needed to aggregate them for the runtime. #[pallet::config] pub trait Config : frame_system :: Config { /// The overarching event type. type RuntimeEvent : From < Event < Self >> + IsType << Self as frame_system :: Config > :: RuntimeEvent > ; } Exposing events to your runtime Any events you define in your pallet must be exposed to the runtime in the /runtime/src/lib.rs file. To expose events to the runtime: Open the /runtime/src/lib.rs file in a text editor. Implement the RuntimeEvent type in the configuration trait for your pallet: impl template :: Config for Runtime { type RuntimeEvent = RuntimeEvent ; } Add the RuntimeEvent type to the construct_runtime! macro: construct_runtime! ( pub enum Runtime where Block = Block , NodeBlock = opaque :: Block , UncheckedExtrinsic = UncheckedExtrinsic { // --snip-- TemplateModule : template :: { Pallet , Call , Storage , Event < T > } , //--add-this-------------------------------------> } ) ; In this example, the event is a generic type and requires the <T> parameter. The <T> parameter isn't needed if your events don't use generic types. Depositing an event Substrate provides a default implementation of how to deposit an event using macros. Depositing an event has the following structure: // 1. Use the `generate_deposit` attribute when declaring the Events enum. #[pallet::event] #[pallet::generate_deposit(pub(super) fn deposit_event)] // <------ here ---- #[pallet::metadata(...)] pub enum Event < T : Config > { // --snip-- } // 2. Use `deposit_event` inside the dispatchable function #[pallet::call] impl < T : Config > Pallet < T > { #[pallet::weight(1_000)] pub ( super ) fn set_value ( origin : OriginFor < T > , value : u64 , ) -> DispatchResultWithPostInfo { let sender = ensure_signed ( origin ) ? ; // --snip-- Self :: deposit_event ( RawEvent :: ValueSet ( value , sender ) ) ; } } The default behavior of this function is to call deposit_event from the FRAME system, which writes the event to storage. This function places the event in the System pallet's runtime storage for that block. At the beginning of a new block, the System pallet automatically removes all events that were stored from the previous block. Events deposited using the default implementation are directly supported by downstream libraries like the Polkadot-JS API . However, you can implement your own deposit_event function if you want to handle events differently. Supported types Events can emit any type which supports type encoding using SCALE codec . In the case where you want to use Runtime generic types like AccountId or Balances , you need to include a where clause to define those types as shown in the example above. Listening to events The Substrate RPC does not directly expose an endpoint for querying events. If you used the default implementation, you can see the list of events for the current block by querying the storage of the System pallet. Otherwise, the Polkadot-JS API supports a WebSocket subscription on runtime events. Errors Runtime code should explicitly and gracefully handle all error cases. Functions in the runtime code must be non-throwing functions that never cause the compiler to panic . A common idiom for writing non-throwing Rust code is to write functions that return Result types . The Result enum type possesses an Err variant that allows a function to indicate that it failed to execute successfully without needing to panic. Function calls that can be dispatched to the runtime in the FRAME development evironment must return a DispatchResult type that could be a DispatchError variant if the function encountered an error. Each FRAME pallet can define a custom DispatchError by using the #[pallet::error] macro. For example: #[pallet::error] pub enum Error < T > { /// Error names should be descriptive. InvalidParameter , /// Errors should have helpful documentation associated with them. OutOfSpace , } The FRAME Support module also includes a helpful ensure! macro that can be used to check pre-conditions and emit an error if they are not met. frame_support :: ensure! ( param < T :: MaxVal :: get ( ) , Error :: < T > :: InvalidParameter ) ; Where to go next Frame macros Polkadot-JS API . construct_runtime! macro #[frame_support::pallet] macro [pallet::error] macro Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Declaring an event Exposing events to your runtime Depositing an event Supported types Listening to events Errors Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Randomness Randomness Randomness Because blockchains require deterministic outcomes‚Äîwhere the same input always produces the same output‚Äîfinding an appropriate source to produce seemingly random results presents a unique challenge. However, there are many applications where randomness can be essential to performing certain operations. For example, you want to include randomness to select the validator responsible for producing a block to prevent validator selection from being predictable, and thereby vulnerable to be exploited in an attack. You might also use randomness in statistical or scientific analyses, in cryptographic operations, or in applications involving gaming or gambling. Deterministic randomness In traditional computer applications, what people call random numbers are actually generated using pseudo-randomness . Pseudo-randomness depends on a sufficiently random seed‚Äîprovided by the user or by an external source‚Äîand an algorithm that manipulates the seed to generate a series of seemingly-random numbers. The result is a pseudo-random number that is difficult to predict because of the algorithm used to generate it, but deterministic because the same seed input always produces the same series of output. However, applications that run on the blockchain are more tightly constrained because all authorities in the network must agree on any on-chain value, including any randomness data that is injected. Because of this constraint, you can't use real randomness directly in blockchain applications. For blockchain applications, the most common approach to providing randomness is a cryptographic primitive called a verifiable random function . A verifiable random function (VRF) is a mathematical operation that takes input and produces a random number and a proof of authenticity that this random number was generated by the submitter. The proof can be verified by any challenger to ensure that the random number generation is valid. In the Polkadot ecosystem and Substrate-based chains, verifiable random functions are part of the consensus mechanism provided by the BABE pallet. For more information about the relationship between verifiable random functions and consensus, see Consensus Generate and consume randomness Substrate provides a Randomness trait called that defines the interface between the logic that generates randomness and the logic that consumes randomness . This trait allows you to write the logic for generating randomness and consuming randomness independently of each other. Generating randomness You can implement the Randomness trait in many different ways, depending on the security guarantees and performance trade-offs that your application requires. Substrate includes two examples of how to implement the Randomness trait in pallets that provide different trade-offs between performance, complexity, and security. The insecure randomness pallet provides a random function that generates pseudo-random values based on the block hashes from the previous 81 blocks. This type of randomness performs well but isn't secure. You should only use this pallet in applications with low security requirements or when testing randomness-consuming applications. You shouldn't use this pallet in a production environment. = The BABE pallet provides randomness by using verifiable random functions. This pallet provides production-grade randomness, and is used in Polkadot. If you select this pallet as the source of randomness your blockchain must use the blind assignment of blockchain extension ( BABE ) slot-based consensus for producing blocks. Consuming randomness The Randomness trait provides the following methods for consuming randomness: The random_seed method takes no parameters and return a raw random value. If you call this method multiple times in a block, it returns the same value each time. Therefore, in most cases, you shouldn't use this method directly. The random method takes a byte-array as a context identifier and returns a result that is unique to this context and independent from other contexts as allowed by the underlying randomness source. Pallets that require random values don't need to provide the randomness source but they do need to specify a randomness source that implements the Randomness trait. Security guarantees It's important to keep in mind that the Randomness trait provides a convenient abstraction for defining a randomness source in the runtimes, but the trait itself does not make any security guarantees. It's up to you as a runtime developer to ensure that the randomness source you use meets the security requirements for all of the pallets that consume its randomness. Where to go next How-to: Randomness Randomness Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Deterministic randomness Generate and consume randomness Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Privileged calls and origins Origins Privileged calls and origins The runtime origin is used by dispatchable functions to check where a call has come from. Raw origins Substrate defines three raw origins which can be used in your runtime pallets: pub enum RawOrigin < AccountId > { Root , Signed ( AccountId ) , None , } Root: A system level origin. This is the highest privilege level and can be thought of as the superuser of the runtime origin. Signed: A transaction origin. This is signed by some on-chain account's private key and includes the account identifier of the signer. This allows the runtime to authenticate the source of a dispatch and subsequently charge transaction fees to the associated account. None: A lack of origin. This needs to be agreed upon by the validators or validated by a module to be included. This origin type is more complex because it is designed to bypass certain runtime mechanisms. For example, this origin type might be used to allow validators to insert data directly into a block. Origin call You can construct calls within your runtime with any origin. For example: // Root proposal . dispatch ( system :: RawOrigin :: Root . into ( ) ) // Signed proposal . dispatch ( system :: RawOrigin :: Signed ( who ) . into ( ) ) // None proposal . dispatch ( system :: RawOrigin :: None . into ( ) ) You can look at the source code of the Sudo module for a practical implementation of this. Custom origins In addition to the three core origin types, runtime developers are also able to define custom origins. These can be used as authorization checks inside functions from specific modules in your runtime, or to define custom access-control logic around the sources of runtime requests. Customizing origins allows runtime developers to specify valid origins depending on their runtime logic. For example, it may be desirable to restrict access of certain functions to special custom origins and authorize dispatch calls only from members of a collective . The advantage of using custom origins is that it provides runtime developers a way to configure privileged access over dispatch calls to the runtime. Next steps Learn more Learn about how origin is used in the #[pallet::call] macro. Examples View the Sudo pallet to see how it allows a user to call with Root and Signed origin. View the Timestamp pallet to see how it validates an a call with None origin. View the Collective pallet to see how it constructs a custom Member origin. View our recipe for creating and using a custom origin. References Visit the reference docs for the RawOrigin enum . Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Raw origins Origin call Custom origins Next steps Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Remote procedure calls Remote procedure calls Remote procedure calls Remote procedure calls, or RPC methods, are a way for an external program‚Äîfor example, a browser or front-end application‚Äîto communicate with a Substrate node. In general, these methods enable an RPC client to connect to an RPC server endpoint to request some type of service. For example, you might use an RPC method to read a stored value, submit a transaction, or request information about the chain a node is connected to. The most convenient way to access the default JSON-RPC methods for a Substrate node is through the Polkadot-JS API . Safe and unsafe RPC methods It's important to be aware that RPC methods can provide access to core node operations, including consensus and storage, and can also be exposed as public interfaces to allow external users to submit transactions to or retrieve information from the blockchain. Therefore, for the security of the blockchain, it's important to consider what different RPC methods expose and whether they should be restricted to running on a local node or made publicly available. Public RPC interfaces Substrate nodes provide the following command-line options that allow you to expose the RPC interfaces publicly: --ws-external --rpc-external --unsafe-ws-external --unsafe-rpc-external By default, the node will refuse to start if you try to expose an RPC interface and run a validator node at the same time. The --unsafe-* flags allow you to suppress this security measure. Exposing RPC interfaces can open up a huge surface of attacks and has to be carefully reviewed. There are quite a few RPC methods that you can use to control the node's behavior, but you should avoid exposing. For example, you should not expose the following RPC methods: submit_extrinsic - allows submitting transactions to local pool. insert_key - allows inserting private keys to local keystore. rotate_keys - session keys rotation. remove_extrinsic - remove and ban extrinsic from the pool. add_reserved_peer - add reserved node. remove_reserved_peer - removed reserved node. You should also avoid exposing RPC methods that can take a long time to execute, potentially blocking the client from syncing. For example, you should avoid using the following RPC methods: storage_keys_paged - get all the keys in the state with a particular prefix and pagination support. storage_pairs - get all the keys in the state with a particular prefix together with their values. These RPCs are declared by using the #[rpc(name = "rpc_method")] macro, where rpc_method is be the name of the function, for example, submit_extrinsic . It's critical to filter out these kind of calls if the requests are coming from untrusted users. The way to do it is through a JSON-RPC proxy that is able to inspect calls and only pass an allowed set of API calls. RPCs for remote_externalities Substrate also provides some specialized RPC methods to call remote_externalities for a node. These specialized methods for remote externalities enable you to make one-off RPC calls to a Substrate node to get information about blocks and headers. The information returned by these calls can be useful for testing purposes with tools like try-runtime . Endpoints When you start a Substrate node locally, there are two endpoints available by default: HTTP endpoint: http://localhost:9933/ WebSocket endpoint: ws://localhost:9944/ Most of the Substrate front-end libraries and tools use the WebSocket endpoint to interact with the blockchain. For example, if you use the Polkadot-JS application to connect to a local node or a public chain, your are typically connecting to the WebSocket endpoint. WebSocket connections allow for bidirectional communication between the front-end application and the backend node responding to requests. However, you can also call RPC methods individually without keeping an open communication channel by connecting to the HTTP endpoint using curl commands. For example, you can use curl commands to get system information or subscribe to a chain to receive notification when there are specific types of changes to the block state. To call RPC methods using the HTTP endpoint: Open a terminal shell and change to the root directory for the Substrate node template. Start the node locally in development mode by running the following command: ./target/release/node-template --dev Connect to the local node and call the rpc_methods endpoint by running the following command: curl -H "Content-Type: application/json" -d '{"id":1, "jsonrpc":"2.0", "method": "rpc_methods"}' http://localhost:9933/ This command returns a list of the JSON-RPC methods exposed for the local node. Call additional methods using the appropriate method name. For example, you can run the following command to get version information about the local node: curl -H "Content-Type: application/json" -d '{"id":1, "jsonrpc":"2.0", "method": "system_version"}' http://localhost:9933/ In most cases, connecting to the RPC endpoint directly returns JSON-formatted results. For example: { "jsonrpc" : "2.0" , "result" : "4.0.0-dev-de262935ede" , "id" :1 } For the return value to be human-readable, you can decode it using SCALE codec. For more information about encoding and decoding information, see Type encoding (SCALE) . Each storage item has a relative storage key associated to it which is used to query storage . This is how RPC endpoints know where to look. Examples state_getMetadata To get metadata for a local node, you can run the following command: curl -H "Content-Type: application/json" -d '{"id":1, "jsonrpc":"2.0", "method": "state_getMetadata"}' http://localhost:9933/ This command returns the metadata in hex-encoded bytes rather than a human-readable format. The JavaScript for this RPC request looks like this: function get_metadata_request ( endpoint ) { let request = new Request ( endpoint , { method : "POST" , body : JSON . stringify ( { id : 1 , jsonrpc : "2.0" , method : "state_getMetadata" , } ) , headers : { "Content-Type" : "application/json" } , } ) ; return request ; } Naive text decoding: function decode_metadata ( metadata ) { return new TextDecoder ( ) . decode ( util . hexToU8a ( metadata ) ) ; } state_getStorage RPC request: Request : { "id" : 1 , "jsonrpc" : "2.0" , "method" : "state_getStorage" , [ "{storage_key}" ] } Where storage_key is a parameter generated by the name of a pallet, function and key (optionally): function get_runtime_storage_parameter_with_key ( module_name , function_name , key ) { // We use xxhash 128 for strings the runtime developer can control let module_hash = util_crypto . xxhashAsU8a ( module_name , 128 ) ; let function_hash = util_crypto . xxhashAsU8a ( function_name , 128 ) ; // We use blake2 256 for strings the end user can control let key_hash = util_crypto . blake2AsU8a ( keyToBytes ( key ) ) ; // Special syntax to concatenate Uint8Array let final_key = new Uint8Array ( [ ... module_hash , ... function_hash , ... key_hash ] ) ; // Return a hex string return util . u8aToHex ( final_key ) ; } Where to go next Rust implementation of JSON-RPC Type encoding (SCALE) Runtime storage Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Safe and unsafe RPC methods RPCs for remote_externalities Endpoints Examples Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Application development Application development Application development As a blockchain developer, you might not be directly involved in building front-end applications. However, most applications that run on a blockchain require some form of front-end or user-facing client to enable users or other programs to access and modify the data that the blockchain stores. For example, you might develop a browser-based, mobile, or desktop application that allows users to submit transactions, post articles, view their assets, or track previous activity. The backend for that application is configured in the runtime logic for your blockchain, but it's the front-end client that makes the runtime features accessible to your users. For your custom chain to be useful to others, you'll need to provide some type of a client application that allows users to view, interact with, or update information that the blockchain keeps track of. In this article, you'll learn how you can expose information about your runtime so that client applications can use it, see examples of the information exposed, and explore tools and libraries that use this information. Exposing runtime information as metadata To interact with a Substrate node or the information stored in the blockchain, you need to know how to connect to the chain and how to access the features the runtime exposes to the outside world. In general, this interaction involves a remote procedure call to request information you're interested in retrieving or updating. As an application developer, however, you typically need to know quite a bit more about the runtime logic, including the following details: The version of the runtime that the application is connecting to. The application programming interfaces that the runtime supports. The pallets that are implemented for that specific runtime. All of the functions and their type signatures that are defined for that specific runtime. All of the custom types that are defined for that specific runtime. All of the parameters that the runtime exposes for users to set. Because Substrate is modular and provides a composable framework for building a blockchain, there's no predefined schema of properties. Instead, every runtime is configured with its own set of properties and those properties‚Äîincluding functions and types‚Äîcan change over time with upgrades. To capture all of the information that's unique to a runtime, Substrate enables you to generate the runtime metadata schema. The metadata for a runtime describes all of the pallets and types that are defined for a specific version of the runtime. For every pallet, the metadata includes information about its storage items, functions, events, errors, and constants. The metadata also includes type definitions for any custom types included in the runtime. Because it provides a complete inventory of the runtime, the metadata is the key to enabling client applications to interact with the node, parse responses, and format message payloads. Generating metadata To minimize the bandwidth required to transmit data over the network, the metadata schema is encoded using the SCALE codec library . This encoding is done automatically for you when you compile a node by using the scale-info crate. At a high level, generating the metadata involves the following steps: The pallets in the runtime logic expose all of the callable functions, types, parameters, and documentation that need to be encoded in the metadata. The scale-info crate collects type information for the pallets in the runtime and builds a registry of the pallets that exist in a particular runtime and the relevant types for each pallet in the registry. The type information is detailed enough to enable encoding and decoding for every type. The frame-metadata crate describes the structure of the runtime based on the registry provided by the scale-info crate. Substrate nodes provide the RPC method state_getMetadata to return a complete description of all the types in the current runtime as a hex-encoded vector of SCALE-encoded bytes. The following diagram provides a simplified overview of how the metadata is generated when the runtime logic is compiled and then accessed by connecting to the node with an RPC request. Getting metadata for a runtime There are several ways you can get the metadata for a runtime. For example, you can do any of the following: Use Polkadot/Substrate Portal to connect to a blockchain or node and select the state endpoint and the getMetadata method to return the metadata in JSON format. Use the command-line polkadot-js-api to call the state_getMetadata RPC method to return the metadata as a hex-encoded vector of SCALE-encoded bytes. Use the subxt metadata command to download the metadata in JSON, hex, or raw bytes. Use the sidecar API and /runtime/metadata endpoint to connect to a node and retrieve the metadata in JSON format. The type information provided by the metadata enables applications to communicate with nodes with different versions of the runtime and across chains that expose different calls, events, types, and storage items. The metadata also allows libraries to generate almost all of the code needed to communicate with a given Substrate node, enabling libraries like subxt to generate front-end interfaces that are specific to a target chain. Client applications and metadata Client applications use the metadata to interact with the node, parse responses, and format message payloads sent to the node. To use the metadata, client applications must use the SCALE codec library to encode and decode RPC payloads. Because the metadata exposes how every type is expected to be decoded, applications can send, retrieve, and process application information without manual encoding and decoding. Metadata format Although the SCALE-encoded bytes can be decoded using the frame-metadata and parity-scale-codec libraries, there are other tools‚Äîsuch as subxt and the Polkadot-JS API‚Äîthat can convert the raw data to human-readable JSON format. The types and type definitions included in the metadata returned by the state_getMetadata RPC call depend on the metadata version of the runtime. In general, the metadata includes the following information: A constant that identifies the file as containing metadata. The version of the metadata format used in the runtime. Type definitions for all types used in the runtime and generated by the scale-info crate. Pallet information for all of the pallets included in the runtime in the order that they are defined in the construct_runtime macro. The following example illustrates a condensed and annotated section of metadata decoded and converted to JSON: [ 1635018093 , { "V14" : { "types" : { "types" : [ { // index of types } ] } , "pallets" : [ { // index of pallets and within each pallet the metadata each pallet exposes } ] , "extrinsic" : { "ty" : 126 , // the type index identifier that defines the format of an extrinsic "version" : 4 , // the transaction version used to encode and decode an extrinsic "signed_extensions" : [ { // index of signed extensions } ] } , "ty" : 141 // the type ID for the system pallet } } ] The constant 1635018093 is a magic number that identifies the file as a metadata file. The rest of the metadata has divided into the types , pallets and extrinsic sections. The types section contains an index of the types and for each type information about its type signature. The pallets section contains information about each of the pallets in the runtime. The extrinsic section describes the type identifier and transaction format version that the runtime is using. Different extrinsic versions can have different formats, especially when considering signed transactions . Pallets The following is a condensed and annotated example of a single element in the pallets array: { "name" : "Sudo" , // name of the pallet "storage" : { // storage information for the pallet "prefix" : "Sudo" , // database prefix for the pallet storage items "entries" : [ { "name" : "Key" , "modifier" : "Optional" , "ty" : { "Plain" : 0 } , "default" : [ 0 ] , "docs" : [ "The `AccountId` of the sudo key." ] } ] } , "calls" : { // pallet call types "ty" : 117 // type identifier in the types section } , "event" : { // pallet event types "ty" : 42 // type identifier in the types section } , "constants" : [ ] , // pallet constants "error" : { // pallet error types "ty" : 124 // type identifier in the types section } , "index" : 8 // index identifier for the pallet in the runtime } , Every element contains the name of the pallet that it represents and information about its storage, calls, events, and errors. You can look up details about the definition of the calls, events, and errors by viewing the type index identifier. The type index identifier for each item is the u32 integer used to access the type information for that item. For example, the type index identifier for calls in the Sudo pallet is 117. If you view information for that type identifier in the types section of the metadata, it provides information about the available calls including the documentation for each call. For example, the following is a condensed excerpt of the calls for the Sudo pallet: { "id" : 117 , "type" : { "path" : [ "pallet_sudo" , "pallet" , "Call" ] , "params" : [ { "name" : "T" , "type" : null } ] , "def" : { "variant" : { "variants" : [ { "name" : "sudo" , "fields" : [ { "name" : "call" , "type" : 114 , "typeName" : "Box<<T as Config>::RuntimeCall>" } ] , "index" : 0 , "docs" : [ "Authenticates the sudo key and dispatches a function call with `Root` origin." , ] } , { "name" : "sudo_unchecked_weight" , "fields" : [ { "name" : "call" , "type" : 114 , "typeName" : "Box<<T as Config>::RuntimeCall>" } , { "name" : "weight" , "type" : 8 , "typeName" : "Weight" } ] , "index" : 1 , "docs" : [ "Authenticates the sudo key and dispatches a function call with `Root` origin." , ] } , { "name" : "set_key" , "fields" : [ { "name" : "new" , "type" : 103 , "typeName" : "AccountIdLookupOf<T>" } ] , "index" : 2 , "docs" : [ "Authenticates the current sudo key and sets the given AccountId (`new`) as the new sudo" , ] } , { "name" : "sudo_as" , "fields" : [ { "name" : "who" , "type" : 103 , "typeName" : "AccountIdLookupOf<T>" } , { "name" : "call" , "type" : 114 , "typeName" : "Box<<T as Config>::RuntimeCall>" } ] , "index" : 3 , "docs" : [ "Authenticates the sudo key and dispatches a function call with `Signed` origin from" , "a given account." , ] } ] } } , } , For each field, you can access type information and metadata for the following: Storage metadata provides the information that is required to enable applications to get information for specific storage items. Call metadata includes information about the runtime calls defined by the #[pallet] macro including call names, arguments and documentation. Event metadata provides the metadata generated by the #[pallet::event] macro, including the name, arguments, and documentation for each pallet event. Constants metadata provides metadata generated by the #[pallet::constant] macro, including the name, type, and hex-encoded value of the constant. Error metadata provides metadata generated by the #[pallet::error] macro, including the name and documentation for each pallet error. You should note that type identifiers change from time to time. You should avoid relying type identifiers in your applications. Extrinsic Extrinsic metadata is generated by the runtime and provides useful information about how transactions are formatted. When decoded, the metadata contains the transaction version and the list of signed extensions. For example: "extrinsic" : { "ty" : 126 , "version" : 4 , "signed_extensions" : [ { "identifier" : "CheckNonZeroSender" , "ty" : 132 , "additional_signed" : 41 } , { "identifier" : "CheckSpecVersion" , "ty" : 133 , "additional_signed" : 4 } , { "identifier" : "CheckTxVersion" , "ty" : 134 , "additional_signed" : 4 } , { "identifier" : "CheckGenesis" , "ty" : 135 , "additional_signed" : 11 } , { "identifier" : "CheckMortality" , "ty" : 136 , "additional_signed" : 11 } , { "identifier" : "CheckNonce" , "ty" : 138 , "additional_signed" : 41 } , { "identifier" : "CheckWeight" , "ty" : 139 , "additional_signed" : 41 } , { "identifier" : "ChargeTransactionPayment" , "ty" : 140 , "additional_signed" : 41 } ] } , "ty" : 141 } } ] The type system is composite. Each type identifier contains a reference to a specific type or to another type identifier that provides information about the associated primitive types. For example, you can encode the BitVec<Order, Store> type, but to decode it properly you must know the types used for the Order and Store types. To find type information for Order and Store , you can use the path in the decoded JSON to locate their type identifiers. RPC APIs Substrate comes with the following APIs to interact with a node: AuthorApiServer : An API to make calls into a full node, including authoring extrinsics and verifying session keys. ChainApiServer : An API to retrieve block header and finality information. OffchainApiServer : An API for making RPC calls for offchain workers. StateApiServer : An API to query information about on-chain state such as runtime version, storage items, and proofs. SystemApiServer : An API to retrieve information about network state, such as connected peers and node roles. Connecting to a node Applications typically connect to Substrate nodes by using JSON-RPC methods through an open HTTP or WebSocket port. Most applications use a WebSocket port because a single connection can be used for multiple messages to and from a node. With an HTTP connection, applications can only send and receive responses one message at a time. The most common reason you would use HTTP to connect to a node is if you want to fetch data using offchain workers. For more information about using offchain workers, see Offchain operations . As an alternative to connecting using RPC, you can use the Substrate Connect and a light client node to connect to Substrate-based blockchains. Substrate Connect runs in a browser and allows applications to create their own light client node and connect directly to the exposed JSON-RPC endpoint. Applications that integrate Substrate Connect rely on in-browser local memory to establish a connection with the light client node. Building front-end applications The following libraries use the JSON-RPC API to enable applications to interact with Substrate nodes: Name Description Language Chain API Provides a TypeScript toolkit for crafting interactions with Substrate-based chains. The toolkit includes FRAME utilities, a functional effect system, and a fluent API to facilitate multi-step, multi-chain interactions for end users without compromising performance or safety. Polkadot JS API Provides a Javascript library for building applications that can dynamically adapt to changes in a node‚Äîsuch as block explorers or chain-agnostic services‚Äîwhen interacting with Substrate-based chains. You can use this library in combination with popular front-end frameworks such as React. Javascript Polkadot JS extension Provides an API for interacting with browser extensions and providers built with the Polkadot JS API. Javascript Substrate Connect Provides a library and a browser extension to build applications that connect directly to Substrate-based chains using an in-browser light client node. Substrate Connect enables you to build applications that connect to multiple chains, providing end users with a single experience if they use your application to interact with multiple chains. Javascript subxt Provides a Rust library that generates a statically-typed Rust interface to interact with a node's RPC APIs based on a target chain's metadata. The subxt ‚Äîsubmit extrinsics‚Äîlibrary enables you to build lower-level applications‚Äîsuch as non-browser graphical user interfaces, chain-specific CLIs, or user-facing applications that require type-safe communication between the node and the generated interface‚Äîthat prevent users from constructing transactions with bad inputs or submitting calls that don't exist. Rust txwrapper Provides a Javascript library for generating signed Substrate transactions offline. This library enables you to write scripts to generate signed transactions while offline that can later by submitted to a node. This functionality is especially useful for testing and decoding transactions. Javascript For more information about the JSON-RPC API and the latest interface specification, see the JSON-RPC specification . Where to go next Substrate Connect Install the front-end template Generate a metadata QR code Get backwards-compatible metadata (desub) Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Exposing runtime information as metadata Generating metadata Getting metadata for a runtime Client applications and metadata Metadata format RPC APIs Connecting to a node Building front-end applications Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Chain specification Chain spec Chain specification In Substrate, a chain specification is the collection of information that describes a Substrate-based blockchain network. For example, the chain specification identifies the network that a blockchain node connects to, the other nodes that it initially communicates with, and the initial state that nodes must agree on to produce blocks. The chain specification is defined using the ChainSpec struct . The ChainSpec struct separates the information required for a chain into two parts: A client specification that contains information used by the Substrate outer node to communicate with network participants and send data to telemetry endpoints. Many of these chain specification settings can be overridden by command-line options when starting a node or can be changed after the blockchain has started. The initial genesis state that all nodes in the network agree on. The genesis state must be established when the blockchain is first started and it cannot be changed thereafter without starting an entirely new blockchain. Customizing outer node settings For the outer node, the chain specification controls information such as: The boot nodes the node communicates with. The server endpoints for the node to send telemetry data to. The human- and machine-readable names for the network the node connects to. Because the Substrate framework is extensible, you can also customize the chain specification to include additional information. For example, you can to configure the outer node to connect to specific blocks at specific heights to prevent long range attacks when syncing a new node from genesis. Note that you can customize outer node settings after genesis. However, nodes only add peers that use the same protocolId . Customizing the genesis configuration All nodes in the network must agree on the genesis state before they can agree on any subsequent blocks. The information configured in the genesis portion of a chain specification is used to create a genesis block. It takes effect when you start the first node and cannot be overridden with command-line options. However, you can configure some information in the genesis portion of a chain specification. For example, you can customize the genesis portion of the chain specification to include information such as: Initial token holder balances. Accounts that are initially part of a governance council. The administrative account that controls the sudo key. Substrate nodes also include the compiled WebAssembly for the runtime logic on the chain, so the initial runtime must also be supplied in the chain specification. Storing chain specification information The information in the chain specification can be stored as Rust code or as a JSON file. Substrate nodes typically include at least one, and often many, hard-coded chain specifications. Including this information as Rust code directly in the node ensures that the node can connect to at least one chain without any additional information supplied by the node operator. If you are building a blockchain with the intent to define a main network, this main network specification is usually hard-coded in the outer node. Alternatively, you can use the build-spec subcommand to serialize the chain specification into a JSON file. It is common to distribute a JSON-encoded chain specification with a node binary when launching a test network or a private chain. Providing the chain specification to start a node Each time you start a node, you provide the chain specification that the node should use. In the simplest case, the node uses a default chain specification that is hard-coded into the node binary. You can choose an alternative hard-coded chain specification by using the --chain command-line option when you start a node. For example, you can instruct the node to use the chain specification associated with the string "local" by specifying --chain local as a command-line option. If you don't want to start a node with a hard-coded chain specification, you can provide it as a JSON file. For example, you can instruct the node to use the chain specification in the someCustomSpec.json file by specifying --chain=someCustomSpec.json as a command-line option. If you specify a JSON file, the node attempts to de-serialize the provided JSON chain specification, and then use it. Declaring storage items for a runtime In most cases, a Substrate runtime requires some storage items to be configured at genesis. For example, if you are developing the runtime with FRAME, any storage item that is declared with the Config trait in the runtime requires configuration at genesis. These storage values are configured in the genesis portion of the chain specification. For information about how to set initial values for storage items in a pallet, see Genesis configuration . Creating a custom chain specification If you are creating a one-off network for development, testing, or demonstration purposes, you might want a fully customized chain specification. To create a completely customized chain spec, you can export the default chain spec to JSON format, then edit the fields in the JSON file. For example, you can use the build-spec sub-command to export the chain specification to a JSON file: substrate build-spec > myCustomSpec.json After you export the chain spec, you can modify any of its fields in a text editor. For example, you might want to change the network name, bootnodes, and any genesis storage items, such as token balances. After editing the JSON file, you can start the node using the customized JSON. For example: substrate --chain = myCustomSpec.json Raw chain specifications Substrate nodes support runtime upgrades. With runtime upgrades, the blockchain's runtime can be different than when the chain began. Chain specifications contain information structured in a way that can be understood by the node's runtime. For example, consider this excerpt from the default chain specification for the Substrate node template: "sudo" : { "key" : "5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY" } In the JSON file, this key and its associated value are human-readable text. However, this information can't be stored in this format in the underlying storage structures that Substrate uses. Before you can use the chain specification to initialize the genesis storage for a node, the human-readable keys must be transformed into actual storage keys that allow the values to be stored in the storage trie . This transformation is straight-forward, but it requires that the chain specification to be encoded in a format that node runtime can read. To enable a node with an upgraded runtime to synchronize with a chain from genesis, the human-readable chain specification is encoded in a raw format. The raw format enables you distribute chain specifications that all nodes can use to synchronize the chain even after runtime upgrades. Substrate-based nodes support the --raw command-line option to produce the raw chain specifications. For example, you can produce the raw chain specification for a human-readable myCustomSpec.json file by running the following command: substrate build-spec --chain = myCustomSpec.json --raw > customSpecRaw.json After the conversion to the raw format, the sudo key snippet looks like this: "0x50a63a871aced22e88ee6466fe5aa5d9" : "0xd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d" , Where to go next Add trusted nodes How-to: Configure genesis state How-to: Customize a chain specification Node template chain specification ChainSpec struct ProtocolId struct Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Customizing outer node settings Customizing the genesis configuration Storing chain specification information Providing the chain specification to start a node Declaring storage items for a runtime Raw chain specifications Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Genesis configuration Genesis configuration Genesis configuration The first block produced by any blockchain is referred to as the genesis block. The hash associated with this block is the top-level parent of all blocks produced after that first block. The Substrate node template provides the genesis configuration‚Äîthe initial state‚Äîfor a subset of pallets by default. As you add custom logic to the runtime‚Äîfor example, by adding predefined or custom pallets‚Äîyou might find that you want to modify the genesis configuration to include other storage items or set different initial values. As you learned in Chain specification , the chain specification you use to start a node determines the genesis configuration for that node. However, the chain specification doesn't create the storage items that get initialized when you start a node. Instead, the storage items are defined in the pallets included in the runtime as described in Runtime storage . After you create storage items for the runtime, you can choose whether they should be set to some initial value as part of the genesis configuration and included in the genesis block. To specify the storage items that you want to set an initial state for, Substrate provides two specialized FRAME attribute macros. The macros you can use to initialize storage items as part of the genesis configuration for a chain are: The #[pallet::genesis_config] macro defines the GenesisConfig data type and initializes storage items. The #[pallet::genesis_build] macro builds the genesis configuration. These macros are used in combination with the chain specification to define the initial state of the runtime. Configure a simple storage value The following example demonstrates adding a single storage value to the genesis configuration for the pallet_template . By default, the pallet_template has one storage item that isn't initialized in the genesis block. This example illustrates how you can use the #[pallet::genesis_config] and #[pallet::genesis_build] macros to set an initial value for the storage value as part of the genesis configuration for your chain. Configure macros in the pallet To initialize a storage item for the pallet_template : Open a new terminal shell and navigate to the root directory for the node template. Open the pallets/template/src/lib.rs file in a text editor. Add the #[pallet::genesis_config] macro and add the storage value something as the GenesisConfig storage item for the pallet. For example, add the following macro to the file: // Test Genesis Configuration #[pallet::genesis_config] #[derive(Default)] pub struct GenesisConfig { pub something : u32 , } In this example, the `#[derive(Default)]` macro is required to satisfy a trait bound requirement in `frame_support::traits::GenesisBuild`. 4. Add the `#[pallet::genesis_build]` macro: ```rust #[pallet::genesis_build] impl<T: Config> GenesisBuild<T> for GenesisConfig { fn build(&self) { } } In this example, there's no special handling for the build function to perform. Save your changes and close the file. Verify that the pallet compiles by running the following command: cargo build --package pallet-template Configure the chain specification Now that you have configured the pallet to initialize a storage value in the genesis block, you can set an initial value for that storage item in the chain specification. Open the node/src/chain_spec.rs file in a text editor. Add the TemplateModuleConfig to the node_template_runtime . For example: use node_template_runtime :: { AccountId , AuraConfig , BalancesConfig , RuntimeGenesisConfig , GrandpaConfig , Signature , SudoConfig , SystemConfig , TemplateModuleConfig , WASM_BINARY , } ; Locate the GenesisConfig and set the initial value for the something storage item. For example, in the node/src/chain_spec.rs file: -> GenesisConfig { GenesisConfig { system : SystemConfig { // Add Wasm runtime to storage. code : wasm_binary . to_vec ( ) , } , template_module : TemplateModuleConfig { something : 221u32 , } , transaction_payment : Default :: default ( ) , } } Adding genesis configuration to the runtime After you use the #[pallet::genesis_config] macro to add the GenesisConfig to each pallet where it's needed, you must include the Config trait for each pallet in the runtime to enable the runtime to initialize storage items in the genesis block. All of the GenesisConfig types for the pallets that included in the construction of the runtime are then aggregated into a single RuntimeGenesisConfig type for that runtime. The aggregated RuntimeGenesisConfig implements the BuildStorage trait to build all of the initial storage items for the runtime. For example, the node template runtime builds storage items for the following pallets that have a RuntimeGenesisConfig specified by default: System pallet Aura pallet Grandpa pallet Balances pallet TransactionPayment pallet Sudo pallet System pallet #[pallet::genesis_config] pub struct GenesisConfig { #[serde(with = "sp_core::bytes" )] pub code : Vec < u8 > , } Aura pallet #[pallet::genesis_config] pub struct GenesisConfig < T : Config > { pub authorities : Vec < T :: AuthorityId > , } Grandpa pallet #[pallet::genesis_config] pub struct GenesisConfig { pub authorities : AuthorityList , } Balances pallet #[pallet::genesis_config] pub struct GenesisConfig < T : Config < I > , I : 'static = ( ) > { pub balances : Vec < ( T :: AccountId , T :: Balance ) > , } TransactionPayment pallet #[pallet::genesis_config] pub struct GenesisConfig { pub multiplier : Multiplier , } Sudo pallet #[pallet::genesis_config] pub struct GenesisConfig < T : Config > { /// The `AccountId` of the sudo key. pub key : Option < T :: AccountId > , } Because these pallets include the #[pallet::genesis_config] macro with a GenesisConfig and have the Config trait defined in the runtime, they are aggregated into node_template_runtime::RuntimeGenesisConfig struct for the runtime: pub struct RuntimeGenesisConfig { pub system : SystemConfig , pub aura : AuraConfig , pub grandpa : GrandpaConfig , pub balances : BalancesConfig , pub transaction_payment : TransactionPaymentConfig , pub sudo : SudoConfig , } Ultimately, the RuntimeGenesisConfig is exposed by way of the ChainSpec trait. For a more complete example of genesis storage configuration for Substrate, see the chain specification that ships with the Substrate code base . Initialize storage items within a pallet You can use the #[pallet::genesis_build] macro to define the initial state of storage items within the pallet itself. Defining the genesis configuration within a pallet allows you to access the pallet's private functions. The following example demonstrates using #[pallet::genesis_config] and #[pallet::genesis_build] to set the initial value of a storage item. In this example, there are two storage items: A list of member account identifiers. A specific account identifier that designates a member from the list to be the prime member. The macros and data types for this example are defined in the my_pallet/src/lib.rs file: #[pallet::genesis_config] struct GenesisConfig { members : Vec < T :: AccountId > , prime : T :: AccountId , } #[pallet::genesis_build] impl < T : Config > GenesisBuild < T > for GenesisConfig { fn build ( & self ) { Pallet :: < T > :: initialize_members ( & self . members ) ; SomeStorageItem :: < T > :: put ( self . prime ) ; } } The genesis configuration is defined in the node/src/chain_spec.rs file: GenesisConfig { my_pallet : MyPalletConfig { members : LIST_OF_IDS , prime : ID , } , } You can also use the genesis_build macro to define a GenesisConfig attribute that is not bound to a particular storage item. This can be useful if you want to invoke a private helper function within your pallet that sets several storage items, or to invoke a function defined in some other pallets included within your pallet. For example, using an imaginary private function called intitialize_members , the code might look like this: In my_pallet/src/lib.rs : #[pallet::genesis_config] struct GenesisConfig { members : Vec < T :: AccountId > , prime : T :: AccountId , } #[pallet::genesis_build] impl < T : Config > GenesisBuild < T > for GenesisConfig { fn build ( & self ) { Pallet :: < T > :: initialize_members ( & config . members ) ; SomeStorageItem :: < T > :: put ( self . prime ) ; } } In chain_spec.rs : GenesisConfig { my_pallet : MyPalletConfig { members : LIST_OF_IDS , prime : ID , } , } Last edit: June 27, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Configure a simple storage value Adding genesis configuration to the runtime Initialize storage items within a pallet Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Build process Build process Build process In Architecture , you learned that a Substrate node consists of an outer node host and a runtime execution environment. These node components communicate with each other through runtime API calls and host function calls. In this section, you'll learn more about how the Substrate runtime is compiled into a platform-native executable and into a WebAssembly (Wasm) binary that is stored on the blockchain. After you see the inner-working of how the binaries are compiled, you'll learn more about why there are two binaries, when they are used, and how you can change the execution strategies, if you need to. Compiling an optimized artifact You probably already know that you can compile a Substrate node by running the cargo build --release command in the root directory for a Substrate node project. This command builds both the platform-specific executable and WebAssembly binaries for the project and produces an optimized executable artifact. Producing the optimized executable artifact includes some post-compilation processing. As part of the optimization process, the WebAssembly runtime binary is compiled and compressed through a series of internal steps before it's included in the genesis state for a chain. To give you a better understanding of the process, the following diagram summarizes the steps. The following sections describe the build process in more detail. Build the WebAssembly binary The wasm-builder is a tool that integrates the process of building the WebAssembly binary for your project into the main cargo build process. This tool is published in the substrate-wasm-builder crate. When you start the build process, cargo builds a dependency graph from all of the Cargo.toml in the project. The runtime build.rs module then uses the substrate-wasm-builder crate to compile the Rust code for the runtime into a WebAssembly binary, creating the initial binary artifact. Features included in WebAssembly By default, the wasm-builder enables all of the features defined for the project in both the WebAssembly binary and platform-native executable except for the default and std features that are only enabled for the native build. Environment variables to customize the build process You can use the following environment variables to customize how the WebAssembly binaries are built: Use this variable If you want to do this SKIP_WASM_BUILD Skip building the WebAssembly binary. This is useful when you only need to recompile the native binary. However, if the WebAssembly binary doesn't exist, no binaries are compiled. You can skip WebAssembly builds for individual projects by including the PROJECT NAME in the environment variable. For example, to skip build the WebAssembly binary for the cargo project node-runtime, you can use the environment variable SKIP NODE RUNTIME WASM_BUILD. WASM_BUILD_TYPE Specify whether the WebAssembly binary is a release build or a debug build. By default, the build type you specify for the cargo command is used. FORCE_WASM_BUILD Force a WebAssembly build. This environment variable is rarely required because the wasm-builder instructs cargo to check for file changes. WASM_BUILD_RUSTFLAGS Extend the RUSTFLAGS passed to the cargo build command while building the WebAssembly binary. WASM_BUILD_NO_COLOR Disable color output of the WebAssembly build. WASM_TARGET_DIRECTORY Copy the WebAssembly binary to the specified directory. The path needs to be absolute. WASM_BUILD_TOOLCHAIN Specify the toolchain to use to build the WebAssembly binaries. The format needs to be the same as used by cargo , for example, nightly-2020-02-20 . CARGO_NET_OFFLINE Prevent network access for some or all processes launched to support offline environments. Compact and compress the WebAssembly binary The substrate-wasm-builder crate uses lower-level processes to optimize instruction sequences and remove any unnecessary code‚Äîsuch as the code used for debugging‚Äîto create a compact WebAssembly binary. The binary is then further compressed to minimize the size of the final WebAssembly binary. When the compiler processes the runtime/src/lib.rs file for the node, it sees the requirement to include the generated WebAssembly binary: include! ( concat! ( env! ( "OUT_DIR" ) , "/wasm_binary.rs" ) ) ; This code includes the compact WebAssembly binary ( WASM_BINARY ) and the uncompressed WebAssembly binary generated by the compiler ( WASM_BINARY_BLOATY ) in its compilation result, and the final executable binary for the project is generated. At each stage of the build process, the WebAssembly binary is compressed to a smaller and smaller size. For example, you can compare the sizes of each WebAssembly binary artifact for Polkadot: .rw-r--r-- 1 .2M pep 1 Dec 16 :13 ‚îÇ ‚îú‚îÄ‚îÄ polkadot_runtime.compact.compressed.wasm .rw-r--r-- 5 .1M pep 1 Dec 16 :13 ‚îÇ ‚îú‚îÄ‚îÄ polkadot_runtime.compact.wasm .rwxr-xr-x 5 .5M pep 1 Dec 16 :13 ‚îÇ ‚îî‚îÄ‚îÄ polkadot_runtime.wasm You should always use the fully compressed runtime ( *_runtime.compact.compressed.wasm ) WebAssembly binaries for on-chain upgrades and relay chain validation. In most cases, there's no need to use the initial WebAssembly binary or interim compact artifacts. Execution strategies After you have compiled the node with the native and WebAssembly runtime, you use command-line options to specify how the node should operate. For details about the command-line options you can use to start the node, see the node-template command-line reference. When you start the node, the node executable uses the command-line options you specify to initialize the chain and generate the genesis block. As part of this process, the node adds the WebAssembly runtime as a storage item value and a corresponding :code key. After you start the node, the running node selects the runtime to use. By default, the node always uses the WebAssembly runtime for all operations, including: Synchronization Authoring new block Importing blocks Interacting with offchain workers Selection of the WebAssembly runtime Using the WebAssembly runtime is important because the WebAssembly and native runtimes can diverge. For example, if you make changes to the runtime, you must generate a new WebAssembly blob and update the chain to use the new version of the WebAssembly runtime. After the update, the WebAssembly runtime differs from the native runtime. To account for this difference, all of the execution strategies treat the WebAssembly representation of the runtime as the canonical runtime. If the native runtime and the WebAssembly runtime versions are different, the WebAssembly runtime is always selected. Because the WebAssembly runtime is stored as part of the blockchain state, the network must come to consensus about the representation of this binary. To reach consensus about the binary, the blob that represents the WebAssembly runtime must be exactly the same across all synchronizing nodes. WebAssembly execution environment The WebAssembly execution environment can be more restrictive than the Rust execution environment. For example, the WebAssembly execution environment is a 32-bit architecture with a maximum 4GB of memory. Logic that can be executed in the WebAssembly runtime can always be executed in the Rust execution environment. However, not all logic that can be executed in the Rust runtime can be executed in the WebAssembly runtime. Block authoring nodes typically use the WebAssembly execution environment to help ensure that they produce valid blocks. Native runtime Although the WebAssembly runtime is selected by default, it is possible for you to override the runtime selected for all or specific operations by specifying an execution strategy as a command-line option. If the native runtime and the WebAssembly runtime share the same version , you can selectively use the native runtime instead of the WebAssembly runtime, in addition to the WebAssembly runtime, or as a fallback if using the WebAssembly runtime fails. In general, you would only choose to use the native runtime for performance reasons or because it's a less restrictive environment than the WebAssembly runtime. For example, you might want to use the native runtime for initial synchronization. To use the native runtime for synchronizing blocks, you can start the node using the --execution-syncing native or --execution-syncing native-else-wasm command-line option. For information about using the command-line options to specify an execution strategy for all or specific operations, see node-template . For information about the execution strategy variant, see ExecutionStrategy Building WebAssembly without a native runtime A WebAssembly runtime is required to start a new chain. After an initial WebAssembly runtime is provided, the blob that represents the WebAssembly runtime can be passed to other nodes as part of a chain specification . In some rare cases, you might want to compile the WebAssembly target without the native runtime. For example, if you're testing a WebAssembly runtime to prepare for a forkless upgrade, you might want to compile just the new WebAssembly binary. Although it's a rare use case, you can use the build-only-wasm.sh script to build the no_std WebAssembly binary without compiling the native runtime. You can also use the wasm-runtime-overrides command-line option to load the WebAssembly from the file system. Compiling Rust without WebAssembly If you want to compile the Rust code for a node without building a new WebAssembly runtime, you can use the SKIP_WASM_BUILD as a build option. This option is primarily used for faster compile time when you don't need to update the WebAssembly. Where to go next Wasm-builder README Rust compilation options Discussion: Removing the native runtime Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Compiling an optimized artifact Execution strategies Building WebAssembly without a native runtime Compiling Rust without WebAssembly Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Build a deterministic runtime Build a deterministic runtime Build a deterministic runtime By default, the Rust compiler produces optimized WebAssembly binaries. These binaries are fine for working in an isolated environment where you are doing local development. However, the WebAssembly binaries that the compiler builds by default aren't guaranteed to be deterministically reproducible. Each time the compiler generates the WebAssembly runtime, it might produce slightly different WebAssembly bytecode. This is problematic in a blockchain network where all nodes must use exactly the same raw chain specification file. Working with builds that aren't guaranteed to be deterministically reproducible can cause other problems, too. For example, if you want to automate the build process for your blockchain, you'll want to ensure that the same code always produces the same result. Without a deterministic build, compiling the WebAssembly runtime with every push would produce inconsistent and unpredictable results, making it difficult to integrate with any automation and likely to continuously break your CI/CD pipeline. Deterministic builds‚Äîcode that always compiles to exactly the same bytecode‚Äîalso ensure that the WebAssembly runtime can be inspected, audited, and independently verified. Tooling for WebAssembly runtime To help you compile the WebAssembly runtime in a deterministic way, you can use the same tooling that produces the runtime for Polkadot, Kusama, and other Substrate-based chains. This tooling‚Äîreferred to collectively as the Substrate runtime toolbox or srtool ‚Äîensures that the same source code consistently compiles to an identical WebAssembly blob. The core component of the Substrate runtime toolbox ( srtool ) is a Docker container. This container is executed as part of a Docker image. The name of the srtool Docker image specifies the version of the Rust compiler used to compile the code included in the image. For example, the image paritytech/srtool:1.62.0 indicates that the code in the image was compiled with version 1.62.0 of the rustc compiler. Working with the Docker container Because srtool is a Docker container, you must have Docker available in your build environment to use it. However, you don't need to know anything about using Docker to build a Substrate-based chain using srtool because you can use the srtool-cli command-line interface to work with Docker images. The srtool-cli package is a command-line utility written in Rust that installs an executable program called srtool on your computer. This program simplifies your interactions with the srtool Docker container. Over time, the tooling around the srtool Docker image has expanded to include the following tools and helper programs: srtool-cli provides a command-line interface to pull the srtool Docker image, get information about the image and tooling used to interact with it, and build the runtime using the srtool Docker container. subwasm provides command-line options for working with the metadata and WebAssembly runtime built using srtool. The subwasm program is also used internally to perform tasks in the srtool image. srtool-actions provides GitHub actions to integrate builds produced using the srtool image with your GitHub CI/CD pipelines. srtool-app provides a simple graphical user interface for building the runtime using the srtool Docker image. Prepare the environment To work with the Docker image that executes the code in the srtool Docker container, you must have a Docker account and Docker command-line or desktop tools available. You should also prepare your development environment with the specific command-line tools you want to use. At a minimum, you should install the srtool-cli program to enable you to work with the Docker image using a simple command-line interface. To prepare the environment: Open a terminal shell in your Substrate development environment. Verify that you have Docker installed by running the following command: docker --version If Docker is installed, the command displays version information. For example: Docker version 20.10.17, build 100c701 Install the srtool command-line interface by running the following command: cargo install --git https://github.com/chevdor/srtool-cli View usage information for the srtool command-line interface by running the following command: srtool help Download the latest srtool Docker image by running the following command: srtool pull Start a deterministic build After you have prepared the environment, you can start compiling the WebAssembly runtime using the srtool Docker image. To build the runtime: Open a terminal shell in your Substrate development environment. Compile the runtime for a project by running a command similar to the following: srtool build --app --package node-template-runtime --runtime-dir runtime The name you specify for the --package should be the name defined in the Cargo.toml file for the runtime. The path you specify for the --runtime-dir should be the path to the Cargo.toml file for the runtime. If the Cargo.toml file for the runtime is located in a runtime subdirectory‚Äîfor example, runtime/kusama‚Äîyou can omit the --runtime-dir command-line option. Add workflow actions If you use a GitHub repository for your Substrate-based project, you can set up a GitHub workflow to start compiling the WebAssembly runtime automatically. To add a workflow for building the runtime: Create a .github/workflows directory in your Substrate repository. In the .github/workflows directory, click Add file , then select Create new file . Copy the sample GitHub action from basic.yml example in the srtools-actions repository and paste it into the file you created in the previous step. Modify the settings in the sample action to suit your chain. For example, modify the following settings: the name of the chain the name of the runtime package the location of the runtime Type a name for the action file in your Substrate repository. Click Commit new file . Downloading from Docker Hub You must have a Docker account and Docker installed in your build environment to use the Substrate runtime toolbox. If you sign in to Docker Hub, you can search for the paritytech/srtool container and find the corresponding images with tag names that identify the Rust compiler version and the build script version. If you don't want to use srtool-cli or srtool-app to work with the paritytech/srtool container, you can pull a paritytech/srtool container image directly from Docker Hub. To pull the image from Docker Hub: Sign in to Docker Hub. Type paritytech/srtool in the Search field and press Enter. Click paritytech/srtool , then click Tags . Copy the command for the image you want to pull. Open a terminal shell on your local computer. Paste the command you copied from the Docker Hub. For example, you might run a command similar to the following: docker pull paritytech/srtool:1.62.0 The command downloads and unpacks the image. Naming convention for images Unlike many Docker images, there is no latest tag for the srtool image. If you are downloading an image directly from Docker Hub, you'll need to select the image that's compatible with the version of the Rust compiler you have installed. The naming convention for paritytech/srtool Docker images specifies the version of the Rust compiler used to compile the code included in the image. There are also images that specify both a compiler version and the version of the build script used. For example, an image named paritytech/srtool:1.62.0-0.9.19 was compiled with version 1.62.0 of the rustc compiler but using the version 0.9.19 of the build script. Images that only specifies the compiler version always contains the latest version of the software. In most cases, that's the image you should use. Compiler version If you are downloading an image directly from Docker Hub, you should first check the version of the Rust compiler that you are using to ensure the image is compatible with the version of Rust you have installed. If you aren't sure which version of the Rust compiler you use, you can run the following command to check the version you have installed: rustc --version Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Tooling for WebAssembly runtime Working with the Docker container Prepare the environment Start a deterministic build Add workflow actions Downloading from Docker Hub Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Troubleshoot your code Troubleshoot your code Troubleshoot your code Because Substrate and FRAME provide a flexible and modular framework for building blockchain applications, it‚Äôs important to follow common best practices and basic coding principles to avoid introducing errors or making code difficult to debug. General coding practices The following general principles aren‚Äôt unique to Substrate or to using FRAME but they are particularly important when building complex software with stringent security requirements and constrained resources like a blockchain application: Formatting and readability . Use consistent formatting and follow best practices for writing readable code to make your programs easy for you and others to understand and maintain. Comments . Add clear and concise comments to your code to explain what the code does, and, where applicable, why the code is written the way it is. Style and naming conventions . Follow the Rust style guidelines and naming conventions to make your code consistent with other Rust programs and make it easier for other Rust programmers to read and debug your code. Licencing . Ensure your repository includes the appropriate open source license and any licenses, copyright notices, and attributions required for code you're using that you didn't write. In most cases, if you are using code you didn't write, you should retain the original licenses and mention the authors. Refactoring . Improve the design, structure, or implementation of your code code to create simpler, cleaner, more performant, or more expressive programs. In general, refactoring simplifies code logic with changing the code functionality and results in code that is more readable, maintainable, and extensible. Don‚Äôt repeat yourself (DRY) . Follow the DRY principle of software development and use data abstractions or data normalization to avoid redundancy. Testing . Write and execute unit tests to ensure all individual software components work as intended. For more information about unit testing and testing modules in the runtime, see Test . Errors and warnings . Address all errors and warnings reported by the compiler to ensure you understand the cause of the error or warning and how to resolve it. Dependencies . Update dependencies regularly to ensure your code doesn't fall too far behind new releases and become out of date. You should periodically update the Rust compiler and toolchain and check for changes where there are new releases of Polkadot. Hard coding . Avoid embedding data directly in the source code. Common Substrate issues There are a few common aspects of Substrate that can introduce errors or cause performance issues if not handled correctly. As you are writing the logic for your chain, you should pay particular attention to the following potential trouble spots: Benchmarks Pallet coupling Offchain workers Storage Events Benchmarks The Substrate benchmarking system is designed to help you determine the appropriate weight to assign to the functions in your pallets. Setting an appropriate weight is a critical step to ensure that your blockchain is reliable and secure. Although you can skip benchmarking and setting weights for transactions in the early phases of development, you should be aware that using a weight of zero makes your code vulnerable to attack. If there are no transaction fees associated with the execution of a function, a malicious actor could call the function repeatedly‚Äîessentially spamming the network with transactions‚Äîto halt the chain in a denial of service (DoS) attack. In general, you should ensure that all functions that can be executed in the runtime have a weight defined and subtract a corresponding fee from a calling account. Transaction fees are typically an important economic incentive to prevent denial of service (DoS) attacks and create a sustainable economic model for the chain. For more information about the benchmarking system, see Benchmark . For a simple example of how to write and run benchmarks, see Add benchmarks . Pallet coupling In Substrate, there are two ways that one pallet can call the functions in another palletPallet coupling is about how a pallet can call functions in another pallet. Tight pallet coupling is more restrictive and most often used when one pallet depends on all or a substantial number of the types and methods in the other pallet. Loose pallet coupling is more flexible and most often used when one pallets depends on specific traits or function interfaces an interface that another pallet exposes. Tight pallet coupling requires both pallets to be installed in the runtime and the pallets can't be used independently. In addition, tightly-coupled pallets can be harder to maintain because changes in one pallet often require changes in the other pallet. In most cases, loose coupling is more flexible solution because you can reuse types and interfaces from another pallet without including that pallet in the runtime. For more information about tight and loose pallet coupling, see Pallet coupling and this code example . For simple examples of pallet coupling see Use tight pallet coupling and Use loose pallet coupling . Offchain workers You can use offchain operations to query data from offchain sources or to process data offchain. For example, offchain workers enable you to offload the execution of tasks that might take longer than the maximum block execution time allows. However, some characteristics of offchain operations can have unintended consequences. If you plan to use offchain workers, you should consider the following: By default, offchain workers run on validator nodes when those nodes are performing their block authoring. If you want to run an offchain worker on a node that is not a validator, you must use the --offchain-worker always command-line option. To prevent any node‚Äîvalidator or not‚Äîfrom running an offchain work, you can use the --offchain-worker never command-line option. If you have offchain workers running as parallel processes on he network, you might need to implement concurrent programming techniques to avoid race conditions. By default, offchain workers are triggered for every block import even if the block isn't finalized. Because offchain worker have full access to state, you can create conditions that trigger them to run only in some specific cases. For more information about offchain operations, see Offchain operations . For examples of how to use offchain components, see Offchain workers . Storage As discussed in Runtime storage , the fundamental principle for blockchain storage is to minimize both the number and size of the data items you store. Storing data unnecessarily can lead to slow network performance and resources running out. In planning and reviewing your code for potential issues, keep the following guidelines in mind: Only store critical information. Don‚Äôt store intermediate or transient information. Don‚Äôt store data that won‚Äôt be needed if the operation fails. If possible, don‚Äôt store information that is already stored in another structure Store limited length hashed data where possible. As a general rule, it is better to have one larger data structure than many smaller data structures to reduce complexity and the number of read and write operations. However, this isn't always the case and you should use benchmarking to measure and optimize how you store data on a case-by-case basis. Both lists and storage maps incur storage costs, so you should be conscious about how you use them. The more items you have in a list or a map, the more iterating over the items affects the performance in the runtime. Storage maps often store unbounded sets of data, and‚Äîbecause accessing the elements of a map requires more database reads than accessing the elements of a list‚Äîiterating when using storage maps can be significantly more costly. Being conscious of the time required to iterate over items in a storage map is particularly important if your project is a parachain. If the time required to iterate over storage exceeds the maximum time allowed for block production, the blockchain will stop producing blocks and thus stop working. In general, you should avoid having unbounded data in storage maps and avoid iterating over storage maps that store a large data set. You should use benchmarks to test the performance of all functions in the runtime under different conditions, including iterating over a large number of items in a list or storage map. By testing for specific conditions‚Äîfor example, triggering a function to execute over a large data set with many iteractions‚Äîbenchmarks can help you identify when it's best to enforce boundaries by limiting the number of elements in a list or the number of iterations in a loop. For more guidelines about storage and storage structures, see State transitions and storage and Runtime storage . For more information about iterating over storage, see Iterating over storage maps . Events Pallets typically emit events to send notifications about changes to data or conditions in the runtime to receiving entities‚Äîlike users or applications‚Äîthat are outside of the runtime. In custom pallets, you can define the following event-related information: The type of the event. The information contained within the event. The conditions for emitting the event. In general, events inform users or applications such as a block explorer that a change occurred. Events aren't intended to describe differences in state or to contain detailed information. You should use caution in adding more information to an event than is needed because additional information increases storage and computational overhead involved in producing events. If additional information about a change is needed, users can query the chain state. For information about adding events to a custom pallet, see Declaring an event . Unsafe or insecure patterns Secure operations and coding principles are critical for ensuring data integrity and the viability of a blockchain. There are several common unsafe or insecure coding practices that can introduce errors or make you chain vulnerable to attack if not handled correctly. As you are writing the logic for your chain, you should pay particular attention to the following potential trouble spots: Error handling Unsafe math: floating point numbers Unsafe math: overflows Unbounded Vec data types Error handling Runtime code should explicitly and gracefully handle all error cases. In general, you shouldn't use the panic! macro for error handling except in tests and benchmarks. Functions in the runtime should never generate a panic and must not throw errors. Only bugs detected by the compiler should generate unrecoverable panic errors. In Rust, you should write functions that use the Result type to return errors with the Err variant. The Result type with the Err variant allows the function to indicate failed execution without panicking. As a best practice, you should have many individual and specific error messages to make it easier it to diagnose problems. You should also be aware that using unwrap() in the runtime with the Result type can generate undefined behavior. Instead of using unwrap() , try using ok_or , unwrap_or , ensure or returning Err in a matching pattern. For example: let a = TryInto::<u128>::try_into(id.fee).ok().unwrap(); let b = a.checked_mul(8).ok_or(Error::<T>::Overflow)? .checked_div(10).ok_or(Error::<T>::Overflow)?; let b = id.fee .checked_mul(&8u32.saturated_into()).ok_or(Error::<T>::Overflow)? .checked_div(&10u32.saturated_into()).ok_or(Error::<T>::Overflow)?; For more information about error handling in pallets, see Error pallet attribute and Errors . Unsafe math: Floating point numbers Blockchains require deterministic operations to ensure that independent nodes can reach consensus reliably. Because floating point numbers can lead to non-deterministic results, you should avoid operations involving floating point numbers and always use fixed point arithmetic in the runtime. Substrate provides primitives for use fixed point arithmetic in the sp_arithmetic crate. You can use specific Per methods to represent a part of a whole depending on the precision you need. For example: Percent: Parts per hundred represents [0, 1] from [0, 100]. Permill: Parts per million represents [0, 1] from [0, 1 000 000]. Perbill: Parts per billion represents [0, 1] from [0, 1 000 000_000]. Note that using higher resolution requires data types with higher sizes, so having more precision comes with a cost. Unsafe math: Overflows Overflows happen if the computed value of data to be returned exceeds the limits of its defined data type. There are two ways you can handle data overflows: use of saturating methods or handle the case of an invalid value using checked arithmetic operations. Use saturating methods. If the result of an operation would be too large for the type, saturating methods return the maximum value of the type. If the result would be too small, saturating methods return the minimum value of the type. For more information, see Saturating . Use checked_* methods. These methods perform the calculations in an isolated environment and, based on the result, return Some or None. For more information, see checked_add . Unbounded Vec data types As noted in Runtime storage and Storage , it's important to minimize the number and size of the data items you store to ensure the chain is performant and secure. Using Vec data types without setting bounds for size makes your chain vulnerable to both intentional and unintentional misuse of the limited resources available. For example, a malicious actor or an end-user acting without restrictions could add unlmited data and overwhelm your storage capacity, leading to undefined behavior in the runtime. In general, any storage item with its size determined by user action should have a bound on it. The following code illustrates an unbounded Vec data type: type Proposal < T : Config > = StorageMap < _ , Blake2_128Concat , T :: Hash , Vec < T :: Hash > , ValueQuery > ; For safer code, replace the Vec data type with the BoundedVec data type: type Proposal < T : Config > = StorageMap < _ , Blake2_128Concat , T :: Hash , BoundedVec < T :: Hash , ValueQuery > ; By default, all pallet storage items are limited by the bound defined in the pallet_prelude::MaxEncodedLen attribute. The #[pallet::without_storage_info] attribute macro allows you to override this default behavior if you require unbounded storage for an entire pallet. For example: #[pallet::pallet] #[pallet::generate_store(pub(super) trait Store)] #[pallet::without_storage_info] pub struct Pallet<T>(_); This macro applies to all storage items in your pallet, so you should only use it in a test or development environment. You should never use the #[pallet::without_storage_info] macro in production. By removing this macro after testing, you can ensure that your pallet follows the default behavior. If necessary, you can use the #[pallet::unbounded] attribute macro to declare a specific storage item as unbounded. For more information about limiting storage using BoundedVec data types, see Create bounds and BoundedVec . For more information about pallet macros, see FRAME macros . Secure hashing algorithms Substrate provides the following hashing algorithms by default: xxHash is a fast hashing function, but it is not cryptographically secure. With this hashing algorithm, hash collisions‚Äîin which different inputs hash to the same output‚Äîare possible. You should only use this hashing algorithm in functions that aren't available to outside entities that could try to manipulate the input and attack the system. Blake2 is a relatively fast cryptographic hashing function. In most cases, you can use the Blake2 hashing algorithm in any situations where security matters. However, Substrate can support any hash algorithm that implements the Hasher trait. For more information about hashing algorithms, see Hashing algorithms . Inaccurate weight Weight is a Substrate construct that represents the resources consumed to execute a transaction in a block. The appropriate weight for executing a transaction depends on a number of factors, including the hardware, the computational complexity, the storage requirements, and the database operation performed. Every executable transaction should be assigned an appropriate weight. If you have multiple transactions assigned the same weights, it's likely that the weight assignment doesn't accurately reflect the actual time of execution. Benchmarking helps you evaluate and estimate the resources that each function in the runtime is likely to consume under different circumstances. Modeling the expected weight of each runtime function enables the blockchain to calculate how many transactions or system-level calls it can execute within a certain period of time. If you set the weight for a transaction too low, an attacker or an unsuspecting user can create blocks that are overweight and cause block production timeouts. You should run appropriate benchmark tests for all functions under different conditions to ensure that that all transaction have appropriate weights that take into account the factors that affect the resources consumed. For more information about using benchmarks and calculating weight, see Benchmarking and weight and Weights . Insecure randomness Randomness is used in many different applications on blockchains. Substrate provides two default implementations of randomness. The insecure randomness collective flip pallet generates random values based on the block hashes from the previous 81 blocks. This pallet can be useful when defending against weak adversaries or in low-security situations like testing. For example, you can use this pallet when testing randomness-consuming pallets. You should never use this pallet in production as a true source of randomness. The BABE pallet uses verifiable random functions (VRF) to implement a more secure version of randomness. This pallet provides production-grade randomness. However, it isn't suite for every purpose. For example, the randomness provided by the BABE pallet isn't suitable for gambling applications. As alternative to these pallets, you can use an oracle as a secure source of randomness. For more information about using randomness, see Randomness and Incorporate randomness . Anti-patterns Anti-patterns are solutions meant to perform common tasks that introduce more problems than they solve. There are several coding patterns that you might be inclined to follow that fall into the category of anti-patterns you should avoid. Don't dispatch a function to read from storage In Substrate, you shouldn't use a dispatchable function call to read an item from storage. Instead, you should either define a getter macro for the storage item or to use an RPC method. For example, assume you have the following storage map: #[pallet::storage] #[pallet::getter(fn product_information)] pub type ProductInformation < T : Config > = StorageMap < _ , Blake2_128Concat , T :: Hash , Product < T :: AccountId , T :: Hash >> ; You can read the item by calling Self::product_information(id) instead of writing a separate dispatchable function like this: // !!! Don't create a dispatchable function to read storage state !!! ~~~~ #[pallet::weight(T::WeightInfo::get_product())] pub fn get_product ( origin : OriginFor < T > , id : T :: Hash ) -> DispatchResult { let sender = ensure_signed ( origin ) ? ; let product = if < ProductInformation < T >> :: contains_key ( & id ) { Some ( Self :: product_information ( & id ) ) } else { None } ; match product { Some ( value ) => { Self :: deposit_event ( Event :: ItemFetched ( value ) ) ; } None => return Err ( Error :: < T > :: NotFound . into ( ) ) , } Ok ( ( ) ) } For more information about declaring storage and the getter macro, see Declare storage items . If a storage getter macro is not flexible enough for your requirements, you can create a custom RPC method. For information about creating a custom RPC method, see Add custom RPC to the node . Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE General coding practices Common Substrate issues Unsafe or insecure patterns Anti-patterns Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Test Home Test Test Regardless of the type of chain you ultimately want to build, you are likely to start with a local development environment for unit testing and debugging. As your project progresses, you'll want to evaluate the performance of different operations under different conditions to establish benchmarks around the execution time and storage requirements used by the functions in your chain. If you are building a parachain, you'll also want to simulate the operation of one or more parachains in a local network before you attempt to deploy on a test network like Rococo or a live production network like Kusama or Polkadot where real-world economics are in play. In a local development network, you have full control over your node. You can start and stop the node. You can modify, recompile, and upgrade the runtime at your discretion. You can write to and clear stored state as often as you like. You can use predefined accounts and pallets to try things out. But testing is the key to making your chain accessible to a wider audience and economically viable in the network. The topics in this section highlight tools and techniques for testing your blockchain logic to help you move from local development to deployment on a live test network and ultimately into production as part of the Polkadot ecosystem. Unit test explains how you can use the Rust testing framework and a mock runtime environment to execute unit tests that validate individual functions or modules of code. Debug describes how you can use the Rust logging functions to debug your runtime. Benchmark explains the role of benchmarks, how you can use the benchmarking framework to evaluate the performance of the function calls in your code and to adjust transaction weight to accurately reflect execution time. Simulate parachains steps through how you can set up a local test network to simulate a relay chain network with validators, parachain collator nodes, and XCM messaging. Check runtime explains how you can use the try-runtime command-line tool for testing runtime state against a production snapshot of the chain state. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Unit test Home Test Unit testing Unit test As you build the logic for your runtime, you'll want to routinely test that the logic works as expected. You can create unit tests for the runtime using the unit testing framework provided by Rust. After you create one or more unit tests, you can use the cargo test command to execute the test. For example, you can run all of the tests you have created for a runtime by running the following command: cargo test For more information about using the Rust cargo test command and testing framework, run the following command: cargo help test Test pallet log in a mock runtime In addition to the unit testing you can do with the Rust testing framework, you can verify the logic in your runtime by constructing a mock runtime environment. The configuration type Test is defined as a Rust enum with implementations for each of the pallet configuration traits that are used in the mock runtime. frame_support :: construct_runtime! ( pub enum Test where Block = Block , NodeBlock = Block , UncheckedExtrinsic = UncheckedExtrinsic , { System : frame_system :: { Pallet , Call , Config , Storage , Event < T > } , TemplateModule : pallet_template :: { Pallet , Call , Storage , Event < T > } , } ) ; impl frame_system :: Config for Test { // -- snip -- type AccountId = u64 ; } If Test implements pallet_balances::Config , the assignment might use u64 for the Balance type. For example: impl pallet_balances :: Config for Test { // -- snip -- type Balance = u64 ; } By assigning pallet_balances::Balance and frame_system::AccountId to u64 , testing accounts and balances only requires tracking a (AccountId: u64, Balance: u64) mapping in the mock runtime. Test storage in a mock runtime The sp-io crate exposes a TestExternalities implementation that you can use to test storage in a mock environment. It is the type alias for an in-memory, hashmap-based externalities implementation in substrate_state_machine referred to as TestExternalities . The following example demonstrates defining a struct called ExtBuilder to build an instance of TestExternalities , and setting the block number to 1. pub struct ExtBuilder ; impl ExtBuilder { pub fn build ( self ) -> sp_io :: TestExternalities { let mut t = system :: GenesisConfig :: default ( ) . build_storage :: < TestRuntime > ( ) . unwrap ( ) ; let mut ext = sp_io :: TestExternalities :: new ( t ) ; ext . execute_with ( | | System :: set_block_number ( 1 ) ) ; ext } } To create the test environment in unit tests, the build method is called to generate a TestExternalities using the default genesis configuration. #[test] fn fake_test_example ( ) { ExtBuilder :: default ( ) . build_and_execute ( | | { // ...test logics... } ) ; } Custom implementations of Externalities allow you to construct runtime environments that provide access to features of the outer node. Another example of this can be found in the offchain module. The offchain module maintains its own Externalities implementation. Test events in a mock runtime It can also be important to test the events that are emitted from your chain, in addition to the storage. Assuming you use the default generation of deposit_event with the generate_deposit macro, all pallet events are stored under the system / events key with some extra information as an EventRecord . These event records can be directly accessed and iterated over with System::events() , but there are also some helper methods defined in the system pallet to be used in tests, assert_last_event and assert_has_event . fn fake_test_example ( ) { ExtBuilder :: default ( ) . build_and_execute ( | | { System :: set_block_number ( 1 ) ; // ... test logic that emits FakeEvent1 and then FakeEvent2 ... System :: assert_has_event ( Event :: FakeEvent1 { } . into ( ) ) System :: assert_last_event ( Event :: FakeEvent2 { data : 7 } . into ( ) ) assert_eq! ( System :: events ( ) . len ( ) , 2 ) ; } ) ; } Some things to note are: Events are not emitted on the genesis block, and so the block number should be set in order for this test to pass. You need to have a .into() after instantiating your pallet event, which turns it into a generic event. Advanced event testing When testing events in a pallet, often you are only interested in the events that are emitted from your own pallet. The following helper function filters events to include only events emitted by your pallet and converts them into a custom event type. A helper function like this is usually placed in the mock.rs file for testing in a mock runtime. fn only_example_events ( ) -> Vec < super :: Event < Runtime >> { System :: events ( ) . into_iter ( ) . map ( | r | r . event ) . filter_map ( | e | if let RuntimeEvent :: TemplateModule ( inner ) = e { Some ( inner ) } else { None } ) . collect :: < Vec < _ >> ( ) ; } Additionally, if your test performs operations that emit events in a sequence, you might want to only see the events that have happened since the last check. The following example leverages the preceding helper function. parameter_types! { static ExamplePalletEvents : u32 = 0 ; } fn example_events_since_last_call ( ) -> Vec < super :: Event < Runtime >> { let events = only_example_events ( ) ; let already_seen = ExamplePalletEvents :: get ( ) ; ExamplePalletEvents :: set ( events . len ( ) as u32 ) ; events . into_iter ( ) . skip ( already_seen as usize ) . collect ( ) } You can find examples of this type of event testing in the tests for the nomination pool or staking . If you rewrite the previous event test with this new function, the resulting code looks like this: fn fake_test_example ( ) { ExtBuilder :: default ( ) . build_and_execute ( | | { System :: set_block_number ( 1 ) ; // ... test logic that emits FakeEvent1 ... assert_eq! ( example_events_since_last_call ( ) , vec! [ Event :: FakeEvent1 { } ] ) ; // ... test logic that emits FakeEvent2 ... assert_eq! ( example_events_since_last_call ( ) , vec! [ Event :: FakeEvent2 { } ] ) ; } ) ; } Genesis config In the previous examples, the ExtBuilder::build() method used the default genesis configuration for building the mock runtime environment. In many cases, it is convenient to set storage before testing. For example, you might want to pre-seed account balances before testing. In the implementation of frame_system::Config , AccountId and Balance are both set to u64 . You can put (u64, u64) pairs in the balances vec to seed (AccountId, Balance) pairs as the account balances. For example: impl ExtBuilder { pub fn build ( self ) -> sp_io :: TestExternalities { let mut t = frame_system :: GenesisConfig :: default ( ) . build_storage :: < Test > ( ) . unwrap ( ) ; pallet_balances :: GenesisConfig :: < Test > { balances : vec! [ ( 1 , 10 ) , ( 2 , 20 ) , ( 3 , 30 ) , ( 4 , 40 ) , ( 5 , 50 ) , ( 6 , 60 ) ] , } . assimilate_storage ( & mut t ) . unwrap ( ) ; let mut ext = sp_io :: TestExternalities :: new ( t ) ; ext . execute_with ( | | System :: set_block_number ( 1 ) ) ; ext } } In this example, account 1 has a balance of 10, account 2 has a balance of 20, and so on. The exact structure used to define the genesis configuration of a pallet depends on the pallet GenesisConfig struct definition. For example, in the Balances pallet, it is defined as: pub struct GenesisConfig < T : Config < I > , I : 'static = ( ) > { pub balances : Vec < ( T :: AccountId , T :: Balance ) > , } Block production It is useful to simulate block production to verify that expected behavior holds across block production. A simple way of doing this is by incrementing the System module's block number between on_initialize and on_finalize calls from all modules with System::block_number() as the sole input. Although it is important for runtime code to cache calls to storage or the system module, the test environment scaffolding should prioritize readability to facilitate future maintenance. fn run_to_block ( n : u64 ) { while System :: block_number ( ) < n { if System :: block_number ( ) > 0 { ExamplePallet :: on_finalize ( System :: block_number ( ) ) ; System :: on_finalize ( System :: block_number ( ) ) ; } System :: reset_events ( ) ; System :: set_block_number ( System :: block_number ( ) + 1 ) ; System :: on_initialize ( System :: block_number ( ) ) ; ExamplePallet :: on_initialize ( System :: block_number ( ) ) ; } } The on_finalize and on_initialize methods are only called from ExamplePallet if the pallet trait implements the frame_support::traits::{OnInitialize, OnFinalize} traits to execute the logic encoded in the runtime methods before and after each block respectively. Then call this function in the following fashion. #[test] fn my_runtime_test ( ) { with_externalities ( & mut new_test_ext ( ) , | | { assert_ok! ( ExamplePallet :: start_auction ( ) ) ; run_to_block ( 10 ) ; assert_ok! ( ExamplePallet :: end_auction ( ) ) ; } ) ; } Where to go next Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Test pallet log in a mock runtime Test storage in a mock runtime Test events in a mock runtime Genesis config Block production Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Debug Home Test Debug Debug Debugging is a necessity in all walks of software development, and blockchain is no exception. Most of the same tools used for general purpose Rust debugging also apply to Substrate. Logging utilities You can use Rust's logging API to debug your runtimes. This comes with a number of macros, including debug and info . For example, after updating your pallet's Cargo.toml file with the log crate just use log::info! to log to your console: pub fn do_something ( origin ) -> DispatchResult { let who = ensure_signed ( origin ) ? ; let my_val : u32 = 777 ; Something :: put ( my_val ) ; log :: info! ( "called by {:?}" , who ) ; Self :: deposit_event ( RawEvent :: SomethingStored ( my_val , who ) ) ; Ok ( ( ) ) } Printable trait The Printable trait is meant to be a way to print from the runtime in no_std and in std . The print function works with any type that implements the Printable trait . Substrate implements this trait for some types ( u8 , u32 , u64 , usize , &[u8] , &str ) by default. You can also implement it for your own custom types. Here is an example of implementing it for a pallet's Error type using the node-template as the example codebase. use sp_runtime :: traits :: Printable ; use sp_runtime :: print ; #[frame_support::pallet] pub mod pallet { // The pallet's errors #[pallet::error] pub enum Error < T > { /// Value was None NoneValue , /// Value reached maximum and cannot be incremented further StorageOverflow , } impl < T : Config > Printable for Error < T > { fn print ( & self ) { match self { Error :: NoneValue => "Invalid Value" . print ( ) , Error :: StorageOverflow => "Value Exceeded and Overflowed" . print ( ) , _ => "Invalid Error Case" . print ( ) , } } } } /// takes no parameters, attempts to increment storage value, and possibly throws an error pub fn cause_error ( origin ) -> dispatch :: DispatchResult { // Check it was signed and get the signer. See also: ensure_root and ensure_none let _who = ensure_signed ( origin ) ? ; print! ( "My Test Message" ) ; match Something :: get ( ) { None => { print ( Error :: < T > :: NoneValue ) ; Err ( Error :: < T > :: NoneValue ) ? } Some ( old ) => { let new = old . checked_add ( 1 ) . ok_or ( { print ( Error :: < T > :: StorageOverflow ) ; Error :: < T > :: StorageOverflow } ) ? ; Something :: put ( new ) ; Ok ( ( ) ) } , } } Run the node binary with the RUST_LOG environment variable to print the values. RUST_LOG = runtime = debug ./target/release/node-template --dev The values are printed in the terminal or the standard output every time that the runtime function gets called. 2020 - 01 - 01 tokio - blocking - driver DEBUG runtime My Test Message < - - str implements Printable by default 2020 - 01 - 01 tokio - blocking - driver DEBUG runtime Invalid Value < - - the custom string from NoneValue 2020 - 01 - 01 tokio - blocking - driver DEBUG runtime DispatchError 2020 - 01 - 01 tokio - blocking - driver DEBUG runtime 8 2020 - 01 - 01 tokio - blocking - driver DEBUG runtime 0 < - - index value from the Error enum definition 2020 - 01 - 01 tokio - blocking - driver DEBUG runtime NoneValue < - - str which holds the name of the ident of the error Keep in mind that adding print functions to the runtime increases the size of the Rust and Wasm binaries with debug code not needed in production. Substrate's own print function For legacy use cases, Substrate provides extra tools for Print debugging (or tracing). You can use the print function to log the status of the runtime execution. use sp_runtime :: print ; // --snip-- pub fn do_something ( origin ) -> DispatchResult { print! ( "Execute do_something" ) ; let who = ensure_signed ( origin ) ? ; let my_val : u32 = 777 ; Something :: put ( my_val ) ; print! ( "After storing my_val" ) ; Self :: deposit_event ( RawEvent :: SomethingStored ( my_val , who ) ) ; Ok ( ( ) ) } // --snip-- Start the chain using the RUST_LOG environment variable to see the print logs. RUST_LOG = runtime = debug ./target/release/node-template --dev The values are printed in the terminal or the standard output if the Error gets triggered. 2020 -01-01 00:00:00 tokio-blocking-driver DEBUG runtime Execute do_something 2020 -01-01 00:00:00 tokio-blocking-driver DEBUG runtime After storing my_val If std The legacy print function allows you to print and have an implementation of the Printable trait. However, in some legacy cases you may want to do more than print, or not bother with Substrate-specific traits just for debugging purposes. The if_std! macro is useful for this situation. One caveat of using this macro is that the code inside will only execute when you are actually running the native version of the runtime. use sp_std :: if_std ; // Import into scope the if_std! macro. The println! statement should be inside of the if_std macro. #[pallet::call] impl < T : Config < I > , I : 'static > Pallet < T , I > { // --snip-- pub fn do_something ( origin ) -> DispatchResult { let who = ensure_signed ( origin ) ? ; let my_val : u32 = 777 ; Something :: put ( my_val ) ; if_std! { // This code is only being compiled and executed when the `std` feature is enabled. println! ( "Hello native world!" ) ; println! ( "My value is: {:#?}" , my_val ) ; println! ( "The caller account is: {:#?}" , who ) ; } Self :: deposit_event ( RawEvent :: SomethingStored ( my_val , who ) ) ; Ok ( ( ) ) } // --snip-- } The values are printed in the terminal or the standard output every time that the runtime function gets called. $ 2020 -01-01 00:00:00 Substrate Node 2020 -01-01 00:00:00 version x.y.z-x86_64-linux-gnu 2020 -01-01 00:00:00 by Anonymous, 2017 , 2020 2020 -01-01 00:00:00 Chain specification: Development 2020 -01-01 00:00:00 Node name: my-node-007 2020 -01-01 00:00:00 Roles: AUTHORITY 2020 -01-01 00:00:00 Imported 999 ( 0x3d7a‚Ä¶ab6e ) # --snip-- - > Hello native world ! - > My value is: 777 - > The caller account is: d43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d ( 5GrwvaEF .. . ) # --snip-- 2020 -01-01 00:00:00 Imported 1000 ( 0x3d7a‚Ä¶ab6e ) Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Logging utilities Printable trait Substrate's own print function If std Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Benchmark Home Test Benchmark Benchmark Substrate and FRAME provide a flexible framework for developing custom logic for your blockchain. This flexibility enables you to design complex and interactive pallets and implement sophisticated runtime logic. However, determining the appropriate weight to assign to the functions in your pallets can be a difficult task. Benchmarking enables you to measure the time it takes to execute different functions in the runtime and under different conditions. If you use benchmarking to assign accurate weights to function calls, you can prevent your blockchain from being overloaded and unable to produce blocks or vulnerable to denial of service (DoS) attacks by malicious actors. Why benchmark a pallet It is important to understand the computational resources required to execute different functions‚Äîincluding runtime functions like on_initialize and verify_unsigned ‚Äîto keep the runtime safe and to enable the runtime to include or exclude transactions based on the resources available. The ability to include or exclude transactions based on available resources ensures that the runtime can continue to produce and import blocks without service interruptions. For example, if you have a function call that requires particularly intensive computation, executing the call might exceed the maximum time allowed for producing or importing a block, disrupting the block handling process or stopping blockchain progress altogether. Benchmarking helps you validate that the execution time required for different functions is within reasonable boundaries. Similarly, a malicious user might attempt to disrupt network service by repeatedly executing a function call that requires intensive computation or that doesn't accurately reflect the computation it requires. If the cost for executing a function call doesn't accurately reflect the computation involved, there's no incentive to deter a malicious user from attacking the network. Because benchmarking helps you evaluate the weight associated with executing transactions, it also helps you to determine appropriate transaction fees. Based on your benchmarks, you can set fees that represent the resources consumed by executing specific calls on the blockchain. Developing a linear model At a high level, benchmarking requires you to perform the following steps: Write custom benchmarking logic that executes a specific code path for a function. Execute the benchmark logic in the WebAssembly execution environment on a specific set of hardware and with a specific runtime configuration. Execute the benchmark logic across a controlled range of possible values that might affect the execution time a function requires. Execute the benchmark multiple times for each component in a function to isolate and remove outliers. From the results generated by executing the benchmark logic, the benchmarking tool creates a linear model of the function across all of its components. The linear model for a function enables you to estimate how long it takes to execute a specific code path and to make informed decisions without actually spending any significant resources at runtime. Benchmarking assumes all transactions have linear complexity because higher complexity functions are considered to be dangerous to the runtime as the weight of these functions may explode as the runtime state or input becomes too complex. Benchmarking and weight As discussed in Transactions, weights, and fees , Substrate-based chains use the concept of weight to represent the time it takes to execute the transactions in a block. The time required to execute any particular call in a transaction depends on a several factors, including the following: Computational complexity. Storage complexity. Database read and write operations required. Hardware used. To calculate an appropriate weight for a transaction, you can use benchmark parameters to measure the time it takes to execute the function calls on different hardware, using different variable values, and repeated multiple times. You can then use the results of the benchmarking tests to establish an approximate worst case weight to represent the resources required to execute each function call and each code path. Fees are then based on the worst case weight. If the actual call performs better than the worst case, the weight is adjusted and any excess fees can be returned. Because weight is a generic unit of measurement based on computation time for a specific physical machine, the weight of any function can change based on the specific hardware used for benchmarking. By modeling the expected weight of each runtime function, the blockchain is able to calculate how many transactions or system level calls it can execute within a certain period of time. Within FRAME, each function call that can be dispatched must have a #[weight] annotation that can return the expected weight for the worst case scenario execution of that function given its inputs. The benchmarking framework automatically generates a file with those formulas for you. Benchmarking tools The benchmarking framework provides tools that help you add, test, run, and analyze benchmarks for the functions in the runtime. The benchmarking tools that help you determine the time it takes to execute function calls include the following: Benchmark macros to help you write, test, and add runtime benchmarks. Linear regression analysis functions for processing benchmark data. Command-line interface (CLI) to enable you to execute benchmarks on your node. The end-to-end benchmarking pipeline is disabled by default when compiling a node. If you want to run benchmarks, you need to compile a node with the runtime-benchmarks Rust feature flag. Writing benchmarks Writing a runtime benchmark is similar to writing a unit test for your pallet. Like unit tests, benchmarks must execute specific logical paths in your code. In unit tests, you check the code for specific success and failure results. For benchmarks, you want to execute the most computationally intensive path. In writing benchmarks, you should consider the specific conditions‚Äîsuch as storage or runtime state‚Äîthat might affect the complexity of the function. For example, if triggering more iterations in a for loop increases the number of database read and write operations, you should set up a benchmark that triggers this condition to get a more accurate representation of how the function would perform. If a function executes different code paths depending on user input or other conditions, you might not know which path is the most computationally intensive. To help you see where complexity in the code might become unmanageable, you should create a benchmark for each possible execution path. The benchmarks can help you identify places in the code where you might want to enforce boundaries‚Äîfor example, by limiting the number of elements in a vector or limiting the number of iterations in a for loop‚Äîto control how users interact with your pallet. You can find examples of end-to-end benchmarks in all of the prebuilt FRAME pallets . Testing benchmarks You can test benchmarks using the same mock runtime that you created for unit testing your pallet. The benchmarking macro you use in your benchmarking.rs module automatically generates test functions for you. For example: fn test_benchmark_ [ benchmark_name ] < T > :: ( ) -> Result < ( ) , & 'static str > You can add the benchmark functions to a unit test and ensure that the result of the function is Ok(()) . Verify blocks In general, you only need to check that a benchmark returned Ok(()) because that result indicates that the function was executed successfully. However, you can optionally include a verify block with your benchmarks if you want to verify any final conditions, such as the final state of your runtime. The additional verify blocks don't affect the results of your final benchmarking process. Run the unit tests with benchmarks To run the benchmarking tests, you need to specify the package to test and enable the runtime-benchmarks feature. For example, you can test the benchmarks for the Balances pallet by running the following command: cargo test --package pallet-balances --features runtime-benchmarks Adding benchmarks The benchmarks included with each pallet are not automatically added to your node. To execute these benchmarks, you need to implement the frame_benchmarking::Benchmark trait. You can see an example of how to do this in the Substrate node . Assuming there are already some benchmarks set up on your node, you just need to add the pallet to the define_benchmarks! macro: #[cfg(feature = "runtime-benchmarks" )] mod benches { define_benchmarks! ( [ frame_benchmarking , BaselineBench :: < Runtime > ] [ pallet_assets , Assets ] [ pallet_babe , Babe ] ... [ pallet_mycustom , MyCustom ] ... After you have added your pallet, compile your node binary with the runtime-benchmarks feature flag. For example: cd bin/node/cli cargo build --profile = production --features runtime-benchmarks The production profile applies various compiler optimizations. These optimizations slow down the compilation process a lot . If you are just testing things out and don't need final numbers, use the --release command-line option instead of the production profile. Running benchmarks After you have compiled a node binary with benchmarks enabled, you need to execute the benchmarks. If you used the production profile to compile the node, you can list the available benchmarks by running the following command: ./target/production/node-template benchmark pallet --list Benchmark all functions in all pallets To execute all benchmarks for the runtime, you can run a command similar to the following: ./target/production/node-template benchmark pallet \ --chain dev \ --execution = wasm \ --wasm-execution = compiled \ --pallet "*" \ --extrinsic "*" \ --steps 50 \ --repeat 20 \ --output pallets/all-weight.rs This command creates an output file‚Äîin this case, a file named all-weight.rs ‚Äîthat implements the WeightInfo trait for your runtime. Benchmark a specific functions in a pallet To execute the benchmark for a specific function in a specific pallet, you can run a command similar to the following: ./target/production/node-template benchmark pallet \ --chain dev \ --execution = wasm \ --wasm-execution = compiled \ --pallet pallet_balances \ --extrinsic transfer \ --steps 50 \ --repeat 20 \ --output pallets/transfer-weight.rs This command creates an output file for the selected pallet‚Äîfor example, transfer-weight.rs ‚Äîthat implements the WeightInfo trait for the pallet_balances pallet. Use a template to format benchmarks The benchmarking command-line interface uses a Handlebars template to format the final output file. You can optionally pass the --template command-line option to specify a custom template instead of the default. Within the template, you have access to all the data provided by the TemplateData struct in the benchmarking command-line interface. There are some custom Handlebars helpers included with the output generation: underscore : Add an underscore to every 3rd character from the right of a string. Primarily to be used for delimiting large numbers. join : Join an array of strings into a space-separated string for the template. Primarily to be used for joining all the arguments passed to the CLI. To get a full list of benchmark subcommands, run: ./target/production/node-template benchmark --help To get a full list of available options for the benchmark pallet subcommand, run: ./target/production/node-template benchmark pallet --help Where to go next frame-benchmarking README Substrate Seminar: Benchmarking Your Substrate Pallet How-to: Add benchmarks Command reference: node-template benchmark Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Why benchmark a pallet Developing a linear model Benchmarking and weight Benchmarking tools Writing benchmarks Testing benchmarks Adding benchmarks Running benchmarks Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Simulate parachains in a test network Home Test Simulate parachains Simulate parachains in a test network You can use the zombienet command-line tool to set up a local test network to simulate a relay chain with validators and parachain collator nodes. You can configure the test network to include multiple validators and parachains with multiple collators. This tutorial illustrates how to set up a basic test network with the following configuration: Four validators Two parachains One collator per parachain One message passing channel that enables the parachains to exchange messages Prepare a working folder with the binaries The zombienet command-line interface relies on a configuration file to specify the characteristics of the test network, including the name and location of the binaries, Docker image, or Kubernetes deployment to use. This tutorial illustrates how to configure a test network that uses the native relay chain and collator binaries, so the first step in setting up your test network is to prepare a working folder with the binaries you'll need. To prepare a working folder with the binaries for the test network: Open a new terminal shell on your computer, if needed. Change to your home directory and create a new folder to hold the binaries required to generate a test network. For example: mkdir binaries If you‚Äôre setting up the test network on Linux, you can download the Polkadot binary from Releases into your working folder. If you‚Äôre setting up the test network on macOS or want to compile the binary yourself, continue to the next step. Clone the Polkadot repository by running a command similar to the following: git clone https://github.com/paritytech/polkadot Change to the root of the polkadot directory by running the following command: cd polkadot Checkout the latest release of Polkadot. Release branches use the naming convention release-v<n.n.n> . For example, the release branch used in this tutorial is release-v0.9.37 . You can check out a more recent release branch instead of using release-v0.9.37 . You can find information about recent releases and what's included in each release on the Releases tab. For example: git checkout release-v0.9.37 Compile the relay chain node by running the following command: cargo build --release Copy the Polkadot binary into your working binaries folder by running a command similar to the following: cp ./target/release/polkadot .. /binaries/polkadot-v0.9.37 As this example illustrates, it's generally a good practice to append the version of polkadot to the binary name to keep the files in the binaries folder organized. Change to your home directory. Add the parachain binary Your working folder now has the binary for the relay chain, but you also need the binary for the parachain collator nodes. You can add the parachain collator binary to your working folder by cloning the substrate-parachain-template repository. By default, compiling the substrate-parachain-template creates a parachain collator binary that is configured with the paraId 1000. You can use this paraId for the first parachain in the test network. To add the parachain collator binary to the working folder: Clone the substrate-parachain-template repository by running the following command: git clone https://github.com/substrate-developer-hub/substrate-parachain-template Change to the root of the parachain template directory by running the following command: cd substrate-parachain-template Check out the release branch that matches the release branch you used to configure the relay chain. For example: git checkout polkadot-v0.9.37 Compile the parachain template collator by running the following command: cargo build --release You now have a parachain collator binary for paraId 1000. Copy the parachain binary into your working binaries folder by running a command similar to the following: cp ./target/release/parachain-template-node .. /binaries/parachain-template-node-v0.9.37 In this example, your working directory is $HOME/binaries or ~/binaries so you must navigate up a directory level from your current substrate-parachain-template root directory. As this example illustrates, it's generally a good practice to append the version to the binary name to keep the files in the binaries folder organized. Configure the test network settings Now that you have the binaries you need in a working folder, you are ready to configure the settings for the test network that Zombienet will use. To download and configure Zombienet: Download the appropriate Zombienet executable for the Linux or macOS operating system. Depending on your security settings, you might need to explicitly allow access to the executable. If you want the executable to be available system-wide, run commands similar to the following after downloading the executable: chmod +x zombienet-macos cp zombienet-macos /usr/local/bin Verify that Zombienet is installed correctly by running the following command: ./zombienet-macos --help If command-line help is displayed, the Zombienet is ready to configure. Create a configuration file for Zombienet by running the following command: touch config.toml You are going to use the configuration file to specify the following information: Location of the binaries for the test network. The relay chain specification‚Äî rococo-local ‚Äîto use. Information about the four relay chain validators. Identifiers for parachains included in the test network. Information about the collators for each parachains. WebSocket endpoint ports to use to connect to each node. For example: [ relaychain ] default_command = "../binaries/polkadot-v0.9.37" default_args = [ "-lparachain=debug" ] chain = "rococo-local" [ [ relaychain.nodes ] ] name = "alice" [ [ relaychain.nodes ] ] name = "bob" [ [ relaychain.nodes ] ] name = "charlie" [ [ relaychain.nodes ] ] name = "dave" [ [ parachains ] ] id = 1000 cumulus_based = true [ parachains.collator ] name = "parachain-A-1000-collator01" command = "../binaries/parachain-template-node-v0.9.37" [ [ parachains ] ] id = 1001 cumulus_based = true [ parachains.collator ] name = "parachain-B-1001-collator01" command = "../binaries/parachain-template-node-v0.9.37" Save your changes and close the file. Start the test network using this configuration file by running a command similar to the following: ./zombienet-macos spawn config.toml -p native The command displays information about the test network nodes being started. Take note of the relay chain and parachain node endpoints. For example, the direct link to the relay chain endpoints should look similar to the following: alice: https://polkadot.js.org/apps/?rpc=ws://127.0.0.1:52190#/explorer bob: https://polkadot.js.org/apps/?rpc=ws://127.0.0.1:52194#/explorer charlie: https://polkadot.js.org/apps/?rpc=ws://127.0.0.1:52198#/explorer dave: https://polkadot.js.org/apps/?rpc=ws://127.0.0.1:52202#/explorer The direct link to the parachain collator endpoints should look similar to the following: parachain-1000-collator: https://polkadot.js.org/apps/?rpc=ws://127.0.0.1:52206#/explorer parachain-1001-collator: https://polkadot.js.org/apps/?rpc=ws://127.0.0.1:52210#/explorer After all of the nodes are running, you can interact with your nodes by opening the Polkadot/Substrate Portal and connecting to any of the node endpoints. Open a message passing channel Now that you have your test network up, you can open horizontal relay message passing channels to enable communication between parachain A (1000) and parachain B (1001). Because channels are unidirectional, you need to Send a request to open channel from parachain A (1000) to parachain B (1001). Accept the request on parachain B (1001). Send a request to open channel from parachain B (1001) to parachain A (1000). Accept the request on parachain A (1000). Zombienet simplifies opening these channels by enabling you to include basic channel settings in the configuration file for testing purposes. To set up communication between the parachains in the test network: Open the config.toml file in a text editor. Add channel information similar to the following to the configuration file: [ [ hrmp_channels ] ] sender = 1000 recipient = 1001 max_capacity = 8 max_message_size = 8000 [ [ hrmp_channels ] ] sender = 1001 recipient = 1000 max_capacity = 8 max_message_size = 8000 Note that the values you set for max_capacity and max message size shouldn't exceed the values defined for the hrmpChannelMaxCapacity and hrmpChannelMaxMessageSize parameters for the relay chain. To check the configuration settings for the current relay chain using the Polkadot/Substrate Portal : Click Developer and select Chain State . Select configuration , then select activeConfig() . Check the following parameter values: hrmpChannelMaxCapacity: 8 hrmpChannelMaxTotalSize: 8,192 hrmlChannelMaxMessageSize: 1,048,576 Save your changes and close the file. Restart Zombienet by running the following command: ./zombienet-macos spawn config.toml -p native You now have a test network with a bidirectional HRMP channel open between the parachains A (1000) and parachain B (1001). You can use the Polkadot/Substrate Portal to connect to the parachains and send messages. Click Developer and select Extrinsics . Select polkadotXcm or xcmPallet , then select sent(dest, message) to craft the XCM messages you want to send. You should note that XCM messages are like other transactions and require the sender to pay for the execution of the operation. All of the information required must be included in the message itself. For information about how to craft messages using XCM after you've opened HRMP channels, see Cross-consensus communication and Transfer assets with XCM . Where to go next For a more complex preconfigured environment that uses Zombienet, download and explore the Trappist playground . For more information about the properties you can set in the configuration file, see the Network definition specification . Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Prepare a working folder with the binaries Configure the test network settings Open a message passing channel Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Check runtime Home Test Check runtime Check runtime The try-runtime command-line tool enables you to query a snapshot of runtime storage using an in-memory-externalities data structure to store state. By using the in-memory storage, you can write tests for a specified runtime state so that you can test against real chain state before going to production. In its simplest form, you can use try-runtime to test the runtime state by doing the following: Connect to a remote node. Call a specific runtime API. Retrieve state from the node at a specific block. Write tests for the data retrieved. Motivation The initial motivation for try-runtime came from the need to test runtime changes against state from a real chain. Prior TestExternalities and BasicExternalities existed for writing unit and integrated tests with mock data, but lacked the ability to test against a chain's actual state. The try-runtime tool extends TestExternalities and BasicExternalities by retrieving state using the following RPC endpoints for the node: rpc_get_storage rpc_get_keys_paged (see remote externalities lib for more details;) After using the key-value database to retrieve state, try-runtime inserts the data into TestExternalities . How it works The try-runtime tool has its own implementation of externalities called remote_externalities which is just a wrapper around TestExternalities that uses a generic key-value store where data is type encoded . The diagram below illustrates the way externalities sits outside a compiled runtime as a means to capture the storage of that runtime. With remote_externalities , you can capture some chain state and run tests on it. Essentially, RemoteExternalities will populate a TestExternalities with a real chain's data. To query state, try-runtime uses the RPC methods provided by StateApiClient . In particular: storage This method returns the storage value for the key that represents the block you specify. storage_key_paged This method returns the keys that match a prefix you specify with pagination support. Usage The most common use case for try-runtime is to help you prepare for storage migration and runtime upgrades. Because the RPC calls that query storage are computationally expensive, there are a number of command-line options you should set for a running node before you use the try-runtime command. To prepare a node for try-runtime testing, set the following options: Set --rpc-max-payload 1000 to ensure large RPC queries can work. Set --rpc-cors all to ensure WebSocket connections can come through. You can combine try-runtime with fork-off-substrate to test your chain before production. Use try-runtime to test your chain's migration and its pre and post states. Then, use fork-off-substrate if you want to check that block production continues after the migration. Runtime upgrade hooks By default, runtime upgrade hooks‚Äîwhich can be defined inside of the runtime or inside pallets‚Äîspecify what should happen when there's been a runtime upgrade. That is, the default on_runtime_upgrade method only describes runtime state after the upgrade. However, it is possible to use methods provided by try-runtime to inspect and compare the runtime state before and after a runtime upgrade for testing purposes. If you enable the try-runtime feature for the runtime, you can define pre-upgrade and post-upgrade hooks for the runtime as follows: #[cfg(feature = "try-runtime" )] fn pre_upgrade ( ) -> Result < Vec < u8 > , & 'static str > { Ok ( Vec :: new ( ) ) } #[cfg(feature = "try-runtime" )] fn post_upgrade ( _state : Vec < u8 > ) -> Result < ( ) , & 'static str > { Ok ( ( ) ) } With these function, you can use the pre_upgrade hook to retrieve the runtime state and return it as a Vec result. You can the pass the Vec as input parameter to the post_upgrade hook. Command-line examples To use try-runtime from the command line, run your node with the --features=try-runtime flag. For example: cargo run --release --features = try-runtime try-runtime You can use the following subcommands with try-runtime : on-runtime-upgrade : Executes tryRuntime_on_runtime_upgrade against the given runtime state. offchain-worker : Executes offchainWorkerApi_offchain_worker against the given runtime state. execute-block : Executes core_execute_block using the given block and the runtime state of the parent block. follow-chain : Follows a given chain's finalized blocks and applies to all its extrinsics. This allows the behavior of a new runtime to be inspected over a long period of time, with real transactions coming as input. To view usage information for a specific try-runtime subcommand, specify the subcommand and the --help flag. For example, to see usage information for try-runtime on-runtime-upgrade , you can run the following command: cargo run --release --features = try-runtime try-runtime on-runtime-upgrade --help For example, you can run try-runtime with the on-runtime-upgrade subcommand for a chain running locally with a command like this: cargo run --release --features = try-runtime try-runtime on-runtime-upgrade live ws://localhost:9944 You can use try-runtime to re-execute code from the ElectionProviderMultiPhase offchain worker on localhost:9944 with a command like this: cargo run -- --release \ --features = try-runtime \ try-runtime \ --execution Wasm \ --wasm-execution Compiled \ offchain-worker \ --header-at 0x491d09f313c707b5096650d76600f063b09835fd820e2916d3f8b0f5b45bec30 \ live \ -b 0x491d09f313c707b5096650d76600f063b09835fd820e2916d3f8b0f5b45bec30 \ -m ElectionProviderMultiPhase \ --uri wss://localhost:9944 You can run the migrations of the local runtime on the state of SomeChain with a command like this: RUST_LOG = runtime = trace,try-runtime::cli = trace,executor = trace \ cargo run try-runtime \ --execution Native \ --chain somechain-dev \ on-runtime-upgrade \ live \ --uri wss://rpc.polkadot.io You can run try-runtime against the state for a specific block number with a command like this: RUST_LOG = runtime = trace,try-runtime::cli = trace,executor = trace \ cargo run try-runtime \ --execution Native \ --chain dev \ --no-spec-name-check \ on-runtime-upgrade \ live \ --uri wss://rpc.polkadot.io \ --at < block-hash > Notice that this command requires the --no-spec-name-check command-line option. Where to go next Storage keys OnRuntimeUpgrade try-runtime-upgrade Staking pallet Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Motivation How it works Usage Command-line examples Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Deploy Build Test Deploy Prepare to deploy Deployment options Keys and network operations Maintain Tutorials Reference Community Documentation Search documentation Quick start Learn Install Build Test Deploy Prepare to deploy Deployment options Keys and network operations Maintain Tutorials Reference Community Home Deploy Deploy Moving a project from a test network into production requires a clear view of your infrastructure and network operations. The topics in this section are intended to provide some guidance on how to deploy a parachain and how to prepare for network maintenance after you deploy. Before diving into specific deployments options and operations, however, there are a few important considerations that should act as the foundation for your deployment process. At a high level, you should apply the following general principles throughout all phases of your network deployment: Infrastructure operations should be code-driven and kept under version control. Security should be a central concern in all of your infrastructure decisions. Securing on-chain operations is critical to a successful deployment. Build a code-driven infrastructure As a general rule, you should take an infrastructure-as-code approach to your deployment planning and preparation. All of the infrastructure components used to deploy the network should be written in code and managed through a version control system. Following an infrastructure-as-code process helps to ensure that: Your infrastructure components are always in a known state. You can track changes, enforce reviews, and audit activity through version control. You can easily roll back to a known state, if needed. You can automate operations and won't need to perform manual tasks to redeploy. Build security into the infrastructure Security is one of the most important properties of a blockchain, so making your infrastructure secure by default should be a top priority when you plan to deploy a new network. Having strict access control rules and blocking any non-required communication is a good starting point, but you should also consider how you can enhance security through the following: Log monitoring Bastion hosts for access Two-factor authentication for secure shell (ssh) access Build security into chain operations In addition to the infrastructure that your network relies on, it's important to keep on-chain operations as secure as possible. For example, by setting up your chain to use cold, warm, and hot keys, you can limit the damage that the compromise of a session key could do. The following diagram illustrates using keys for different operations to help prevent funds from being stolen. You can also use multi-signature accounts on offline devices to improve security and to provide an on-chain audit trail of all actions. The record of on-chain activity could then be used to send alert notification for specific on-chain actions. Proxy accounts also let you limit the permission that an account or multi-signature account has on a particular target account. For example, you might use a proxy account to specify that a multi-signature staking controller account can only execute staking transactions and the transaction can only be executed if three of the five people that make up the multi-signature account approve. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Build a code-driven infrastructure Build security into the infrastructure Build security into chain operations Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Prepare to deploy Build Test Deploy Prepare to deploy Deployment options Keys and network operations Maintain Tutorials Reference Community Documentation Search documentation Quick start Learn Install Build Test Deploy Prepare to deploy Deployment options Keys and network operations Maintain Tutorials Reference Community Home Deploy Prepare to deploy Prepare to deploy To prepare for deployment, you'll want to look more carefully at your network architecture, including how and where to deploy different nodes, the type of infrastructure you are deploying on, and the command-line options you'll use to manage node operations. The topics in this section highlight some of these key considerations for moving from a local development and test environment into production. Node roles and responsibilities As discussed in Networks and nodes , you can use Substrate to build blockchains that can be deployed as solo chains or parachains, as private networks, or as custom relay chains. You also learned that nodes can take on different roles depending on your chain and project requirements. As you prepare for deployment, it's important to keep in mind how different types of nodes are used to ensure they are configured with appropriate hardware and software required to perform their duties. The following table summarizes the most common node types and responsibilities: Node type What it does Validator node Secures the relay chain by staking DOT, processes the validating proofs from parachain collator node, and votes on consensus along with other validators. Collator node Maintains a parachain by collecting parachain transactions and producing state transition proofs for the validators. Boot node Provides a static address and peer-to-peer ( libp2p ) public key that is used to bootstrap a node onto the network‚Äôs distributed hash table and to find peer nodes. RPC node Exposes an RPC interface over HTTP or WebSocket ports for the relay chain or the parachain so that users can read the blockchain state and submit transactions. There are often multiple RPC nodes behind a load balancer. Archive node Maintains all blocks starting from the genesis block with complete state available for every block. Full node Synchronizes with the relay chain or parachain to store the most recent block state and block headers for older blocks. Role-specific command-line options The command-line settings you use to start a node typically reflect the type of node you are starting. For example, if you are starting a validator node, you should always specify at least the --validator command-line option to enable block validation. You should also ensure that the node keys are injected using the command line or by calling the author_rotateKeys RPC method. For collator nodes, you should always specify at least the --collator command-line option to enable parachain collation. Bootnodes use a static key file to ensure the public address for peer-to-peer networking is always the same. You should store the private node key in a file and use the --node-key-file command-line option to specify the path to the file. For RPC nodes, you should specify the following command-line options to allow up to 5000 public RPC or WebSocket connections: --unsafe-ws-external --rpc-methods Safe --rpc-cors ‚Äò*‚Äô --ws-max-connections 5000 If you want a node to be an archive node, you must specify the -‚Äìpruning=archive command-line option to prevent the node from discarding any block state. Parachain-specific syntax For parachain nodes, you must specify two sets of command-line options. The first set of command-line options apply to the parachain. The second set of command-line options apply to the relay chain that runs locally in parallel with the parachain. For example: ./statemine $PARACHAIN_OPTIONS -- $RELAYCHAIN_OPTIONS For example, the following command illustrates starting a Statemine collator node: ./statemine --chain statemine --in-peers 25 --out-peers 25 --db-cache 512 --pruning = 1000 --unsafe-pruning -- --chain kusama -db-cache 512 --pruning = 1000 --wasm-execution Compiled Common deployment targets There are many different ways you could deploy the nodes that define your network topology, and different deployment scenarios have different hardware and software requirements. The following table provides a non-exhaustive list of the common deployment targets: Type Description Physical server Physical hardware is typically deployed as a rack-mounted server located in a data center. Virtual machine Virtual hardware is typically deployed as a virtual image hosted on a cloud provider platforms or self-hosted onsite. Kubernetes cluster Kubernetes is a container orchestration engine that can be used to host your blockchain instances. This option is only recommended if you already have prior experience with Kubernetes, especially in production environments. Local container Local container engines‚Äîsuch as containerd , Docker, and Pod Manager‚Äîprovide simpler containerization services than Kubernetes and can be used to create and deploy node images. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Node roles and responsibilities Parachain-specific syntax Common deployment targets Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Deployment options and tools Build Test Deploy Prepare to deploy Deployment options Keys and network operations Maintain Tutorials Reference Community Documentation Search documentation Quick start Learn Install Build Test Deploy Prepare to deploy Deployment options Keys and network operations Maintain Tutorials Reference Community Home Deploy Deployment options Deployment options and tools As you saw in Common deployment targets , there are several common deployment scenarios. How you go about managing the nodes for the services you provide depends a great deal on whether you are deploying on physical hardware, using a cloud provider, or configuring a Kubernetes cluster. This section describes some of the most common options and tools for deploying and managing nodes in different environments. Linux servers If you are deploying on physical or virtual machines that use a distribution of the Linux operating system, you typically use systemd to manage most services. You can use systemd to ensure processes are enabled and running, set policies for restarting services, specify the user account for hosts to run under, and configure system parameters to limit memory usage and other properties. The following example illustrates configuring the systemd file for a node running a local development chain using the account associated with Alice and the local user name polkadot : [Unit] Description=Polkadot Validator [Service] User=polkadot ExecStart=/home/polkadot/polkadot --dev --alice Restart=always RestartSec=90 [Install] WantedBy=multi-user.target For demonstration purposes, this file is named polkadot.service and placed in the /etc/systemd/system directory. You can then enable the service by running the following command: systemctl enable polkadot To start the service after it's enabled, you can run the following command: systemctl start polkadot Using environment variables If you want to remove some settings from the systemd configuration‚Äîfor example, the --dev and --alice command-line options‚Äîyou can configure those settings in an environment variable file. With environment variable files, you can configure the appropriate settings for each server‚Äîin its own server-specific file‚Äîbut still manage the service using systemd commands. In this example, you would create a new environment variable file for the local host in /etc/default/polkadot that looks like this: START_OPTIONS="--dev --alice" You would then modify the systemd service to look like this: [Unit] Description=Polkadot Validator [Service] User=polkadot EnvironmentFile=/etc/default/polkadot ExecStart=/home/polkadot/polkadot $START_OPTIONS Restart=always RestartSec=90 [Install] WantedBy=multi-user.target You can use this technique with multiple variables to abstract the configuration details away from the systemd file that runs on your node hosts. Local logging By default, the systemd service writes output to the local syslog file, typically /var/log/syslog or /var/log/messages . You can also view this output using the journalctl command. For example, to see the most recent output of the polkadot process, you can run the following command: journalctl -u polkadot -f To remove logs older than two days ago, you can run a command similar to the following: journalctl -u polkadot --vacuum-time = 2d To retain only the past 1G of data, run: journalctl --vacuum-size = 1G Remote logging If your deployment includes many hosts, you can use Loki or Elasticsearch to aggregate the data from multiple sources. Loki To log to a remote loki instance: Install the promtail server package on each server. Create the configuration file that specifies the server and client information to enable each server to send logs to a remote host. For example: # promtail server config server : http_listen_port : 9080 grpc_listen_port : 0 log_level : info positions : filename : /var/lib/promtail/positions.yaml # loki servers clients : - url : http : //myloki.mycompany.com/loki/api/v1/push backoff_config : min_period : 1m max_period : 1h max_retries : 10000 scrape_configs : - job_name : journald journal : max_age : 1m path : /var/log/journal labels : job : journald pipeline_stages : - match : selector : '{job="journald"}' stages : - multiline : firstline : '^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}' max_lines : 2500 - regex : expression : '(?P<date>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3})\s+(?P<level>(TRACE|DEBUG|INFO|WARN|ERROR))\s+(?P<worker>([^\s]+))\s+(?P<target>[\w-]+):?:?(?P<subtarget>[\w-]+)?:[\s]?(?P<chaintype>\[[\w-]+\]+)?[\s]?(?P<message>.+)' - labels : level : target : subtarget : Elasticsearch To log to a remote Elasticsearch cluster: Install the logstash package. Create the configuration file that specifies the server and client information to enable each server to send logs to a remote host. An example configuration would look like this: nput { journald { path => "/var/log/journal" seekto => "tail" thisboot => true filter => { "_SYSTEMD_UNIT" => "polkadot.service" } type => "systemd" } } filter { date { match => ["timestamp", "YYYY-mm-dd HH:MM:ss.SSS"] target => "@timestamp" } mutate { rename => [ "MESSAGE", "message" ] remove_field => [ "cursor", "PRIORITY", "SYSLOG_FACILITY", "SYSLOG_IDENTIFIER", "_BOOT_ID", "_CAP_EFFECTIVE", "_CMDLINE", "_COMM", "_EXE", "_GID", "_HOSTNAME", "_MACHINE_ID", "_PID", "_SELINUX_CONTEXT", "_STREAM_ID", "_SYSTEMD_CGROUP", "_SYSTEMD_INVOCATION_ID", "_SYSTEMD_SLICE", "_SYSTEMD_UNIT", "_TRANSPORT", "_UID" ] } if ([message] =~ ".*TRACE .*") { drop{ } } grok { match => { "message" => "%{NOTSPACE:thread} %{LOGLEVEL:log-level} %{NOTSPACE:namespace} %{GREEDYDATA:message}" } } } output { elasticsearch { hosts => ["https://myelasticsearch.mycompany.com:9243"] user => "username" password => "password" index => "logstash-polkadot-%{+YYYY.MM.dd}" } } Logging command-line options When you start a node, you can use command-line options to specify the log level and target components you want to log activity for. All target components are set to the info logging level by default. You can adjust log levels for individual components using the --log or -l command-line option. For example, to change the logging level for the afg and sync components: --log afg = trace,sync = debug To change the logging level to debug for all components: -ldebug. The valid log levels from least to most verbose are error , warn , info , debug , and trace . The valid targets for logging are: afg aura babe beefy db gossip header peerset pow rpc runtime runtime::contracts sc_offchain slots state-db state_tracing sub-libp2p sync telemetry tracing trie txpool Cloud provisioning There are multiple options for provisioning nodes on cloud providers. Some of the tools for deploying using cloud resources are provider-specific and some tools are provider-agnostic. The following provider-specific tools are the most commonly-used for deploying on AWS, Microsoft Azure, or Google Cloud: Amazon Cloud Formation Azure Resource Manager Google Cloud Deployment Manager These provider-specific deployment tools are easy to use and provide important resources, including sample code, documentation, and support. However, if you use more than one multiple provider‚Äîeach with its own scripting format and configuration requirements‚Äîmaking even basic changes to your infrastructure can require changes to multiple sections of code for each provider to do the same thing. As an alternative to provider-specific tooling, Terraform offers a more general solution to infrastructure provisioning. With Terraform, you can specify a change once and apply the change across multiple providers. Terraform Terraform uses the HashiCorp Configuration Language (HCL) to support more than 2000 different cloud resource providers , including the main three providers AWS, Azure, and GCP. The configuration language enables you to abstract configuration details and use the same code for development, test, and production environments regardless of the provider you use and manage all changes to your infrastructure through source code version control. Terraform also enables you to incorporate independent resources into your infrastructure using a common language. For example, you can deploy RPC nodes along with a frontend load balancer using a single configuration file. After you prepare a host for deployment, you can use Terraform to preconfigure the host with required software from an image or run a script that preconfigures the base image. You can find examples of using Terraform with multiple providers in polkadot-validator-setup . Ansible After base hosts are deployed, they must be configured with the required software components, configuration files, and system settings. In addition to Terraform or cloud provider tooling, Ansible provides another flexible way you can automate infrastructure deployment. Ansible uses playbooks to orchestrate, configure, administer, and deploy system components. Using a combination of playbooks and roles , you can implement a specific configuration or behavior for a group of nodes. When deploying blockchain nodes, Ansible enables you to define an inventory that describes the hosts and how to group the hosts according to their role‚Äîfor example, in groups that identify hosts as validator, collator, or rpc nodes. You can then call a playbook to link the hosts and groups in the inventory with roles to execute on each host. You can find examples of using ansible in ansible-galaxy and the node role . Kubernetes You should only deploy on a Kubernetes cluster if you have previous experience managing Kubernetes configurations. The main tools for managing Substrate-based nodes in Kubernetes are helm charts that you can use to deploy nodes and the Testnet Manager that you can use to deploy and maintain test networks in a Kubernetes cluster. Note that you must have access to a Kubernetes cluster, a local copy of kubectl , and Helm installed before you can use these tools. Helm charts Parity Helm Charts is a collection of helm charts that define, install, manage, and upgrade Substrate and Polkadot components. Within the collection, the node chart is used to deploy substrate or polkadot node binary. All of the parameters for the chart are documented in the node chart README.md . The most important parameters to be aware of are: Option Description node.chain Network to connect to. node.command Binary to use. node.flags Command-line options to use with the binary in the container. node.customChainspecUrl Custom chain specification URL. There‚Äôs also an example values.yml configuration file that you can start working from. The following example illustrates how to deploy a rococo-local test network chain in Kubernetes with two validators and two full nodes. To deploy the rococo-local chain using the helm chart: Verify you have access to a Kubernetes cluster and the Helm client installed. Add the Parity chart repository to Helm by running the following command: helm repo add parity https://paritytech.github.io/helm-charts/ Install the node chart by running the following command: helm install polkadot-node parity/node Deploy the validator node using the Alice account and a custom node key by running the following command: helm install rococo-alice parity/node --set node.role = "validator" \ --set node.customNodeKey = "91cb59d86820419075b08e3043cd802ba3506388d8b161d2d4acd203af5194c1" \ --set node.chain = rococo-local \ --set node.perNodeServices.relayP2pService.enabled = true \ --set node.perNodeServices.relayP2pService.port = 30333 \ --set node.flags = "--alice --rpc-external --ws-external --rpc-cors all --rpc-methods=unsafe" This command deploys the node alice as a stateful service with an example custom node key and with a service to be used as a boot node for all other hosts. Deploy the validator node using the Bob account and alice as a boot node by running the following command: helm install rococo-bob parity/node --set node.role = "validator" \ --set node.chain = rococo-local \ --set node.flags = "--bob --bootnodes '/dns4/rococo-alice-node-0-relay-chain-p2p/tcp/30333/p2p/12D3KooWMeR4iQLRBNq87ViDf9W7f6cc9ydAPJgmq48rAH116WoC'" After both validators are running, the chain should start producing blocks. Deploy two full nodes by running the following command: helm install rococo-pool parity/node --set node.chain = rococo-local \ --set node.replicas = 2 \ --set node.flags = "--bootnodes '/dns4/rococo-alice-node-0-relay-chain-p2p/tcp/30333/p2p/12D3KooWMeR4iQLRBNq87ViDf9W7f6cc9ydAPJgmq48rAH116WoC'" After completing these steps, you have a working rococo-local test chain with two validators and two full nodes. The following tools‚Äîlisted from simplest to most advanced are useful for managing helm releases. Helmfile Terraform Helm provider Flux CD ArgoCD Docker If you are deploying nodes as virtual machines in a network, you can use Docker images to prepare the node configuration for different types of nodes. For example, you can prepare Docker images for validator nodes and RPC provider nodes then deploy multiple nodes of each type without configuring a separate virtual machine for each node. External node operators can then use the Docker images you provide to deploy new nodes whenever and wherever they are needed. Sample Dockerfile The following sample Dockerfile illustrates the best practices for building the Docker image in a secure way that minimizes the attack surface. This example is similar version to the Dockerfile used to create the official Polkadot images. You can also consult Docker documentation for additional information about Best practices for writing Dockerfiles . # This is an example build stage for the node template. Here we create the binary in a temporary image. # This is a base image to build substrate nodes FROM docker.io/paritytech/ci-linux:production as builder WORKDIR /node-template COPY . . RUN cargo build --locked --release # This is the 2nd stage: a very small image where we copy the binary." FROM docker.io/library/ubuntu:20.04 LABEL description= "Multistage Docker image for Substrate Node Template" \ image.type= "builder" \ image.authors= "you@email.com" \ image.vendor= "Substrate Developer Hub" \ image.description= "Multistage Docker image for Substrate Node Template" \ image.source= "https://github.com/substrate-developer-hub/substrate-node-template" \ image.documentation= "https://github.com/substrate-developer-hub/substrate-node-template" # Copy the node binary. COPY --from = builder /node-template/target/release/node-template /usr/local/bin RUN useradd -m -u 1000 -U -s /bin/sh -d /node-dev node-dev && \ mkdir -p /chain-data /node-dev/.local/share && \ chown -R node-dev:node-dev /chain-data && \ ln -s /chain-data /node-dev/.local/share/node-template && \ # unclutter and minimize the attack surface rm -rf /usr/bin /usr/sbin && \ # check if executable works in this container /usr/local/bin/node-template --version USER node-dev EXPOSE 30333 9933 9944 9615 VOLUME [ "/chain-data" ] ENTRYPOINT [ "/usr/local/bin/node-template" ] Automated build pipeline The following sample GitHub action builds and publishes a Docker image to DockerHub. In most cases, you trigger this action using a manual workflow or when a new release is published. Note that you must add secrets to your GitHub repository or organization as described in Encrypted secrets to publish images securely. You'll also need to save the credentials for your DockerHub account in your GitHub secrets. If you instead want to use another image repository‚Äîfor example, the GitHub image registry‚Äîyou can amend the Build and push Docker images step. # You need to add the following secrets to your GitHub Repository or Organization to make this work # - DOCKER_USERNAME: The username of the DockerHub account. E.g. parity # - DOCKER_TOKEN: Access token for DockerHub, see https://docs.docker.com/docker-hub/access-tokens/. E.g. VVVVVVVV-WWWW-XXXXXX-YYYY-ZZZZZZZZZ # The following are setup as an environment variable below # - DOCKER_REPO: The unique name of the DockerHub repository. E.g. parity/polkadot name : Build & Publish Docker Image # Controls when the action will run. on : # Triggers the workflow on push events but only for the main branch # push: # branches: [ main ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch : # Set an environment variable (that can be overriden) for the Docker Repo env : DOCKER_REPO : parity/polkadot # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs : build : # The type of runner that the job will run on runs-on : ubuntu - 22.04 # Steps represent a sequence of tasks that will be executed as part of the job steps : # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - name : Check out the repo uses : actions/checkout@v2.5.0 # Login to Docker hub using the credentials stored in the repository secrets - name : Log in to Docker Hub uses : docker/login - action@v2.1.0 with : username : $ { { secrets.DOCKER_USERNAME } } password : $ { { secrets.DOCKER_TOKEN } } # Get the commit short hash, to use as the rev - name : Calculate rev hash id : rev run : echo "value=$(git rev - parse - - short HEAD)" > > $GITHUB_OUTPUT # Build and push 2 images, One with the version tag and the other with latest tag - name : Build and push Docker images uses : docker/build - push - action@v3.2.0 with : context : . push : true tags : $ { { env.DOCKER_REPO } } : v$ { { steps.rev.outputs.value } } , $ { { secrets.DOCKER_REPO } } : latest Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Linux servers Cloud provisioning Kubernetes Docker Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Key signatures and network operations Build Test Deploy Prepare to deploy Deployment options Keys and network operations Maintain Tutorials Reference Community Documentation Search documentation Quick start Learn Install Build Test Deploy Prepare to deploy Deployment options Keys and network operations Maintain Tutorials Reference Community Home Deploy Keys and network operations Key signatures and network operations In Accounts, addresses, and keys , the discussion of public and private keys focused on identifying accounts and addresses associated with users interacting with different networks. However, keys and different key signatures are also fundamental to deploying Substrate nodes and performing specific node operations. This section recaps the encryption schemes and where they are used by different node components. Digital signature schemes Most digital signature schemes provide the following features: Key generation . The signature scheme must provide a method for generating a random private key from the set of all possible private keys and a corresponding public key that can be used to verify the authenticity of the private key. Message signing . The signature scheme must provide a method that produces a signature for a given message and private key. Signature verification . The signature scheme must provide a method to accept or reject the authenticity of a message based on the message, public key, and signature being evaluated. Different signature schemes use different algorithms to perform these operations. Regardless of the mathematics used, all signature schemes are designed to achieve two main outcomes: Authenticity of the signature generated for a given message and private key is verified by using the corresponding public key. Integrity of the message can be reasonably assumed because generating a valid signature without the private key isn't computationally feasible. The following signature schemes are supported in Substrate-based chains: Scheme Description ed25519 The Ed25519 signature scheme is based on the Edwards-curve Digital Signature Algorithm (EdDSA) signature scheme‚Äîa variant of Schnorr signatures‚Äîthat uses SHA-512 and Curve25519. This signature scheme produces more secure signatures and is significantly faster at signing messages than the ECDSA signature scheme. sr25519 The Sr25519 signature scheme is the default signature scheme for Substrate. This signature scheme is based on the Schnorrkel variant that uses Schnorr signatures with Ristretto point compression. The Sr25519 signature scheme supports additional features‚Äîsuch as hierarchical deterministic key derivation, multi-signature signing, and a verifiable random function‚Äîthat are particularly useful in a blockchain environment. ecdsa The Elliptic Curve Digital Signature Algorithm (ECDSA) is a variant of the Digital Signature Algorithm (DSA) that uses the Secp256k1 elliptic curve cryptography. This signature scheme was used initially in Bitcoin and Ethereum because of the patent protecting Schnorr signatures. Using the ECDSA signature scheme complicates some advanced cryptographic techniques, such as threshold signatures. Session keys and types Session keys are private online keys that are used by validators to sign consensus-related messages. Session keys must be available online to enable the validator to perform certain network operations. These keys aren't used to control funds and they should only be used for their intended purpose. They can be changed regularly To create session keys, a validator node operator must use a controller account too generate a certificate signed with the session's public key. The certificate attests that the key acts on behalf of the validator's staking account and nominators. After creating the session key, the validator node operator informs the chain that this key represents the controller key by publishing the session certificate in a transaction on the chain. In most cases, node operators use the Session ) pallet to manage their session keys. The SessionKeys trait is a generic, indexable type and you can declare any number of session keys in the runtime. The default Substrate node template uses four session keys. Other chains can have more or fewer depending on what operations the chain expects its validators to perform. In practice, validators combine all of the session public keys into a single object, sign the set of public keys with a controller account, and submit a transaction to register the keys on chain. This on-chain registration links a validator node with an account that holds funds. As such, the account associated with the session keys object can be credited with rewards or slashed based on the node's behavior. The runtime declares what session keys implemented with the help of the impl_opaque_keys! macro: impl_opaque_keys! { pub struct SessionKeys { pub grandpa : Grandpa , pub babe : Babe , pub im_online : ImOnline , pub authority_discovery : AuthorityDiscovery , } } Polkadot uses the following session keys: Name Type Authority discovery sr25519 GRANDPA ed25519 BABE sr25519 I'm online sr25519 Parachain assignment sr25519 Parachain validator ed25519 BABE requires keys suitable for use in a Verifiable Random Function as well as for digital signatures. Sr25519 keys have both capabilities and so are used for BABE. Commands-line interface You can use polkadot keys or the subkey command to generate and inspect keys. Two important subcommands are: generate to create a new random account and print the private key to standard output or save the key to a file. inspect to view the account and address information for an account by passing a secret phrase or seed. Some important options are: --network to specify the network the keys will be used on (default is substrate ). --scheme to specify the signature scheme for the keys (default is sr25519 ). For example, you can generate a Polkadot random key by running the following command: polkadot key generate -n polkadot The command displays output similar to the following: Secret phrase: settle whisper usual blast device source region pumpkin ugly beyond promote cluster Network ID: polkadot Secret seed: 0x2e6371e04b45f16cd5c2d66fc47c8ad7f2881215287c374abfa0e07fd003cb01 Public key (hex): 0x9e65e97bd8ba80095440a68d1be71adff107c73627c8b85d29669721e02e2b24 Account ID: 0x9e65e97bd8ba80095440a68d1be71adff107c73627c8b85d29669721e02e2b24 Public key (SS58): 14agqii5GAiM5z4yzGhJdyWQ3a6HeY2oXvLdCrdhFXRnQ77D SS58 Address: 14agqii5GAiM5z4yzGhJdyWQ3a6HeY2oXvLdCrdhFXRnQ77D You can inspect account and address information for a key by running a command similar to the following with a secret phrase: ./polkadot key inspect -n polkadot "settle whisper usual blast device source region pumpkin ugly beyond promote cluster" The command displays output similar to the following: Secret phrase: settle whisper usual blast device source region pumpkin ugly beyond promote cluster Network ID: polkadot Secret seed: 0x2e6371e04b45f16cd5c2d66fc47c8ad7f2881215287c374abfa0e07fd003cb01 Public key (hex): 0x9e65e97bd8ba80095440a68d1be71adff107c73627c8b85d29669721e02e2b24 Account ID: 0x9e65e97bd8ba80095440a68d1be71adff107c73627c8b85d29669721e02e2b24 Public key (SS58): 14agqii5GAiM5z4yzGhJdyWQ3a6HeY2oXvLdCrdhFXRnQ77D SS58 Address: 14agqii5GAiM5z4yzGhJdyWQ3a6HeY2oXvLdCrdhFXRnQ77D To inspect a key created with the hard derivation //Stash//0 , you would run a command similar to the following: polkadot key inspect -n polkadot "settle whisper usual blast device source region pumpkin ugly beyond promote cluster//Stash//0" The command displays output similar to the following: Secret Key URI `settle whisper usual blast device source region pumpkin ugly beyond promote cluster//Stash//0` is account: Network ID: polkadot Secret seed: 0xe9437b365161e8228e8abd53d64e6b31058dcddcd0b96f895045ecc41579ee3e Public key (hex): 0xd8ed7b942f6e590b06e99951ac10e3312f65f01df5b3f250b70374fc2da1046d Account ID: 0xd8ed7b942f6e590b06e99951ac10e3312f65f01df5b3f250b70374fc2da1046d Public key (SS58): 15uRtdeE4MyMHV9LP1UHKqTx4f8Qa8uVZUpxWWw8VKSroucK SS58 Address: 15uRtdeE4MyMHV9LP1UHKqTx4f8Qa8uVZUpxWWw8VKSroucK For more information about using subkey comands and command-line options, see subkey . Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Digital signature schemes Session keys and types Commands-line interface Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Maintain Build Test Deploy Maintain Monitor Runtime upgrades Tutorials Reference Community Documentation Search documentation Quick start Learn Install Build Test Deploy Maintain Monitor Runtime upgrades Tutorials Reference Community Home Maintain Maintain In many ways, deploying your project as a parachain, parathread, paraobject, or private solo chain is the beginning of your journey. To keep your application viable and evolving, it's important to build your community and manage your network, including staying up-to-date with hardware and software upgrades, managing new releases, and maintaining your infrastructure. The topics in this section are intended to highlight‚Äîin a generalized form‚Äîsome of the most important maintenance tasks. However, every project is different and most projects will require their own specialized set of instructions, processes, and policies. Monitor describes the default telemetry available for Substrate nodes and the Polkadot ecosystem, including how to view and filter the information available. Runtime upgrades explains how runtime versioning and storage migration support runtime upgrades, enabling your blockchain to evolve over time. The following topics are planned but not available at this time: Operate Client upgrades Network upgrades Storage migration Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Monitor Build Test Deploy Maintain Monitor Runtime upgrades Tutorials Reference Community Documentation Search documentation Quick start Learn Install Build Test Deploy Maintain Monitor Runtime upgrades Tutorials Reference Community Home Maintain Monitor Monitor Monitoring network operations and performance is an important part of maintaining a healthy, stable, and secure ecosystem. By default, Substrate nodes provide low-level telemetry components to automatically collect and transmit detailed information about the operation of each node in the network. The backend telemetry information is streamed in real time‚Äîas it occurs‚Äîto an endpoint so that you can observe, monitor, and alert on system behavior. As a developer or node operator, there's rarely any need to know the implementation details of how these low-level telemetry components provide information about computer and network operations. At a high level, the information collected from each public node is sent to a default telemetry server and aggregated for display in a front-end dashboard. The following diagram provides a simplified overview of the workflow. View telemetry data The Polkadot telemetry dashboard provides a real-time view of how currently online nodes are performing. You can choose the information you want to display by turning visible columns on and off from the list of columns available. To see the list of columns and modify which columns are displayed, click Settings . The columns you can toggle on and off include: Validator to display an indicator to differentiate validator nodes from other nodes. Location to display the location of a node. Implementation to display the version of the software running on the node. Network ID to display the public network identifier for the node. Peer Count to display the number of peers connected to the node. Transactions in Queue to display the number of transactions in the Ready queue waiting for a block author. Upload Bandwidth to display a graph of recent upload activity in MB per second. Download Bandwidth to display a graph of recent download activity in MB per second. State Cache Size to display a graph of the state cache size in MB. Block to display the current best block number for the node to ensure it's staying synchronized with its peers. Block Hash to display the block hash for the current best block number for the node. Finalized Block to display the most recently finalized block number for the node to ensure it's staying synchronized with its peers. Finalized Block Hash to display the block hash for the most recently finalized block number for the node. Block Time to display the block execution time between blocks. Block Propagation Time to display the time it took for the node to import the most recent block. Last Block Time to display the time it took for the node to author the most recent block. Node Uptime to display the number of days the node has been online without being restarted. Pin node information You can keep the information for one or more nodes in view in the dashboard by clicking the line displayed for the node in the dashboard list. For example, if you want to keep the information for two nodes visible while the information for other nodes scrolls by, you can select the nodes in the list and pin the information in place. Display nodes on a map As an alternative to the list of nodes, you can display nodes on a global map. The map provides a visual representation of where nodes are located, where blocks are being produced and imported in real time, and where there are higher concentrations of nodes geographically. Note that only nodes that have a known location are displayed in the map. To see represented on a map, click Map . In this view, you can hover over any node location on the map to display a subset of the information available for the node, including the node name, location, best block number, best block hash, and block time statistics. Display node statistics In addition to the information available for individual nodes, you can view statistics that describe the number and percentage of nodes that share specific attributes. For example, you can see the percentage of nodes that are using each release of Polkadot, the percentage of nodes using the Linux operating system and the most common distributions currently deployed. You can also view statistics about the CPU and memory configuration across nodes in the network. To see the full set of statistics available, click Statistics . The statistics available include details about the hardware and software running on the nodes in the network, including: Software version. Operating system. CPU architecture and model. Number of physical CPU cores. Total memory. Whether the node is a virtual machine. Linux distribution and kernel version. CPU and memory speed. Disk speed. Filter by chain By default, the telemetry dashboard displays information about Polkadot and provides quick links to a subset of other chains. To display information for other chains, click More and select a different chain. Customize the monitoring stack The default telemetry dashboard provides visibility into node and chain operations without requiring you to host or configure any backend monitoring or front-end services. However, many projects choose to augment or replace the default telemetry server with their own backend server and front-end dashboards. In general, setting up your own telemetry server involves establishing monitoring and alerting policies for both on-chain events and individual node operations . On-chain activity You can monitor on-chain activity for specific events, such as transactions submitted from a certain address, a change to the current validator set. On-chain monitoring typically involves connecting to RPC nodes to check for specific values, identify processing delays, or track the timing of events. In most cases, you only need two RPC instances to handle requests for all of your hosts. However, it's recommended that you run your own RPC servers to service these requests in case there are issues with the public RPC nodes. Some examples of applications that query on-chain information are polkabot and polkadot-basic-notification . Node operations You should monitor each node that you run on the network for basic information about its operation such as the current block height, the number of peer-to-peer connections, CPU usage, and the available free memory. By default, Substrate exposes many useful metrics on the host:9615/metrics endpoint. For example, if Substrate is running locally, you can see the metrics on the http://localhost:9615/metrics endpoint. This endpoint outputs metrics using a simple key-value format. For example: polkadot_database_cache_bytes 0 However, keys can also include descriptive tags. For example: susbtrate_block_height{status="best"} 136 susbtrate_block_height{status="finalized"} 133 By default, the metrics endpoint is only be exposed on the local network interface. However, you can expose it on all interfaces by using the --prometheus-external command-line option to start a node. Configure monitoring tools To set up monitoring and alerting policies, you typically configure a set of tools to create your own monitoring stack. For example, the default metrics endpoint doesn't include host metrics‚Äîsuch as CPU, memory, bandwidth usage‚Äîso you can complement it by installing the Prometheus node_exporter on each host. The following diagram illustrates an open source set of tools that are often used as a monitoring stack. As this diagram illustrates, there are different tools available for each layer of the stack. In this example, the following tools are configured for monitoring on-chain activity and node operations: Prometheus is a monitoring engine that collects metrics from specified targets at specified intervals and evaluates the data collected using rules you define. Its time series database can hold large amounts of data that can be accessed very quickly. Grafana is an observability platform that allows you to query, visualize, and analyze the data you collect through dashboards and graphs. Node exporter is process that listens on a port and reports application-specific metrics to Prometheus. Alertmanager is a tool that enables you to create and route alerts based on the rules you specify. Alertmanager allows you to configure how and where to send alert if something goes wrong. For example, you can send instant messages for warning alerts, but page an on-call technician for critical alerts Loki is a scalable log aggregation system that allows you to view and search logs from all components in your infrastructure in one place. For a simple example of setting up node monitoring using Prometheus, Grafana, and node exporter, see Monitor node metrics . For a simplified example of using Loki, see Remote logging . Change the telemetry server After you have configured the backend monitoring rules for your nodes, you can use the --telemetry-url command-line option when you start a node to specify the telemetry server that you want to send telemetry data to. You can pass this option multiple times to specify multiple telemetry endpoints. If you specify the --telemetry-url command-line option, you must also specify how verbose metrics should be, with level 0 denoting the least verbose through level 9 denoting the most verbose. For example, to specify your own telemetry server URL with a verbosity level of 5, you would run a command similar to the following: ./target/release/node-template --dev \ --telemetry-url "wss://192.168.48.1:9616 5" \ --prometheus-port 9616 \ --prometheus-external For more information about the backend components for telemetry or configuring your own server, see substrate-telemetry or the telemetry helm chart for Kubernetes deployments. Disable telemetry Telemetry is enabled for all global chain nodes by default. You can use the --no-telemetry command-line option to prevent a node from connecting to the Substrate telemetry server. For example, to prevent telemetry data from being send to the default telemetry server, you would run a command similar to the following: ./target/release/node-template --chain myCustomChain \ --no-telemetry Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE View telemetry data Customize the monitoring stack Disable telemetry Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Runtime upgrades Build Test Deploy Maintain Monitor Runtime upgrades Tutorials Reference Community Documentation Search documentation Quick start Learn Install Build Test Deploy Maintain Monitor Runtime upgrades Tutorials Reference Community Home Maintain Runtime upgrades Runtime upgrades Forkless runtime upgrades are a defining feature of the Substrate framework for blockchain development. The ability to update the runtime logic without forking the code base enables your blockchain to evolve and improve over time. This capability is made possible by including the definition of the runtime execution environment‚Äîthe runtime WebAssembly blob‚Äîas an element in the blockchain's runtime state. Because the runtime is part of the blockchain state, network maintainers can leverage the blockchain's capabilities for trustless, decentralized consensus to securely make enhancements to the runtime. In the FRAME system for runtime development, the system library defines the set_code call that is used to update the definition of the runtime. The tutorial Upgrade a running network demonstrates two ways that you can upgrade a runtime without shutting down a node or interrupting operations. However, both of the upgrades in the tutorial illustrate adding functionality to the runtime as opposed to updating the existing runtime state. If a runtime upgrade requires changes to the existing state, it is likely to require storage migration. Runtime versioning In Build process , you learned that compiling a node generated both a platform-native binary and a WebAssembly binary and that selecting which binary to use at different points in the block production process can be controlled by execution strategy command-line options. The component that selects the runtime execution environment to communicate with is called the executor . Although you can override the default execution strategies for custom scenarios, in most cases, or the executor select the appropriate binary to use by evaluating the following information for both the native and WebAssembly runtime binaries: spec_name spec_version authoring_version To provide this information to the executor process, the runtime includes a runtime version struct similar to the following: pub const VERSION : RuntimeVersion = RuntimeVersion { spec_name : create_runtime_str! ( "node-template" ) , impl_name : create_runtime_str! ( "node-template" ) , authoring_version : 1 , spec_version : 1 , impl_version : 1 , apis : RUNTIME_API_VERSIONS , transaction_version : 1 , } ; The parameters in the struct provide the following information: This parameter Provides this spec_name The identifier for the different Substrate runtimes. impl_name The name of the implementation of the spec. This is of little consequence for the node and serves only to differentiate code of different implementation teams. authoring_version The version of the authorship interface. An authoring node will not attempt to author blocks unless this is equal to its native runtime. spec_version The version of the runtime specification. A full node will not attempt to use its native runtime in substitute for the on-chain Wasm runtime unless all of spec_name , spec_version , and authoring_version are the same between the Wasm and native binaries. Updates to the spec_version can be automated as a CI process, as is done for the Polkadot network . This paramenter is typically incremented when there's an update to the transaction_version . impl_version The version of the implementation of the specification. Nodes can ignore this. It is only used to indicate that the code is different. As long as the authoring_version and the spec_version are the same, the code itself might have changed, but the native and Wasm binaries do the same thing. In general, only non-logic-breaking optimizations would result in a change of the impl_version . transaction_version The version of the interface for handling transactions. This parameter can be useful to synchronize firmware updates for hardware wallets or other signing devices to verify that runtime transactions are valid. The parameter allows hardware wallets to know which transactions they can safely sign. This number must be bumped if there is a change in the index of the pallets in the construct_runtime! macro or if there are any changes to dispatchable functions, such as the number of parameters or parameter types. If this number is updated, then the spec_version must also be updated. apis A list of supported runtime APIs along with their versions. The orchestration engine‚Äîsometimes referred to as the executor‚Äîverifies that the native runtime has the same consensus-driven logic as the WebAssembly before it chooses to execute it. However, because the runtime versioning is set manually, the orchestration engine can still make inappropriate decisions if the runtime version is misrepresented. Accessing the runtime version The FRAME system exposes the runtime version information through the state.getRuntimeVersion RPC endpoint. The endpoint accepts an optional block identifier. However, in most cases, you use the runtime metadata to understand the APIs the runtime exposes and how to interact with these APIs. The runtime metadata should only change when the chain's runtime spec_version changes. Forkless runtime upgrades Traditional blockchains require a hard fork when upgrading the state transition function of their chain. A hard fork requires all node operators to stop their nodes and manually upgrade to the latest executable. For distributed production networks, coordination of hard fork upgrades can be a complex process. The runtime versioning properties enable Substrate-based blockchains to upgrade the runtime logic in real time without causing a fork in the network. To perform a forkless runtime upgrade, Substrate uses existing runtime logic to update the Wasm runtime stored on the blockchain to a new consensus-breaking version with new logic. This upgrade gets pushed out to all full nodes on the network as a part of the consensus process. After the Wasm runtime is upgraded, the orchestration engine sees that the native runtime spec_name , spec_version , or authoring_version no longer matches the new Wasm runtime. As a result, the orchestration engine executes the canonical Wasm runtime instead of using the native runtime in any of the execution processes. Storage migration Storage migrations are custom, one-time functions that allow you to update storage to adapt to changes in the runtime. For example, if a runtime upgrade changes the data type used to represent user balances from an unsigned integer to a signed integer, the storage migration would read the existing value as an unsigned integer and write back an updated value that has been converted to a signed integer. If you don't make these kinds of changes to how data is stored when needed, the runtime can't properly interpret the storage values to include in the runtime state and is likely to lead to undefined behavior. Storage migrations with FRAME FRAME storage migrations are implemented using the OnRuntimeUpgrade trait. The OnRuntimeUpgrade trait specifies a single function‚Äî on_runtime_upgrade ‚Äîthat allows you to specify logic to run immediately after a runtime upgrade but before any on_initialize functions or transactions are executed. Preparing for storage migration Preparing for a storage migration means understanding the changes that are defined by a runtime upgrade. The Substrate repository uses the E1-runtimemigration label to designate such changes. Writing a migration Every storage migration is different, with different requirements and different levels of complexity. However, you can use the following recommended practices to guide you when you need to perform storage migration: Extract migrations into reusable functions and write tests for them. Include logging in migrations to assist in debugging. Remember that migrations are executed within the context of the upgraded runtime. The migration code might need to include deprecated types, as in this example . Use storage versions to make migrations safer by making them more declarative, as in this example . Ordering migrations By default, FRAME orders the execution of on_runtime_upgrade functions based on the order in which the pallets appear in the construct_runtime! macro. For upgrades, the functions run in reverse order, starting with the last pallet executed first. You can impose a custom order, if needed (see an example here ). FRAME storage migrations run in this order: Custom on_runtime_upgrade functions if using a custom order. System frame_system::on_runtime_upgrade functions. All on_runtime_upgrade functions defined in the runtime starting with the last pallet in the construct_runtime! macro. Testing migrations It is important to test storage migrations. A few of the tools available for you to test storage migration include the following: The Substrate debug kit includes a remote externalities tool that allows storage migration unit testing to be safely performed on live chain data. The fork-off-substrate script makes it easy to create a chain specification to bootstrap a local test chain for testing runtime upgrades and storage migrations. Where to go next Upgrade a running network Substrate migrations Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Runtime versioning Accessing the runtime version Forkless runtime upgrades Storage migration Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Tutorials Build a blockchain Build application logic Build a parachain Reference Community Documentation Search documentation Quick start Learn Install Build Test Deploy Maintain Tutorials Build a blockchain Build application logic Build a parachain Reference Community Home Tutorials Tutorials If you are someone who likes to learn by doing, the Substrate tutorials are a great place to start your journey. Tutorials don't provide much background about why you are performing certain steps or explain the coding details. Instead, the tutorials give you hands-on experience performing the tasks essential to building your own blockchain and focus on ensuring a successful result. If you prefer to experiment on your own with less guidance, you might want to explore the Substrate Playground . If you prefer to dig into the details of the code directly, you might want to start with the Rust API documentation. But if you want to start your journey with a guided tour, try out the following tutorials. Build a blockchain The Build a blockchain tutorials illustrate the basics for working with blockchains nodes and networks, including how to make nodes communicate with each other in a network of peers and how to collect metrics about node operations. You'll learn how to: Build a local node in your development environment. Simulate a two-node network using predefined accounts. Start a small network of trusted validator nodes. Monitor node operations by collecting metrics. Upgrade the runtime for a node. Build application logic The Build application logic tutorials focus on how you can customize the runtime using pallets, including how to add simple and complex pallets to the runtime and how to use pallets in combination with smart contracts. You'll learn how to: Add a simple predefined pallet to the runtime. Create a custom pallet using macros. Specify the account to use as the originator of a function call. Add an offchain worker and submit transactions using an offchain worker. Publish a custom pallet for others to use. For a deeper dive into how to create a custom pallet from start to finish, you might want to explore the first part of the Collectibles workshop . Future iterations are intended to include an introduction to front-end development and how to convert an application from a solo chain to a parachain project. Build a parachain The Build a parachain tutorials delve into more advanced topics for moving beyond solo chain development, including how to connect your chain to other chains. You'll learn how to: Turn a solo chain into a parachain. Deploy a local test network and interact with multiple connected parachains. Connect your parachain to a public test network. Work with the cross-consensus messaging format. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Build a blockchain Build application logic Build a parachain Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Build a blockchain Build a blockchain Build a local blockchain Simulate a network Add trusted nodes Authorize specific nodes Monitor node metrics Upgrade a running network Build application logic Build a parachain Reference Community Documentation Search documentation Quick start Learn Install Build Test Deploy Maintain Tutorials Build a blockchain Build a local blockchain Simulate a network Add trusted nodes Authorize specific nodes Monitor node metrics Upgrade a running network Build application logic Build a parachain Reference Community Home Tutorials Build a blockchain Build a blockchain The Build a blockchain tutorials illustrate the basics for working with Substrate-based blockchain nodes, including how to make nodes communicate with each other in a network of peers and how to collect metrics about node operations. In general, you should complete the tutorials in the order listed because they lay the foundation for trying later tutorials or performing more complex tasks. Later tutorials reinforce or expand on the basic topics you learn in the Get started tutorials. Build a local blockchain shows you how to set up and interact with a local node in your development environment. Simulate a network helps you simulate a two-node network using predefined accounts. Add trusted nodes demonstrates how to generate keys and distribute a chain specification to create a small network of trusted validator nodes. Authorize specific nodes illustrates how you can configure a network that has both authorized nodes and nodes with restricted access. Monitor node metrics highlights how you can take advantage of the node metrics Substrate exposes. Upgrade a running network illustrates forkless upgrades by modifying the runtime for a running Substrate node. Last edit: April 3, 2023 ‚Üê Previous Next ‚Üí Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Build application logic Build a blockchain Build application logic Add a pallet to the runtime Specify the origin for a call Use macros in a custom pallet Add offchain workers Publish custom pallets Collectibles workshop Build a parachain Reference Community Documentation Search documentation Quick start Learn Install Build Test Deploy Maintain Tutorials Build a blockchain Build application logic Add a pallet to the runtime Specify the origin for a call Use macros in a custom pallet Add offchain workers Publish custom pallets Collectibles workshop Build a parachain Reference Community Home Tutorials Build application logic Build application logic The Build application logic tutorials focus on how you can customize the runtime using pallets, including how to add simple and complex pallets to the runtime and how to use pallets in combination with smart contracts. You'll learn how to: Add a pallet to the runtime introduces the common steps for adding a simple predefined pallet to the node template runtime. Use macros in a custom pallet illustrates how to create a custom pallet using macros. Specify the origin for a call demonstrates how you can specify the account to use as the originator of a function call. Add offchain workers illustrates how to modify a pallet to include an offchain worker and configure the pallet and runtime to enable the offchain worker to submit transactions that update the on-chain state. Publish custom pallets illustrates how to publish custom pallets and crates so they are available to the community. If you want to experiment with smart contract development for your blockchain, you should use the preconfigured substrate-contracts-node instead of the standard node template. There are compatibility issues between the pallets that support smart contracts and the current node template. To resolve these compatibility issues would require extensive changes to the configuration of the standard node template, including using older versions of all crates and modifying multiple files. To dive directly into smart contract development, see the tutorials in Develop smart contracts . Last edit: April 3, 2023 ‚Üê Previous Next ‚Üí Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Build a parachain Build a blockchain Build application logic Build a parachain Prepare a local relay chain Connect a local parachain Acquire a testnet slot Open message passing channels Transfer assets with XCM Reference Community Documentation Search documentation Quick start Learn Install Build Test Deploy Maintain Tutorials Build a blockchain Build application logic Build a parachain Prepare a local relay chain Connect a local parachain Acquire a testnet slot Open message passing channels Transfer assets with XCM Reference Community Home Tutorials Build a parachain Build a parachain The Build a parachain tutorials delve into more advanced topics for moving beyond solo chain development, including how to connect your chain to other chains. Prepare a local relay chain helps you prepare a local relay chain to use in testing a local parachain. Connect a local parachain highlights the steps necessary to connect your local parachain to a relay chain. Acquire a testnet slot summarizes the steps for connecting a parachain to the Rococo test network. Open message passing channels demonstrates how to construct cross-consensus messages to open channels that allow parachains to communicate with each other. Transfer assets with XCM illustrates how to use cross-consensus messages to execute a remote transfer to a parachain through the relay chain. Last edit: April 3, 2023 ‚Üê Previous Next ‚Üí Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Reference Reference This section contains links to the Rust API documentation and technical reference materials for Substrate and FRAME tools and libraries. Rust API highlights entry points into the Rust API documentation that is generated directly from the Substrate source code. Glossary defines and explains concepts and terminology that are specific to blockchain technology or the Substrate ecosystem. Command-line tools provides reference information for the most commonly-used command-line interface (CLI) tools. Runtime APIs highlights the application programming interfaces that the runtime provides to support communication with outer node services. FRAME pallets lists all of the pre-built pallets that you can include in your runtime. FRAME macros describes the all of the FRAME macros you can use to build custom pallets. Account data structures describes account-related data structures and how account properties are used to manage the account lifecycle in the runtime logic. Type encoding (SCALE) describes the encoding scheme used in Substrate to minimize the data transferred over the network. How-to reference guides lists the How-to quick reference guides that provide abbreviated instructions for accomplishing specific tasks. The How-to guides assume that you are already familiar with Substrate and programming in Rust. XCM reference provides basic reference information for the cross-consensus message format, including descriptions of XCM instructions, registers, origins, and errors. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Rust API Rust api Rust API The Rust documentation is the single source of truth for the Substrate code base. However, if you aren't sure what you are looking for‚Äîfor example, a specific crate, trait, or data structure‚Äîit can be tricky to navigate or to know the best place to start. As discussed in Architecture and Rust libraries , Substrate libraries use a naming convention to indicate whether a library is part of the Substrate core client ( sc_* ), FRAME and the runtime ( frame_* and pallet_* ), or a Substrate primitive ( sp_* ). If you don't have a specific crate in mind, use the following links as entry points to explore the core Rust libraries. Substrate core client libraries Use the following links to explore Substrate libraries that are part of the Substrate core client ( sc_* ). sc_authority_discovery sc_block_builder sc_chain_spec sc_cli sc_client_api sc_client_db sc_consensus sc_network sc_rpc sc_service sc_state_db sc_transaction_pool FRAME libraries Use the following links to explore the core FRAME libraries that are used in the Substrate runtime ( frame_* and pallet_* . frame_benchmarking frame_executive frame_remote_externalities frame_support frame_system pallet_assets pallet_balances pallet_collective pallet_identity pallet_membership pallet_proxy Substrate primitive libraries Use the following links to explore Substrate primitive libraries ( sp_* ). sp_api sp_blockchain sp_core sp_io sp_runtime sp_state_machine sp_storage Other libraries Use the following links to explore other libraries. kitchensink_runtime node_primitives node_rpc node_template node_template_runtime Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Substrate core client libraries FRAME libraries Substrate primitive libraries Other libraries Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Glossary Glossary Glossary This glossary defines and explains concepts and terminology that are specific to blockchain technology or the Substrate ecosystem. adaptive quorum biasing (AQB) Provides a mechanism for adjusting the passing threshold for a referendum based on voter turnout. Adaptive quorum biasing allows for more flexible governance by removing the requirement to have an arbitrary quorum for voting purposes, which create undesirable governance mechanics. Adaptive quorum biasing is implemented in the Democracy pallet . The Democracy pallet provides the interfaces for on-chain bodies such as a collective or individual token holder‚Äîto call referenda with positive, negative, or neutral biases. With a positive turnout bias , the passing threshold decreases as more votes are cast, so that a higher turnout increases the likelihood of a referendum passing. With a negative turnout bias , the passing threshold increases as more votes are cast. Negative turnout bias is also sometimes called a "default carries" position because if there's an apathetic voting body, the referendum passes by default. A neutral turnout bias specifies a simple majority passing threshold. aggregation Used in the context of FRAME , aggregation or pallet aggregation is the process of combining analogous types from multiple runtime modules into a single type. Pallet aggregation allows each module's analogous types to be represented. The call containing the aggregated types is sometimes referred to as an outer call or a call to an outer object . Currently, there are six data types that can be aggregated: Call for published functions that can be called with a set of arguments. Error for messages that indicate why a function invocation ( Call ) failed. Event for pallet-emitted events that describe state changes. Log for extensible header items. Metadata for information that allows inspection of the above. Origin for the source of a function invocation ( Call ). approval voting Voting system where voters can vote for as many candidates as desired. The candidate with the highest overall number of votes wins. With approval voting, it is worth noting the following: Voting for all candidates is the same as voting for none. It is possible to vote against a single candidate by voting for all other candidates. Approval voting is used by the FRAME Elections Phragmen pallet as a governing Council on a number of Substrate-based chains. author Describes the node that is responsible for the creation of a block . Block authors are also referred to as block producers . In a proof-of-work blockchain, these nodes are called miners . authority The nodes that act as a collective to manage consensus on a blockchain network. In a proof-of-stake blockchain‚Äîfor example, a blockchain that uses the Staking pallet from FRAME ‚Äîauthorities are determined through a token-weighted nomination and voting system. The terms authorities and validators sometimes seem to refer the same thing. However, validators is a broader term that can include other aspects of chain maintenance such as parachain validation. In general, authorities are a (non-strict) subset of validators and many validators are authorities. authority round (Aura) Deterministic consensus protocol where block production is limited to a rotating list of authorities that take turns creating blocks. With authority round (Aura) consensus, the majority of online authorities are assumed to be honest. Learn more by reading the official wiki article for the Aura consensus algorithm. The Aura protocol is often used in combination with GRANDPA as a hybrid consensus protocol where Aura is used for block production and short-term probabilistic finality , with deterministic finality provided by GRANDPA . blind assignment of blockchain extension (BABE) A block authoring protocol similar to Aura . However, with the blind assignment of blockchain extension (BABE) protocol, authorities win slots based on a verifiable random function (VRF) as opposed to the round-robin selection method. The winning authority can select a chain and submit a new block for it. Learn more about BABE by referring to its official Web3 Foundation research document . block A block is a single element of a blockchain that contains an ordered set of instructions‚Äîoften in the form of transactions‚Äîthat might result in a state change. Each block is identified by a cryptographic digest ‚Äîa hash‚Äîand includes a pointer back to the hash of its parent block. Every block has a header and a body that contains the executed instructions that made it into the block within certain parameters. Blocks are most often visualized as a vertical stack with each block referencing its parent block until there are conflicts that must be resolved using a fork-choice rule and an optional finality mechanism. block height Because blocks are often visualized as a vertical stack, block height is used to describe the distance a specific block is away from the first block. The block height for any particular block in a blockchain indicates how many blocks preceded that block. Similarly, the terms top or tip are used to describe the most recently-added block in a chain. block number Similar to block height, a block number is used to describe the distance a specific block is away from the first block. The block number for any particular block in a blockchain indicates how many blocks preceded that block. blockchain Describes a distributed network of computers that uses cryptography to allow a group of participants to trustlessly come to consensus on the state of a system as it evolves over time. The computers that compose the blockchain network are called nodes . byzantine fault tolerance (BFT) Defines the ability of a distributed computer network to remain operational if a certain proportion of its nodes or authorities are defective or behaving maliciously. Typically, a distributed network is considered byzantine fault tolerant if it can remain functional with up to one-third of nodes assumed to defective, offline, actively malicious, and acting as part of a coordinated attack. byzantine failure The loss of a network service due to node failures that exceed the proportion of nodes required to reach consensus. practical byzantine fault tolerance (pBFT) An early approach to byzantine fault tolerance. pBFT systems tolerate byzantine behavior from up to one-third of participants. The communication overhead for such systems is O(n¬≤) , where n is the number of nodes (participants) in the system. call In a general context, a call describes the act of invoking a function to be executed. In the context of pallets that contain functions to be dispatched to the runtime, Call is an enumeration data type that describes the functions that can be dispatched with one variant per pallet. The object that a Call represents is a dispatch data structure or a dispatchable. collator An author of a parachain network. They are not authorities in themselves, as they require a relay chain to coordinate consensus . More details are found on the Polkadot Wiki on collators . consensus In the context of a blockchain , consensus is the process nodes use to agree on the canonical fork of a chain. Consensus is comprised of authorship , finality , and fork-choice rule . In the Substrate ecosystem, these three components are separated from one another, and the term consensus often refers specifically to authorship. In the context of a Substrate node , the term consensus engine describes the node subsystem that is responsible for consensus tasks. See also hybrid consensus . consensus algorithm An algorithm that ensures that a set of actors ‚Äîwho don't necessarily trust each other‚Äîcan reach agreement about state as the result of some computation. Most consensus algorithms assume that up to one-third of the actors or nodes can be byzantine fault tolerant . Consensus algorithms are generately concerned with ensuring two properties: safety indicating that all honest nodes eventually agreed on the state of the chain. liveness indicating the ability for the chain to keep making progress. For detailed information about the consensus strategies of the Polkadot network , see the Polkadot Consensus blog series. See also hybrid consensus . cryptographic primitives A general term used to describe fundamental cryptographic concepts such as signature schemes and hashing algorithms. Cryptographic primitives are essential to many aspects of the Substrate ecosystem. For example: Hashing algorithms produce blocks of hashed data and each block uses the hash generated by the hashing algorithm to reference its parent block. Hashing is used to encode state as a trie data structure to facilitate efficient verification. Digital signature schemes are used to secure different consensus models such as authorities . Cryptographic schemes identify and authenticate the accounts used to perform transactions in the Substrate runtime. council Most often used to refer to an instance of the Collective pallet on Substrate-based networks such as Kusama or Polkadot if the Collective pallet is part of the FRAME -based runtime for the network. A council primarily serves to optimize and balance the more inclusive referendum system. database backend The means by which the state of a blockchain network is persisted between invocations of the blockchain node application. For information about how the database backend is implemented and used by Substrate-based chains, see Runtime storage . dev phrase A mnemonic phrase that is intentionally made public. All of the well-known development accounts‚ÄîAlice, Bob, Charlie, Dave, Eve, and Ferdie‚Äîare generated from the same secret phrase. The secret phrase is: bottom drive obey lake curtain smoke basket hold race lonely fit walk Many tools in the Substrate ecosystem, such as subkey , allow you to implicitly specify an account using a derivation path such as //Alice . digest An extensible field of the block header that encodes information needed by several actors in a blockchain network including: Light clients for chain synchronization. Consensus engines for block verification. The runtime itself in the case of pre-runtime digests. dispatch The execution of a function with a predefined set of arguments. In the context of runtime development with FRAME , a dispatch takes pure data‚Äîthe Call type‚Äîand uses that data to execute a published function in a runtime module ( pallet ) with predefined arguments. The published functions take one additional parameter, known as origin , that allows the function to securely determine the provenance of its execution. equivocating A type of erroneous or malicious behavior that involves backing multiple mutually-exclusive options within the consensus mechanism. ethash A function used by some proof-of-work consensus systems, such as the Ethereum blockchain. It was developed by a team led by Tim Hughes . events A means of recording, for the benefit of the offchain world, that some particular state transition happened. In the context of FRAME , events are a composable data types that each pallet can individually define. Events in FRAME are implemented as a set of transient storage items that are inspected immediately after a block has executed and reset during block-initialization. executor A means of executing a function call in a given runtime with a set of dependencies. There are two orchestration engines in Substrate, WebAssembly and native . The native executor uses a natively compiled runtime embedded in the node to execute calls. This is a performance optimization that up-to-date nodes can take advantage of. The WebAssembly executor uses a Wasm binary and a Wasm interpreter to execute calls. The binary is guaranteed to be up-to-date regardless of the version of the blockchain node because it is persisted in the state of the Substrate-based chain. extrinsic Data that is external to the blockchain and included in a block . Typical Substrate chains have extrinsics which contain a Call value. In general, there are two types of extrinsics: signed or unsigned transactions . inherent data that is inserted by a block author . existential deposit The minimum balance an account is allowed to have in the Balances pallet . Accounts cannot be created with a balance less than the existential deposit amount. If an account balance drops below this amount, the Balances pallet uses a FRAME System API to drop its references to that account. If all of the references to an account are dropped, the account can be reaped . finality The part of consensus that makes the ongoing progress of the blockchain irreversible. After a block is finalized, all of the state changes it encapsulates are irreversible without a hard fork. The consensus algorithm must guarantee that finalized blocks never need reverting. However, different consensus algorithms can define different finalization methods. In a consensus protocol that uses deterministic finality , each block is guaranteed to be the canonical block for that chain when the block is included. Deterministic finality is desirable in situations where the full chain is not available, such as in the case of light clients . GRANDPA is the deterministic finality protocol that is used by the Polkadot Network . In a consensus protocol that uses probabilistic finality , finality is expressed in terms of a probability, denoted by p , that a proposed block, denoted by B , will remain in the canonical chain. As more blocks are produced on top of B , p approaches 1. In a consensus protocol that uses instant finality , finality is guaranteed immediately upon block production. This type of non-probabilistic consensus tends to use practical byzantine fault tolerance (pBFT) and have expensive communication requirements. fork Indicates that there are divergent paths a blockchain might take. If two or more blocks have the same parent but different state, the blockchain cannot continue to progress until the differences are resolved. An unresolved fork would split the blockchain into two separate chains. By resolving divergent forks, you can ensure that only one canonical chain exists. Flaming Fir A Substrate-based blockchain test network that exists for developing and testing the Substrate blockchain development framework. For more information about accessing Substrate networks and flaming fir, see the Polkadot wiki . FRAME An acronym for the Framework for Runtime Aggregation of Modularized Entities that enables developers to create blockchain runtime environments from a modular set of components called pallets . Runtime developers interact with FRAME using macros such as the following: #[pallet::event] #[pallet::error] #[pallet::storage] #[frame_support::pallet] The macros make it easy to define custom pallets and compose pallets to create a working runtime using the construct_runtime! macro to deploy a Substrate-based blockchain. The convention used in the Substrate codebase is to preface core FRAME modules with frame_ and the optional pallets with pallet_* . For example, the preceding macros are all defined in the frame_support module and all FRAME-based runtimes must include the frame_system module. After the frame_support::construct_runtime macro has been used to create a runtime that includes the frame_system module, optional pallets such as the Balances pallet can be used to extend the core capabilities of the runtime. full node A node that is able to synchronize a blockchain in a secure manner through execution and verification of all logic. Full nodes stand in contrast to light clients . genesis configuration A mechanism for specifying the initial state of a blockchain . By convention, this initial state or first block is commonly referred to as the genesis state or genesis block. The genesis configuration for Substrate-based chains is accomplished by way of a chain specification file. The chain specification file makes it easy to use a single Substrate codebase as the foundation for multiple independently-configured chains. GRANDPA A deterministic finality mechanism for blockchains that is implemented in the Rust programming language. The formal specification is maintained by the Web3 Foundation . header The structure that aggregates the information used to summarize a block . A header consists primarily of cryptographic information that is used by light clients to get a minimally-secure but very efficient synchronization of the chain. hybrid consensus A blockchain consensus protocol that consists of independent or loosely-coupled mechanisms for block production and finality . Hybrid consensus allows the chain to grow as fast as probabilistic consensus protocols, such as Aura , while maintaining the same level of security as deterministic finality consensus protocols, such as GRANDPA . In general, block production algorithms tend to be faster than finality mechanisms. Making block production separate from block finalization gives Substrate developers greater control of their chain's performance. inherent transactions Inherent transactions‚Äîsometimes referred to as inherents‚Äîare a special type of unsigned transaction. This type of transaction enables a block authoring node to insert information that doesn't require validation directly to a block. Only the block authoring node that calls the inherent transaction function can insert data into its block. In general, validators assume the data inserted using an inherent transaction is valid and reasonable even if it can't be deterministically verified. JSON-RPC A stateless, lightweight remote procedure call protocol that is encoded in JSON. JSON-RPC provides a standard way to call functions on a remote system by using JavaScript Object Notation. For Substrate, this protocol is implemented thrinough the Parity JSON-RPC crate. keystore A subsystem in Substrate for managing keys for the purpose of producing new blocks. Kusama Kusama is a Substrate-based blockchain that implements a design similar to the Polkadot network . Kusama is a canary network and is referred to as Polkadot's "wild cousin" . As a canary network, Kusama is expected to be more stable than a test network like Westend , but not as stable as a production network like Polkadot . As a canary network, Kusama is controlled by its network participants and is intended to be stable enough to encourage meaningful experimentation. libp2p A peer-to-peer networking stack that allows use of many transport mechanisms, including WebSockets (usable in a web browser). Substrate uses the Rust implementation of the libp2p networking stack. light client A type of blockchain node that does not store the chain state or produce blocks. A light client is capable of verifying cryptographic primitives and exposes a remote procedure call (RPC) server that allows blockchain users to interact with the blockchain network. macro A programming language feature that enables developers to write a sequence of instructions that can be named and executed together. The FRAME development environment provides several macros for Rust that you can use to compose a runtime . metadata Data that provides information about one or more aspects of a system. The metadata that exposes information about a Substrate blockchain enables you to interact with that system. node A running instance of a blockchain client. Each node is part of the peer-to-peer network that allows blockchain participants to interact with one another. Substrate nodes can fill a number of roles in a blockchain network. For example, the nodes that produce blocks fulfill the validator role for the blockchain. Nodes that run light-clients facilitate scalable interactions in resource-constrained environments like user interfaces or embedded devices. nominated proof-of-stake (NPoS) A method for determining validators or authorities based on a willingness to commit their stake to the proper functioning of one or more block producing nodes. oracle In a blockchain network, an oracle is a mechanism for connecting the blockchain to a non-blockchain data source. Oracles enable the blockchain to access and act upon information from existing data sources and incorporate data from non-blockchain systems and services. origin A FRAME primitive that identifies the source of a dispatched function call into the runtime . The FRAME system module defines three built-in origins . As a pallet developer, you can also define custom origins, such as those defined by the Collective pallet . pallet A module that can be used to extend the capabilities of a FRAME -based runtime . Pallets bundle domain-specific logic with runtime primitives like events , and storage items . parachain A parachain is a blockchain that derives shared infrastructure and security from a relay chain . You can learn more about parachains on the Polkadot Wiki . Polkadot network The Polkadot network is a blockchain that serves as the central hub of a heterogeneous blockchain network. It serves the role of the relay chain and supports other chains‚Äîthe parachains ‚Äîby providing shared infrastructure and security. proof-of-finality Data that can be used to prove that a particular block is finalized. proof-of-work A consensus mechanism that deters attacks by requiring work on the part of network participants. For example, some proof-of-work systems require participants to use the Ethash function to calculate a hash as a proof of completed work. relay chain The central hub in a heterogenous network of multiple blockchains. Relay chains are blockchains that provide shared infrastructure and security to the other blockchains‚Äîthe parachains ‚Äîin the network. In addition to providing consensus capabilities, relay chains also allow parachains to communicate and exchange digital assets without needing to trust one another. remote procedure call (RPC) A mechanism for interacting with a computer program. Remote procedure calls enable developers to query the remote computer programs or invoke program logic with parameters they supply. Substrate nodes expose an RPC server on HTTP and WebSocket endpoints. rhododendron An instant finality , byzantine fault tolerant (BFT) consensus algorithm. One of a number of adaptions of pBFT for blockchains. Refer to its implementation on GitHub . rococo A parachain test network for the Polkadot network. The Rococco network is a Substrate-based blockchain that is an evolving testbed for the capabilities of heterogeneous blockchain networks. runtime The block execution logic of a blockchain. The runtime provides the state transition function for a node. In Substrate, the runtime is stored as a WebAssembly binary in the chain state . slot A fixed, equal interval of time used by consensus engines such as Aura and BABE . In each slot, a subset of authorities is permitted‚Äîor obliged‚Äîto author a block . sovereign account The unique account identifier for each chain in the relay chain ecosystem. The sovereign account for each chain is a root-level that can only be accessed using the Sudo pallet or through governance. The account identifier is calculated by concatenating the Blake2 hash of a specific text string and the registered parachain identifier. For the relay chain, the parachain account identifier is calculated as the concatenation of (blake2(para+ParachainID) with the hash truncated to the correct length. For example, the account identifier for the parachain with the parachain identifier of 1012 on the relay chain is: String to hex para: 0x70617261 Encoded parachain identifier 1012: f4030000 0x70617261f4030000000000000000000000000000000000000000000000000000 ccount address: 5Ec4AhPc9b6e965pNRSsn8tjTzuKaKambivxcL7Gz9Gne9YB For other parachains, the parachain account identifier is calculated as the concatenation of (blake2(sibl+ParachainID) with the hash truncated to the correct length. For example, the account identifier for the parachain with the parachain identifier of 1012 on the relay chain is: String to hex sibl: 0x7369626c Encoded parachain identifier 1012: f4030000 0x7369626cf4030000000000000000000000000000000000000000000000000000 Account address: 5Eg2fntREKHYGgoxvRPxtnEYiUadHjdsfNaPsHdmrsJMVugs The sovereign account is most often used to sign XCM messages that are sent to either the relay chain or other chains in the ecosystem. SS58 address format The SS58 address format is a public key address based on the Bitcoin Base-58-check encoding. Each Substrate SS58 address uses a base-58 encoded value to identify a specific account on a specific Substrate-based chain. These are represented by a base-58 encoded value to identify a specific account on a specific Substrate chain. The canonical ss58-registry provide additional details about the address format used by different Substrate-based chains, including the network prefix and website used for different networks. stake-weighted voting A democratic voting system that uses a one-vote-per-token method for tallying votes rather than a one-vote-per-head method. state Cryptographically-secure data that persists between blocks and can be used to create new blocks as part of the state transition function. In Substrate-based blockchains, state is stored in a trie data structure that supports the efficient creation of incremental digests. This trie is exposed to the runtime as a simple key/value map where both keys and values can be arbitrary byte arrays. state transition function (STF) The logic of a blockchain that determines how the state changes when a block is processed. In Substrate, the state transition function is effectively equivalent to the runtime . storage item FRAME primitives that provide type-safe data persistence capabilities to the runtime . Learn more about storage items in this article about runtime storage . Substrate A flexible framework for building modular, efficient, and upgradeable blockchains . Substrate is written in the Rust programming language and is maintained by Parity Technologies . transaction A type of extrinsic that includes a signature that can be used to verify the account authorizing it inherently or via signed extensions . transaction era A definable period‚Äîexpressed as a range of block numbers‚Äîduring which a transaction can be included in a block. Transaction eras are used to protect against transaction replay attacks in the event that an account is reaped and its replay-protecting nonce is reset to zero. transaction pool A collection of transactions that are not yet included in blocks but have been determined to be valid. A tagged transaction pool is a transaction pool implementation that allows the runtime to specify whether a given transaction is valid, how it should be prioritized, and how it relates to other transactions in the pool in terms of dependency and mutual-exclusivity. The tagged transaction pool implementation is designed to be extensible and general enough to express both unspent transaction output (UTXO) and account-based transaction models. trie (Patricia Merkle Tree) A data structure that is used to represent sets of key-value pairs. The Patricia Merkle trie data structure enables the items in the data set to be stored and retrieved using a cryptographic hash. Because incremental changes to the data set result in a new hash, retrieving data is efficient even if the data set is very large. With this data structure, you can also prove whether the data set includes any particular key-value pair without the access to the entire data set. validator A semi-trusted‚Äîor untrusted but well-incentivized‚Äîactor that helps maintain a blockchain network. In Substrate, validators broadly correspond to the authorities running the consensus system. In Polkadot , validators also manage other duties such as guaranteeing data availability and validating parachain candidate blocks . WebAssembly (Wasm) An execution architecture that allows for the efficient, platform-neutral expression of deterministic, machine-executable logic. WebAssembly can be compiled from many languages, including the Rust programming language. Substrate-based chains use a WebAssembly binary to provide portable runtimes that can be included as part of the chain's state . weight A convention used in Substrate-based blockchains to measure and manage the time it takes to validate a block. Substrate defines one unit of weight as one picosecond of execution time on reference hardware. The maximum block weight should be equivalent to one-third of the target block time with an allocation of: One third for block construction One third for network propagation One third for import and verification By defining weights, you can make trade-off decisions between the number of transactions per second and the hardware required to maintain the target block time as appropriate for your use case. Because weights are defined in the runtime, you can tune them using runtime updates to keep up with hardware and software improvements. Westend Westend is a Parity-maintained, Substrate-based blockchain that serves as a test network for the Polkadot network . Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE adaptive quorum biasing (AQB) aggregation approval voting author authority authority round (Aura) blind assignment of blockchain extension (BABE) block block height block number blockchain byzantine fault tolerance (BFT) call collator consensus consensus algorithm cryptographic primitives council database backend dev phrase digest dispatch equivocating ethash events executor extrinsic existential deposit finality fork Flaming Fir FRAME full node genesis configuration GRANDPA header hybrid consensus inherent transactions JSON-RPC keystore Kusama libp2p light client macro metadata node nominated proof-of-stake (NPoS) oracle origin pallet parachain Polkadot network proof-of-finality proof-of-work relay chain remote procedure call (RPC) rhododendron rococo runtime slot sovereign account SS58 address format stake-weighted voting state state transition function (STF) storage item Substrate transaction transaction era transaction pool trie (Patricia Merkle Tree) validator WebAssembly (Wasm) weight Westend Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Command-line tools Reference Rust API Glossary Command-line tools archive memory-profiler node-template polkadot-apps sidecar srtool subkey subxt try-runtime tx-wrapper Runtime APIs FRAME pallets FRAME macros Account data structures Type encoding (SCALE) Address formats Transaction formats XCM reference How-to reference guides Community Documentation Search documentation Quick start Learn Install Build Test Deploy Maintain Tutorials Reference Rust API Glossary Command-line tools archive memory-profiler node-template polkadot-apps sidecar srtool subkey subxt try-runtime tx-wrapper Runtime APIs FRAME pallets FRAME macros Account data structures Type encoding (SCALE) Address formats Transaction formats XCM reference Command line tools Command-line tools This section provides reference information for Substrate command-line tools. Command entry point Description archive Index and store all blocks, state, and transaction data for a Substrate-based chain in a relational SQL database. memory-profiler Collect information about memory allocation and the behavior of blockchain applications over time. node-template Start and manage a Substrate node preconfigured with a subset of commonly-used FRAME pallets. polkadot-launch Launch a local Polkadot test network. polkadot-apps Interact with Polkadot or a Substrate node using a browser. sidecar Use a REST service to interact with blockchain nodes built using FRAME. srtool Build WASM runtime in a deterministic way, allowing continuous integration pipelines and users to produce a strictly identical WASM runtime. subkey Generate and manage public and private key pairs for accounts. subxt Submit extrinsics to a Substrate node using RPC. try-runtime Query a snapshot of runtime storage to retrieve state. tx-wrapper Publish chain specific offline transaction generation libraries. Last edit: June 25, 2022 ‚Üê Previous Next ‚Üí Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Runtime APIs Runtime apis Runtime APIs As discussed in Architecture , Substrate nodes consist of outer node services and a runtime and this separation of responsibilities is an important concept for designing Substrate-based chains and building upgradeable logic. However, the outer node services and the runtime must communicate with each other to complete many critical operations, including reading and writing data and performing state transitions. The outer node services communicate with the runtime by calling runtime application programming interfaces to perform specific tasks. By default, the Substrate runtime provides the following traits for outer node services to call: AccountNonceApi AuraApi Benchmark BlockBuilder GrandpaApi NominationPoolsApi OffchainWorkerApi SessionKeys TaggedTransactionQueue TransactionPaymentApi AccountNonceApi Use the AccountNonceApi to get the nonce for a specified account identifier. The nonce for each account is incremented each time that account is used to complete a transaction. Therefore, the nonce is also sometimes referred to as a transaction index. This API provides the following methods: account_nonce to get the current account nonce for a specified AccountId. account_nonce_with_context to get the current account nonce for a specified AccountId and execution context. AuraApi Use the AuraApi to manage block authoring with the slot-based consensus that uses a round-robin rotation of authorities. Although most consensus-related tasks are handled by outer node services, the runtime must provide this API for consensus-related tasks that are part of the state transition logic. This API provides the following methods for authority-based round-robin scheduling ( Aura ): slot_duration to get the slot duration for Aura consensus. slot_duration_with_context to get the slot duration for Aura consensus within a specified execution context. authorities to get the authorities set for Aura consensus. authorities_with_context to get the authorities set for Aura consensus within a specified execution context. Benchmark Use the Benchmark API to provide the information required for benchmarking function execution in a FRAME runtime. This API provides the following methods: benchmark_metadata to get the benchmark metadata available for this runtime. benchmark_metadata_with_context to get the benchmark metadata available for this runtime within a specified execution context. dispatch_benchmark to dispatch the specified benchmark. dispatch_benchmark_with_context to dispatch the specified benchmark within a specified execution context. BlockBuilder Use the BlockBuilder API to provide the functionality required for building and finalizing a block. The runtime is responsible for checking transaction validity and executing the transactions to construct blocks. For the outer node, transactions are an opaque vector array (Vec ). This API provides the following methods: apply_extrinsic to include the specified extrinsic in the current block. The method also returns a result that indicates whether the transaction was included in the block or not. apply_extrinsic_with_context to include the specified extrinsic in the current block and specified execution context. The method also returns a result that indicates whether the transaction was included in the block or not. finalize_block to finish construction of the current block. finalize_block_with_context to finish construction of the current block within the specified execution context. inherent_extrinsics to include inherent extrinsic transactions in the current block. Inherent transaction types vary from chain to chain. inherent_extrinsics_with_context to include inherent extrinsic transactions in the current block and specified execution context. Inherent transaction types vary from chain to chain. check_inherents to check that the inherent transactions are valid. check_inherents_with_context to check that the inherent transactions are valid within the specified execution contex. GrandpaApi Use the GrandpaApi to integrate authority-set changes from the GRANDPA finalization protocol into the runtime. The GRANDPA finalization protocol signals changes to the authority sets by specifying a delay of some number of blocks. The changes ar then automatically applied in the runtime after the specified number of blocks have been finalized. This API provides the following methods: grandpa_authorities to get the current authorities and weights for GRANDPA finalization. grandpa_authorities_with_context to get the current authorities and weights for GRANDPA finalization in the specified execution context. current_set_id to get the current GRANDPA authority set identifier. current_set_id_with_context to get the current GRANDPA authority set identifier in the specified execution context. The GrandpaApi also provides methods for submitting transactions to report evidence of misbehavior and related proof of key ownership. For information about these methods, see GrandpaApi . NominationPoolsApi Use the NominationPoolsApi to get information about about nomination pools and nomination pool members. This API provides the following methods: pending_rewards to get the pending rewards for the nomination pool member with the specified AccountId. pending_rewards_with_context to get the pending rewards for the nomination pool member with the specified AccountId within the specified execution context. OffchainWorkerApi Use the OffchainWorkerApi to start offchain worker operations . This API provides the following methods: offchain_worker to start the off-chain task for a specified block header. offchain_worker_with_context to start the off-chain task for a specified block header and execution context. SessionKeys Use the SessionKeys API to generate and decode session keys ( https://paritytech.github.io/substrate/master/sp_session/trait.SessionKeys.) This API provides the following methods: generate_session_keys to generate a set of session keys. If you generate the keys using a specified seed, the seed must to be a valid utf8 string. You should store the generated keys in the keystore exposed by the runtime externalities. The method returns the public keys as concatenated SCALE-encoded values. generate_session_keys_with_context to generate a set of session keys within the specified execution context. If you generate the keys using a specified seed, the seed must to be a valid utf8 string. You should store the generated keys in the keystore exposed by the runtime externalities. The method returns the public keys as concatenated SCALE-encoded values. decode_session_keys to decode the specified public session keys. The method returns the list of raw public keys and the key type. decode_session_keys_with_context to decode the specified public session keys within the specified execution context. The method returns the list of raw public keys and the key type. TaggedTransactionQueue Use the TaggedTransactionQueue API to validate transactions in the transaction queue. This API provides the following methods: validate_transaction to verify the specified transaction is a valid transaction given the state specified by the block_hash parameter. validate_transaction_with_context to verify the specified transaction is a valid transaction given the state specified by the block_hash parameter within the specified execution context. TransactionPaymentApi Use the TransactionPaymentApi to query the runtime for information about transactions and transaction fees. This API provides the following methods: query_info to return information about a specified transaction dispatched to the runtime. query_info_with_context to return information about a specified transaction dispatched to the runtime within the specified execution context. query_fee_details to return information about the transaction fees for a specified transaction. query_fee_details_with_context to return information about the transaction fees for a specified transaction within the specified execution context. Where to go next Runtime development FRAME macros impl runtime apis Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE AccountNonceApi AuraApi Benchmark BlockBuilder GrandpaApi NominationPoolsApi OffchainWorkerApi SessionKeys TaggedTransactionQueue TransactionPaymentApi Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
FRAME pallets Frame pallets FRAME pallets The FRAME development environment provides modules‚Äîcalled pallets‚Äîand support libraries that you can use, modify, and extend to build the runtime logic to suit the needs of your blockchain. This section provides an overview of the predefined pallets and links to the Rust API reference documentation, where you can find details about each pallet's interfaces. System pallets The FRAME system pallets are integral to the Substrate runtime and provide core functionality that all other pallets depend on. System pallet name What it's for frame_benchmarking Contains common runtime patterns for benchmarking and testing purposes. frame_executive Orchestrates incoming function calls by sending them to the appropriate pallets in the runtime. frame_support Provides Rust macros, types, traits, and modules that generate boilerplate code for the pallet structure when compiled. frame_system Defines low-level types for Substrate primitives, storage items, and core functions for the blockchain. All other pallets depend on the frame_system crate. Functional pallets The Substrate development framework includes many functional pallets that provide features you might find generally useful as composable components of your blockchain. These functional pallets are prebuilt and freely available to enable the community to share, reuse, and improve the interfaces to address common use cases. The following table highlights some of the most commonly used pallets available. However, the number of pallets that provide specialized functionality and how they provide that functionality changes frequently. You should check the Rust documentation for the most up-to-date listing of pallets and for details about using them. Prebuilt pallet name What it's for pallet_alliance Initializes a collective for community members to join in an effort to establish rules against bad behavior and to provide recognition and influence for teams that contribute to the ecosystem. pallet_assets Provides simple and secure methods for dealing with fungible assets. pallet_atomic_swap Enables sending funds from an origin to a target. A proof is used to allow the target to claim the swap. If the swap is not claimed within a specified duration of time, the sender may cancel it. pallet_aura Extends the authority round (Aura) consensus model by managing offline reporting. pallet_authority_discovery Retrieves the current set of authorities, learns its own authority ID, and signs and verifies messages to and from other authorities. pallet_authorship Tracks the current author of the block and recent uncles. pallet_babe Extends BABE consensus by collecting on-chain randomness from VRF outputs and managing epoch transitions. pallet_balances Provides functionality for handling accounts and balances. pallet_bounties Manages rewards for performing specified work or for achieving a specified set of objectives. pallet_collective Allows a set of account IDs to make their collective feelings known through dispatched calls from specialized origins. pallet_contracts Provides functionality for the runtime to deploy and execute WebAssembly smart contracts. pallet_contracts_primitives Provides common definitions that are used by the pallet_contracts crate for smart contracts. pallet_democracy Provides a democratic system that handles administration of general stakeholder voting. pallet_election_provider_multi_phase Enables an election provider to conduct an election consisting of signed and unsigned phases. pallet_elections_phragmen Provides an election module based on sequential Phragm√©n . pallet_example_basic Demonstrates concepts, APIs, and structures that are applicable for most pallets. pallet_example_offchain_worker Demonstrates concepts, APIs, and structures that are applicable for most offchain workers. pallet_grandpa Extends the GRANDPA consensus by managing the GRANDPA authority set ready for the native code. pallet_identity Enables a federated naming system that allows multiple registrars to be added from a specified origin. Registrars can set a fee to provide identity-verification service. pallet_im_online Allows validators to gossip a heartbeat transaction with each new session to signal that the node is online. pallet_indices Allocates indices for newly created accounts. An index is a short form of an address. pallet_lottery Configures a lottery that enables network participants to purchase tickets. pallet_membership Allows control of membership of a set of AccountId s, useful for managing the membership of a collective. pallet_multisig Enables multi-signature dispatches. pallet_nicks Demonstrates simplified account naming on-chain. It makes no effort to create a name hierarchy, be a DNS replacement, or provide reverse lookups. pallet_offences Tracks reported offences. pallet_proxy Allows accounts to give permission to other accounts to dispatch types of calls from their signed origin. pallet_randomness_collective_flip Provides a random function that can be used in tests and generates low-influence random values based on the block hashes from the previous 81 blocks. This pallet is not intended for use in production. pallet_recovery Provides a social recovery tool for users to gain access to their accounts if their private key or other authentication mechanism is lost. This pallet enables an account owner to identify trusted parties who can act on the owner's behalf to recover access to an account. pallet_scheduler Exposes capabilities for scheduling dispatches to occur at a specified block number or at a specified period. These scheduled dispatches can be named or anonymous and can be canceled. pallet_scored_pool Maintains a scored membership pool where the highest scoring entities are made members. pallet_session Allows validators to manage their session keys, provides a function for changing the session length, and handles session rotation. pallet_society Provides economic incentives for users to participate and maintain a membership society. pallet_staking Manages funds that have been staked by network maintainers. pallet_sudo Allows for a single account‚Äîcalled the sudo key‚Äîto execute dispatchable functions that require a Root origin or designate a new account to replace them as the sudo key. pallet_timestamp Provides functionality to get and set the on-chain time. pallet_transaction_payment Provides the basic logic to compute pre-dispatch transaction fees. pallet_treasury Provides a reserve of funds that can be managed by stakeholders in the system and a structure for making spending proposals from this reserve. pallet_uniques Provides methods for managing non-fungible collections and items. pallet_utility Provides a stateless helper module for managing dispatches. pallet_vesting Places a linear curve on an account's locked balance. This module ensures that there is a lock in place to prevent the balance to drop below the unvested amount for any reason other than transaction fee payment. Parachain pallets In addition to the functional pallets that are generally useful for any blockchain, there are prebuilt pallets that provide features specifically for blockchains that are intended to connect to a relay chain. The following pallets provide features for parachain development. Prebuilt pallet name What it's for cumulus-pallet-aura-ext Provides AURA consensus for parachains. pallet-collator-selection Manages collators in a parachain. cumulus-pallet-dmp-queue Implements a message queue for receiving messages from the relay chain. cumulus-pallet-parachain-system Provides basic functionality for cumulus-based parachains. cumulus-pallet-solo-to-para Enables migration from a solo chain to a parachain. cumulus-pallet-xcm Adds support for cross-chain message passing (XCMP) to a parachain. cumulus-pallet-xcmp-queue Enables the XCMP transport layer to handle both incoming and outgoing message sending and dispatch, queuing, signalling, and backpressure. Additional information For detailed information about any pallet, refer to the Rust-generated API documentation or the source code for the individual pallet. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE System pallets Functional pallets Parachain pallets Additional information Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
FRAME macros Frame macros FRAME macros Substrate uses customized Rust macros to generate code and aggregate the logic from the pallets you implement for a runtime. These runtime macros allow you to focus on your runtime logic rather than spending time on encoding and decoding on-chain variables or duplicating the code required for basic blockchain development . This section provides an overview of the types of macros available in Rust and highlights how the specific FRAME macros are used in runtime development. Macro basics In computer programming, macros are lines of code that encapsulate a preset sequence of instructions to execute. As code that writes code, macros enable you to abstract repetitive operations and simplify the code you need to write. With macros, you can declare complex data structures implicitly. In Rust, macros can be declarative macros or procedural macros . Declarative macros enable you to declare a pattern and compare the result of an expression to the pattern, then execute any additional lines of code based on whether the pattern was matched. Declarative macros are widely-used in Rust programming. Procedural macros are similar to functions. Unlike the pattern-matching done in declarative macros, procedural macros take code as input, perform some set of instructions on the input, and produce code as output. There are three types of procedural macros: Custom derive macros enable you to define and reuse the implementation of a trait for a given type. The derive macro is particularly useful for defining the implementation for custom runtime types that must satisfy specific traits. Attribute-like macros enable you to create new attributes to generate code. Function-like macros enable you to define macros that operate like function calls to generate code. Because macros are expanded before the compiler interprets the lines of code they contain, they can define complex data structures and operations. FRAME macros take advantage of the different types of macros to provide shortcut abstractions to what is often complex blocks of code. However, the abstraction that macros provide can make the runtime code somewhat difficult to follow. To learn more about the FRAME macros used in the Substrate runtime, you can install and use cargo-expand . After you install cargo-expand, you can use the cargo expand command to display the results of the code contained in the macros the runtime uses. FRAME support and system macros Substrate primitives and FRAME both rely on a collection of different macros. This section provides an overview of the macros provided in the FRAME support and FRAME system libraries. In most cases, these macros provide the framework that other pallets depend on and you should be familiar with how and where they are used in the runtime logic. After the overview, this section describes the specific macros that you are most likely to use as a runtime developer. FRAME support macros The frame_support crate provides many of the most important declarative, derive, attribute-like, and function-like macros used in the runtime. A few of the important macros that you should be familiar with from the frame_support crate include the following: construct_runtime used to construct runtime from the list of pallets you have implemented. match_types used to create a type that implements the Contains trait with syntax similar to matches! . parameter_types used to create new implementations of the Get trait. For additional information about the macros in the frame_support crate, see the Rust documentation for Macros , Derive macros , and Attribute macros . FRAME system macros The frame_system crate uses macros to define primitives that provide access to core data types and shared utilities. These primitives and associated macros form the foundation for many node operations both in the outer node and in the runtime and act as the base layer for other pallets to interact with the Substrate framework. A few of the important primitives and macros that you should be familiar with from the frame_system crate include the following: sp_core map used to initialize a key-value collection from array. RuntimeDebug used to debug the runtime. For more information about sp_core function-like macros, see Macros . sp_runtime bounded_btree_map used to build a bounded btree-map from given literals. bounded_vec used to build a bounded vec from given literals. impl_opaque_keys used to implement OpaqueKeys for a described data structure. parameter_types used to create new implementations of the Get trait. For more information about sp_runtime function-like macros, see Macros . For information about sp_runtime derive macros, see Derive macros . sp_api decl_runtime_apis used to declare specified traits as runtime APIs. impl_runtime_apis used to tag specified trait implementations as runtime APIs. For more information about sp_api function-like macros, see Macros . sp_std if_std used to indicate code that should only be run when the std feature set is enabled. map used to initialize a key-value collection from array. vec used to create a vector containing the arguments. For more information about sp_std function-like macros, see Macros . sp_version create_apis_vec used to create a vector of API declarations. create_runtime_str used to create a RuntimeString constant. runtime_version used as an attribute that accepts the version declaration of a runtime and generates a custom WebAssembly section with the equivalent contents. You'll see these many of these crates listed as dependencies in the runtime and node Cargo.toml file for the node template. Macros for composing pallets As discussed in Building custom pallets , most FRAME pallets are composed using a common set of sections. Macros make building each of those sections more modular and extensible. This section describes the macros available and how to use them to build your custom runtime. #[pallet] The #[pallet] macro is required to declare a pallet. This attribute macro is an attribute of the pallet module ( mod pallet ). Within the pallet module, the #[pallet] macro serves as an entry point for additional #[pallet::*] macros that describe the attributes used to identify the specific items the pallet requires. For example, a pallet typically includes a set of types, functions, and trait implementations that are aggregated by the construct_runtime! macro to build the runtime. #[pallet] pub mod pallet { ... } Development mode You can specify dev_mode as an argument on the #[pallet] or #[frame_support::pallet] attribute macro to enable development mode for a pallet. For example, replace #[pallet] with #[pallet(dev_mode)] or #[frame_support::pallet] with #[frame_support::pallet(dev_mode)] to enable development mode for the pallet you're working on. Development mode loosens some of the restrictions and requirements placed on production pallets to make it easier to iterate on your code during development and testing cycles. For example, if you enable development mode for a pallet: You don't need to specify a weight on every #[pallet::call] declaration. By default, development mode assigns a weight of zero ( 0 ) to calls that don't have a weight explicitly specified. You don't need to implement MaxEncodedLen on storage types. By default, development mode marks all storage items as unbounded. Note that you can only add the dev_mode argument to the #[pallet] or #[frame_support::pallet] attribute macro that encloses your pallet module. You can't specify this argument for any of the #[pallet::*] attribute macros. You should never deploy pallets with development mode enabled in a production network. Before deploying a pallet in a production runtime, be sure to remove the dev_mode argument from the #[pallet] declaration, fix any compiler errors, and complete testing with the development mode disabled. Using the pallet module Inside the module, the macro parses items with the attribute #[pallet::*] . Some #[pallet::*] attributes are mandatory and some are optional. You can import system-level types from the frame_support and frame_system crates automatically by using the pallet_prelude from those crates. For example: #[pallet] pub mod pallet { use frame_support :: pallet_prelude :: * ; use frame_system :: pallet_prelude :: * ; ... } The #[pallet] macro is similar to a derive macro in that it expands the pallet types and trait implementations by reading the input. In most cases, the macro doesn't modify any input. However, there are a few specific scenarios where‚Äîunlike a derive macro‚Äîthis macro modifies its input. The macro will modify the input in the following circumstances: If a generic is replaced with a type For example, this can occur if the inner type of an item in pub struct Pallet<..>(_) is replaced in the pallet::storage macro with a type that implements the StorageInstance trait. If a function or data structure is changed For example, this can occur if the pallet::type_value macro changes a function item into a struct and trait implementation. If docs are not provided by the user For example, if no documentation is provided, the macro pallet::pallet modifies the input to add documentation above the struct Pallet<T>(_); item. #[pallet::config] The #[pallet::config] macro is required to define the generic data types that the pallet uses. This macro provides the constants that are part of the system-level Config trait for the pallet. The Config trait for this macro must be defined as a regular trait definition named Config that includes the system-level frame_system::Config trait. The definition can include other top-level traits and a where clause. For example: #[pallet::config] pub trait Config : frame_system :: Config + $optionally_some_other_supertraits $optional_where_clause { ... } To bypass the frame_system::Config requirement, you can use the attribute #[pallet::disable_frame_system_supertrait_check] . For example: #[pallet::config] #[pallet::disable_frame_system_supertrait_check] pub trait Config : pallet_timestamp :: Config { } #[pallet::constant] The #[pallet::constant] macro provides the Config trait‚Äîinside the #[pallet::config] macro‚Äîwith the types and attributes it needs for the runtime and generates associated metadata. This macro adds information about the constants used in a pallet to the runtime metadata, including: the constant name the name of the associated types the constant value the value returned by Get::get() for the constant For example, you can use #[pallet::constant] to add type MyGetParam to the metadata: #[pallet::config] pub trait Config : frame_system :: Config { #[pallet::constant] // puts attributes in metadata type MyGetParam : Get < u32 > ; } #[pallet::extra_constants] The #[pallet::extra_constants] macro enables you to add constants to the metadata. For example, you can declare a function that returns a generated value. You can then use the #[pallet::extra_constants] macro to add the information for the generated value to the metadata: #[pallet::extra_constants] impl < T : Config > Pallet < T > { //Example function using extra_constants fn example_extra_constants ( ) -> u128 { 4u128 } } #[pallet::pallet] The #[pallet::pallet] macro is required to declare the pallet data structure placeholder to be used by construct_runtime! macro. This macro must be defined as a struct named Pallet with a generic type and no where clause. For example: #[pallet::pallet] pub struct Pallet < T > ( _ ) ; This macro can generate the Store trait to contain an associated type for each storage item if you provide the #[pallet::generate_store($vis trait Store)] attribute macro. For example: #[pallet::pallet] #[pallet::generate_store(pub(super) trait Store)] pub struct Pallet < T > ( _ ) ; For more information about working with storage and this macro, see the macro expansion added to the struct Pallet<T> definition. #[pallet::without_storage_info] The #[pallet::without_storage_info] macro enables you to define pallet storage items that don't have a fixed size. By default, all pallet storage items are required to implement traits::StorageInfoTrait , so that all key and value types have a fixed size based on the bound defined in the pallet_prelude::MaxEncodedLen attribute. This size limitation is required for parachain development to estimate the size of the Proof of Validity (PoV) blob. The #[pallet::without_storage_info] attribute macro allows you to override the default behavior if you require unbounded storage for an entire pallet. To use it, add the #[pallet::without_storage_info] attribute to the pallet struct like so: #[pallet::pallet] #[pallet::generate_store(pub(super) trait Store)] #[pallet::without_storage_info] pub struct Pallet < T > ( _ ) ; Note that you should only use the #[pallet::without_storage_info] macro if you need to make all of the storage items in your pallet unbounded. If you only need undefined storage for a specific storage item, you can use the #[pallet::unbounded] attribute macro to override the fixed size constraint. Because the #[pallet::without_storage_info] macro applies to all storage items in your pallet, you should only use it in a test or development environment. You should never use the #[pallet::without_storage_info] attribute macro in a production environment. For more information about working with storage and this macro, see the macro expansion added to the struct Pallet<T> definition. #[pallet::unbounded] The #[pallet::unbounded] attribute macro enables you to declare a specific storage item as unbounded. By default, all pallet storage items are required to have a fixed size. You can use this attribute macro to override the default requirement on a specific storage item. If you are a parachain developer, you can use this macro for storage items that will never go into the Proof of Validity (PoV) blob. #[pallet::hooks] The #[pallet::hooks] macro allows you to declare optional pallet hooks to implement pallet-specific logic at specific points in the block making process. Within the #[pallet::hooks] macro, you can implement the Hooks trait to execute logic when a block is being initialized or finalized, before a runtime is upgraded, or after a runtime upgrade has been completed. For example: #[pallet::hooks] impl < T : Config > Hooks < BlockNumberFor < T >> for Pallet < T > { // Hooks functions and logic goes here. } For more information about using hooks, see the Rust documentation for pallet::hooks and macro expansion . #[pallet::call] The #[pallet::call] is required to implement the functions that can be dispatched to the runtime for a pallet. Each function must: define a weight with the #[pallet::weight($expr)] attribute have its first argument as origin: OriginFor<T> use compact encoding for arguments using #[pallet::compact] return DispatchResultWithPostInfo or DispatchResult Extrinsic requests coming into the runtime can use calls to trigger specific logic. Calls can also be used in on-chain governance, demonstrated by the democracy pallet where calls can be voted on. The #[pallet::call] aggregates all of the function call logic using the Call enum . The aggregation enables FRAME to batch functions of the same type into a single runtime call. The runtime then generates the associated items from the implementation defined in the impl code blocks. For more information. see the Rust documentation for pallet::call . #[pallet::error] The #[pallet::error] macro allows you to define the error types that can be returned from the function calls dispatched to the runtime. The error information is included in the runtime metadata. The macro must be defined as an enumeration named Error with a generic type and variants with or without fields. For example: #[pallet::error] pub enum Error < T > { /// $some_optional_doc $SomeFieldLessVariant , /// $some_more_optional_doc $SomeVariantWithOneField ( FieldType ) , ... } Any field type you specify for an enumeration variant must implement the scale_info::TypeInfo trait and its encoded size should be as small as possible. Field types in enum variants must also implement the PalletError trait to compile. For more information, see the Rust documentation for pallet::error . #[pallet::event] The #[pallet::event] macro allows you to define event types for a pallet. This macro is similar to the pallet::error macro but it can hold more information. The macro is defined as an enumeration named Event. For example: #[pallet::event] #[pallet::generate_deposit($visibility fn deposit_event)] // Optional pub enum Event < $some_generic > $optional_where_clause { /// Some doc $SomeName ( $SomeType , $YetanotherType , ... ) , ... } For more information, see the Rust documentation for pallet::event . #[pallet::storage] The #[pallet::storage] macro enables you to define abstract storage inside runtime storage and to set metadata for that storage. This attribute macro can be used multiple times. The [pallet::storage] macro can be defined using named or unnamed generics with a type alias of StorageValue, StorageMap or StorageDoubleMap. #[pallet::storage] #[pallet::getter(fn $getter_name)] // optional $vis type $StorageName < $some_generic > $optional_where_clause = $StorageType < $generic_name = $some_generics , $other_name = $some_other , ... > ; For more information, see the Rust documentation for pallet::storage and the following storage data structures: StorageDoubleMap StorageMap StorageValue #[pallet::type_value] The #[pallet::type_value] macro enables you to define a struct that implements a Get trait for storage types. This attribute macro can be used multiple times in combination with the #[pallet::storage] macro to define default values in storage. #[pallet::type_value] fn MyDefault < T : Config > ( ) -> T :: Balance { 3 . into ( ) } For more information about using this macro, see the Rust documentation for pallet::type_value . #[pallet::genesis_build] The #[pallet::genesis_build] macro allows you to define how a genesis configuration is built. The macro is defined as a Rust trait implementation with a generic type <T: Config> of trait GenesisBuild on type GenesisConfig. For example: #[pallet::genesis_build] impl < T : Config > GenesisBuild < T > for GenesisConfig { fn build ( & self ) { } } For more information, see the Rust documentation for pallet::genesis_build . #[pallet::genesis_config] The #[pallet::genesis_config] macro allows you to define the genesis configuration of the pallet. The macro can be defined as an enumeration or a struct, but must be public and implement trait the GenesisBuild with the #[pallet::genesis_build] macro. For example: #[pallet::genesis_config] pub struct GenesisConfig < T : Config > { _myfield : BalanceOf < T > , } For more information, see the Rust documentation for pallet::genesis_config . #[pallet::inherent] The #[pallet::inherent] macro allows the pallet to provide data in an unsigned inherent transaction. The macro is defined as a trait implementation with bound <T: Config,> of trait ProvideInherent for type Pallet . For example: #[pallet::inherent] impl < T : Config > ProvideInherent for Pallet < T > { // ... regular trait implementation } For more information, see the Rust documentation for pallet::inherent . #[pallet::origin] The #[pallet::origin] macro allows you to define an origin for the pallet. The macro must be defined as a type alias, enumeration, or struct. The macro must be public. For example: #[pallet::origin] pub struct Origin < T > ( PhantomData < ( T ) > ) ; For more information, see the Rust documentation for pallet::origin . #[pallet::validate_unsigned] The #[pallet::validate_unsigned] macro allows the pallet to validate unsigned transactions. The macro is defined as a trait implementation with bound <T: Config> of trait ValidateUnsigned for type Pallet . For example: #[pallet::validate_unsigned] impl < T : Config > ValidateUnsigned for Pallet < T > { // ... regular trait implementation } For more information, see the Rust documentation for pallet::validate_unsigned . Runtime construction macros As an introduction to the use of macros, Substrate runtime macros highlighted several of the macros that are provided in the frame_support and frame_system crates. However, a few of those macros introduced in Substrate runtime macros warrant additional attention because of the role they play in constructing the runtime logic. construct_runtime! The construct_runtime! macro generates all of the data structures that have been declared and implemented for the pallets to be included in the runtime. For example, the construct_runtime! macro extrapolates information from the pallet attribute macros to generate details such as: Runtime struct to represent the Substrate runtime. Call enumeration variants and metadata for each pallet included in the runtime that has callable functions. Event enumeration variants for each pallet included in the runtime that emits events. Genesis configuration struct for building storage for each pallet included in the runtime that defines storage items. Inherent data for pallets that support inherent transactions. Origin enumeration variants for pallets that identify the function caller using the default origin definition or pallet-specific custom origins. Validation for unsigned transactions for pallets that support unsigned transactions. The construct_runtime! macro also implements helper traits for the data structures and types exposed, generates an index of the pallets listed in the runtime, and maps events, errors, and callable functions from the runtime back to the pallet they originate from. By default, the construct_runtime! macro includes all pallet attributes for all pallets listed in the macro definition. You can override the default behavior to exclude specific parts or to only include specific parts. For example, if you don't want the runtime to expose the function calls defined in specific pallet, you can explicitly exclude them in the construct_runtime! macro with an entry similar to the following: MyCustomPallet : pallet_my_custom_pallet exclude_parts { Call } In this example, MyCustomPallet is the name that uniquely identifies the pallet that requires special handling and pallet_my_custom_pallet represents the path to the pallet. The exclude_parts keyword specifies that you only want to exclude certain attributes for the specified pallet. In this case, only callable functions for MyCustomPallet are excluded. Similarly, you can override the default to explicitly include certain parts in the runtime with the use_parts keyword. For example, if you only want the runtime to expose a subset of attributes for a specific pallet, you can explicitly include them in the construct_runtime! macro with an entry similar to the following: MyCustomPallet : pallet_my_custom_pallet use_parts { Pallet , Call , Storage , RuntimeEvent , RuntimeOrigin , Config } You should note the the order in which pallets are listed in the construct_runtime! macro is significant. By default, the pallet index starts at zero for the first pallet and is incremented for each pallet thereafter. You can manually adjust the indexing in the construct_runtime! macro by adding an index number for a pallet. For example, you can generate all pallet attributes for the MyCustomPallet and set the index to eight with syntax like this: MyCustomPallet : pallet_my_custom_pallet use_parts = 8 , However, you should also note that the order used for defining pallets in the construct_runtime! macro affects the genesis storage configuration. If you have one pallet that depends on another pallet, be sure the pallet that is depended upon comes before‚Äîthat is, is listed before or has a lower index value‚Äîthan the pallet that depends on it. For more information, see the Rust documentation for construct_runtime parameter_types! The parameter_types! macro declares the parameter types that are to be assigned to the configuration trait for each pallet during runtime construction. This macro converts each parameter specified into a struct type with a get() function that returns the specified type. Each parameter struct type also implements a frame_support::traits::Get<I> trait to convert the type to its specified value. For more information, see the Rust documentation for parameter_types . impl runtime apis! The impl_runtime_apis! macro generates the runtime API for all of the traits that are implemented by the macro. The traits implemented in this macro must first be declared in the decl_runtime_apis macro. The macro generates the RuntimeApi and RuntimeApiImpl structs to expose these traits as runtime APIs . The traits exposed by the macro enable outer node components to communicate with the runtime through the RuntimeApi type. The macro declares the RuntimeApi and RuntimeApiImpl structs and implements various helper traits for the RuntimeApiImpl struct. If you define additional interfaces for the runtime to expose in the impl_runtime_apis! macro, they are appended to the default RuntimeApiImpl implementation. The macro also generates the RUNTIME_API_VERSIONS constant to expose version information about all of the implemented api traits. This constant is used to instantiate the apis field of RuntimeVersion . For more information, see the Rust documentation for impl runtime apis . app_crypto! The app_crypto! macro generates application-specific cryptographic key pairs using the specified signature algorithm. The macro declares the following struct types: Public For the Public type, the macro implements the sp_application_crypto::AppKey trait to define the public key type and the sp_application_crypto::RuntimeAppPublic trait enable generating key pairs, signing transactions, and verifying signatures. Signature For the Signature type, the macro implements the core::hash::Hash trait to specify the signature algorithm‚Äîfor example, SR25519 or ED25519‚Äîused to hash the signature. Pair For the Pair type, the macro implements the sp_application_crypto::Pair and sp_application_crypto::AppKey traits to generate public-private key pairs from a secret phrase or seed. In addition to the traits for these structs, the macro implements helper traits. For more information, see the Rust documentation for app_crypto . Benchmarking macros The FRAME benchmarking framework defines several macros for benchmarking pallets. The following macros are used for benchmarking: add_benchmark to add pallet benchmarks to a Vec<BenchmarkBatch> object using the pallet crate name and generated module struct. benchmarks to construct the benchmark logic for testing the execution time for function calls. benchmarks_instance to provide the same functionality as the benchmarks macro for instantiable modules. benchmarks_instance_pallet to provide the same functionality as the benchmarks macro for instantiable pallets that are declared with the [ frame_support::pallet ] macro. cb_add_benchmarks to call add_benchmark as a callback for the define_benchmarks macro. cb_list_benchmarks to call list_benchmark as a callback for the define_benchmarks macro. define_benchmarks to define all of the benchmarked pallets for the runtime. impl_benchmark_test_suite to create a test suite that runs the benchmarks defined in the benchmarking module. list_benchmark to generate a list of benchmarks for the pallets configured in the runtime. whitelist to add accounts to an allow list for testing purposes. References The Rust Programming Language The Little Book of Rust Macros Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Macro basics FRAME support and system macros Macros for composing pallets Runtime construction macros Benchmarking macros References Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Account data structures Account data structures Account data structures Accounts are a fundamental part of any blockchain, and, in Substrate, accounts can be used in many different ways. This article describes how accounts are stored in Substrate and how the account data structure is used to manage the account lifecycle in the runtime logic. Account The Account data type is a storage map that is defined generically in the frame-system pallet : /// The full account information for a particular account ID. #[pallet::storage] #[pallet::getter(fn account)] pub type Account < T : Config > = StorageMap < _ , Blake2_128Concat , T :: AccountId , AccountInfo < T :: Index , T :: AccountData > , ValueQuery , > ; The StorageMap for an Account consists of the following parameters: The first parameter (_) is used in macro expansion. Blake2_128Concat specifies the hashing algorithm to use. T::AccountId is used as the key for over the AccountInfo<T::Index, T::AccountData> struct. See StorageMap API for details. AccountInfo The AccountInfo for an account is defined in the frame_system pallet : #[derive(Clone, Eq, PartialEq, Default, RuntimeDebug, Encode, Decode)] pub struct AccountInfo < Index , AccountData > { /// The number of transactions this account has sent. pub nonce : Index , /// The number of other modules that currently depend on this account's existence. The account /// cannot be reaped until this is zero. pub consumers : RefCount , /// The number of other modules that allow this account to exist. The account may not be reaped /// until this and `sufficients` are both zero. pub providers : RefCount , /// The number of modules that allow this account to exist for their own purposes only. The /// account may not be reaped until this and `providers` are both zero. pub sufficients : RefCount , /// The additional data that belongs to this account. Used to store the balance(s) in a lot of /// chains. pub data : AccountData , } Every account has an AccountInfo consisting of: The nonce indicating the number of transactions the account has sent. The consumers reference counter indicating the number of other modules that currently depend on this account's existence. The providers reference counter indicating the number of other modules that allow this account to exist. The sufficients reference counter indicating the number of modules that allow this account to exist for their own purposes only. The AccountData structure that you can configure to hold different kinds of data. Account reference counters The account reference counters track account dependencies in the runtime. For example, if you store data under a map controlled by an account, you wouldn't want to delete the account until the data stored under the map the account controls has been deleted. The consumers and providers reference counters are designed to be used together. For example, the consumers reference counter in the Session pallet is incremented when an account sets its session keys prior of becoming a validator. The providers reference counter must be greater than zero before the consumer counter can be incremented. The providers reference counter indicates if an account is ready to be depended upon. For example, the providers reference counter is incremented when a new account is created with more than the existential deposit [2] . The providers reference counter prevents Substrate pallets from storing data about an account until the account is active ( providers > 0). The consumers reference counter prevents Substrate pallets from removing an account until data about the account is removed in all pallets ( consumers == 0). The consumers reference counter holds users accountable for the data stored on-chain. If users want to remove their accounts and get back the existential deposit, they need to remove all of the data they have stored on all on-chain pallets to decrement consumers counter. Pallets also have cleanup functions to decrement the providers reference counter to mark the account as deactivated within the pallet-managed scope. When the account providers reference counter is zero and the consumers is zero, the account is considered deactivated by all on-chain pallets. The sufficients reference counter indicates if an account is self-sufficient and can exist by itself. For example, in the Assets pallet, an account can have sufficient number of certain assets but without owning any native account balance. Runtime developers can update these counters using the inc_consumers() , dec_consumers() , inc_providers() , dec_providers() , inc_sufficients() , and dec_sufficients() methods exposed by the frame-system pallet. Each increment call of a certain counter should be accompanied by a corresponding decrement call of the counter in an account life cycle. There are also three query functions to ease usage on these counters: can_inc_consumer() to check if an account is ready to be used ( providers > 0). can_dec_provider() to check if an account is no longer referenced in runtime whatsoever ( consumers == 0) before decrementing providers to 0. is_provider_required() to check if an account has outstanding consumer references ( consumers > 0). See frame-system API for details. AccountData trait and implementation The AccountInfo can be any struct as long as the struct satisfies the associated type AccountData trait bound defined in the frame-system::pallet::Config trait . By default, the Substrate runtime configures AccountInfo to be as defined in pallet-balances . Where to go next For addtional technical details, see the following resources: frame_system::AccountInfo API pallet_balances::AccountData API . pallet_session::Pallet::set_keys dispatchable call frame_system::Provider HandleLifetime implementation pallet_assets new_account function Last edit: August 21, 2022 ‚Üê Previous Next ‚Üí ON THIS PAGE Account AccountInfo Account reference counters AccountData trait and implementation Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Type encoding (SCALE) Scale codec Type encoding (SCALE) Substrate uses a lightweight and efficient encoding and decoding program to optimize how data is sent and received over the network. The program used to serialize and deserialize data is called the SCALE codec, with SCALE being an acronym for s imple c oncatenated a ggregate l ittle- e ndian. The SCALE codec is a critical component for communication between the runtime and the outer node. It is designed for high-performance, copy-free encoding and decoding of data in resource-constrained execution environments like the Substrate WebAssembly runtime . The SCALE codec is not self-describing in any way. It assumes the decoding context has all type knowledge about the encoded data. Front-end libraries maintained by Parity use the parity-scale-codec crate‚Äîwhich is a Rust implementation of the SCALE codec‚Äîto encode and decode interactions between RPCs and the runtime. SCALE codec is advantageous for Substrate and blockchain systems because: It is lightweight relative to generic serialization frameworks like serde , which add significant boilerplate that can bloat the size of the binary. It does not use Rust libstd making it compatible with no_std environments that compile to Wasm, such as the Substrate runtime. It is built to have great support in Rust for deriving codec logic for new types using: #[derive(Encode, Decode)] . It's important to define the encoding scheme used in Substrate rather than reuse an existing Rust codec library because this codec needs to be re-implemented on other platforms and languages that want to support interoperability among Substrate blockchains. The following table shows how the Rust implementation of the Parity SCALE codec encodes different types. SCALE codec examples of different types Type Description Example SCALE decoded value SCALE encoded value Fixed-width integers Basic integers are encoded using a fixed-width little-endian (LE) format. signed 8-bit integer 69 0x45 unsigned 16-bit integer 42 0x2a00 unsigned 32-bit integer 16777215 0xffffff00 Compact/general integers 1 A "compact" or general integer encoding is sufficient for encoding large integers (up to 2**536) and is more efficient at encoding most values than the fixed-width version. (Though for single-byte values, the fixed-width integer is never worse.) unsigned integer 0 0x00 unsigned integer 1 0x04 unsigned integer 42 0xa8 unsigned integer 69 0x1501 unsigned integer 65535 0xfeff0300 BigInt(100000000000000) 0x0b00407a10f35a Boolean Boolean values are encoded using the least significant bit of a single byte. false 0x00 true 0x01 Results 2 Results are commonly used enumerations which indicate whether certain operations were successful or unsuccessful. Ok(42) 0x002a Err(false) 0x0100 Options One or zero values of a particular type. Some 0x01 followed by the encoded value None 0x00 followed by the encoded value Vectors (lists, series, sets) A collection of same-typed values is encoded, prefixed with a compact encoding of the number of items, followed by each item's encoding concatenated in turn. Vector of unsigned 16-bit integers: [4, 8, 15, 16, 23, 42] 0x18040008000f00100017002a00 Strings Strings are Vectors of bytes ( Vec<u8> ) containing a valid UTF8 sequence. Tuples A fixed-size series of values, each with a possibly different but predetermined and fixed type. This is simply the concatenation of each encoded value. Tuple of compact unsigned integer and boolean: (3, false) 0x0c00 Structs For structures, the values are named, but that is irrelevant for the encoding (names are ignored - only order matters). All containers store elements consecutively. The order of the elements is not fixed, depends on the container, and cannot be relied on at decoding. This implicitly means that decoding some byte-array into a specified structure that enforces an order and then re-encoding it could result in a different byte array than the original that was decoded. A SortedVecAsc<u8> structure that always has byte-elements in ascending order: SortedVecAsc::from([3, 5, 2, 8]) [3, 2, 5, 8] Enumerations (tagged-unions) A fixed number of variants, each mutually exclusive and potentially implying a further value or series of values. Encoded as the first byte identifying the index of the variant that the value is. Any further bytes are used to encode any data that the variant implies. Thus, no more than 256 variants are supported. Int(42) and Bool(true) where enum IntOrBool { Int(u8), Bool(bool),} 0x002a and 0x0101 SCALE Codec has been implemented in other languages, including: Python: polkascan/py-scale-codec Golang: itering/scale.go C: MatthewDarnell/cScale C++: soramitsu/scale-codec-cpp JavaScript: polkadot-js/api TypeScript: parity-scale-codec-ts , scale-ts , soramitsu/scale-codec-js-library AssemblyScript: LimeChain/as-scale-codec Haskell: airalab/hs-web3 Java: emeraldpay/polkaj Ruby: wuminzhe/scale_rb Compact/general integers are encoded with the two least significant bits denoting the mode: 0b00 : single-byte mode; upper six bits are the LE encoding of the value (valid only for values of 0-63). 0b01 : two-byte mode: upper six bits and the following byte is the LE encoding of the value (valid only for values 64-(2**14-1) ). 0b10 : four-byte mode: upper six bits and the following three bytes are the LE encoding of the value (valid only for values (2**14)-(2**30-1) ). 0b11 : Big-integer mode: The upper six bits are the number of bytes following, plus four. The value is contained, LE encoded, in the bytes following. The final (most significant) byte must be non-zero. Valid only for values (2**30)-(2**536-1) . ‚Ü© Results are encoded as: 0x00 if the operation was successful, followed by the encoded value. 0x01 if the operation was unsuccessful, followed by the encoded error. ‚Ü© Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Address formats Address formats Address formats The default Substrate address format is SS58. The SS58 encoded address format is based on the Bitcoin Base-58-check format, but with a few modification specifically designed to suite Substrate-based chains. You can use other address formats for Substrate-based chains. However, the SS58 address format provides a base-58 encoded value that can identify a specific account on any Substrate chain. Because different chains can have different ways of identifying accounts, the SS58 address is designed to be extensible. Basic format You can find the implementation for the SS58 address format in Ss58Codec . The basic format of the address can be described as: base58encode ( concat ( <address-type>, <address>, <checksum> ) ) The address is the concatenated byte series consisting of an address type, the encoded address, and a checksum that is passed into a base-58 encoder. The base58encode function is implemented exactly as defined in Bitcoin and IPFS specifications, using the same alphabet as both of those implementations. The Base-58 alphabet eliminates characters that might look ambiguous when printed, for example: non-alphanumerical characters (+ and /) zero (0) capital i (I) capital o (O) lower-case L (l) Address type The address-type in the SS58 address format is one or more bytes that describe the precise format of the address bytes that follow it. Currently, the valid values are: 00000000b..=00111111b (0 to 63 inclusive) Simple account/address/network identifier. The byte can be interpreted directly as such an identifier. 01000000b..=01111111b (64 to 127 inclusive) Full address/address/network identifier. The lower 6 bits of this byte should be treated as the upper 6 bits of a 14 bit identifier value, with the lower 8 bits defined by the following byte. This works for all identifiers up to 2**14 (16,383). 10000000b..=11111111b (128 to 255 inclusive) Reserved for future address format extensions. The latter (42) address that is intended to be valid on all Substrate networks that support fixed-length addresses. For production networks, however, a network-specific version might be desirable to help avoid the key-reuse between networks and some of the problems that re-use can cause. By default, Substrate nodes print keys in address type 42 However, Substrate-based chains with alternative node implementations‚Äîfor example, nodes in the Polkadot ecosystem‚Äîcan default to some other address type. Address length in bytes There are 16 different address formats, identified by the length in bytes of the total payload including the checksum. Total Type Raw account Checksum 3 1 1 1 4 1 2 1 5 1 2 2 6 1 4 1 7 1 4 2 8 1 4 3 9 1 4 4 10 1 8 1 11 1 8 2 12 1 8 3 13 1 8 4 14 1 8 5 15 1 8 6 16 1 8 7 17 1 8 8 35 1 32 2 Checksum types Several potential checksum strategies exist within Substrate, giving different length and longevity guarantees. There are two types of checksum preimages (known as SS58 and AccountID) and many different checksum lengths (1 to 8 bytes). In all cases for Substrate, the Blake2b-512 (Spec, Wiki) hash function is used (OID 1.3.6.1.4.1.1722.12.2.1.16). The variants simply select the preimage used as the input to the hash function and the number of bytes taken from its output. The bytes used are always the left-most bytes. The input to be used is the non-checksum portion of the SS58 byte series used as input to the base-58 function, for example concat( <address-type>, <address> ) . A context prefix of 0x53533538505245, (the string SS58PRE) is prepended to the input to give the final hashing preimage. The advantage of using more checksum bytes is simply that more bytes provide a greater degree of protection against input errors and index alteration at the cost of widening the textual address by an extra few characters. For the account ID format, this is insignificant and therefore no 1-byte alternative is provided. For the shorter account-index formats, the extra byte represents a far greater portion of the final address, so it is left for further up the stack (though not necessarily the user themselves) to determine the best tradeoff for their purposes. Address types and the network registry The SS58 registry is the canonical listing of all address type identifiers and how they map to Substrate-based networks. Encoding address and network identifiers Identifiers up to value 64 can be expressed using a simple address format. For the simple address format, the least significant byte of the network identifier value is expressed as the first byte of the encoded address. For identifiers between 64 and 16,383, the full address format must be used. The full address encoding requires special handling because SCALE encoding as little endian requires the first two bits to be used for the 01 prefix. To encode the network identifier, the full address format treats the first two bytes as a 16-bit sequence, and disregards the first two bits of that sequence to account for the 01 prefix. The remaining 14 bits encode the network identifier value as little endian, with the assumption that the two missing higher order bits are zero. This effectively spreads the low-order byte across the boundary between the two bytes. For example, the 14-bit identifier 0b00HHHHHH_MMLLLLLL is expressed in two bytes as: 0b01LLLLLL 0bHHHHHHMM Identifiers of 16384 and beyond are not currently supported. Validating addresses You can verify that a value is a valid SS58 address by using the subkey inspect command or though the Polkadot-JS API. Using subkey The basic syntax for the subkey inspect command is: subkey inspect [flags] [options] uri For the uri command-line argument, you can specify the secret seed phrase, a hex-encoded private key, or an SS58 address. If the input is a valid address, the subkey program displays the corresponding hex-encoded public key, account identifier, and SS58 addresses. For example, to inspect the public keys derived from a secret seed phrase, you can run a command similar to the following:: subkey inspect "caution juice atom organ advance problem want pledge someone senior holiday very" The command displays output similar to the following: Secret phrase `caution juice atom organ advance problem want pledge someone senior holiday very` is account: Secret seed: 0xc8fa03532fb22ee1f7f6908b9c02b4e72483f0dbd66e4cd456b8f34c6230b849 Public key (hex): 0xd6a3105d6768e956e9e5d41050ac29843f98561410d3a47f9dd5b3b227ab8746 Public key (SS58): 5Gv8YYFu8H1btvmrJy9FjjAWfb99wrhV3uhPFoNEr918utyR Account ID: 0xd6a3105d6768e956e9e5d41050ac29843f98561410d3a47f9dd5b3b227ab8746 SS58 Address: 5Gv8YYFu8H1btvmrJy9FjjAWfb99wrhV3uhPFoNEr918utyR The subkey program assumes that an address is based on a public/private key pair. If you inspect an address, the command returns the 32-byte account identifier. However, not all addresses in Substrate-based networks are based on keys. Depending on the command-line options you specify and the input you provided, the command output might also display the network for which the address has been encoded. For example: subkey inspect "12bzRJfh7arnnfPPUZHeJUaE62QLEwhK48QnH9LXeK2m1iZU" The command displays output similar to the following: Public Key URI `12bzRJfh7arnnfPPUZHeJUaE62QLEwhK48QnH9LXeK2m1iZU` is account: Network ID/Version: polkadot Public key (hex): 0x46ebddef8cd9bb167dc30878d7113b7e168e6f0646beffd77d69d39bad76b47a Account ID: 0x46ebddef8cd9bb167dc30878d7113b7e168e6f0646beffd77d69d39bad76b47a Public key (SS58): 12bzRJfh7arnnfPPUZHeJUaE62QLEwhK48QnH9LXeK2m1iZU SS58 Address: 12bzRJfh7arnnfPPUZHeJUaE62QLEwhK48QnH9LXeK2m1iZU Using Polkadot-JS API To verify an address in JavaScript or TypeScript projects, you can use the functions built into the Polkadot-JS API. For example: // Import Polkadot.js API dependencies. const { decodeAddress, encodeAddress } = require ( '@polkadot/keyring' ) const { hexToU8a, isHex } = require ( '@polkadot/util' ) // Specify an address to test. const address = '<addressToTest>' // Check address. const isValidSubstrateAddress = ( ) = > { try { encodeAddress ( isHex ( address ) ? hexToU8a ( address ) : decodeAddress ( address )) return true } catch ( error ) { return false } } // Query result. const isValid = isValidSubstrateAddress ( ) console.log ( isValid ) If the function returns true , the address you specified is a valid address. Other SS58 implementations Support for encoding and decoding Substrate SS58 addresses has been implemented in several other languages and libraries. Crystal: wyhaines/base58.cr Go: itering/subscan Python: polkascan/py-scale-codec Typescript: subsquid/squid-sdk Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Basic format Address type Address length in bytes Checksum types Address types and the network registry Encoding address and network identifiers Validating addresses Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Transaction formats Transaction format Transaction formats This article describes in detail the data structure of signed and unsigned transactions in Substrate. This is particularly useful for understanding how the transaction pool checks incoming transactions. Parachain builders will find this useful for customizing how their transactions are formatted as well as writing client applications that need to adhere to a chosen format. Extrinsics normally contain a signature, some data to describe if the extrinsic has passed some validity checks and a reference to the pallet and call that it is intended for. This format provides a way for applications to ensure the requirements for an extrinsic are met and correctly constructed. Unchecked: signed transactions that require some validation check before they can be accepted in the transaction pool. Any unchecked extrinsic contains the signature for the data being sent plus some extra data. Checked: inherent extrinsics which by definition don't require signature verification. Instead, they carry information on where the extrinsic comes from and some extra data. Opaque: used for cases when an extrinsic hasn't yet been committed to a format but can still be decoded. Extra data can be any additional information that would be useful to attach to a transaction or inherent. For example, the nonce of the transaction, the tip for the block author, or how long the extrinsic is valid for. This information is provided by a specialized extensions that help determine the validity and ordering of extrinsics before they get included in a block. A signed transaction might be constructed like so: node_runtime :: UncheckedExtrinsic :: new_signed ( function . clone ( ) , // some call sp_runtime :: AccountId32 :: from ( sender . public ( ) ) . into ( ) , // some sending account node_runtime :: Signature :: Sr25519 ( signature . clone ( ) ) , // the account's signature extra . clone ( ) , // the signed extensions ) How transactions are constructed Substrate defines its transaction formats generically to allow developers to implement custom ways to define valid transactions. In a runtime built with FRAME however (assuming transaction version 4), a transaction must be constructed by submitting the following encoded data: <signing account ID> + <signature> + <additional data> When submitting a signed transaction, the signature is constructed by signing: The actual call, composed of: The index of the pallet. The index of the function call in the pallet. The parameters required by the function call being targeted. Some extra information, verified by the signed extensions of the transaction: What's the era for this transaction, i.e. how long should this call last in the transaction pool before it gets discarded? The nonce, i.e. how many prior transactions have occurred from this account? This helps protect against replay attacks or accidental double-submissions. The tip amount paid to the block producer to help incentive it to include this transaction in the block. Then, some additional data that's not part of what gets signed is required, which includes: The spec version and the transaction version. This ensures the transaction is being submitted to a compatible runtime. The genesis hash. This ensures that the transaction is valid for the correct chain. The block hash. This corresponds to the hash of the checkpoint block, which enables the signature to verify that the transaction doesn't execute on the wrong fork, by checking against the block number provided by the era information. The SCALE encoded data is then signed (i.e. ( call , extra , additional )) and the signature, extra data and call data is attached in the correct order and SCALE encoded, ready to send off to a node that will verify the signed payload. If the payload to be signed is longer than 256 bytes, it is hashed just prior to being signed, to ensure that the size of the signed data does not grow beyond a certain size. This process can be broken down into the following steps: Construct the unsigned payload. Create a signing payload. Sign the payload. Serialize the signed payload. Submit the serialized transaction. An extrinsic is encoded into the following sequence of bytes just prior to being hex encoded: [ 1 ] + [ 2 ] + [ 3 ] + [ 4 ] where: [1] contains the compact encoded length in bytes of all of the following data. Learn how compact encoding works using SCALE . [2] is a u8 containing 1 byte to indicate whether the transaction is signed or unsigned (1 bit), and the encoded transaction version ID (7 bits). [3] if a signature is present, this field contains an account ID, an SR25519 signature and some extra data. If unsigned this field contains 0 bytes. [4] is the encoded call data. This comprises of 1 byte denoting the pallet to call into, 1 byte denoting the call to make in that pallet, and then as many bytes as needed to encode the arguments expected by that call. The way applications know how to construct a transaction correctly is provided by the metadata interface . An application will know how to correctly encode a transaction by using the metadata types and transaction format. If a call doesn't need to be signed, then the first bit in [2] will be 0 and so an application will know not to try decoding a signature. Polkadot JS Apps example: Here we demonstrate a detailed, manual extrinsic construction and submission of a balance transfer from Bob to Dave: Bob sends 42 UNIT s to Dave. Start a node template in --dev mode (see the quick start guide for how to set this up) Navigate to https://polkadot.js.org/apps/?rpc=ws%3A%2F%2F127.0.0.1%3A9944#/extrinsics Set Bob as sender and select balances pallet and the transfer(dest, value) call Set MultiAddress to Id and the AccountID to Dave for the dest Set value to 42000000000000 (this is 42 UNIT as defined in the chain spec of the node template) Click Submit Transaction button (lower right) and un-check sign and submit to generate a signed transaction with the default nonce = 0 and Lifetime = 64 for inspection Encoded call data: 0x050300306721211d5404bd9da88e0204360a1a9ab8b87c66c1bc2fcdd37f3c2222cc200b00a014e33226 Encoded call hash: 0x26c333c22ec93ac431ee348168530b7d77e85d766f130af60890c0fd6ab20d5b Resulting signed transaction call hash: 0x450284008eaf04151687736326c9fea17e25fc5287613693c912909cb226aa4794f26a48018eeaeb6a3496444c08b5c3e10e0c5f94776774591504ef4ef26e3873799831285a1a7cbd8ba2babe6fba94ea3585bf20e46c80ce7baeb25b149529ece931478c45020c00050000306721211d5404bd9da88e0204360a1a9ab8b87c66c1bc2fcdd37f3c2222cc200b00a014e33226 Here you can Copy the Signed transaction data to submit via RPC directly, or for inspection on the Developer -> Extrinsics -> Decode section. We will use this window now to submit the transaction and watch the result. Close the authorize transaction card Click Submit Transaction button (lower right) and keep sign and submit checked Navigate to the Developer -> RPC Calls tab In the RPC tab, you should see the result of your author_submitAndWatchExtrinsic call as something similar to: { dispatchInfo : { weight : 159 , 200 , 000 class : Normal paysFee : Yes } events : [ { phase : { ApplyExtrinsic : 1 } event : { method : Withdraw section : balances index : 0x0508 data : { who : 5FHneW46xGXgs5mUiveU4sbTyGBzmstUspZC92UhjJM694ty amount : 125 , 000 , 147 } } topics : [ ] } { phase : { ApplyExtrinsic : 1 } event : { method : NewAccount section : system index : 0x0003 data : { account : 5DAAnrj7VHTznn2AWBemMuyBwZWs6FNFjdyVXUeYum3PTXFy } } topics : [ ] } { phase : { ApplyExtrinsic : 1 } event : { method : Endowed section : balances index : 0x0500 data : { account : 5DAAnrj7VHTznn2AWBemMuyBwZWs6FNFjdyVXUeYum3PTXFy freeBalance : 42 , 000 , 000 , 000 , 000 } } topics : [ ] } { phase : { ApplyExtrinsic : 1 } event : { method : Transfer section : balances index : 0x0502 data : { from : 5FHneW46xGXgs5mUiveU4sbTyGBzmstUspZC92UhjJM694ty to : 5DAAnrj7VHTznn2AWBemMuyBwZWs6FNFjdyVXUeYum3PTXFy amount : 42 , 000 , 000 , 000 , 000 } } topics : [ ] } { phase : { ApplyExtrinsic : 1 } event : { method : ExtrinsicSuccess section : system index : 0x0000 data : { dispatchInfo : { weight : 159 , 200 , 000 class : Normal paysFee : Yes } } } topics : [ ] } ] status : { InBlock : 0x501c8f15883bb2b686fb5ea1ca35e99dace8bd6216bfc571a31d7088aea000f7 } } Navigate to the Network -> Explorer tab Open the balances.Transfer extrinsic details by clicking the <block number>-<extrinsic number> in the top right of the card Inspect the on-chain details of your transaction Click the #/extrinsics/decode/0x.... link to open the decoded details of the above Signed transaction data and notice it is identical to what we submitted. Thus before or after submitting an extrinsic, this tool can be used to decode and introspect transaction call data. Signed extensions Substrate provides the concept of signed extensions to extend an extrinsic with additional data, provided by the SignedExtension trait. The transaction queue regularly calls signed extensions to keep checking that a transaction is valid before it gets put in the ready queue. This is a useful safeguard for verifying that transactions won't fail in a block. They are commonly used to enforce validation logic to protect the transaction pool from spam and replay attacks. In FRAME, a signed extension can hold any of the following types by default: AccountId : to encode the sender's identity. Call : to encode the pallet call to be dispatched. This data is used to calculate transaction fees. AdditionalSigned : to handle any additional data to go into the signed payload. This makes it possible to attach any custom logic prior to dispatching a transaction. Pre : to encode the information that can be passed from before a call is dispatch to after it gets dispatched. FRAME's system pallet provides a set of useful SignedExtensions out of the box. Practical examples An important signed extension for validating transactions is CheckSpecVersion . It provides a way for the sender to provide the spec version as a signed payload attached to the transaction. Since the spec version is already known in the runtime, the signed extension can perform a simple check to verify that the spec versions match. If they don't, the transaction fail before it gets put in the transaction pool. Other examples include the signed extensions used to calculate transaction priority. These are: CheckWeight : sets the value for priority to 0 for all dispatch classes. ChargeTransactionPayment : calculates the overall priority, modifying the priority value accordingly. The priority depends on the dispatch class and the amount of tip-per-weight or tip-per-length (whatever is more limiting) the sender is willing to pay. Transactions without a tip use a minimal tip value of 1 for priority calculations to make sure that not all transactions end up having a priority of 0 . The consequence of this is that smaller transactions are preferred over larger ones. Where to go next Now that you have seen how transactions are constructed, you might want to review how they progress from the transaction pool to the runtime and get added to blocks or how to use tools that enable you to submit transactions offline or using a REST API. Transaction lifecycle Transactions, weights, and fees tx-wrapper for offline transactions sidecar for REST-based transactions Last edit: April 3, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE How transactions are constructed Signed extensions Where to go next Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
XCM reference Xcm reference XCM reference This section provides reference information for the cross-consensus message (XCM) format. Instructions As you learned in Cross-consensus communication , the XCM executor is a program that executes an ordered set of instructions in a virtual machine running on the recipient consensus system. It's worth noting that some instructions are dependent on other instructions. The order in which instructions are listed in the Instruction enumeration reflects some of these dependencies. For example, an asset must be added to the holding register before it can be deposited somewhere else. In general, you also use a similar order for instructions when you construct a message to be sent to the receiving system. However, for your convenience, this reference section lists the instructions in alphabetic order instead of the order in which they are defined. BuyExecution Pays for the execution of the current message by removing assets from the holding register. You must specify the fees parameter to identify the asset to remove from the holding register to pay execution fees. You can also specify a weight_limit for the maximum fee to be purchased. If the weight_limit you specify is lower than the estimated weight of the message, the XCM executor stops execution with the TooExpensive error. Parameter Description fees Specifies the assets to be removed from the holding register to pay transaction fees. weight_limit Specifies the maximum weight to be purchased to pay execution fees. If you don't specify a limit, the weight is treated as unlimited up to the maximum you specify to be removed from the holding register. The following example illustrates the settings for a BuyExecution instruction: { BuyExecution: { fees: { id: { Concrete: { parents: 0 interior: Here } } fun: { Fungible: 1,000,000 } } weightLimit: { Limited: 1,000,000 } } } The following example illustrates using the instruction in a Rust program: BuyExecution { fees , weight_limit } => { if let Some ( weight ) = Option :: < u64 > :: from ( weight_limit ) { // pay for `weight` using up to `fees` of the holding register. let max_fee = self . holding . try_take ( fees . into ( ) ) . map_err ( | _ | XcmError :: NotHoldingFees ) ? ; let unspent = self . trader . buy_weight ( weight , max_fee ) ? ; self . holding . subsume_assets ( unspent ) ; } Ok ( ( ) ) } , ClaimAsset Creates assets that are being held on behalf of the location identified in the origin register. You must specify the assets parameter to identify the assets to be claimed. The asset you specify must match exactly with the assets available to be claimed by the origin with the given ticket . You must specify the ticket using the MultiLocation type. The claim ticket for the asset is an abstract identifier to help locate the asset to be claimed. Parameter Description assets Specifies the assets to be claimed. ticket Specifies a location to help identify the asset to be claimed. ClearError Clears the error register. You can use this instruction to manually clear the last error from the error register. ClearOrigin Clears the origin register. You can use this instruction to ensure that later instructions cannot take over the authority of the original origin. For example, if you have instructions that are being relayed from an untrusted source, as is often the case with ReserveAssetDeposited, you can use ClearOrigin to prevent the original origin from being used to execute the instruction. The following example appends the ReserveAssetDeposited and ClearOrigin instructions to an existing message: let mut message = vec! [ ReserveAssetDeposited ( assets ) , ClearOrigin ] ; message . extend ( xcm .0 . into_iter ( ) ) ; DepositAsset Subtracts the specified asset from the holding register and deposits on-chain equivalent assets under the ownership of the specified beneficiary . You must specify the assets to remove using the MultiAssetFilter type. Parameter Description assets Specifies the asset to remove from the holding register. max_assets Specifies the maximum number of unique assets or asset instances to remove from the holding register. Only the first max_assets number of unique assets or asset instances that match the assets specified are removed, prioritized under standard asset ordering. If there are any additional unique assets or asset instances, they will remain in the holding register. beneficiary Specifies the new owner of the assets. The following example illustrates a simple message that includes a DepositAsset instruction: ParaA :: execute_with ( | | { let message = Xcm ( vec! [ WithdrawAsset ( ( Here , send_amount ) . into ( ) ) , buy_execution ( ( Here , send_amount ) ) , DepositAsset { assets : All . into ( ) , max_assets : 1 , beneficiary : Parachain ( 2 ) . into ( ) } , ] ) ; // Send withdraw and deposit assert_ok! ( ParachainPalletXcm :: send_xcm ( Here , Parent , message . clone ( ) ) ) ; } ) ; DepositReserveAsset Removes the assets from the holding register and deposits on-chain equivalent assets into the sovereign account under the ownership of the destination. This instruction also sends a follow-up message to destination of the ReserveAssetDeposited instruction with the given effects. Parameter Description assets Specifies the asset to remove from the holding register. max_assets Specifies the maximum number of unique assets or asset instances to remove from the holding register. Only the first max_assets number of unique assets or asset instances that match the assets specified are removed, prioritized under standard asset ordering. If there are any additional unique assets or asset instances, they will remain in the holding register. destination Specifies the location whose sovereign account will own the assets and thus the effective beneficiary for the assets and the notification target for the reserve asset deposit message. xcm Specifies additional instructions to execute on the destination location following the ReserveAssetDeposited instruction. DescendOrigin Changes the origin to some interior location within the context of the current value in the origin register. Parameter Description interior Specifies an interior location to place in the origin register. ExchangeAsset Reduces assets in the holding register up to the amount specified by the give parameter and increases assets in the holding register with a minimum amount of alternative assets specified by the receive parameter. Parameter Description give Specifies the assets to be removed from the holding register. receive Specifies the assets to be increased in the holding register. Any fungible assets specified for the receive parameter can be increased by an amount greater than expressed, but the holding register can't accrue assets not stated in receive. HrmpChannelAccepted Sends a notification message that an open channel request has been accepted by the recipient. After sending this notification, the channel is opened when the relay chain session changes. This message should originate directly on the relay chains and is intended to be sent by the relay chain to a parachain. Parameter Description recipient Specifies the parachain identifier for the recipient parachain that has accepted the previous open-channel request. HrmpChannelClosing Sends a message to notify a recipient that the sender who initiated the request to open a channel has decided to close the channel. After sending this notification, the channel is closed when the relay chain session changes. This message should originate directly on the relay chains and is intended to be sent by the relay chain to a parachain. Parameter Description initiator Specifies the parachain identifier for the parachain that initiated the channel close operation. sender Specifies the parachain identifier of the sender side of the channel being closed. recipient Specifies the parachain identifier of the receiver side of the channel being closed. HrmpNewChannelOpenRequest Sends a request from a parachain to the relay chain to open a new channel for communication with another parachain. Messages passed using the HRMP transport protocol are always routed through a relay chain. This message should originate directly on the relay chains and is intended to be sent by the relay chain to a parachain. Parameter Description sender Specifies the sender in the to-be opened channel. Also, the initiator of the channel opening. max_message_size Specifies the maximum size of a message proposed by the sender. max_capacity Specifies the maximum number of messages that can be queued in the channel. InitiateReserveWithdraw Reduces the value of the holding register by the assets and sends XCM instructions beginning with WithdrawAsset to a reserve location. Parameter Description assets Specifies the assets to remove from the holding register. reserve Specifies a valid location that acts as a reserve for all specified assets. The sovereign account of this consensus system on the reserve location will have appropriate assets withdrawn and effects will be executed on them. There will typically be only one valid location on any given asset/chain combination. xcm Specifies additional instructions to execute on the assets after they are withdrawn on the reserve location. InitiateTeleport Removes the assets from the holding register and sends a message beginning with ReceiveTeleportedAsset instruction to a specified destination location. NOTE: The destination location MUST respect the origin for the instruction as a valid teleportation origin for all assets. If it does not, then the assets may be lost. Parameter Description assets Specifies the assets to remove from the holding register. destination Specifies a valid location that respects teleports coming from this location. xcm Specifies additional instructions to execute on the destination location following the ReceiveTeleportedAsset instruction. QueryHolding Sends a QueryResponse message with the assets value equal to the holding contents, or a portion thereof. Parameter Description query_id Specifies the identifier for the query that is used for the query_id field of the QueryResponse message. destination Specifies the location to where the QueryResponse message should be sent. assets Specifies a filter for the assets that should be reported back. max_response_weight Specifies the value to be used for the max_weight field of the QueryResponse message. The following example illustrates a QueryHolding instruction that returns a QueryResponse with the identifier query_id_set : ParaA :: execute_with ( | | { let message = Xcm ( vec! [ QueryHolding { query_id : query_id_set , dest : Parachain ( 1 ) . into ( ) , assets : All . into ( ) , max_response_weight : 1_000_000_000 , } , ] ) ; } ) ; QueryResponse Provides expected information from the origin. Parameter Description query_id Specifies the identifier for the query that resulted in this message being sent. | response : Expresses the content of the message that resulted from the query instruction. | max_weight | Specifies the maximum weight that handling this response should take. If proper execution requires more weight than you specify, an error is returned. If the execution requires less than you specify, the difference might be added to the surplus weight register at run time. The Response type is used to express information content in the QueryResponse XCM instruction. Depending on the query, it can represent the following different data types: Null Assets { assets: MultiAssets } ExecutionResult { result: Result<(), (u32, Error)> } Version { version: Compact } The following example illustrates checking that a QueryResponse message was received and the information in the response is a newly deposited asset: ParaA :: execute_with ( | | { assert_eq! ( parachain :: MsgQueue :: received_dmp ( ) , vec! [ Xcm ( vec! [ QueryResponse { query_id : query_id_set , response : Response :: Assets ( MultiAssets :: new ( ) ) , max_weight : 1_000_000_000 , } ] ) ] , ) ; } ) ; ReceiveTeleportedAsset Accrues assets into the holding register equivalent to the assets removed from the current origin. The origin must be trusted to have removed the assets as a consequence of sending this message. Parameter Description assets Specifies the assets that have been removed from the origin. RefundSurplus Increases the refunded weight register to the value of the surplus weight register. This instruction enables fees previously paid using the BuyExecution instruction to be moved into holding register to match the amount added to the refunded weight register. ReportError Reports the contents of the error register to the specified destination using XCM. A QueryResponse message of type ExecutionOutcome is sent to the destination with the given query_id and the outcome of the XCM. Parameter Description query_id Specifies the value to use for the query_id field of the QueryResponse message. destination Specifies the location to where the QueryResponse message should be sent. max_response_weight Specifies the value to be used for the max_weight field of the QueryResponse message. ReserveAssetDeposited Adds derivative assets to the holding register to represent the assets received from the value in the origin register. The origin must be trusted to act as a reserve for the assets. Parameter Description assets Specifies the assets that have been received into the sovereign account of the local consensus system on from ‚Ä†he origin. SetAppendix Sets the appendix register. The appendix register provides any code that should run after the current program completes execution. After the current program ends successfully, or after an error where the Error Handler is empty, the appendix register is cleared and its contents are used to replace the programme register. The estimated weight of this instruction must include the estimated weight of the appendix code. At run-time,the surplus weight register should be increased by the estimated weight of the appendix prior to being changed. Parameter Description appendix : Xcm: The value to which to set the Appendix Register. SetErrorHandler Sets the error handler register. The error handler register provides any code that should run if the program encounters an error. After a program encounters an error, this register is cleared and its contents are used to replace the programme register. The estimated weight of this instruction must include the estimated weight of the error_handler code. At run-time, the surplus weight register should be increased by the estimated weight of the error handler prior to being changed. Parameter Description error_handler Specifies the value to set in the error handler register. SubscribeVersion Sends a QueryResponse message to Origin specifying XCM version in the response field. Any upgrades to the local consensus that result in a later version of XCM being supported should elicit a similar response. Parameter Description query_id Specifies the value to be used for the query_id field of the QueryResponse message. max_response_weight Specifies the value to be used for the max_weight field of the QueryResponse message. Transact Dispatches the encoded call using the dispatch-origin for the call as expressed by the context you specify using the origin_type parameter. Parameter Description origin_type Specifies a context for expressing the message origin as a dispatch origin. max_weight Specifies the maximum amount of weight to expend while dispatching the encoded call. If the dispatch requires more weight than you specify, execution stops and an error is returned. If the dispatch requires less than you specify, the difference might be added to the surplus weight register at run time. call Specifies the encoded transaction to execute on the receiving system. TransferAsset Withdraws assets from the ownership of Origin and deposit equivalent assets under the ownership of beneficiary. Parameter Description assets Specifies the asset to be transferred. beneficiary Specifies the new owner for the assets. TransferReserveAsset Withdraws assets from the ownership of the current origin and deposits equivalent assets under the ownership of the sovereign account for the destination. This instruction also sends an additional message with ReserveAssetDeposited and any instructions specified with the xcm parameter to the specified destination. Parameter Description assets Specifies the asset to be transferred. destination Specifies the location whose sovereign account will own the assets and thus the effective beneficiary for the assets and the notification target for the reserve asset deposit message. xcm Specifies the instructions that should follow the ReserveAssetDeposited instruction, which is sent onwards to destination. The following example illustrates using the TransferReserveAsset instruction that contains a message with two additional instructions: let mut message = Xcm ( vec! [ TransferReserveAsset { assets , dest , xcm : Xcm ( vec! [ BuyExecution { fees , weight_limit : Unlimited } , DepositAsset { assets : Wild ( All ) , max_assets , beneficiary } , ] ) , } ] ) ; Trap Throws an error of type Trap. Parameter Description id Specifies the value to be used for the parameter of the thrown error. UnsubscribeVersion Cancels the effect of a previous SubscribeVersion instruction from Origin. WithdrawAsset Removes the specified assets from the sending account on the local consensus system and adds them to the value in the holding register. Parameter Description assets Specifies the asset to remove from the sender. The asset must be owned by the must be owned by the account in the origin register. Registers Most XCVM registers can't be modified directly. They are set to specific values to start and are only be manipulated by specific instructions, under certain circumstances, or according to certain rules. The XCVM includes the following registers: Register Description Origin Stores the location for the authority the current program is running under. Holding Stores the number of assets that exist under the control of the XCVM but have no on-chain representation. Surplus weight Stores the overestimation of weight previously calculated. Refunded weight Stores the portion of surplus weight that has been refunded. Programme Stores the set of XCM instructions currently executing. This register holds the complete message‚Äîthe program‚Äîin cross-consensus virtual machine. Programme counter Stores the instruction index for currently executing instruction. The value is incremented by one at the end of every successfully executed instruction. The register is reset to zero when the Programme register changes. Error Stores information about the last known error during program execution. Error handler Stores code that should run if there's an error. Appendix Stores code that should run after the current program ends. Origins In some cases, you might want to manipulate the origin stored in the origin register to grant more or fewer permissions for performing specific operations or executing specific XCM instructions. You can use the following origin types to manipulate how the origin for XCM instructions is interpreted. Origin type Description Native Use the native dispatch origin representation for the sender in the local runtime framework. For most chains, this is the Parachain or Relay origin if coming from a chain. SovereignAccount Use the sovereign account of the sender. For most chains, this is the Signed origin. Superuser Use the superuser account. Normal account can't use this origin and in many cases it isn't available for any account to use. For most chains, this is the Root origin. Xcm Use the XCM native origin and the MultiLocation encoded directly in the dispatch origin unchanged. For most chains, this is the pallet_xcm::Origin::Xcm type. The implications of using different origins depend on the code you are calling. Depending on your chain logic and permissions, you might not be able to generate the origin you want to use. For example, most users can't generate the Superuser origin, but it's possible for the chain logic to do so. The following example illustrates specifying the origin type in a Transact instruction: Transact { origin_type: OriginKind::SovereignAccount, require_weight_at_most: weight, call: call.encode().into(), }, For additional examples of converting origins, see the prigin_conversion module. Errors If the program executing XCM instructions encounters a problem, it stops execution and identifies the type of problem encountered with one of the following errors: Error type Description Overflow = 0 An instruction caused an arithmetic overflow. Unimplemented = 1 The instruction is intentionally unsupported. UntrustedReserveLocation = 2 The origin register doesn't contain a valid value for a reserve transfer notification. UntrustedTeleportLocation = 3 The origin register doesn't contain a valid value for a teleport notification. MultiLocationFull = 4 The MultiLocation value too large to descend further. MultiLocationNotInvertible = 5 The MultiLocation value ascends more parents than the known number of ancestors of the local location. BadOrigin = 6 The origin register doesn't contain a valid value for executing the instruction. InvalidLocation = 7 The location parameter is not a valid value for the instruction. AssetNotFound = 8 The specified asset can't be found or isn't valid in the location specified for the instruction. FailedToTransactAsset = 9 An asset transaction‚Äîfor example, an instruction to withdraw or deposit an asset‚Äîfailed. In most cases, this type of error is caused by problems with type conversions. NotWithdrawable = 10 The specified asset can't be withdrawn, potentially due to lack of ownership, asset availability, or permissions. LocationCannotHold = 11 The specified asset can't be deposited under the ownership of a particular location. ExceedsMaxMessageSize = 12 There was an attempt to send a message that exceeded the maximum message size supported by the transport protocol. DestinationUnsupported = 13 There was an attempt to send a message that can't be translated into a format supported by the destination. Transport = 14 The destination is routable, but there's an issue with the transport mechanism. Unroutable = 15 The destination is known to be unroutable. UnknownClaim = 16 The claim specified for the ClaimAsset instruction isn't recognized or can't be found. FailedToDecode = 17 The function specified for the Transact instruction can't be decoded. TooMuchWeightRequired = 18 The function specified for the Transact instruction might exceed the weight limit allowed. NotHoldingFees = 19 The holding register doesn't contain any payable fees for the BuyExecution instruction to use. TooExpensive = 20 The fees declared to purchase weight in the BuyExecution instruction are insufficient. Trap(u64) = 21 Used by the Trap instruction to force an error intentionally. Its code is included. ExpectationFalse = 22 Used by ExpectAsset, ExpectError, and ExpectOrigin if the expectation was not true. Concrete identifiers uniquely identify a single asset through its location in a consensus system relative to the context where an instruction is executed. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Instructions Registers Origins Errors Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
How-to quick reference guides How to guides How-to quick reference guides Substrate How-to quick reference guides provide instructions for achieving specific goals. Each guide explains how to perform a specific task with the assumption that you are already familiar with Substrate and programming in Rust. Basics See the following guides to learn common patterns in runtime development. Import a pallet Configure runtime constants Configure genesis state Customize a chain specification Use helper functions Pallet design See the following guides for best practices on building pallets using FRAME. Implement lockable currency Incorporate randomness Configure crowdfunding Create a storage structure (struct) Use tight pallet coupling Use loose pallet coupling Weights See the following guides for help with benchmarking and weight configurations. Calculate fees Add benchmarks Use custom weights Use conditional weights Testing See the following guides for help with testing pallets and runtime logic. Set up basic tests Test a transfer function Storage migration See the following guides for help with a storage migration. Basic storage migration Trigger migration Consensus models See the following guides to implement consensus mechanisms in your runtimes. Create a hybrid node Add proof-of-work consensus Parachains See the following guides for help working with Substrate parachains. Convert a solo chain Connect to a relay chain Select collators Prepare to launch Upgrade a parachain Auctions and crowdloans Add HRMP channels Tools See the following guides for add-on tools that help you manage Substrate chains in production. Use try-runtime Create a txwrapper for a chain Use REST endpoints to get chain data Verify a Wasm binary Offchain workers See the following guides for help working with offchain data. Make offchain HTTP requests Offchain local storage Offchain indexing Last edit: February 27, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Basics Pallet design Weights Testing Storage migration Consensus models Parachains Tools Offchain workers Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
{"payload":{"allShortcutsEnabled":false,"fileTree":{"":{"items":[{"name":".github","path":".github","contentType":"directory"},{"name":".editorconfig","path":".editorconfig","contentType":"file"},{"name":"LICENSE","path":"LICENSE","contentType":"file"},{"name":"README.md","path":"README.md","contentType":"file"},{"name":"contributing.md","path":"contributing.md","contentType":"file"}],"totalCount":5}},"fileTreeProcessingTime":1.5396640000000001,"foldersToFetch":[],"reducedMotionEnabled":null,"repo":{"id":178537578,"defaultBranch":"master","name":"awesome-substrate","ownerLogin":"substrate-developer-hub","currentUserCanPush":false,"isFork":false,"isEmpty":false,"createdAt":"2019-03-30T09:20:07.000Z","ownerAvatar":"https://avatars.githubusercontent.com/u/47530779?v=4","public":true,"private":false,"isOrgOwned":true},"refInfo":{"name":"master","listCacheKey":"v0:1674589692.221209","canEdit":false,"refType":"branch","currentOid":"408938183b85cc873b12e949172c1603f145124e"},"path":"README.md","currentUser":null,"blob":{"rawLines":null,"stylingDirectives":null,"csv":null,"csvError":null,"dependabotInfo":{"showConfigurationBanner":false,"configFilePath":null,"networkDependabotPath":"/substrate-developer-hub/awesome-substrate/network/updates","dismissConfigurationNoticePath":"/settings/dismiss-notice/dependabot_configuration_notice","configurationNoticeDismissed":null,"repoAlertsPath":"/substrate-developer-hub/awesome-substrate/security/dependabot","repoSecurityAndAnalysisPath":"/substrate-developer-hub/awesome-substrate/settings/security_analysis","repoOwnerIsOrg":true,"currentUserCanAdminRepo":false},"displayName":"README.md","displayUrl":"https://github.com/substrate-developer-hub/awesome-substrate/blob/master/README.md?raw=true","headerInfo":{"blobSize":"23.2 KB","deleteInfo":{"deletePath":null,"deleteTooltip":"You must be signed in to make or propose changes"},"editInfo":{"editTooltip":"You must be signed in to make or propose changes"},"ghDesktopPath":"https://desktop.github.com","gitLfsPath":null,"onBranch":true,"shortPath":"41e58dd","siteNavLoginPath":"/login?return_to=https%3A%2F%2Fgithub.com%2Fsubstrate-developer-hub%2Fawesome-substrate%2Fblob%2Fmaster%2FREADME.md","isCSV":false,"isRichtext":true,"toc":[{"level":1,"text":"Awesome Substrate ","anchor":"awesome-substrate-","htmlText":"Awesome Substrate "},{"level":2,"text":"Contents","anchor":"contents","htmlText":"Contents"},{"level":2,"text":"Resources","anchor":"resources","htmlText":"Resources"},{"level":2,"text":"Support","anchor":"support","htmlText":"Support"},{"level":2,"text":"Social","anchor":"social","htmlText":"Social"},{"level":2,"text":"Events","anchor":"events","htmlText":"Events"},{"level":2,"text":"Blogs","anchor":"blogs","htmlText":"Blogs"},{"level":2,"text":"Videos","anchor":"videos","htmlText":"Videos"},{"level":2,"text":"Templates","anchor":"templates","htmlText":"Templates"},{"level":2,"text":"FRAME Pallets","anchor":"frame-pallets","htmlText":"FRAME Pallets"},{"level":2,"text":"Framework Extensions","anchor":"framework-extensions","htmlText":"Framework Extensions"},{"level":2,"text":"Client Libraries","anchor":"client-libraries","htmlText":"Client Libraries"},{"level":2,"text":"Mobile","anchor":"mobile","htmlText":"Mobile"},{"level":2,"text":"Tools","anchor":"tools","htmlText":"Tools"},{"level":2,"text":"Products and Services","anchor":"products-and-services","htmlText":"Products and Services"},{"level":2,"text":"Alternative Implementations","anchor":"alternative-implementations","htmlText":"Alternative Implementations"},{"level":2,"text":"SCALE Codec","anchor":"scale-codec","htmlText":"SCALE Codec"}],"lineInfo":{"truncatedLoc":"240","truncatedSloc":"206"},"mode":"file"},"image":false,"isCodeownersFile":null,"isValidLegacyIssueTemplate":false,"issueTemplateHelpUrl":"https://docs.github.com/articles/about-issue-and-pull-request-templates","issueTemplate":null,"discussionTemplate":null,"language":"Markdown","large":false,"loggedIn":false,"newDiscussionPath":"/substrate-developer-hub/awesome-substrate/discussions/new","newIssuePath":"/substrate-developer-hub/awesome-substrate/issues/new","planSupportInfo":{"repoIsFork":null,"repoOwnedByCurrentUser":null,"requestFullPath":"/substrate-developer-hub/awesome-substrate/blob/master/README.md","showFreeOrgGatedFeatureMessage":null,"showPlanSupportBanner":null,"upgradeDataAttributes":null,"upgradePath":null},"publishBannersInfo":{"dismissActionNoticePath":"/settings/dismiss-notice/publish_action_from_dockerfile","dismissStackNoticePath":"/settings/dismiss-notice/publish_stack_from_file","releasePath":"/substrate-developer-hub/awesome-substrate/releases/new?marketplace=true","showPublishActionBanner":false,"showPublishStackBanner":false},"renderImageOrRaw":false,"richText":" Awesome Substrate \n \n An awesome list is a list of awesome things curated by the Substrate community. \n \n Substrate is a framework for building upgradable, modular and efficient\nblockchains. Substrate is an open-source library of Rust code that is\nmaintained by Parity Technologies . Source code available on\n GitHub . \n Contents \n \n Resources \n Support \n Social \n Events \n Blogs \n Videos \n Templates \n FRAME Pallets \n Framework Extensions \n Client Libraries \n Mobile \n Tools \n Products and Services \n Alternative Implementations \n SCALE Codec \n \n Resources \n \n DotJobs - A job board for the Substrate and Polkadot ecosystem projects, maintained by Stateless.Money . \n Developer Hub GitHub - Substrate Developer Hub repositories. \n Ecosystem Projects - Projects and teams building with Substrate. \n Polkadot Stack - An awesome list maintained by our friends at Web3 Foundation . \n Official Homepage - Vision, ecosystem, opportunities, and much more.\n \n Docs - Developer documentation. \n Tutorials - Guided exercises to get you started. \n How-to guides - Workflows outlined to achieve a specific goal. \n Reference Docs - Versioned API documentation. \n \n \n Technical Papers\n \n Polkadot Lightpaper \n Polkadot: Vision for a heterogeneous multi-chain framework \n Overview of Polkadot and its Design Considerations \n \n Chinese Translation (by community) \n \n \n \n \n \n Support \n \n Builders Program - White-glove solutions and dedicated support team for visionary teams using Substrate. \n Stack Exchange - The best place for all technical questions. \n Web3 Foundation Grants - Funding for ecosystem development. \n Polkadot Treasury - The Treasury funds are allocated through the voting on spending proposal. \n \n Social \n \n Substrate Developers Chat (Telegram) - Chat with other Substrate developers, also bridged to matrix . \n Twitter - Follow us to stay up-to-date. \n Polkaverse - A decentralized news-feed style social platform for the Polkadot community to discuss, share knowledge, post ecosystem updates and interact with posts. Built on top of Subsocial . \n \n Events \n \n Sub0 Developer Conference - Semiannual, online and in-person for all\nthings Substrate. \n Substrate Seminar - Bi-weekly\ncollaborative learning sessions. \n \n Blogs \n \n DotLeap - Polkadot and Substrate Community blog and newsletter. \n Official - Published by Parity. \n \n Videos \n \n Parity YouTube \n \n Substrate Seminar (YouTube Archive) \n Sub0 Conference Nov. 2022 \n Sub0 Conference Oct. 2020 \n Sub0 Conference Dec. 2019 \n Sub0 Conference Apr. 2019 \n \n \n Polkadot Network Technical Explainers \n Substrate Seminar Twitch - Biweekly stream hosted by Polkadot Developers Twitch.\n \n Old Seminar Crowdcast - Seminar archive. \n Older Seminar Crowdcast - Older still Seminar archive. \n \n \n Substrate: A Rustic Vision for Polkadot by Gavin Wood at Web3 Summit 2018 \n \n Templates \n \n Base - Minimal FRAME-based\nnode, derived from upstream . \n Frontier - Fronter enabled EVM and Ethereum RPC compatible Substrate node, ready for hacking. \n Front-End - Polkadot-JS API and React app to build front-ends for Substrate-based chains. \n Parachain - Cumulus enabled Substrate node, derived from upstream . \n substrate-stencil - A template for a Substrate node that includes staking and governance capabilities. \n polkadot-js-api-ts-template - A template project to kickstart hacking on top of @polkadot/api . \n INK!athon - Full-stack dApp boilerplate with ink! smart contracts and a React frontend using the useInkathon (listed below) hooks library maintained by Scio Labs . \n Subsocial Starter Kit - A starter-kit for building web3 social apps for the Polkadot ecosystem, powered by the Subsocial blockchain . \n \n FRAME Pallets \n \n Chainlink Feed Pallet - Chainlink feed token interface. \n Official in Substrate - Large collection, Parity maintained. \n Open Runtime Module Library (ORML) - Community maintained collection of Substrate runtime modules. \n Sunshine Bounty - Distributed autonomous organization (DAO) for administering a bounty program. \n Sunshine Identity - Keybase-inspired identity management. \n Sunshine Faucet - Dispense resources for a development chain. \n RMRK Pallets - Nested, conditional & Multi-resourced NFTs. \n \n Framework Extensions \n \n Bridges - A collection of tools for cross-chain communication. \n Cumulus - A set of tools for writing Substrate-based Polkadot parachains. \n FRAME - A system for building Substrate runtimes. \n Frontier - End-to-end Ethereum emulation for Substrate chains. \n ink! - Rust smart contract language for Substrate chains. \n IntegriTEE - Trusted off-chain execution framework that uses Intel SGX trusted execution environments. \n Polkadot-JS - Rich JavaScript API framework for front-end development. \n \n Client Libraries \n \n .Net API - Maintained by Usetech . \n .NET Substrate API - Used in nuget , and Unity Example ; Maintained by Ajuna Network . \n .NET Toolchain/SDK - Toolchain for Substrate .NET. Pre-generated SubstrateNET ; Maintained by Ajuna Network. \n go-substrate-gen - Generate Go (de)serialization/client code from Substrate metadata. \n sube - Lightweight Rust client library and CLI with support for type information. \n subxt - Official Rust client. \n C++ API - Maintained by Usetech. \n Go RPC Client - Maintained by Centrifuge . \n Kotlin Client - Maintained by Nodle.io . \n Polkadot-JS API - Semi-official JavaScript library for Substrate-based chains. \n Python Interface - Maintained by Polkascan Foundation . \n Rust API Client - Rust client maintained by Supercomputers Systems AG . \n Subscan Go Utilities - SS58 and more, developed by Subscan. \n Sub-Api - Friendly wrapper for Polkadot.js API maintained by KodaDot. \n useInkathon - Typesafe React Hooks library abstracting functionality by polkadot.js for working with Substrate-based networks and ink! smart contracts maintained by Scio Labs. \n Subsocial JS SDK - A JS SDK for developers to build web3 social apps on top of Subsocial. \n \n Mobile \n \n Fearless Utils Android - Android Substrate tools. \n Fearless Utils iOS - iOS Substrate tools. \n Nova Substrate SDK Android - Substrate SDK and tools for Android. \n Nova Substrate SDK iOS - Substrate SDK and tools for iOS. \n Polkadot-Dart - Dart Substrate API. \n PolkaWallet SDK - Flutter SDK for Substrate-based App. \n React-Native-Substrate-Sign - Rust library for React Native. \n \n Tools \n \n offline-election - Tool to predict nominated proof-of-stake elections. \n offchain::ipfs - Substrate infused with IPFS . \n polkadot-js-bundle - A standalone JS bundle that contains Polkadot{JS} libraries. \n polkadot-launch - Simple CLI tool to launch a local Polkadot test network. \n polkadot-runtime-prom-exporter - A Prometheus exporter for Polkadot runtime metrics (modifiable for Substrate use). \n polkadot-scripts - A collection of scripts Parity uses to diagnose Polkadot/Kusama. \n polkadot-starship - Another tool to launch a local Polkadot test network, with emphasis on the ability to run big testnets. \n srtool-actions - GitHub actions to easily use the srtool Docker image to build your own runtime. \n srtool-cli - CLI frontend for the srtool Docker image. \n srtool - Docker image to deterministically build a runtime. \n subsee - CLI to inspect metadata of a Substrate node as JSON. \n subalfred - An all-in-one Substrate development toolbox. \n substrate-balance-calculator - Breakdown the balances of your Substrate account. \n substrate-balance-graph - Create a graph of the token balance over time of a Substrate address. \n substrate-graph-benchmarks - Graph the benchmark output of FRAME pallets. \n substrate-js-utils - A set of useful JavaScript utilities for Substrate that uses the Polkadot{JS} API; Also deployed as a website . \n substrate-society - A basic front-end for the FRAME Society pallet. \n substrate-toml-lint - A toml parser and checker to avoid common errors in Substrate projects. \n subwasm - CLI to inspect a runtime WASM blob offline. It shows information, metadata and can compare runtimes. It can also help you fetch a runtime directly from a node. \n sup - Command line tool for generating or upgrading a Substrate node. \n scale-value - Analogous to serde_json but for SCALE; Library to decode arbitrary SCALE encoded bytes into a dynamic Value given type info from scale-info . \n scale-decode - Decode SCALE bytes into arbitrary custom types by implementing a visitor trait. \n Aleph.im - Scalable, decentralized database, file storage, and computation services for Substrate chains and more. \n Archive - Indexing engine for Substrate chains. \n Dev Hub Utils - Unofficial utilities for working with official Substrate Developer Hub resources. \n Europa - A sandbox for the Substrate runtime execution environment. \n Fork Off Substrate - Script to help bootstrap a new chain with the state of a running chain. \n fudge - Core lib for accessing and (arbitrarily) manipulating substrate databases, including the building and importing of local blocks. \n Gantree Library - A suite of technologies for managing Substrate-powered parachain networks via rapid spin-up & tear-down. \n Halva - A toolchain for improving the experience of developing on Substrate. \n Hydra - A GraphQL framework for Substrate nodes. \n Jupiter - Testnet for smart contracts written for the FRAME Contracts pallet and ink!. \n Megaclite - Zero-knowledge tools for the Polkadot ecosystem. \n Metadata Portal - A self-hosted webpage that shows the latest metadata and chain specs for any given network. \n Minimark - Implementation of RMRK NFT v1/v2 protocol maintained by KodaDot. \n Nova Polkadot Utils - Contains static info & metadata to support client apps in Polkadot ecosystem to map it to various netowrks. \n Parity Signer - Upcycle an unused mobile phone into an air-gapped hardware wallet. \n Polkadot PANIC - Monitoring and alerting solution for Polkadot nodes by Simply VC, compatible with many Substrate chains. \n Polkadot Tool Index - List of tools available for your development with Polkadot and any Substrate chain including Block Explorers, Wallets, Network Monitoring & Reporting, Clients, Benchmarking, Fuzzing, Forking, SCALE Codec, CLI Tools and much more. \n Polkadot-JS Apps UI - Semi-official block explorer & front-end for Substrate-based chains. \n Polkadot-JS Extension - Browser extension for interacting with Substrate-based chains. \n Polkascan - Multi-chain block explorer maintained by Polkascan Foundation. \n Proxy Hot Wallet Demo - A demonstration of a secure, convenient, and flexible hot wallet architecture built on Substrate primitives. \n Redspot - A Truffle -like toolkit for smart contracts for the FRAME Contracts pallet and ink!. \n Sidecar - REST service that runs alongside Substrate nodes. \n SS58 Transform - Display key's addressees with all SS58 prefixes. \n Staking Rewards Collector - A script to parse and output staking rewards for a given Kusama or Polkadot address and cross-reference them with daily price data. \n Subkey - Command line utility for working with cryptographic keys. \n SubQuery - A GraphQL indexer and query service that allows users to easily create indexed data sources and host them online for free.\n \n Nova SubQuery API - A SubQuery API implementation for operation history and staking analytics. \n \n \n Subscan - Multi-network explorer for Substrate-based chains. \n Subsquid - An indexing framework (SDK + infrastructure) to quickly and easily turn Substrate and EVM on-chain data into APIs and host them. \n Substate - 100% no-std/wasm compatible Substrate storage key generator library for Rust. \n Substrate debug-kit - A collection of tools and libraries for debugging Substrate-based chains. \n Substrate Docker Builders - A set of Dockerfiles and GitHub Actions to auto-build and push a Docker image for Substrate-based chains. \n Substrate Faucet Bot - Python-based faucet for development purposes. \n Substrate Graph - GraphQL indexer for Substrate-based chains. \n Typechain-Polkadot - Hepls users to generate typescript types from contract ABIs (ink!) and generate runtime code to interact with contracts and deploy them. \n TxWrapper - Helpful library for offline transaction creation. \n VSCode Substrate - Plugin for Visual Studio Code. \n Polkaholic.io - Multi-chain block explorer with API and DeFi support across 40+ parachains. \n SubID - An advanced cross-chain portfolio management tool for the Polkadot ecosystem, allowing any user to see their balances across chains; view their crowdloan history; view their NFTs across Polkadot ecosystem chains; claim their vested tokens and perform cross-chain transfers. \n Subsocial SDK Playground - Subsocial JS SDK Playground allows you to fetch spaces, send transactions on blockchain and test the SDK code snippets on the go without the need to download or setup anything locally. \n \n Products and Services \n \n OnFinality - Free and paid services to shared Substrate based nodes. \n PrivHost - Public Tor .onion supported nodes for Polkadot, Kusama and Edgeware. \n Substrate Devops Guide - Parity devops team's configuration and guidance on deploying, monitoring, and maintaining node infrastructure. \n \n Alternative Implementations \n \n Gossamer - A Polkadot client implemented in Go; from ChainSafe . \n Kagome - A C++17 implementation of the Polkadot client; from Soramitsu . \n LimeChain AssemblyScript Runtime - An account-based Substrate proof-of-concept runtime written in AssemblyScript; from LimeChain . \n \n SCALE Codec \n \n AssemblyScript - Maintained by LimeChain. \n C - Maintained by Matthew Darnell. \n C++ - Maintained by Soramitsu. \n Codec Definition - Official codec documentation. \n Go - Maintained by Itering . \n Haskell - Maintained by Robonomics Network . \n Java - Maintained by Emerald . \n Parity SCALE Codec - Reference implementation written in Rust. \n Python - Maintained by Polkascan Foundation. \n Ruby - Maintained by Itering. \n Scales - Serializing SCALE using type information from a type registry. \n JavaScript / TypeScript implementations:\n \n paritytech/parity-scale-codec-ts - Maintained by Parity Technologies. \n polkadot-js/api - Maintained by Polkadot-JS. \n scale-ts - Maintained by Josep M Sobrepere. \n soramitsu/scale-codec-js-library - Maintained by Soramitsu. \n \n \n \n ","renderedFileInfo":null,"tabSize":2,"topBannersInfo":{"overridingGlobalFundingFile":false,"globalPreferredFundingPath":null,"repoOwner":"substrate-developer-hub","repoName":"awesome-substrate","showInvalidCitationWarning":false,"citationHelpUrl":"https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/about-citation-files","showDependabotConfigurationBanner":false,"actionsOnboardingTip":null},"truncated":false,"viewable":true,"workflowRedirectUrl":null,"symbols":{"timedOut":false,"notAnalyzed":true,"symbols":[]}},"copilotUserAccess":null,"csrf_tokens":{"/substrate-developer-hub/awesome-substrate/branches":{"post":"Aeoh1-r_jnHCBF1CxBBIWjW1yNSxwQ4nVqJ2K2ghyyWb7iJNq_32xgbqeJqIW6X3H5K-10vo4R8jctMD29VwNQ"}}},"title":"awesome-substrate/README.md at master ¬∑ substrate-developer-hub/awesome-substrate","locale":"en"}
Content style guide Content style guide Content style guide This guide focuses on best practices for writing technical documentation and on the style conventions to use when developing documentation for Parity Technologies products and audiences. The goal of this guide is to help members of the documentation team and any one interested in contributing to documentation write material that is clear, concise, and consistent. This guide also includes peculiarities to working with this repo specifically for active contributors to be aware of available in the Builder notes section. If you can't find the answer to a style, voice, or terminology question in this guide, consult the following resources: Google developer documentation style guide Microsoft Style Guide Chicago Manual of Style Merriam-Webster Dictionary If you can't the answer in any of these resources, open an issue. General guidance for writing engaging content There are three keys to writing content that engages the audience: Use the second person point of view to directly address the reader. Use an active voice and present tense whenever possible. Use a conversational tone that is not too formal or too chummy. Point of view In most cases, address the reader directly. For tutorials, use either first person plural‚Äîwe, us, our, ours‚Äîor second person point of view. Because tutorials provide a more guided approach to a topic, using the first person plural is a more natural and commonly-accepted practice than in other types of documentation. Use the first person point of view sparingly and with intention. When overused, the first person narrative can overwhelm the sense of a shared experience and obscure the reader‚Äôs journey. Do not use ‚ÄúI‚Äù or ‚Äúme‚Äù unless it appears in the text of a user interface element. Do not use ‚Äúwe‚Äù to refer to Parity or the Substrate Developer Hub team. For example, if you are documenting a recommended setting or practice, use ‚ÄúParity Technologies recommends....‚Äù not ‚ÄúWe recommend...‚Äù. Passive constructions In spite of the axiom to never use the passive voice, there are situations where a passive sentence structure might be appropriate. Don‚Äôt twist a sentence into knots just to avoid a passive construction. Passive voice does have its place, but be wary of using it. When writing about software, it‚Äôs often tempting to describe what‚Äôs happening from the code point of view. However, there‚Äôs almost always a human being with a goal or task to complete who is initiating the activity that the software is executing. If you keep this human presence in mind, your writing will be more dynamic, easier to follow, and more interesting to read. Contractions and conversational tone Contractions are generally acceptable because they give documentation a more natural conversational tone‚Äîat least for English speakers. Be conscious of when and why you use contractions. To keep the tone conversational but concise, adhere to the following common-sense guidelines: Use common, well-known words whenever possible. Don‚Äôt use flowery language or literary flourish words and phrases like ‚Äúand so forth‚Äù, ‚Äúalbeit‚Äù, ‚Äúheretofore‚Äù, or ‚Äúthus‚Äù. Try to be precise in word choice. For example: Don‚Äôt use ‚Äúwhen‚Äù‚Äîimplying eventuality and time‚Äîas interchangeable with ‚Äúif‚Äù, which implies the possibility of something happening. Don‚Äôt use phrases that introduce ambiguity. For example, instead of ‚ÄúWhen the process completes...‚Äù use ‚ÄúAfter the process completes...‚Äù. Think carefully about word choices like using ‚Äúsince‚Äù (implying a period of time) instead of ‚Äúbecause‚Äù (implying cause and result) or using ‚Äúonce‚Äù (single occurrence) instead of ‚Äúafter‚Äù (each time). Avoid using dead language words and phrases even if they are generally accepted as English words in practice. For example: Instead of ‚Äúi.e.‚Äù, use ‚Äúthat is‚Äù or rewrite the sentence to make the meaning clear without needing extra qualification. Instead of ‚Äúe.g.‚Äù, use ‚Äúfor example‚Äù. Instead of ‚Äúvia‚Äù, use an appropriate English substitute such as ‚Äúby‚Äù, ‚Äúthrough‚Äù, or ‚Äúusing‚Äù. Instead of ‚Äúetc.‚Äù, use ‚Äúand so on‚Äù or revise the content to make the term unnecessary. For example, revise to use such as or like followed by an example or two. Instead of ‚Äúcaveat‚Äù, use an appropriate English substitute such as ‚Äúnotice‚Äù, ‚Äúcaution‚Äù, or ‚Äúwarning‚Äù. Avoid adding unnecessary words or phrases. For example: Instead of ‚ÄúIn order to‚Äù, just use ‚Äúto‚Äù. Instead of ‚Äúas well as‚Äù, just use ‚Äúand‚Äù. Instead of ‚Äúand then‚Äù, just use ‚Äúthen‚Äù. Avoid jargon, colloquialisms, and idiomatic phrases. Avoid adverbs and subjective statements. For example, don‚Äôt use words and phrases that include easily, rapidly, simply, quickly. Experienced developers who truly prefer to skip the tutorial... We can quickly test if this code is functioning as expected... Headings All heading levels should use the following conventions: Use sentence style case. Use active, present tense verbs in headings wherever appropriate, especially in the context of tutorials and how-to guides. Serve as a summary of the content they contain. Avoid generic headings like overview and introduction, if possible. While generic heading can be conceptually useful, they add no value to the content or the navigational experience. Always contain content. A heading should never be immediately followed by another heading. As a best practice, avoid using headings strictly for navigation. Avoid using code, proper names, product-specific jargon, and backticks in headings. Keep headings as succinct as possible while providing meaningful clues about the content. Limit heading levels As a best practice, avoid building an information hierarchy with more than three heading levels. Most content can be effectively organized using two internal heading levels, making it easier to navigate and scan for relevant topics. Topic titles Avoid using gerunds (verbs that end with -ing) in titles and headings. Procedure titles and headings should answer the question: What are you trying to do? For example, if the answer to What are you trying to do? is I want to create an account , the article heading should be Create an account . In most cases, concept and reference topics are named with a noun phrase, such as Event hooks . Lists Introduce lists with a heading, a sentence, or a fragment that ends with a colon. Use numbered lists for processes and procedures that must be completed in sequential order. Use bulleted lists for items that don't need to appear in order. Make all list items parallel in structure. For example, start each item in the list using a noun or a phrase that starts with a verb. Bullets Bullets are for unordered lists. The order of items in a bulleted list can imply importance, but generally all list items are peers. Each list item should start with a capital letter and end with a period unless all of the list items are single words or short phrases of no more than four words. Use parallel structure in phrasing the items in a list. For example, each list item might start with a verb, noun, or gerund. Numbered steps Only use numbered paragraphs for steps in procedures. If a procedure has more than nine steps, always consider breaking it into subsections with headings. Ideally, each procedure or subtask should be three to six steps, not have nested sub-steps, and have minimal embedded paragraphs describing what happens‚Äîthe result or outcome to expect‚Äîin an unnumbered paragraph following the step. Don't combine different actions into one step except when two actions complete a task, such as "Enter the user name, then click Next ." Pronouns Use gender-neutral pronouns, like ‚Äúthey‚Äù whenever possible. Generally, you can change any noun from singular to plural to have subject-verb-pronoun agreement and avoid the use of gender-specific pronouns like ‚Äúhe‚Äù, ‚Äúhim‚Äù, ‚Äúhis‚Äù or ‚Äúshe‚Äù, ‚Äúher‚Äù, ‚Äúhers‚Äù. Be wary of impersonal and potentially ambiguous pronouns such as: all, another, any each, either few, many, neither, none, one, other same, several, some, such that, them, these, those If you use any of these impersonal pronouns, be sure you answer ‚Äúof what?‚Äù, ‚Äúof which?‚Äù, or ‚Äúas what?‚Äù in the sentence. Terminology and usage conventions This section covers common terminology, style, and usage questions and recommended practices. Above and below Don't use above to mean earlier or as an adjective preceding a noun ( the above section ) or following a noun ( the code above ). Use a hyperlink, or use previous , preceding , or earlier . Don't use below to mean later or as an adjective preceding a noun ( the below section ) or following a noun ( the code below ). Use a hyperlink, or use later or the following . For example: Use the preceding code to display information about the database. Use the following code to display information about the database. Dates and numbers Use the DD Mon YYYY or DD Month YYYY format for dates. In body text, spell out whole numbers from zero through nine. Use numerals for 10 or greater. Use commas in numbers with four or more digits. Use more than instead of over (over is a spatial term). Emphasis and admonitionments Use bold formatting for user interface elements that the user interacts with, including: Dialog titles Field labels Buttons labels Options displayed in the user interface Don't use bold, italics, or underlining for emphasis. If there's text that requires more attention than the surrounding body, consider isolating it as a standalone note or tip. Use admonishment components sparingly! They are generally disruptive to the reader‚Äôs experience. Ask yourself if it is really necessary to stop the reader‚Äôs forward progress by adding a Note, Caution, or Tip component. Note Indicates neutral or positive information that emphasizes or supplements important points of the main text. A note supplies information that may apply only in special cases. Examples are memory limitations, equipment configurations, or details that apply to specific versions of a program. Tip Helps users apply the techniques and procedures described in the text to their specific needs. A tip suggests alternative methods that may not be obvious and helps users understand the benefits and capabilities of the product. A tip is not essential to the basic understanding of the text. Caution Advises users that failure to take or avoid a specific action could result in loss of data. Images Diagrams and illustrations can help readers visualize and internalize complex ideas and processes. So, use them liberally but with intention. Images also help to break up long text flows, but they should always reinforce and reflect the text immediately preceding or immediately following the image. If you include screenshots, only include the relevant parts of the screen and use callouts to highlight how what is captured in the image is relevant to the text. Be wary of using diagrams or illustrations that include any information‚Äîvisual or textual‚Äîthat is likely to get stale. Log in formats Most Linux distributions and macOS use log in to describe how a user initiates an interactive session. Windows uses log on. Use log in as two words with no hyphen when describing an action (verb usage). Use login as one word when used as a noun (rare but some platforms use login to mean user or an identity). Use log-in with a hyphen when modifying a noun (adjective usage). Optional steps Use (Optional) to the beginning of steps that are optional. For example: Open a new terminal. Open the attributes file in a text editor. (Optional) Add a custom field. Punctuation Element How to use it apostrophe (‚Äò) Use in contractions for a conversational tone. Avoid using the possessive form. capitalization Use sentence style capitalization for all headings. When referring to elements in the user interface, follow the capitalization that is used in the labels or text. Do not capitalize common terms. colon (:) Use a colon at the end of the statement that introduces a procedure, bulleted list, or table. comma (,) Use a serial comma to separate three or more items in a series, including the item before the conjunction. em dash (‚Äî) Use an em dash (‚Äî) to set off a parenthetical phrase with more emphasis than parentheses provide. Don‚Äôt add spaces around an em dash. Don‚Äôt capitalize the first word after an em dash unless the word is a proper noun. hyphenation (-) Avoid using hyphenated compound words. Use hyphens only if the meaning is unclear without them or if the only recognized form of the word includes a hyphen. quotation marks (" ") Avoid using quotation marks unless you need to quote a message or as tring that would otherwise be confusing given its surrounding context. semicolons (;) Don't use semicolons instead of commas to separate items in a list. If you think the content should use semicolons, consider rewriting it into subtopics or an unordered bullet list. Slashes (/) and backslashes () Avoid using slashes or backslashes except when documenting paths that require either forward or backward slashes. Never use and/or in documentation. Software versions Use or later or and later to refer to multiple versions of software. For example: Firefox 3.6 or later Rust compiler (rustc) version 1.55.0 and later Tense Use present tense whenever possible. Use past tense only if you must describe something that has already occurred. Use future tense only if you must describe something that has not yet occurred but can be safely assumed. User interface elements In general, you should avoid writing about user interface elements. Instead, documentation should always focus on what the audience needs to do or wants to accomplish and not what is displayed on the screen. Element What to do button Use bold for the button label. Don't include "button" in the description. For example: Click Submit . checkbox Use checkbox, not box or check box, if you need to refer to a checkbox in the user interface. Use select and clear with checkboxes, not turn on and turn off, mark and unmark, check and uncheck, or unselect and deselect. click Use click to describe taking action on a standalone button. Do not use click on. Click and select are not interchangeable. dialog If you need to refer to a dialog box, use dialog. Don't use pop-up window, dialog box, or dialogue box. dropdown Use dropdown as an adjective, not as a noun. For example, use dropdown list . Verb usage Verb How to use it allow, enable Avoid using software as a point of view and consider rewriting to focus on the human interacting with the software. can, may, might Use the verb can when describing ability, capability, or capacity. Avoid using the verb may because it implies permission. Use the past tense might when describing the possibility or eventuality of an outcome. clear Use _ clear rather than deselect or unselect if you have to describe removing a selection from a checkbox. displays Use the transitive verb displays rather than the intransitive verb appears. Use displays with a direct object. For example, The command displays log messages. ensure Use ensure to mean to make sure or to guarantee. Remember that this is not interchangeable with assure (to make confident) and insure (to provide insurance). enter, type Use enter to instruct the user to input a value by pressing the Enter or Return key. Use type to instruct the user to type a value in a field. select Use select to describe taking action on a menu item, checkbox, or radio button. Note that click and select are not interchangeable. set up, setup Use set up ‚Äîtwo words, no hyphen‚Äîwhen used as a verb. Don't hyphenate. Use setup ‚Äîone word, no hyphen‚Äîwhen used as an adjective or as a noun. want, wish Use want instead of wish or desire when the user has a choice of actions. Word choice Word in question How to use it affect, effect Use affect as a verb and use effect as a noun. app, application Use application or applications unless there‚Äôs a specific reason for using the shorthand term app or apps . back-end, front-end Using the hyphen in these terms is still more common than not using it. Both forms are acceptable, but for consistency use the hyphen. email It hasn‚Äôt been e-mail for thirty-plus years. Never use emails. Don‚Äôt use email as a verb. file name Use file name as two words, not filename . its, it‚Äôs Use its as a possessive meaning belonging to or associated with an object or idea previously mentioned. Because it is a vague pronoun, be sure what it refers to can be easily identified. Use it‚Äôs only as a contraction for it is or it has . please Avoid using please in documentation unless there‚Äôs a specific reason for using it. For example, you might use please if quoting the content of a message that asks the user to do something inconvenient. prerequisite As a section title, use Before you begin instead. If you use prerequisite in the text, there‚Äôs no hyphen. that, which Use that at the beginning of a clause that‚Äôs necessary for the sentence to make sense. Don‚Äôt put a comma before it. Don't use that when introducing a clause referring to people. Use who . Use which at the beginning of a clause that adds supporting or parenthetical information to a sentence. If you can omit the clause and the sentence still makes sense, use which , and put a comma before it. user name Use user name as two words, not username. Best practices and common mistakes This section highlights best practices and common mistakes to avoid. Make every word count Concise sentences are easier to read, comprehend, and translate. Use simple words with precise meanings. Remove words that don‚Äôt add substance. Avoid using passive to be verbs like been and being. Avoid weak or vague verbs, such as have , make , and do . When in doubt, choose the simple word or phrase over a more formal or complex one. For example: Use this Not this use utilize, make use of remove extract, take away, eliminate tell inform, let know to in order to, as a means to also in addition connect establish connectivity Whenever possible, choose words that have one clear meaning. Omit unnecessary adverbs‚Äîwords that describe how, when, or where. Unless they're important to the meaning of a statement, leave them out. Be consistent Use one term consistently to represent one concept. For example, if you use extrinsic, dispatchable, and transaction interchangeably or ambiguously, you‚Äôll leave the reader confused and uncertain. If terminology changes, be prepared to root out old terminology. If you use words that can be both nouns and verbs‚Äîfor example, words like file, post, mark, screen, record, and report‚Äîuse sentence structure and context to eliminate ambiguity. Avoid dangling participles Participles are modifiers so they must have a noun to modify. A dangling participle is a participle that doesn‚Äôt have a noun to modify. If you misplace or leave out the word being modified, you‚Äôll end up with a sentence that is difficult to understand, illogical, or ambiguous (though potentially amusing). Here are a few examples of sentences with dangling participles: Looking around the yard, dandelions sprouted in every corner. Walking through the kitchen, the smoke alarm was going off. Driving like a maniac, the deer was hit and killed. You can correct these sentences by bringing the participle phrase closer to the subject the phrase is intended to modify or changing the word order of the sentence to clarify who is doing what. You can also fix these types of problems by changing the tense or using the active voice. For example: Looking around the yard, I saw dandelions had sprouted in every corner. As I was walking through the kitchen, the smoke alarm was going off. Driving like a maniac, he hit a deer and killed it. Dangling prepositions In modern English, it‚Äôs perfectly acceptable to end a sentence with a preposition. Don‚Äôt twist a sentence into knots just to avoid using a preposition at the end. This is something you might be interested in. This is an example you should pay attention to. Cross-reference formats Most cross-references should include information that clarifies what the reader can expect to be found in the referenced topic. For cross-references to topics in the Substrate documentation, use the following formats: For more information about [task or concept], see [topic-title]. For cross-references in a glossary entry to other glossary entries, use the following format: See [topic]. For cross-references to external resources, use the title of the destination instead of the URL of the destination. Avoid using links to unnamed destinations. For example, don‚Äôt use links like click here or see this article . Writing concept topics Concept topics answer ‚Äúwhy?‚Äù and ‚Äúwhat is‚Ä¶?‚Äù questions. Use concept topics to: Explain abstract ideas. Introduce new terminology. Offer analysis. Provide background information. The goal of a concept topic is to help the reader understand the bigger picture, the key components of a system or architecture, relationships between components. Concept topics tend to be relatively stable, requiring little, if any, ongoing maintenance. At a minimum, a concept topic includes at least one heading and one or more body paragraphs. A concept topic can also include: One or more examples. Two or more subsections, marked by subheadings. A list of related topics. Builder notes This repository has some conventions and peculiarities that you need to take into account when modifying it (in any way). Please read this entire section to avoid common gotchas and help make your life and the maintainers lives easier. New pages and moving files If you are adding or renaming a page, you must add it correctly in src/components/DevNavMenu.tsx and possibly gatsby-config.js and gatsby-node.js . Your index.mdx file's title is not the source for the navigation rendering. Internal link conventions All /rustdoc/ internal links must end with .or .html#some-ID . Reasoning can be found in #425 . Check rendering images by clearing .cache From time to time, the development server local cache becomes corrupted. To fix this in a one liner: yarn clean && yarn dev PLEASE do this when reviewing a page before every PR - this ensures your state is what the build CI see as well. New or updated yarn packages From time to time, BREAKING changes happen in the yarn dependencies. To fix this in a one liner on main : git checkout main && git pull && yarn install && yarn clean && yarn dev Change the branch above to your working brach of choice, or start a new on for a new PR based on latest main this way. NOTE: please discard the "private": false, field this adds to package.json . Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE General guidance for writing engaging content Headings Lists Pronouns Terminology and usage conventions Best practices and common mistakes Writing concept topics Builder notes Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Contributor guidelines Contributor guidelines Contributor guidelines Thank you for your interest in contributing to documentation for the Substrate development framework. As a member of the community, you are invited and encouraged to contribute to the ever-evolving technical documentation and to the code base. There are a lot of ways to get involved. For example, you can contribute by: Submitting issues Offering suggestions for improvements to existing content Adding review comments to existing pull requests Proposing new content Creating new pull requests to fix issues yourself Creating pull request for new content other community members might find useful We value, respect, and appreciate all contributions from the developer community and only ask that you agree to abide by our Code of conduct and follow these Contributor guidelines. To learn more about how to contribute, see the following topics: Before you contribute How to contribute What to contribute Before you contribute Before contributing, please take a few minutes to review these contributor guidelines. The contributor guidelines are intended to make the contribution process easy and effective for everyone involved in addressing your issue, assessing changes, and finalizing your pull requests. Before contributing, consider the following: If you want to report an issue, click Issues and provide as much information about the problem as possible. If you have a technical question or need help finding specific information, post your question on Stack Exchange . If you want to contribute directly to this repository, typical fixes might include any of the following: Spelling, grammar, or typo fixes. Code indentation, white space, or formatting changes. Broken or missing links. Note that any contribution to this repository must be submitted in the form of a pull request . Before you create a pull request, be sure that the pull request only implements one bug fix. If you are new to working with GitHub repositories and creating pull requests, consider exploring First Contributions or How to Contribute to an Open Source Project on GitHub . How to contribute This repository contains the documentation source for the docs.substrate.io site and is focused primarily on documentation for Substrate blockchain developers and parachain developers. Making simple edits For simple changes, like fixing a typo or making minor changes to a sentence: Click Edit this page . Edit the page in GitHub. Replace the default commit message with a short description of your change, then click Commit Changes . Select Create a new branch for this commit and start a pull request . Creating branches and pull requests If your contribution is more than a simple change, you'll need to follow the full work flow for creating a working branch and submitting a pull request. Here's a summary of what you need to do: Navigate to the substrate-docs public repository in a web browser. Clone or fork the repository to create a copy of the repository on your local machine. Create a new branch for your fix by running a command similar to the following: git switch -c my-initials/my-branch-name-here Open the file you want to fix in a text editor and make the appropriate changes for the issue you are trying to address. Add the file contents of the changed files by running a command similar to the following: git add path-to-changed-file Commit your changes to store the contents you added along with a descriptive message by running a command similar to the following: git commit -m "Description of the fix being committed." Push the changes to the remote repository by running a command similar to the following: git push origin my-initials/my-branch-name-here Click Create pull request to start a new pull request and provide any additional information about the changes you made. A maintainer will review your pull request and approve or request changes. If no changes are required, the maintainer will merge your pull request. If a maintainer requested changes or clarification, update your pull request and request another reviewer. When you see your changes have been merged, celebrate your success! ü•Ç What to contribute The most valuable contributions from the community typically take the form of how-to guides or tutorials that help other developers solve specific problems, learn specific skills, or demonstrate specific tasks. If you would like to contribute, you might be wondering ‚ÄúWhat is the difference between a ‚Äòhow-to‚Äô guide and a tutorial?‚Äù. How-to guides A how-to guide describes how to achieve a goal or complete a task. Only the information that is pertinent to achieving that goal or completing the task is included. With how-to guides, readers have enough information to know what they want to do‚Äîfor example, open a bank account‚Äîbut not necessarily enough information to know how to do it. For example, the how-to guide for opening a bank account wouldn't explain what a bank account is or why you might want to open one, but would focus on specific steps such as: Select an institution. Fill out an application. Deposit a minimum amount of currency. How-to guides often include links to additional information, but should not include explanations that take the focus away from what the reader wants to accomplish. For more information about writing How-to guides, see the Template - How-to guide and the Markdown template . Tutorials A tutorial is a hands-on illustration or lesson that enables the reader to achieve a highly-predictable result. Tutorials assume that readers have no prior knowledge on the subject being covered and that they require explicit guidance to complete each step to reach a well-known outcome. Typically, a tutorial is a guided tour that helps the reader complete one organic task from start to finish. There are no detours and the information should not be broken out into subtopics because the steps must be completed in order, not in a sequence of the reader‚Äôs choosing. The single most important aspect of a tutorial is that it should always result in a successful, expected outcome. The successful outcome is what inspires confidence and delight in the reader. The single most important distinction between a how-to guide and a tutorial is that, in a tutorial, the author decides what the goal should be and the author eliminates all distractions that would detract from the successful achievement of the goal. Last edit: May 7, 2023 ‚Üê Previous Next ‚Üí ON THIS PAGE Before you contribute How to contribute What to contribute Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
Template - How-to guide Template how to guide Template - How-to guide We recommend you use the following template to structure articles that you want to submit as to ‚Äúhow-to‚Äù topics. You can download a copy of the Markdown how-to-template directly from here . After downloading the template, rename the file and replace the description of each section with the relevant content. Front matter Your How-to guide should start with a front matter section delineated by typing three dashes on the first line. The front matter consists of the following fields: --- title: Keep titles short description: Write a descriptive sentence to summarize the content of the article. keywords: --- Only the title field is required and you can specify additional metadata fields. You close the front matter section by typing three dashes on the line following the last front matter entry. [Guide title] The guide title should summarize the goal of the article. For ‚Äúhow-to‚Äù guides, the title should complete the ‚ÄúHow do I ‚Ä¶?‚Äù sentence. For example, if the goal of the guide is to illustrate "How do I mint a token supply?," you can set the title in the front matter like this: title: Mint a token supply In general, you should keep titles short so they are easy to scan for keywords. [Guide description] The guide description is optional, but if you include it, use a single sentence to convey any additional information about the content that the title doesn't convey. For example: description: Illustrates how you can mint a token supply owned by a single account. [Guide keywords] Keyword are optional, but if you include them, indent two spaces, then use a dash and a single keyword per line. For an example, see the how-to template . Introductory paragraph The first paragraph of the article should provide a brief overview of what the article is about and why this information is useful to its audience. The overview section does not require an Overview heading and it might be more than one paragraph. The opening section of each article sets the stage for what follows and should answer the obvious questions so readers can decide whether the content is relevant to them. Readers should know‚Äîjust from reading this section‚Äîwhether they should continue or the content doesn‚Äôt apply to them and they should move on to something else. For your overview, try to answer the following questions: What is this article about? What is the purpose or goal to be accomplished by following the procedure or technique? Why would someone want to use this procedure or technique? For example, are there specific use case scenarios that are applicable? When would someone use this procedure? For example, is this an activity that is done once or repeated? Is it a pattern or a unique case? Where is the procedure or technique applicable? Who would use this procedure or technique? For example, are special skills required? Do specific permissions or restrictions apply? The overview section is also a good place to link to other resources, including other guides. As the content creator, you want readers to have confidence that the guide will be useful for them. Use cases This section is optional because the guide title might be sufficient if your article is tightly focused on a single use case. If your article has more than one practical application, use this section to briefly describe each of them. If the only use case is a repetition of the article title or sufficiently covered in the overview section, skip this section. If your article only has one use case but it requires more explanation than the title provides, add this section and one or more sentences to provide the additional explanation. For example: This guide illustrates implementing a second currency for paying fees. If you want to support multiple currencies in your runtime, this guide provides practical advice and detailed steps that you can apply for the additional currencies you want to support. This guide shows you how to perform a runtime migration from a Vec<u32> to SomeStruct If your article has more than one use case, use a bulleted list. Before you begin This section is optional but recommended . Use the Before you begin heading and use the section body to describe any prerequisites that apply to your article. This section should answer the following questions: What should someone have before reading this article? What should someone know before reading this article? What should someone do before reading this article? Procedural steps Use Steps as a heading only if the article has one set of steps that achieve a single goal. For example, use Steps if an article is tightly focused on a single use case and a more descriptive heading would simply repeat the article title. For more complex procedures and techniques, use clear, concise headings to describe each part of the procedure or technique. Each step should be action driven. In most cases, each step starts with a verb and ends with a period. The paragraph following a step should describe the result or outcome the reader should expect. If you feel a step needs any additional information, link to that information rather than embedding too much extraneous detail within the step. Code snippets can help illustrate the steps but should not overwhelm the focus on "how do I do this" (not on "what do I do"). Keep in mind that most steps have results and readers like confirmation that they have taken the correct action as they progress through a procedure. Examples This section is optional but recommended . You can use this section to provide links to one or more code-based examples that make practical use of your article. This section should include at least one reference to a repository that exposes what this guide covers in the form of a working example. You can use reference an existing codebase within Substrate or new code in any publicly-available repository. For example, if you have a repository where you have tested the procedure you are writing about, include a link to it in this section. Related resources This section is optional. If you include it, add a bulleted list of links to similar guides, other Developer Hub resources, or related material. For example, you might add links to other how-to guides, tutorials, or Rust docs. Last edit: May 7, 2023 ‚Üê Previous ON THIS PAGE Front matter Introductory paragraph Before you begin Procedural steps Examples Related resources Technology Overview Flexible Open Interoperable Future-Proof Developers Home Docs Rust Docs Marketplace Playground Smart Contracts Substrate Connect Rococo Network Vision Substrate & Polkadot Ecosystem Home Projects Builders Program Hackathons Grants Careers Substrate Seminar Past Seminars Awesome Substrate Contact Events Newsletter Blog Square One ¬© 2023 Parity Technologies All Rights Reserved Privacy Policy Terms of Services
